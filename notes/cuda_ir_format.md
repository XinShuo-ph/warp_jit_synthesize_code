# CUDA IR Format Documentation

## Overview
CUDA intermediate representation generated by Warp differs from CPU IR in several key ways. This document compares CPU vs CUDA code generation.

## Key Differences

### 1. Function Signature

**CPU:**
```cpp
void kernel_name_hash_cpu_kernel_forward(
    wp::launch_bounds_t dim,
    size_t task_index,
    wp_args_kernel_name_hash *_wp_args)
```

**CUDA:**
```cpp
void kernel_name_hash_cuda_kernel_forward(
    wp::launch_bounds_t dim,
    wp::array_t<T> var_a,
    wp::array_t<T> var_b,
    ...)
```

**Differences:**
- CPU: Arguments passed via struct pointer `_wp_args`
- CUDA: Arguments passed directly as function parameters
- CPU: Has `task_index` parameter (for task-based parallelism)
- CUDA: No `task_index` (uses grid/block indexing)

### 2. Argument Handling

**CPU:**
```cpp
// Extract from args struct
wp::array_t<wp::float32> var_a = _wp_args->a;
wp::array_t<wp::float32> var_b = _wp_args->b;
wp::array_t<wp::float32> var_out = _wp_args->out;
```

**CUDA:**
```cpp
// Arguments already in scope (passed directly)
// No extraction needed
```

### 3. Thread Loop Structure

**CPU:**
```cpp
// Implicit single-threaded execution
// No loop - executes once per task
```

**CUDA:**
```cpp
for (size_t _idx = static_cast<size_t>(blockDim.x) * static_cast<size_t>(blockIdx.x) + static_cast<size_t>(threadIdx.x);
     _idx < dim.size;
     _idx += static_cast<size_t>(blockDim.x) * static_cast<size_t>(gridDim.x))
{
    // Kernel body
}
```

**Key points:**
- CUDA uses grid-stride loop pattern
- `blockDim.x`: Threads per block
- `blockIdx.x`: Block index
- `threadIdx.x`: Thread index within block
- `gridDim.x`: Total blocks in grid
- Allows kernels to handle any array size

### 4. Shared Memory

**CPU:**
```cpp
// No shared memory management
```

**CUDA:**
```cpp
wp::tile_shared_storage_t tile_mem;

// Inside loop
wp::tile_shared_storage_t::init();  // Reset per iteration
```

### 5. Thread ID Computation

**Both CPU and CUDA:**
```cpp
var_0 = builtin_tid1d();
```

**Note:** Same API, but `builtin_tid1d()` is implemented differently:
- CPU: Simple counter
- CUDA: Uses `blockIdx.x * blockDim.x + threadIdx.x`

### 6. Memory Operations

**Both use same API:**
```cpp
var_ptr = wp::address(var_array, var_idx);  // Get pointer
var_val = wp::load(var_ptr);                // Load value
wp::array_store(var_array, var_idx, var_val); // Store value
```

## Complete Example Comparison

### Python Source
```python
@wp.kernel
def add_kernel(a: wp.array(dtype=float), 
               b: wp.array(dtype=float), 
               out: wp.array(dtype=float)):
    tid = wp.tid()
    out[tid] = a[tid] + b[tid]
```

### CPU IR
```cpp
void add_kernel_hash_cpu_kernel_forward(
    wp::launch_bounds_t dim,
    size_t task_index,
    wp_args_add_kernel_hash *_wp_args)
{
    // argument vars
    wp::array_t<wp::float32> var_a = _wp_args->a;
    wp::array_t<wp::float32> var_b = _wp_args->b;
    wp::array_t<wp::float32> var_out = _wp_args->out;
    
    // primal vars
    wp::int32 var_0;
    wp::float32* var_1;
    wp::float32* var_2;
    wp::float32 var_3;
    wp::float32 var_4;
    wp::float32 var_5;
    
    // forward
    var_0 = builtin_tid1d();
    var_1 = wp::address(var_a, var_0);
    var_2 = wp::address(var_b, var_0);
    var_4 = wp::load(var_1);
    var_5 = wp::load(var_2);
    var_3 = wp::add(var_4, var_5);
    wp::array_store(var_out, var_0, var_3);
}
```

### CUDA IR
```cpp
void add_kernel_hash_cuda_kernel_forward(
    wp::launch_bounds_t dim,
    wp::array_t<wp::float32> var_a,
    wp::array_t<wp::float32> var_b,
    wp::array_t<wp::float32> var_out)
{
    wp::tile_shared_storage_t tile_mem;

    for (size_t _idx = static_cast<size_t>(blockDim.x) * static_cast<size_t>(blockIdx.x) + static_cast<size_t>(threadIdx.x);
         _idx < dim.size;
         _idx += static_cast<size_t>(blockDim.x) * static_cast<size_t>(gridDim.x))
    {
        // reset shared memory allocator
        wp::tile_shared_storage_t::init();

        // primal vars
        wp::int32 var_0;
        wp::float32* var_1;
        wp::float32* var_2;
        wp::float32 var_3;
        wp::float32 var_4;
        wp::float32 var_5;
        
        // forward
        var_0 = builtin_tid1d();
        var_1 = wp::address(var_a, var_0);
        var_2 = wp::address(var_b, var_0);
        var_4 = wp::load(var_1);
        var_5 = wp::load(var_2);
        var_3 = wp::add(var_4, var_5);
        wp::array_store(var_out, var_0, var_3);
    }
}
```

## Type System

Both CPU and CUDA use the same Warp type system:
- Scalars: `wp::float32`, `wp::int32`, etc.
- Vectors: `wp::vec_t<N, T>` (e.g., `wp::vec_t<3, wp::float32>`)
- Matrices: `wp::mat_t<M, N, T>` (e.g., `wp::mat_t<3, 3, wp::float32>`)
- Arrays: `wp::array_t<T>`

## Operations

Both CPU and CUDA use the same operation APIs:
- Arithmetic: `wp::add()`, `wp::sub()`, `wp::mul()`, `wp::div()`
- Math: `wp::sin()`, `wp::cos()`, `wp::sqrt()`, `wp::abs()`, `wp::exp()`, `wp::log()`
- Vector: `wp::dot()`, `wp::cross()`, `wp::length()`, `wp::normalize()`
- Matrix: `wp::transpose()`, matrix multiplication
- Atomic: `wp::atomic_add()`, `wp::atomic_min()`, `wp::atomic_max()`
- Comparison: `wp::min()`, `wp::max()`

## Summary

| Aspect | CPU | CUDA |
|--------|-----|------|
| Function params | Args struct pointer | Direct parameters |
| Task parameter | Yes (`task_index`) | No |
| Thread loop | Implicit (single) | Explicit grid-stride loop |
| Shared memory | Not used | `tile_shared_storage_t` |
| Thread ID | `builtin_tid1d()` | `builtin_tid1d()` (different impl) |
| Memory ops | Same API | Same API |
| Type system | Same | Same |
| Operations | Same API | Same API |
| Code size | ~30% smaller | ~30% larger (loop overhead) |

## Implications for Training Data

1. **Structural Differences**: CUDA IR has more boilerplate (loop, shared memory)
2. **Core Logic Identical**: Actual computation code is the same
3. **Device-Specific Patterns**: Models should learn both patterns
4. **API Consistency**: Warp's abstraction keeps most code identical

## Generated Samples

- **CPU samples**: 10 in `/workspace/data/cpu_samples/`
- **CUDA samples**: 5 in `/workspace/data/cuda_samples/`
- All 10 kernel types work with both devices
