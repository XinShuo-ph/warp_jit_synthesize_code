{"id": 0, "kernel_name": "scalar_arr_qahf", "python": "@jax.jit\ndef scalar_arr_qahf(alpha, x, y):\n    return alpha * x + y", "type": "generate_scalar_array_op", "hlo": "module @jit_scalar_arr_qahf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<1024xf32>, %arg2: tensor<1024xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @scalar_arr_qahf(%arg0, %arg1, %arg2) : (tensor<f32>, tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @scalar_arr_qahf(%arg0: tensor<f32>, %arg1: tensor<1024xf32>, %arg2: tensor<1024xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<1024xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<1024xf32>\n    return %3 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_scalar_arr_qahf, is_scheduled=true, entry_computation_layout={(f32[], f32[1024]{0}, f32[1024]{0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmpq0a_c1cl.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"scalar_arr_qahf\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=11 end_column=20}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[1024], param_1.2: f32[1024], param_2.1: f32[]) -> f32[1024] {\n  %param_2.1 = f32[] parameter(2)\n  %mul.5 = f32[1024]{0} broadcast(%param_2.1), dimensions={}, metadata={op_name=\"jit(scalar_arr_qahf)/jit(scalar_arr_qahf)/mul\" stack_frame_id=5}\n  %param_1.2 = f32[1024]{0} parameter(1)\n  %mul.4 = f32[1024]{0} multiply(%mul.5, %param_1.2), metadata={op_name=\"jit(scalar_arr_qahf)/jit(scalar_arr_qahf)/mul\" stack_frame_id=5}\n  %param_0.1 = f32[1024]{0} parameter(0)\n  ROOT %add.2 = f32[1024]{0} add(%mul.4, %param_0.1), metadata={op_name=\"jit(scalar_arr_qahf)/jit(scalar_arr_qahf)/add\" stack_frame_id=5}\n}\n\nENTRY %main.2 (alpha.1: f32[], x.1: f32[1024], y.1: f32[1024]) -> f32[1024] {\n  %alpha.1 = f32[] parameter(0), metadata={op_name=\"alpha\"}\n  %x.1 = f32[1024]{0} parameter(1), metadata={op_name=\"x\"}\n  %y.1 = f32[1024]{0} parameter(2), metadata={op_name=\"y\"}\n  ROOT %multiply_add_fusion = f32[1024]{0} fusion(%y.1, %x.1, %alpha.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(scalar_arr_qahf)/jit(scalar_arr_qahf)/add\" stack_frame_id=5}\n}\n\n", "ptx": null}
{"id": 1, "kernel_name": "elementwise_bsdm", "python": "@jax.jit\ndef elementwise_bsdm(a, b):\n    return a * b", "type": "generate_simple_elementwise", "hlo": "module @jit_elementwise_bsdm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>, %arg1: tensor<1024xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @elementwise_bsdm(%arg0, %arg1) : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @elementwise_bsdm(%arg0: tensor<1024xf32>, %arg1: tensor<1024xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_elementwise_bsdm, is_scheduled=true, entry_computation_layout={(f32[1024]{0}, f32[1024]{0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmpuvqxqpzr.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"elementwise_bsdm\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=11 end_column=16}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n\n\n%wrapped_multiply_computation (param_0: f32[1024], param_1: f32[1024]) -> f32[1024] {\n  %param_0 = f32[1024]{0} parameter(0)\n  %param_1 = f32[1024]{0} parameter(1)\n  ROOT %mul.2 = f32[1024]{0} multiply(%param_0, %param_1), metadata={op_name=\"jit(elementwise_bsdm)/jit(elementwise_bsdm)/mul\" stack_frame_id=5}\n}\n\nENTRY %main.2 (a.1: f32[1024], b.1: f32[1024]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  %b.1 = f32[1024]{0} parameter(1), metadata={op_name=\"b\"}\n  ROOT %wrapped_multiply = f32[1024]{0} fusion(%a.1, %b.1), kind=kLoop, calls=%wrapped_multiply_computation, metadata={op_name=\"jit(elementwise_bsdm)/jit(elementwise_bsdm)/mul\" stack_frame_id=5}\n}\n\n", "ptx": null}
{"id": 2, "kernel_name": "vec_kowe", "python": "@jax.jit\ndef vec_kowe(a, b):\n    # vector dot product along last axis\n    return jnp.sum(a * b, axis=-1)", "type": "generate_vector_kernel", "hlo": "module @jit_vec_kowe attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024x3xf32>, %arg1: tensor<1024x3xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @vec_kowe(%arg0, %arg1) : (tensor<1024x3xf32>, tensor<1024x3xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @vec_kowe(%arg0: tensor<1024x3xf32>, %arg1: tensor<1024x3xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<1024x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<1024x3xf32>, tensor<f32>) -> tensor<1024xf32>\n    return %1 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_vec_kowe, is_scheduled=true, entry_computation_layout={(f32[1024,3]{1,0}, f32[1024,3]{1,0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmpcxqv7iqn.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"vec_kowe\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=11 end_column=34}\n6 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=19 end_column=24}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n\n\n%region_0.1 (reduce_sum.3: f32[], reduce_sum.4: f32[]) -> f32[] {\n  %reduce_sum.3 = f32[] parameter(0), metadata={op_name=\"reduce_sum\"}\n  %reduce_sum.4 = f32[] parameter(1), metadata={op_name=\"reduce_sum\"}\n  ROOT %reduce_sum.5 = f32[] add(%reduce_sum.3, %reduce_sum.4), metadata={op_name=\"reduce_sum\" stack_frame_id=5}\n}\n\n%fused_computation (param_0.2: f32[1024,3], param_1.2: f32[1024,3]) -> f32[1024] {\n  %param_0.2 = f32[1024,3]{1,0} parameter(0)\n  %param_1.2 = f32[1024,3]{1,0} parameter(1)\n  %mul.2 = f32[1024,3]{1,0} multiply(%param_0.2, %param_1.2), metadata={op_name=\"jit(vec_kowe)/jit(vec_kowe)/mul\" stack_frame_id=6}\n  %constant.2 = f32[] constant(0), metadata={op_name=\"jit(vec_kowe)/jit(vec_kowe)\"}\n  ROOT %reduce_sum.1 = f32[1024]{0} reduce(%mul.2, %constant.2), dimensions={1}, to_apply=%region_0.1, metadata={op_name=\"jit(vec_kowe)/jit(vec_kowe)/reduce_sum\" stack_frame_id=5}\n}\n\nENTRY %main.3 (a.1: f32[1024,3], b.1: f32[1024,3]) -> f32[1024] {\n  %a.1 = f32[1024,3]{1,0} parameter(0), metadata={op_name=\"a\"}\n  %b.1 = f32[1024,3]{1,0} parameter(1), metadata={op_name=\"b\"}\n  ROOT %multiply_reduce_fusion = f32[1024]{0} fusion(%a.1, %b.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(vec_kowe)/jit(vec_kowe)/reduce_sum\" stack_frame_id=5}\n}\n\n", "ptx": null}
{"id": 3, "kernel_name": "loop_hmci", "python": "@jax.jit\ndef loop_hmci(a, n):\n    def body_fun(i, val):\n        return val + a\n    # Initial value matches a's shape/type but zeroed\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "hlo": "module @jit_loop_hmci attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>, %arg1: tensor<i32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @loop_hmci(%arg0, %arg1) : (tensor<1024xf32>, tensor<i32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @loop_hmci(%arg0: tensor<1024xf32>, %arg1: tensor<i32>) -> tensor<1024xf32> {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<1024xf32>, tensor<i32>, tensor<i32>, tensor<1024xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.add %iterArg_2, %iterArg : tensor<1024xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<1024xf32>, tensor<i32>, tensor<i32>, tensor<1024xf32>\n    }\n    return %1#3 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_loop_hmci, is_scheduled=true, entry_computation_layout={(f32[1024]{0}, s32[])->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmp1jlydf2w.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"loop_hmci\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=9 end_line=9 column=15 end_column=32}\n6 {file_name_id=3 function_name_id=5 line=10 end_line=10 column=11 end_column=54}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n\n\n%wrapped_add_computation (param_0.1: s32[], param_1: s32[]) -> s32[] {\n  %param_0.1 = s32[] parameter(0)\n  %param_1 = s32[] parameter(1)\n  ROOT %add.0 = s32[] add(%param_0.1, %param_1), metadata={op_name=\"jit(loop_hmci)/jit(loop_hmci)/while/body/add\" stack_frame_id=6}\n}\n\n%wrapped_add_computation.1 (param_0.2: f32[1024], param_1.1: f32[1024]) -> f32[1024] {\n  %param_0.2 = f32[1024]{0} parameter(0)\n  %param_1.1 = f32[1024]{0} parameter(1)\n  ROOT %add.1 = f32[1024]{0} add(%param_0.2, %param_1.1), metadata={op_name=\"jit(loop_hmci)/jit(loop_hmci)/while/body/add\" stack_frame_id=6}\n}\n\n%region_0.1 (arg_tuple.1: (s32[], f32[1024], s32[], f32[1024])) -> (s32[], f32[1024], s32[], f32[1024]) {\n  %arg_tuple.1 = (s32[], f32[1024]{0}, s32[], f32[1024]{0}) parameter(0), metadata={op_name=\"jit(loop_hmci)/jit(loop_hmci)\"}\n  %constant.3 = s32[] constant(1), metadata={op_name=\"jit(loop_hmci)/jit(loop_hmci)\"}\n  %get-tuple-element.19 = f32[1024]{0} get-tuple-element(%arg_tuple.1), index=3, metadata={op_name=\"jit(loop_hmci)/jit(loop_hmci)\"}\n  %get-tuple-element.18 = s32[] get-tuple-element(%arg_tuple.1), index=2, metadata={op_name=\"jit(loop_hmci)/jit(loop_hmci)\"}\n  %get-tuple-element.9 = f32[1024]{0} get-tuple-element(%arg_tuple.1), index=1\n  %get-tuple-element.8 = s32[] get-tuple-element(%arg_tuple.1), index=0\n  %wrapped_add.1 = f32[1024]{0} fusion(%get-tuple-element.9, %get-tuple-element.19), kind=kLoop, calls=%wrapped_add_computation.1, metadata={op_name=\"jit(loop_hmci)/jit(loop_hmci)/while/body/add\" stack_frame_id=6}\n  %wrapped_add = s32[] fusion(%get-tuple-element.8, %constant.3), kind=kLoop, calls=%wrapped_add_computation, metadata={op_name=\"jit(loop_hmci)/jit(loop_hmci)/while/body/add\" stack_frame_id=6}\n  ROOT %tuple.3 = (s32[], f32[1024]{0}, s32[], f32[1024]{0}) tuple(%wrapped_add, %wrapped_add.1, %get-tuple-element.18, %get-tuple-element.19)\n}\n\n%wrapped_compare_computation (param_0.3: s32[], param_1.2: s32[]) -> pred[] {\n  %param_0.3 = s32[] parameter(0)\n  %param_1.2 = s32[] parameter(1)\n  ROOT %lt.0 = pred[] compare(%param_0.3, %param_1.2), direction=LT, metadata={op_name=\"jit(loop_hmci)/jit(loop_hmci)/while/cond/lt\" stack_frame_id=6}\n}\n\n%region_1.2 (arg_tuple.3: (s32[], f32[1024], s32[], f32[1024])) -> pred[] {\n  %arg_tuple.3 = (s32[], f32[1024]{0}, s32[], f32[1024]{0}) parameter(0), metadata={op_name=\"jit(loop_hmci)/jit(loop_hmci)\"}\n  %get-tuple-element.14 = s32[] get-tuple-element(%arg_tuple.3), index=2, metadata={op_name=\"jit(loop_hmci)/jit(loop_hmci)\"}\n  %get-tuple-element.12 = s32[] get-tuple-element(%arg_tuple.3), index=0, metadata={op_name=\"jit(loop_hmci)/jit(loop_hmci)\"}\n  ROOT %wrapped_compare = pred[] fusion(%get-tuple-element.12, %get-tuple-element.14), kind=kLoop, calls=%wrapped_compare_computation, metadata={op_name=\"jit(loop_hmci)/jit(loop_hmci)/while/cond/lt\" stack_frame_id=6}\n}\n\n%wrapped_broadcast_computation (param_0: f32[]) -> f32[1024] {\n  %param_0 = f32[] parameter(0)\n  ROOT %broadcast_in_dim.2 = f32[1024]{0} broadcast(%param_0), dimensions={}, metadata={op_name=\"jit(loop_hmci)/jit(loop_hmci)/broadcast_in_dim\" stack_frame_id=5}\n}\n\nENTRY %main.4 (a.1: f32[1024], n.1: s32[]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  %n.1 = s32[] parameter(1), metadata={op_name=\"n\"}\n  %constant.0 = s32[] constant(0), metadata={op_name=\"jit(loop_hmci)/jit(loop_hmci)\"}\n  %constant.1 = f32[] constant(0), metadata={op_name=\"jit(loop_hmci)/jit(loop_hmci)\"}\n  %copy.6 = s32[] copy(%constant.0)\n  %wrapped_broadcast = f32[1024]{0} fusion(%constant.1), kind=kLoop, calls=%wrapped_broadcast_computation, metadata={op_name=\"jit(loop_hmci)/jit(loop_hmci)/broadcast_in_dim\" stack_frame_id=5}\n  %tuple = (s32[], f32[1024]{0}, s32[], f32[1024]{0}) tuple(%copy.6, %wrapped_broadcast, %n.1, %a.1)\n  %while.1 = (s32[], f32[1024]{0}, s32[], f32[1024]{0}) while(%tuple), condition=%region_1.2, body=%region_0.1, metadata={op_name=\"jit(loop_hmci)/jit(loop_hmci)/while\" stack_frame_id=6}\n  ROOT %while.2 = f32[1024]{0} get-tuple-element(%while.1), index=1, metadata={op_name=\"jit(loop_hmci)/jit(loop_hmci)/while\" stack_frame_id=6}\n}\n\n", "ptx": null}
{"id": 4, "kernel_name": "scalar_arr_xkpw", "python": "@jax.jit\ndef scalar_arr_xkpw(alpha, x, y):\n    return alpha + x * y", "type": "generate_scalar_array_op", "hlo": "module @jit_scalar_arr_xkpw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<1024xf32>, %arg2: tensor<1024xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @scalar_arr_xkpw(%arg0, %arg1, %arg2) : (tensor<f32>, tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @scalar_arr_xkpw(%arg0: tensor<f32>, %arg1: tensor<1024xf32>, %arg2: tensor<1024xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<1024xf32>\n    %1 = stablehlo.convert %arg0 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %3 = stablehlo.add %2, %0 : tensor<1024xf32>\n    return %3 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_scalar_arr_xkpw, is_scheduled=true, entry_computation_layout={(f32[], f32[1024]{0}, f32[1024]{0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmpim7bnoq2.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"scalar_arr_xkpw\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=19 end_column=24}\n6 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=11 end_column=24}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[1024], param_1.2: f32[1024], param_2.1: f32[]) -> f32[1024] {\n  %param_2.1 = f32[] parameter(2)\n  %add.5 = f32[1024]{0} broadcast(%param_2.1), dimensions={}, metadata={op_name=\"jit(scalar_arr_xkpw)/jit(scalar_arr_xkpw)/add\" stack_frame_id=6}\n  %param_0.1 = f32[1024]{0} parameter(0)\n  %param_1.2 = f32[1024]{0} parameter(1)\n  %mul.2 = f32[1024]{0} multiply(%param_0.1, %param_1.2), metadata={op_name=\"jit(scalar_arr_xkpw)/jit(scalar_arr_xkpw)/mul\" stack_frame_id=5}\n  ROOT %add.4 = f32[1024]{0} add(%add.5, %mul.2), metadata={op_name=\"jit(scalar_arr_xkpw)/jit(scalar_arr_xkpw)/add\" stack_frame_id=6}\n}\n\nENTRY %main.2 (alpha.1: f32[], x.1: f32[1024], y.1: f32[1024]) -> f32[1024] {\n  %alpha.1 = f32[] parameter(0), metadata={op_name=\"alpha\"}\n  %x.1 = f32[1024]{0} parameter(1), metadata={op_name=\"x\"}\n  %y.1 = f32[1024]{0} parameter(2), metadata={op_name=\"y\"}\n  ROOT %multiply_add_fusion = f32[1024]{0} fusion(%x.1, %y.1, %alpha.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(scalar_arr_xkpw)/jit(scalar_arr_xkpw)/add\" stack_frame_id=6}\n}\n\n", "ptx": null}
{"id": 5, "kernel_name": "reduce_jlli", "python": "@jax.jit\ndef reduce_jlli(a):\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "hlo": "module @jit_reduce_jlli attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %0 = call @reduce_jlli(%arg0) : (tensor<1024xf32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n  func.func private @reduce_jlli(%arg0: tensor<1024xf32>) -> tensor<f32> {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<1024xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "llvm": "HloModule jit_reduce_jlli, is_scheduled=true, entry_computation_layout={(f32[1024]{0})->f32[]}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmp45nmsd1s.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"reduce_jlli\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=11 end_column=21}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n\n\n%region_0.1 (reduce_sum.3: f32[], reduce_sum.4: f32[]) -> f32[] {\n  %reduce_sum.3 = f32[] parameter(0), metadata={op_name=\"reduce_sum\"}\n  %reduce_sum.4 = f32[] parameter(1), metadata={op_name=\"reduce_sum\"}\n  ROOT %reduce_sum.5 = f32[] add(%reduce_sum.3, %reduce_sum.4), metadata={op_name=\"reduce_sum\" stack_frame_id=5}\n}\n\n%wrapped_reduce-window_computation (param_0: f32[1024], param_1: f32[]) -> f32[32] {\n  %param_0 = f32[1024]{0} parameter(0)\n  %param_1 = f32[] parameter(1)\n  ROOT %reduce-window.1 = f32[32]{0} reduce-window(%param_0, %param_1), window={size=32 stride=32}, to_apply=%region_0.1\n}\n\n%region_0.1.clone (reduce_sum.1: f32[], reduce_sum.2: f32[]) -> f32[] {\n  %reduce_sum.1 = f32[] parameter(0), metadata={op_name=\"reduce_sum\"}\n  %reduce_sum.2 = f32[] parameter(1), metadata={op_name=\"reduce_sum\"}\n  ROOT %reduce_sum.6 = f32[] add(%reduce_sum.1, %reduce_sum.2), metadata={op_name=\"reduce_sum\" stack_frame_id=5}\n}\n\n%wrapped_reduce_computation (param_0.1: f32[32], param_1.1: f32[]) -> f32[] {\n  %param_0.1 = f32[32]{0} parameter(0)\n  %param_1.1 = f32[] parameter(1)\n  ROOT %reduce_sum.8 = f32[] reduce(%param_0.1, %param_1.1), dimensions={0}, to_apply=%region_0.1.clone, metadata={op_name=\"jit(reduce_jlli)/jit(reduce_jlli)/reduce_sum\" stack_frame_id=5}\n}\n\nENTRY %main.3 (a.1: f32[1024]) -> f32[] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  %constant.0 = f32[] constant(0), metadata={op_name=\"jit(reduce_jlli)/jit(reduce_jlli)\"}\n  %wrapped_reduce-window = f32[32]{0} fusion(%a.1, %constant.0), kind=kLoop, calls=%wrapped_reduce-window_computation\n  ROOT %wrapped_reduce = f32[] fusion(%wrapped_reduce-window, %constant.0), kind=kLoop, calls=%wrapped_reduce_computation, metadata={op_name=\"jit(reduce_jlli)/jit(reduce_jlli)/reduce_sum\" stack_frame_id=5}\n}\n\n", "ptx": null}
{"id": 6, "kernel_name": "nested_odsh", "python": "@jax.jit\ndef nested_odsh(a):\n    # Nested where: if a > t1 then (if a > t2 then ... else ...) else ...\n    inner_true = a * 3.0\n    inner_false = a * 2.0\n    outer_true = jnp.where(a > 0.69, inner_true, inner_false)\n    outer_false = a * 0.5\n    return jnp.where(a > 0.44, outer_true, outer_false)", "type": "generate_nested_branch_kernel", "hlo": "module @jit_nested_odsh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @nested_odsh(%arg0) : (tensor<1024xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @nested_odsh(%arg0: tensor<1024xf32>) -> tensor<1024xf32> {\n    %cst = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %1 = stablehlo.multiply %arg0, %0 : tensor<1024xf32>\n    %cst_0 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<1024xf32>\n    %cst_1 = stablehlo.constant dense<0.689999997> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %5 = stablehlo.compare  GT, %arg0, %4,  FLOAT : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xi1>\n    %6 = call @_where(%5, %1, %3) : (tensor<1024xi1>, tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>\n    %cst_2 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %8 = stablehlo.multiply %arg0, %7 : tensor<1024xf32>\n    %cst_3 = stablehlo.constant dense<4.400000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %10 = stablehlo.compare  GT, %arg0, %9,  FLOAT : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xi1>\n    %11 = call @_where(%10, %6, %8) : (tensor<1024xi1>, tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>\n    return %11 : tensor<1024xf32>\n  }\n  func.func private @_where(%arg0: tensor<1024xi1>, %arg1: tensor<1024xf32>, %arg2: tensor<1024xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<1024xi1>, tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_nested_odsh, is_scheduled=true, entry_computation_layout={(f32[1024]{0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmp7nvd0zdg.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"nested_odsh\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=11 end_line=11 column=21 end_column=29}\n6 {file_name_id=3 function_name_id=5 line=10 end_line=10 column=18 end_column=25}\n7 {file_name_id=3 function_name_id=5 line=9 end_line=9 column=27 end_column=35}\n8 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=18 end_column=25}\n9 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=17 end_column=24}\n10 {file_name_id=3 function_name_id=5 line=9 end_line=9 column=17 end_column=61}\n11 {file_name_id=3 function_name_id=5 line=11 end_line=11 column=11 end_column=55}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=5}\n8 {file_location_id=8 parent_frame_id=5}\n9 {file_location_id=9 parent_frame_id=5}\n10 {file_location_id=10 parent_frame_id=5}\n11 {file_location_id=11 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[1024]) -> f32[1024] {\n  %param_0.1 = f32[1024]{0} parameter(0)\n  %constant.14 = f32[] constant(0.44), metadata={op_name=\"jit(nested_odsh)/jit(nested_odsh)\"}\n  %gt.11 = f32[1024]{0} broadcast(%constant.14), dimensions={}, metadata={op_name=\"jit(nested_odsh)/jit(nested_odsh)/gt\" stack_frame_id=5}\n  %gt.10 = pred[1024]{0} compare(%param_0.1, %gt.11), direction=GT, metadata={op_name=\"jit(nested_odsh)/jit(nested_odsh)/gt\" stack_frame_id=5}\n  %constant.13 = f32[] constant(0.69), metadata={op_name=\"jit(nested_odsh)/jit(nested_odsh)\"}\n  %gt.9 = f32[1024]{0} broadcast(%constant.13), dimensions={}, metadata={op_name=\"jit(nested_odsh)/jit(nested_odsh)/gt\" stack_frame_id=7}\n  %gt.8 = pred[1024]{0} compare(%param_0.1, %gt.9), direction=GT, metadata={op_name=\"jit(nested_odsh)/jit(nested_odsh)/gt\" stack_frame_id=7}\n  %constant.12 = f32[] constant(3), metadata={op_name=\"jit(nested_odsh)/jit(nested_odsh)\"}\n  %mul.17 = f32[1024]{0} broadcast(%constant.12), dimensions={}, metadata={op_name=\"jit(nested_odsh)/jit(nested_odsh)/mul\" stack_frame_id=9}\n  %mul.16 = f32[1024]{0} multiply(%param_0.1, %mul.17), metadata={op_name=\"jit(nested_odsh)/jit(nested_odsh)/mul\" stack_frame_id=9}\n  %constant.11 = f32[] constant(2), metadata={op_name=\"jit(nested_odsh)/jit(nested_odsh)\"}\n  %mul.15 = f32[1024]{0} broadcast(%constant.11), dimensions={}, metadata={op_name=\"jit(nested_odsh)/jit(nested_odsh)/mul\" stack_frame_id=8}\n  %mul.14 = f32[1024]{0} multiply(%param_0.1, %mul.15), metadata={op_name=\"jit(nested_odsh)/jit(nested_odsh)/mul\" stack_frame_id=8}\n  %select_n.7 = f32[1024]{0} select(%gt.8, %mul.16, %mul.14), metadata={op_name=\"jit(nested_odsh)/jit(nested_odsh)/jit(_where)/select_n\" stack_frame_id=10}\n  %constant.10 = f32[] constant(0.5), metadata={op_name=\"jit(nested_odsh)/jit(nested_odsh)\"}\n  %mul.13 = f32[1024]{0} broadcast(%constant.10), dimensions={}, metadata={op_name=\"jit(nested_odsh)/jit(nested_odsh)/mul\" stack_frame_id=6}\n  %mul.12 = f32[1024]{0} multiply(%param_0.1, %mul.13), metadata={op_name=\"jit(nested_odsh)/jit(nested_odsh)/mul\" stack_frame_id=6}\n  ROOT %select_n.6 = f32[1024]{0} select(%gt.10, %select_n.7, %mul.12), metadata={op_name=\"jit(nested_odsh)/jit(nested_odsh)/jit(_where)/select_n\" stack_frame_id=10}\n}\n\nENTRY %main.3 (a.1: f32[1024]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  ROOT %multiply_select_fusion = f32[1024]{0} fusion(%a.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(nested_odsh)/jit(nested_odsh)/jit(_where)/select_n\" stack_frame_id=10}\n}\n\n", "ptx": null}
{"id": 7, "kernel_name": "scalar_arr_bkct", "python": "@jax.jit\ndef scalar_arr_bkct(alpha, x, y):\n    return alpha * x + y", "type": "generate_scalar_array_op", "hlo": "module @jit_scalar_arr_bkct attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<1024xf32>, %arg2: tensor<1024xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @scalar_arr_bkct(%arg0, %arg1, %arg2) : (tensor<f32>, tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @scalar_arr_bkct(%arg0: tensor<f32>, %arg1: tensor<1024xf32>, %arg2: tensor<1024xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<1024xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<1024xf32>\n    return %3 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_scalar_arr_bkct, is_scheduled=true, entry_computation_layout={(f32[], f32[1024]{0}, f32[1024]{0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmp4nov4g5s.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"scalar_arr_bkct\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=11 end_column=20}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[1024], param_1.2: f32[1024], param_2.1: f32[]) -> f32[1024] {\n  %param_2.1 = f32[] parameter(2)\n  %mul.5 = f32[1024]{0} broadcast(%param_2.1), dimensions={}, metadata={op_name=\"jit(scalar_arr_bkct)/jit(scalar_arr_bkct)/mul\" stack_frame_id=5}\n  %param_1.2 = f32[1024]{0} parameter(1)\n  %mul.4 = f32[1024]{0} multiply(%mul.5, %param_1.2), metadata={op_name=\"jit(scalar_arr_bkct)/jit(scalar_arr_bkct)/mul\" stack_frame_id=5}\n  %param_0.1 = f32[1024]{0} parameter(0)\n  ROOT %add.2 = f32[1024]{0} add(%mul.4, %param_0.1), metadata={op_name=\"jit(scalar_arr_bkct)/jit(scalar_arr_bkct)/add\" stack_frame_id=5}\n}\n\nENTRY %main.2 (alpha.1: f32[], x.1: f32[1024], y.1: f32[1024]) -> f32[1024] {\n  %alpha.1 = f32[] parameter(0), metadata={op_name=\"alpha\"}\n  %x.1 = f32[1024]{0} parameter(1), metadata={op_name=\"x\"}\n  %y.1 = f32[1024]{0} parameter(2), metadata={op_name=\"y\"}\n  ROOT %multiply_add_fusion = f32[1024]{0} fusion(%y.1, %x.1, %alpha.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(scalar_arr_bkct)/jit(scalar_arr_bkct)/add\" stack_frame_id=5}\n}\n\n", "ptx": null}
{"id": 8, "kernel_name": "multi_mgqg", "python": "@jax.jit\ndef multi_mgqg(a, b):\n    temp1 = a - b\n    temp2 = jnp.sqrt(temp1)\n    return temp2 - a", "type": "generate_multi_statement_kernel", "hlo": "module @jit_multi_mgqg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>, %arg1: tensor<1024xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @multi_mgqg(%arg0, %arg1) : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @multi_mgqg(%arg0: tensor<1024xf32>, %arg1: tensor<1024xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<1024xf32>\n    %1 = stablehlo.sqrt %0 : tensor<1024xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<1024xf32>\n    return %2 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_multi_mgqg, is_scheduled=true, entry_computation_layout={(f32[1024]{0}, f32[1024]{0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmpjkqj48pz.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"multi_mgqg\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=12 end_column=17}\n6 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=12 end_column=27}\n7 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=11 end_column=20}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[1024], param_1.2: f32[1024]) -> f32[1024] {\n  %param_0.1 = f32[1024]{0} parameter(0)\n  %param_1.2 = f32[1024]{0} parameter(1)\n  %sub.5 = f32[1024]{0} subtract(%param_0.1, %param_1.2), metadata={op_name=\"jit(multi_mgqg)/jit(multi_mgqg)/sub\" stack_frame_id=5}\n  %sqrt.2 = f32[1024]{0} sqrt(%sub.5), metadata={op_name=\"jit(multi_mgqg)/jit(multi_mgqg)/sqrt\" stack_frame_id=6}\n  ROOT %sub.4 = f32[1024]{0} subtract(%sqrt.2, %param_0.1), metadata={op_name=\"jit(multi_mgqg)/jit(multi_mgqg)/sub\" stack_frame_id=7}\n}\n\nENTRY %main.2 (a.1: f32[1024], b.1: f32[1024]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  %b.1 = f32[1024]{0} parameter(1), metadata={op_name=\"b\"}\n  ROOT %sqrt_subtract_fusion = f32[1024]{0} fusion(%a.1, %b.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(multi_mgqg)/jit(multi_mgqg)/sub\" stack_frame_id=7}\n}\n\n", "ptx": null}
{"id": 9, "kernel_name": "branch_gnev", "python": "@jax.jit\ndef branch_gnev(a):\n    return jnp.where(a > -0.54, a * 2.8, a - 1.5)", "type": "generate_branch_kernel", "hlo": "module @jit_branch_gnev attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @branch_gnev(%arg0) : (tensor<1024xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @branch_gnev(%arg0: tensor<1024xf32>) -> tensor<1024xf32> {\n    %cst = stablehlo.constant dense<-5.400000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xi1>\n    %cst_0 = stablehlo.constant dense<2.800000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<1024xf32>\n    %cst_1 = stablehlo.constant dense<1.500000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<1024xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<1024xi1>, tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>\n    return %6 : tensor<1024xf32>\n  }\n  func.func private @_where(%arg0: tensor<1024xi1>, %arg1: tensor<1024xf32>, %arg2: tensor<1024xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<1024xi1>, tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_branch_gnev, is_scheduled=true, entry_computation_layout={(f32[1024]{0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmph8ve_urt.py\"\n4 \"/tmp/tmp7nvd0zdg.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"branch_gnev\"\n6 \"nested_odsh\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=41 end_column=48}\n6 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=32 end_column=39}\n7 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=21 end_column=30}\n8 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=11 end_column=49}\n9 {file_name_id=4 function_name_id=6 line=9 end_line=9 column=17 end_column=61}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=5}\n8 {file_location_id=8 parent_frame_id=5}\n9 {file_location_id=9 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[1024]) -> f32[1024] {\n  %param_0.1 = f32[1024]{0} parameter(0)\n  %constant.9 = f32[] constant(-0.54), metadata={op_name=\"jit(branch_gnev)/jit(branch_gnev)\"}\n  %gt.5 = f32[1024]{0} broadcast(%constant.9), dimensions={}, metadata={op_name=\"jit(branch_gnev)/jit(branch_gnev)/gt\" stack_frame_id=7}\n  %gt.4 = pred[1024]{0} compare(%param_0.1, %gt.5), direction=GT, metadata={op_name=\"jit(branch_gnev)/jit(branch_gnev)/gt\" stack_frame_id=7}\n  %constant.8 = f32[] constant(2.8), metadata={op_name=\"jit(branch_gnev)/jit(branch_gnev)\"}\n  %mul.5 = f32[1024]{0} broadcast(%constant.8), dimensions={}, metadata={op_name=\"jit(branch_gnev)/jit(branch_gnev)/mul\" stack_frame_id=6}\n  %mul.4 = f32[1024]{0} multiply(%param_0.1, %mul.5), metadata={op_name=\"jit(branch_gnev)/jit(branch_gnev)/mul\" stack_frame_id=6}\n  %constant.7 = f32[] constant(-1.5), metadata={op_name=\"jit(branch_gnev)/jit(branch_gnev)\"}\n  %broadcast.1 = f32[1024]{0} broadcast(%constant.7), dimensions={}, metadata={op_name=\"jit(branch_gnev)/jit(branch_gnev)\"}\n  %add.1 = f32[1024]{0} add(%param_0.1, %broadcast.1), metadata={op_name=\"jit(branch_gnev)/jit(branch_gnev)/sub\" stack_frame_id=5}\n  ROOT %select_n.3 = f32[1024]{0} select(%gt.4, %mul.4, %add.1), metadata={op_name=\"jit(branch_gnev)/jit(branch_gnev)/jit(_where)/select_n\" stack_frame_id=9}\n}\n\nENTRY %main.3 (a.1: f32[1024]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  ROOT %add_select_fusion = f32[1024]{0} fusion(%a.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(branch_gnev)/jit(branch_gnev)/jit(_where)/select_n\" stack_frame_id=9}\n}\n\n", "ptx": null}
{"id": 10, "kernel_name": "loop_zbsm", "python": "@jax.jit\ndef loop_zbsm(a, n):\n    def body_fun(i, val):\n        return val - a\n    # Initial value matches a's shape/type but zeroed\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "hlo": "module @jit_loop_zbsm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>, %arg1: tensor<i32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @loop_zbsm(%arg0, %arg1) : (tensor<1024xf32>, tensor<i32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @loop_zbsm(%arg0: tensor<1024xf32>, %arg1: tensor<i32>) -> tensor<1024xf32> {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<1024xf32>, tensor<i32>, tensor<i32>, tensor<1024xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<1024xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<1024xf32>, tensor<i32>, tensor<i32>, tensor<1024xf32>\n    }\n    return %1#3 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_loop_zbsm, is_scheduled=true, entry_computation_layout={(f32[1024]{0}, s32[])->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmpy1x_896n.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"loop_zbsm\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=9 end_line=9 column=15 end_column=32}\n6 {file_name_id=3 function_name_id=5 line=10 end_line=10 column=11 end_column=54}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n\n\n%wrapped_add_computation (param_0.1: s32[], param_1: s32[]) -> s32[] {\n  %param_0.1 = s32[] parameter(0)\n  %param_1 = s32[] parameter(1)\n  ROOT %add.0 = s32[] add(%param_0.1, %param_1), metadata={op_name=\"jit(loop_zbsm)/jit(loop_zbsm)/while/body/add\" stack_frame_id=6}\n}\n\n%wrapped_subtract_computation (param_0.2: f32[1024], param_1.1: f32[1024]) -> f32[1024] {\n  %param_0.2 = f32[1024]{0} parameter(0)\n  %param_1.1 = f32[1024]{0} parameter(1)\n  ROOT %sub.0 = f32[1024]{0} subtract(%param_0.2, %param_1.1), metadata={op_name=\"jit(loop_zbsm)/jit(loop_zbsm)/while/body/sub\" stack_frame_id=6}\n}\n\n%region_0.1 (arg_tuple.1: (s32[], f32[1024], s32[], f32[1024])) -> (s32[], f32[1024], s32[], f32[1024]) {\n  %arg_tuple.1 = (s32[], f32[1024]{0}, s32[], f32[1024]{0}) parameter(0), metadata={op_name=\"jit(loop_zbsm)/jit(loop_zbsm)\"}\n  %constant.3 = s32[] constant(1), metadata={op_name=\"jit(loop_zbsm)/jit(loop_zbsm)\"}\n  %get-tuple-element.19 = f32[1024]{0} get-tuple-element(%arg_tuple.1), index=3, metadata={op_name=\"jit(loop_zbsm)/jit(loop_zbsm)\"}\n  %get-tuple-element.18 = s32[] get-tuple-element(%arg_tuple.1), index=2, metadata={op_name=\"jit(loop_zbsm)/jit(loop_zbsm)\"}\n  %get-tuple-element.9 = f32[1024]{0} get-tuple-element(%arg_tuple.1), index=1\n  %get-tuple-element.8 = s32[] get-tuple-element(%arg_tuple.1), index=0\n  %wrapped_subtract = f32[1024]{0} fusion(%get-tuple-element.9, %get-tuple-element.19), kind=kLoop, calls=%wrapped_subtract_computation, metadata={op_name=\"jit(loop_zbsm)/jit(loop_zbsm)/while/body/sub\" stack_frame_id=6}\n  %wrapped_add = s32[] fusion(%get-tuple-element.8, %constant.3), kind=kLoop, calls=%wrapped_add_computation, metadata={op_name=\"jit(loop_zbsm)/jit(loop_zbsm)/while/body/add\" stack_frame_id=6}\n  ROOT %tuple.3 = (s32[], f32[1024]{0}, s32[], f32[1024]{0}) tuple(%wrapped_add, %wrapped_subtract, %get-tuple-element.18, %get-tuple-element.19)\n}\n\n%wrapped_compare_computation (param_0.3: s32[], param_1.2: s32[]) -> pred[] {\n  %param_0.3 = s32[] parameter(0)\n  %param_1.2 = s32[] parameter(1)\n  ROOT %lt.0 = pred[] compare(%param_0.3, %param_1.2), direction=LT, metadata={op_name=\"jit(loop_zbsm)/jit(loop_zbsm)/while/cond/lt\" stack_frame_id=6}\n}\n\n%region_1.2 (arg_tuple.3: (s32[], f32[1024], s32[], f32[1024])) -> pred[] {\n  %arg_tuple.3 = (s32[], f32[1024]{0}, s32[], f32[1024]{0}) parameter(0), metadata={op_name=\"jit(loop_zbsm)/jit(loop_zbsm)\"}\n  %get-tuple-element.14 = s32[] get-tuple-element(%arg_tuple.3), index=2, metadata={op_name=\"jit(loop_zbsm)/jit(loop_zbsm)\"}\n  %get-tuple-element.12 = s32[] get-tuple-element(%arg_tuple.3), index=0, metadata={op_name=\"jit(loop_zbsm)/jit(loop_zbsm)\"}\n  ROOT %wrapped_compare = pred[] fusion(%get-tuple-element.12, %get-tuple-element.14), kind=kLoop, calls=%wrapped_compare_computation, metadata={op_name=\"jit(loop_zbsm)/jit(loop_zbsm)/while/cond/lt\" stack_frame_id=6}\n}\n\n%wrapped_broadcast_computation (param_0: f32[]) -> f32[1024] {\n  %param_0 = f32[] parameter(0)\n  ROOT %broadcast_in_dim.2 = f32[1024]{0} broadcast(%param_0), dimensions={}, metadata={op_name=\"jit(loop_zbsm)/jit(loop_zbsm)/broadcast_in_dim\" stack_frame_id=5}\n}\n\nENTRY %main.4 (a.1: f32[1024], n.1: s32[]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  %n.1 = s32[] parameter(1), metadata={op_name=\"n\"}\n  %constant.0 = s32[] constant(0), metadata={op_name=\"jit(loop_zbsm)/jit(loop_zbsm)\"}\n  %constant.1 = f32[] constant(0), metadata={op_name=\"jit(loop_zbsm)/jit(loop_zbsm)\"}\n  %copy.6 = s32[] copy(%constant.0)\n  %wrapped_broadcast = f32[1024]{0} fusion(%constant.1), kind=kLoop, calls=%wrapped_broadcast_computation, metadata={op_name=\"jit(loop_zbsm)/jit(loop_zbsm)/broadcast_in_dim\" stack_frame_id=5}\n  %tuple = (s32[], f32[1024]{0}, s32[], f32[1024]{0}) tuple(%copy.6, %wrapped_broadcast, %n.1, %a.1)\n  %while.1 = (s32[], f32[1024]{0}, s32[], f32[1024]{0}) while(%tuple), condition=%region_1.2, body=%region_0.1, metadata={op_name=\"jit(loop_zbsm)/jit(loop_zbsm)/while\" stack_frame_id=6}\n  ROOT %while.2 = f32[1024]{0} get-tuple-element(%while.1), index=1, metadata={op_name=\"jit(loop_zbsm)/jit(loop_zbsm)/while\" stack_frame_id=6}\n}\n\n", "ptx": null}
{"id": 11, "kernel_name": "compound_qxls", "python": "@jax.jit\ndef compound_qxls(a, b, scale):\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "hlo": "module @jit_compound_qxls attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>, %arg1: tensor<1024xf32>, %arg2: tensor<f32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @compound_qxls(%arg0, %arg1, %arg2) : (tensor<1024xf32>, tensor<1024xf32>, tensor<f32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @compound_qxls(%arg0: tensor<1024xf32>, %arg1: tensor<1024xf32>, %arg2: tensor<f32>) -> tensor<1024xf32> {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<1024xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<1024xf32>\n    %4 = stablehlo.floor %3 : tensor<1024xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<1024xf32>\n    %6 = stablehlo.abs %5 : tensor<1024xf32>\n    return %6 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_compound_qxls, is_scheduled=true, entry_computation_layout={(f32[1024]{0}, f32[1024]{0}, f32[])->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmpvv8wa43a.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"compound_qxls\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=14 end_column=19}\n6 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=13 end_column=28}\n7 {file_name_id=3 function_name_id=5 line=9 end_line=9 column=22 end_column=39}\n8 {file_name_id=3 function_name_id=5 line=9 end_line=9 column=13 end_column=39}\n9 {file_name_id=3 function_name_id=5 line=10 end_line=10 column=11 end_column=26}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=5}\n8 {file_location_id=8 parent_frame_id=5}\n9 {file_location_id=9 parent_frame_id=5}\n\n\n%fused_computation (param_0.3: f32[], param_1.3: f32[1024], param_2: f32[1024]) -> f32[1024] {\n  %param_1.3 = f32[1024]{0} parameter(1)\n  %param_2 = f32[1024]{0} parameter(2)\n  %add.2 = f32[1024]{0} add(%param_1.3, %param_2), metadata={op_name=\"jit(compound_qxls)/jit(compound_qxls)/add\" stack_frame_id=5}\n  %param_0.3 = f32[] parameter(0)\n  %mul.5 = f32[1024]{0} broadcast(%param_0.3), dimensions={}, metadata={op_name=\"jit(compound_qxls)/jit(compound_qxls)/mul\" stack_frame_id=6}\n  %mul.4 = f32[1024]{0} multiply(%add.2, %mul.5), metadata={op_name=\"jit(compound_qxls)/jit(compound_qxls)/mul\" stack_frame_id=6}\n  %floor.2 = f32[1024]{0} floor(%mul.4), metadata={op_name=\"jit(compound_qxls)/jit(compound_qxls)/floor\" stack_frame_id=7}\n  %sub.2 = f32[1024]{0} subtract(%mul.4, %floor.2), metadata={op_name=\"jit(compound_qxls)/jit(compound_qxls)/sub\" stack_frame_id=8}\n  ROOT %abs.2 = f32[1024]{0} abs(%sub.2), metadata={op_name=\"jit(compound_qxls)/jit(compound_qxls)/abs\" stack_frame_id=9}\n}\n\nENTRY %main.2 (a.1: f32[1024], b.1: f32[1024], scale.1: f32[]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  %b.1 = f32[1024]{0} parameter(1), metadata={op_name=\"b\"}\n  %scale.1 = f32[] parameter(2), metadata={op_name=\"scale\"}\n  ROOT %subtract_abs_fusion = f32[1024]{0} fusion(%scale.1, %a.1, %b.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(compound_qxls)/jit(compound_qxls)/abs\" stack_frame_id=9}\n}\n\n", "ptx": null}
{"id": 12, "kernel_name": "unary_xdom", "python": "@jax.jit\ndef unary_xdom(a):\n    return jnp.cos(a)", "type": "generate_unary_kernel", "hlo": "module @jit_unary_xdom attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @unary_xdom(%arg0) : (tensor<1024xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @unary_xdom(%arg0: tensor<1024xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.cosine %arg0 : tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_unary_xdom, is_scheduled=true, entry_computation_layout={(f32[1024]{0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmpzxbmqpz4.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"unary_xdom\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=11 end_column=21}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n\n\n%wrapped_cosine_computation (param_0: f32[1024]) -> f32[1024] {\n  %param_0 = f32[1024]{0} parameter(0)\n  ROOT %cos.2 = f32[1024]{0} cosine(%param_0), metadata={op_name=\"jit(unary_xdom)/jit(unary_xdom)/cos\" stack_frame_id=5}\n}\n\nENTRY %main.2 (a.1: f32[1024]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  ROOT %wrapped_cosine = f32[1024]{0} fusion(%a.1), kind=kLoop, calls=%wrapped_cosine_computation, metadata={op_name=\"jit(unary_xdom)/jit(unary_xdom)/cos\" stack_frame_id=5}\n}\n\n", "ptx": null}
{"id": 13, "kernel_name": "scalar_arr_cydt", "python": "@jax.jit\ndef scalar_arr_cydt(alpha, x, y):\n    return alpha + x * y", "type": "generate_scalar_array_op", "hlo": "module @jit_scalar_arr_cydt attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<1024xf32>, %arg2: tensor<1024xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @scalar_arr_cydt(%arg0, %arg1, %arg2) : (tensor<f32>, tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @scalar_arr_cydt(%arg0: tensor<f32>, %arg1: tensor<1024xf32>, %arg2: tensor<1024xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<1024xf32>\n    %1 = stablehlo.convert %arg0 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %3 = stablehlo.add %2, %0 : tensor<1024xf32>\n    return %3 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_scalar_arr_cydt, is_scheduled=true, entry_computation_layout={(f32[], f32[1024]{0}, f32[1024]{0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmpp1fmtxd2.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"scalar_arr_cydt\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=19 end_column=24}\n6 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=11 end_column=24}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[1024], param_1.2: f32[1024], param_2.1: f32[]) -> f32[1024] {\n  %param_2.1 = f32[] parameter(2)\n  %add.5 = f32[1024]{0} broadcast(%param_2.1), dimensions={}, metadata={op_name=\"jit(scalar_arr_cydt)/jit(scalar_arr_cydt)/add\" stack_frame_id=6}\n  %param_0.1 = f32[1024]{0} parameter(0)\n  %param_1.2 = f32[1024]{0} parameter(1)\n  %mul.2 = f32[1024]{0} multiply(%param_0.1, %param_1.2), metadata={op_name=\"jit(scalar_arr_cydt)/jit(scalar_arr_cydt)/mul\" stack_frame_id=5}\n  ROOT %add.4 = f32[1024]{0} add(%add.5, %mul.2), metadata={op_name=\"jit(scalar_arr_cydt)/jit(scalar_arr_cydt)/add\" stack_frame_id=6}\n}\n\nENTRY %main.2 (alpha.1: f32[], x.1: f32[1024], y.1: f32[1024]) -> f32[1024] {\n  %alpha.1 = f32[] parameter(0), metadata={op_name=\"alpha\"}\n  %x.1 = f32[1024]{0} parameter(1), metadata={op_name=\"x\"}\n  %y.1 = f32[1024]{0} parameter(2), metadata={op_name=\"y\"}\n  ROOT %multiply_add_fusion = f32[1024]{0} fusion(%x.1, %y.1, %alpha.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(scalar_arr_cydt)/jit(scalar_arr_cydt)/add\" stack_frame_id=6}\n}\n\n", "ptx": null}
{"id": 14, "kernel_name": "nested_zomn", "python": "@jax.jit\ndef nested_zomn(a):\n    # Nested where: if a > t1 then (if a > t2 then ... else ...) else ...\n    inner_true = a * 3.0\n    inner_false = a * 2.0\n    outer_true = jnp.where(a > 1.49, inner_true, inner_false)\n    outer_false = a * 0.5\n    return jnp.where(a > 0.27, outer_true, outer_false)", "type": "generate_nested_branch_kernel", "hlo": "module @jit_nested_zomn attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @nested_zomn(%arg0) : (tensor<1024xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @nested_zomn(%arg0: tensor<1024xf32>) -> tensor<1024xf32> {\n    %cst = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %1 = stablehlo.multiply %arg0, %0 : tensor<1024xf32>\n    %cst_0 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<1024xf32>\n    %cst_1 = stablehlo.constant dense<1.490000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %5 = stablehlo.compare  GT, %arg0, %4,  FLOAT : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xi1>\n    %6 = call @_where(%5, %1, %3) : (tensor<1024xi1>, tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>\n    %cst_2 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %8 = stablehlo.multiply %arg0, %7 : tensor<1024xf32>\n    %cst_3 = stablehlo.constant dense<2.700000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %10 = stablehlo.compare  GT, %arg0, %9,  FLOAT : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xi1>\n    %11 = call @_where(%10, %6, %8) : (tensor<1024xi1>, tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>\n    return %11 : tensor<1024xf32>\n  }\n  func.func private @_where(%arg0: tensor<1024xi1>, %arg1: tensor<1024xf32>, %arg2: tensor<1024xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<1024xi1>, tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_nested_zomn, is_scheduled=true, entry_computation_layout={(f32[1024]{0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmp2z3a8z27.py\"\n4 \"/tmp/tmp7nvd0zdg.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"nested_zomn\"\n6 \"nested_odsh\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=11 end_line=11 column=21 end_column=29}\n6 {file_name_id=3 function_name_id=5 line=10 end_line=10 column=18 end_column=25}\n7 {file_name_id=3 function_name_id=5 line=9 end_line=9 column=27 end_column=35}\n8 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=18 end_column=25}\n9 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=17 end_column=24}\n10 {file_name_id=3 function_name_id=5 line=9 end_line=9 column=17 end_column=61}\n11 {file_name_id=4 function_name_id=6 line=9 end_line=9 column=17 end_column=61}\n12 {file_name_id=3 function_name_id=5 line=11 end_line=11 column=11 end_column=55}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=5}\n8 {file_location_id=8 parent_frame_id=5}\n9 {file_location_id=9 parent_frame_id=5}\n10 {file_location_id=10 parent_frame_id=5}\n11 {file_location_id=11 parent_frame_id=5}\n12 {file_location_id=12 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[1024]) -> f32[1024] {\n  %param_0.1 = f32[1024]{0} parameter(0)\n  %constant.14 = f32[] constant(0.27), metadata={op_name=\"jit(nested_zomn)/jit(nested_zomn)\"}\n  %gt.11 = f32[1024]{0} broadcast(%constant.14), dimensions={}, metadata={op_name=\"jit(nested_zomn)/jit(nested_zomn)/gt\" stack_frame_id=5}\n  %gt.10 = pred[1024]{0} compare(%param_0.1, %gt.11), direction=GT, metadata={op_name=\"jit(nested_zomn)/jit(nested_zomn)/gt\" stack_frame_id=5}\n  %constant.13 = f32[] constant(1.49), metadata={op_name=\"jit(nested_zomn)/jit(nested_zomn)\"}\n  %gt.9 = f32[1024]{0} broadcast(%constant.13), dimensions={}, metadata={op_name=\"jit(nested_zomn)/jit(nested_zomn)/gt\" stack_frame_id=7}\n  %gt.8 = pred[1024]{0} compare(%param_0.1, %gt.9), direction=GT, metadata={op_name=\"jit(nested_zomn)/jit(nested_zomn)/gt\" stack_frame_id=7}\n  %constant.12 = f32[] constant(3), metadata={op_name=\"jit(nested_zomn)/jit(nested_zomn)\"}\n  %mul.17 = f32[1024]{0} broadcast(%constant.12), dimensions={}, metadata={op_name=\"jit(nested_zomn)/jit(nested_zomn)/mul\" stack_frame_id=9}\n  %mul.16 = f32[1024]{0} multiply(%param_0.1, %mul.17), metadata={op_name=\"jit(nested_zomn)/jit(nested_zomn)/mul\" stack_frame_id=9}\n  %constant.11 = f32[] constant(2), metadata={op_name=\"jit(nested_zomn)/jit(nested_zomn)\"}\n  %mul.15 = f32[1024]{0} broadcast(%constant.11), dimensions={}, metadata={op_name=\"jit(nested_zomn)/jit(nested_zomn)/mul\" stack_frame_id=8}\n  %mul.14 = f32[1024]{0} multiply(%param_0.1, %mul.15), metadata={op_name=\"jit(nested_zomn)/jit(nested_zomn)/mul\" stack_frame_id=8}\n  %select_n.7 = f32[1024]{0} select(%gt.8, %mul.16, %mul.14), metadata={op_name=\"jit(nested_zomn)/jit(nested_zomn)/jit(_where)/select_n\" stack_frame_id=11}\n  %constant.10 = f32[] constant(0.5), metadata={op_name=\"jit(nested_zomn)/jit(nested_zomn)\"}\n  %mul.13 = f32[1024]{0} broadcast(%constant.10), dimensions={}, metadata={op_name=\"jit(nested_zomn)/jit(nested_zomn)/mul\" stack_frame_id=6}\n  %mul.12 = f32[1024]{0} multiply(%param_0.1, %mul.13), metadata={op_name=\"jit(nested_zomn)/jit(nested_zomn)/mul\" stack_frame_id=6}\n  ROOT %select_n.6 = f32[1024]{0} select(%gt.10, %select_n.7, %mul.12), metadata={op_name=\"jit(nested_zomn)/jit(nested_zomn)/jit(_where)/select_n\" stack_frame_id=11}\n}\n\nENTRY %main.3 (a.1: f32[1024]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  ROOT %multiply_select_fusion = f32[1024]{0} fusion(%a.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(nested_zomn)/jit(nested_zomn)/jit(_where)/select_n\" stack_frame_id=11}\n}\n\n", "ptx": null}
{"id": 15, "kernel_name": "elementwise_bpan", "python": "@jax.jit\ndef elementwise_bpan(a, b):\n    return a - b", "type": "generate_simple_elementwise", "hlo": "module @jit_elementwise_bpan attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>, %arg1: tensor<1024xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @elementwise_bpan(%arg0, %arg1) : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @elementwise_bpan(%arg0: tensor<1024xf32>, %arg1: tensor<1024xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_elementwise_bpan, is_scheduled=true, entry_computation_layout={(f32[1024]{0}, f32[1024]{0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmpj0odcbve.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"elementwise_bpan\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=11 end_column=16}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n\n\n%wrapped_subtract_computation (param_0: f32[1024], param_1: f32[1024]) -> f32[1024] {\n  %param_0 = f32[1024]{0} parameter(0)\n  %param_1 = f32[1024]{0} parameter(1)\n  ROOT %sub.2 = f32[1024]{0} subtract(%param_0, %param_1), metadata={op_name=\"jit(elementwise_bpan)/jit(elementwise_bpan)/sub\" stack_frame_id=5}\n}\n\nENTRY %main.2 (a.1: f32[1024], b.1: f32[1024]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  %b.1 = f32[1024]{0} parameter(1), metadata={op_name=\"b\"}\n  ROOT %wrapped_subtract = f32[1024]{0} fusion(%a.1, %b.1), kind=kLoop, calls=%wrapped_subtract_computation, metadata={op_name=\"jit(elementwise_bpan)/jit(elementwise_bpan)/sub\" stack_frame_id=5}\n}\n\n", "ptx": null}
{"id": 16, "kernel_name": "compound_pfqy", "python": "@jax.jit\ndef compound_pfqy(a, b, scale):\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "hlo": "module @jit_compound_pfqy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>, %arg1: tensor<1024xf32>, %arg2: tensor<f32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @compound_pfqy(%arg0, %arg1, %arg2) : (tensor<1024xf32>, tensor<1024xf32>, tensor<f32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @compound_pfqy(%arg0: tensor<1024xf32>, %arg1: tensor<1024xf32>, %arg2: tensor<f32>) -> tensor<1024xf32> {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<1024xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<1024xf32>\n    %4 = stablehlo.floor %3 : tensor<1024xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<1024xf32>\n    %6 = stablehlo.abs %5 : tensor<1024xf32>\n    return %6 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_compound_pfqy, is_scheduled=true, entry_computation_layout={(f32[1024]{0}, f32[1024]{0}, f32[])->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmpsrx1_91h.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"compound_pfqy\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=14 end_column=19}\n6 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=13 end_column=28}\n7 {file_name_id=3 function_name_id=5 line=9 end_line=9 column=22 end_column=39}\n8 {file_name_id=3 function_name_id=5 line=9 end_line=9 column=13 end_column=39}\n9 {file_name_id=3 function_name_id=5 line=10 end_line=10 column=11 end_column=26}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=5}\n8 {file_location_id=8 parent_frame_id=5}\n9 {file_location_id=9 parent_frame_id=5}\n\n\n%fused_computation (param_0.3: f32[], param_1.3: f32[1024], param_2: f32[1024]) -> f32[1024] {\n  %param_1.3 = f32[1024]{0} parameter(1)\n  %param_2 = f32[1024]{0} parameter(2)\n  %add.2 = f32[1024]{0} add(%param_1.3, %param_2), metadata={op_name=\"jit(compound_pfqy)/jit(compound_pfqy)/add\" stack_frame_id=5}\n  %param_0.3 = f32[] parameter(0)\n  %mul.5 = f32[1024]{0} broadcast(%param_0.3), dimensions={}, metadata={op_name=\"jit(compound_pfqy)/jit(compound_pfqy)/mul\" stack_frame_id=6}\n  %mul.4 = f32[1024]{0} multiply(%add.2, %mul.5), metadata={op_name=\"jit(compound_pfqy)/jit(compound_pfqy)/mul\" stack_frame_id=6}\n  %floor.2 = f32[1024]{0} floor(%mul.4), metadata={op_name=\"jit(compound_pfqy)/jit(compound_pfqy)/floor\" stack_frame_id=7}\n  %sub.2 = f32[1024]{0} subtract(%mul.4, %floor.2), metadata={op_name=\"jit(compound_pfqy)/jit(compound_pfqy)/sub\" stack_frame_id=8}\n  ROOT %abs.2 = f32[1024]{0} abs(%sub.2), metadata={op_name=\"jit(compound_pfqy)/jit(compound_pfqy)/abs\" stack_frame_id=9}\n}\n\nENTRY %main.2 (a.1: f32[1024], b.1: f32[1024], scale.1: f32[]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  %b.1 = f32[1024]{0} parameter(1), metadata={op_name=\"b\"}\n  %scale.1 = f32[] parameter(2), metadata={op_name=\"scale\"}\n  ROOT %subtract_abs_fusion = f32[1024]{0} fusion(%scale.1, %a.1, %b.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(compound_pfqy)/jit(compound_pfqy)/abs\" stack_frame_id=9}\n}\n\n", "ptx": null}
{"id": 17, "kernel_name": "branch_fyya", "python": "@jax.jit\ndef branch_fyya(a):\n    return jnp.where(a > 0.69, a * 2.2, a + 0.5)", "type": "generate_branch_kernel", "hlo": "module @jit_branch_fyya attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @branch_fyya(%arg0) : (tensor<1024xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @branch_fyya(%arg0: tensor<1024xf32>) -> tensor<1024xf32> {\n    %cst = stablehlo.constant dense<0.689999997> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xi1>\n    %cst_0 = stablehlo.constant dense<2.200000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<1024xf32>\n    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<1024xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<1024xi1>, tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>\n    return %6 : tensor<1024xf32>\n  }\n  func.func private @_where(%arg0: tensor<1024xi1>, %arg1: tensor<1024xf32>, %arg2: tensor<1024xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<1024xi1>, tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_branch_fyya, is_scheduled=true, entry_computation_layout={(f32[1024]{0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmp52wuwg7c.py\"\n4 \"/tmp/tmp7nvd0zdg.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"branch_fyya\"\n6 \"nested_odsh\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=40 end_column=47}\n6 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=31 end_column=38}\n7 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=21 end_column=29}\n8 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=11 end_column=48}\n9 {file_name_id=4 function_name_id=6 line=9 end_line=9 column=17 end_column=61}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=5}\n8 {file_location_id=8 parent_frame_id=5}\n9 {file_location_id=9 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[1024]) -> f32[1024] {\n  %param_0.1 = f32[1024]{0} parameter(0)\n  %constant.8 = f32[] constant(0.69), metadata={op_name=\"jit(branch_fyya)/jit(branch_fyya)\"}\n  %gt.5 = f32[1024]{0} broadcast(%constant.8), dimensions={}, metadata={op_name=\"jit(branch_fyya)/jit(branch_fyya)/gt\" stack_frame_id=7}\n  %gt.4 = pred[1024]{0} compare(%param_0.1, %gt.5), direction=GT, metadata={op_name=\"jit(branch_fyya)/jit(branch_fyya)/gt\" stack_frame_id=7}\n  %constant.7 = f32[] constant(2.2), metadata={op_name=\"jit(branch_fyya)/jit(branch_fyya)\"}\n  %mul.5 = f32[1024]{0} broadcast(%constant.7), dimensions={}, metadata={op_name=\"jit(branch_fyya)/jit(branch_fyya)/mul\" stack_frame_id=6}\n  %mul.4 = f32[1024]{0} multiply(%param_0.1, %mul.5), metadata={op_name=\"jit(branch_fyya)/jit(branch_fyya)/mul\" stack_frame_id=6}\n  %constant.6 = f32[] constant(0.5), metadata={op_name=\"jit(branch_fyya)/jit(branch_fyya)\"}\n  %add.5 = f32[1024]{0} broadcast(%constant.6), dimensions={}, metadata={op_name=\"jit(branch_fyya)/jit(branch_fyya)/add\" stack_frame_id=5}\n  %add.4 = f32[1024]{0} add(%param_0.1, %add.5), metadata={op_name=\"jit(branch_fyya)/jit(branch_fyya)/add\" stack_frame_id=5}\n  ROOT %select_n.3 = f32[1024]{0} select(%gt.4, %mul.4, %add.4), metadata={op_name=\"jit(branch_fyya)/jit(branch_fyya)/jit(_where)/select_n\" stack_frame_id=9}\n}\n\nENTRY %main.3 (a.1: f32[1024]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  ROOT %add_select_fusion = f32[1024]{0} fusion(%a.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(branch_fyya)/jit(branch_fyya)/jit(_where)/select_n\" stack_frame_id=9}\n}\n\n", "ptx": null}
{"id": 18, "kernel_name": "loop_ipgv", "python": "@jax.jit\ndef loop_ipgv(a, n):\n    def body_fun(i, val):\n        return val - a\n    # Initial value matches a's shape/type but zeroed\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "hlo": "module @jit_loop_ipgv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>, %arg1: tensor<i32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @loop_ipgv(%arg0, %arg1) : (tensor<1024xf32>, tensor<i32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @loop_ipgv(%arg0: tensor<1024xf32>, %arg1: tensor<i32>) -> tensor<1024xf32> {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<1024xf32>, tensor<i32>, tensor<i32>, tensor<1024xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<1024xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<1024xf32>, tensor<i32>, tensor<i32>, tensor<1024xf32>\n    }\n    return %1#3 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_loop_ipgv, is_scheduled=true, entry_computation_layout={(f32[1024]{0}, s32[])->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmp0tcr1uff.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"loop_ipgv\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=9 end_line=9 column=15 end_column=32}\n6 {file_name_id=3 function_name_id=5 line=10 end_line=10 column=11 end_column=54}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n\n\n%wrapped_add_computation (param_0.1: s32[], param_1: s32[]) -> s32[] {\n  %param_0.1 = s32[] parameter(0)\n  %param_1 = s32[] parameter(1)\n  ROOT %add.0 = s32[] add(%param_0.1, %param_1), metadata={op_name=\"jit(loop_ipgv)/jit(loop_ipgv)/while/body/add\" stack_frame_id=6}\n}\n\n%wrapped_subtract_computation (param_0.2: f32[1024], param_1.1: f32[1024]) -> f32[1024] {\n  %param_0.2 = f32[1024]{0} parameter(0)\n  %param_1.1 = f32[1024]{0} parameter(1)\n  ROOT %sub.0 = f32[1024]{0} subtract(%param_0.2, %param_1.1), metadata={op_name=\"jit(loop_ipgv)/jit(loop_ipgv)/while/body/sub\" stack_frame_id=6}\n}\n\n%region_0.1 (arg_tuple.1: (s32[], f32[1024], s32[], f32[1024])) -> (s32[], f32[1024], s32[], f32[1024]) {\n  %arg_tuple.1 = (s32[], f32[1024]{0}, s32[], f32[1024]{0}) parameter(0), metadata={op_name=\"jit(loop_ipgv)/jit(loop_ipgv)\"}\n  %constant.3 = s32[] constant(1), metadata={op_name=\"jit(loop_ipgv)/jit(loop_ipgv)\"}\n  %get-tuple-element.19 = f32[1024]{0} get-tuple-element(%arg_tuple.1), index=3, metadata={op_name=\"jit(loop_ipgv)/jit(loop_ipgv)\"}\n  %get-tuple-element.18 = s32[] get-tuple-element(%arg_tuple.1), index=2, metadata={op_name=\"jit(loop_ipgv)/jit(loop_ipgv)\"}\n  %get-tuple-element.9 = f32[1024]{0} get-tuple-element(%arg_tuple.1), index=1\n  %get-tuple-element.8 = s32[] get-tuple-element(%arg_tuple.1), index=0\n  %wrapped_subtract = f32[1024]{0} fusion(%get-tuple-element.9, %get-tuple-element.19), kind=kLoop, calls=%wrapped_subtract_computation, metadata={op_name=\"jit(loop_ipgv)/jit(loop_ipgv)/while/body/sub\" stack_frame_id=6}\n  %wrapped_add = s32[] fusion(%get-tuple-element.8, %constant.3), kind=kLoop, calls=%wrapped_add_computation, metadata={op_name=\"jit(loop_ipgv)/jit(loop_ipgv)/while/body/add\" stack_frame_id=6}\n  ROOT %tuple.3 = (s32[], f32[1024]{0}, s32[], f32[1024]{0}) tuple(%wrapped_add, %wrapped_subtract, %get-tuple-element.18, %get-tuple-element.19)\n}\n\n%wrapped_compare_computation (param_0.3: s32[], param_1.2: s32[]) -> pred[] {\n  %param_0.3 = s32[] parameter(0)\n  %param_1.2 = s32[] parameter(1)\n  ROOT %lt.0 = pred[] compare(%param_0.3, %param_1.2), direction=LT, metadata={op_name=\"jit(loop_ipgv)/jit(loop_ipgv)/while/cond/lt\" stack_frame_id=6}\n}\n\n%region_1.2 (arg_tuple.3: (s32[], f32[1024], s32[], f32[1024])) -> pred[] {\n  %arg_tuple.3 = (s32[], f32[1024]{0}, s32[], f32[1024]{0}) parameter(0), metadata={op_name=\"jit(loop_ipgv)/jit(loop_ipgv)\"}\n  %get-tuple-element.14 = s32[] get-tuple-element(%arg_tuple.3), index=2, metadata={op_name=\"jit(loop_ipgv)/jit(loop_ipgv)\"}\n  %get-tuple-element.12 = s32[] get-tuple-element(%arg_tuple.3), index=0, metadata={op_name=\"jit(loop_ipgv)/jit(loop_ipgv)\"}\n  ROOT %wrapped_compare = pred[] fusion(%get-tuple-element.12, %get-tuple-element.14), kind=kLoop, calls=%wrapped_compare_computation, metadata={op_name=\"jit(loop_ipgv)/jit(loop_ipgv)/while/cond/lt\" stack_frame_id=6}\n}\n\n%wrapped_broadcast_computation (param_0: f32[]) -> f32[1024] {\n  %param_0 = f32[] parameter(0)\n  ROOT %broadcast_in_dim.2 = f32[1024]{0} broadcast(%param_0), dimensions={}, metadata={op_name=\"jit(loop_ipgv)/jit(loop_ipgv)/broadcast_in_dim\" stack_frame_id=5}\n}\n\nENTRY %main.4 (a.1: f32[1024], n.1: s32[]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  %n.1 = s32[] parameter(1), metadata={op_name=\"n\"}\n  %constant.0 = s32[] constant(0), metadata={op_name=\"jit(loop_ipgv)/jit(loop_ipgv)\"}\n  %constant.1 = f32[] constant(0), metadata={op_name=\"jit(loop_ipgv)/jit(loop_ipgv)\"}\n  %copy.6 = s32[] copy(%constant.0)\n  %wrapped_broadcast = f32[1024]{0} fusion(%constant.1), kind=kLoop, calls=%wrapped_broadcast_computation, metadata={op_name=\"jit(loop_ipgv)/jit(loop_ipgv)/broadcast_in_dim\" stack_frame_id=5}\n  %tuple = (s32[], f32[1024]{0}, s32[], f32[1024]{0}) tuple(%copy.6, %wrapped_broadcast, %n.1, %a.1)\n  %while.1 = (s32[], f32[1024]{0}, s32[], f32[1024]{0}) while(%tuple), condition=%region_1.2, body=%region_0.1, metadata={op_name=\"jit(loop_ipgv)/jit(loop_ipgv)/while\" stack_frame_id=6}\n  ROOT %while.2 = f32[1024]{0} get-tuple-element(%while.1), index=1, metadata={op_name=\"jit(loop_ipgv)/jit(loop_ipgv)/while\" stack_frame_id=6}\n}\n\n", "ptx": null}
{"id": 19, "kernel_name": "multi_movi", "python": "@jax.jit\ndef multi_movi(a, b):\n    temp1 = a - b\n    temp2 = jnp.sqrt(temp1)\n    return temp2 * a", "type": "generate_multi_statement_kernel", "hlo": "module @jit_multi_movi attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>, %arg1: tensor<1024xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @multi_movi(%arg0, %arg1) : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @multi_movi(%arg0: tensor<1024xf32>, %arg1: tensor<1024xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<1024xf32>\n    %1 = stablehlo.sqrt %0 : tensor<1024xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<1024xf32>\n    return %2 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_multi_movi, is_scheduled=true, entry_computation_layout={(f32[1024]{0}, f32[1024]{0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmptp0qt4ih.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"multi_movi\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=12 end_column=17}\n6 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=12 end_column=27}\n7 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=11 end_column=20}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[1024], param_1.2: f32[1024]) -> f32[1024] {\n  %param_0.1 = f32[1024]{0} parameter(0)\n  %param_1.2 = f32[1024]{0} parameter(1)\n  %sub.2 = f32[1024]{0} subtract(%param_0.1, %param_1.2), metadata={op_name=\"jit(multi_movi)/jit(multi_movi)/sub\" stack_frame_id=5}\n  %sqrt.2 = f32[1024]{0} sqrt(%sub.2), metadata={op_name=\"jit(multi_movi)/jit(multi_movi)/sqrt\" stack_frame_id=6}\n  ROOT %mul.2 = f32[1024]{0} multiply(%sqrt.2, %param_0.1), metadata={op_name=\"jit(multi_movi)/jit(multi_movi)/mul\" stack_frame_id=7}\n}\n\nENTRY %main.2 (a.1: f32[1024], b.1: f32[1024]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  %b.1 = f32[1024]{0} parameter(1), metadata={op_name=\"b\"}\n  ROOT %sqrt_multiply_fusion = f32[1024]{0} fusion(%a.1, %b.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(multi_movi)/jit(multi_movi)/mul\" stack_frame_id=7}\n}\n\n", "ptx": null}
{"id": 20, "kernel_name": "compound_yezg", "python": "@jax.jit\ndef compound_yezg(a, b, scale):\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "hlo": "module @jit_compound_yezg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>, %arg1: tensor<1024xf32>, %arg2: tensor<f32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @compound_yezg(%arg0, %arg1, %arg2) : (tensor<1024xf32>, tensor<1024xf32>, tensor<f32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @compound_yezg(%arg0: tensor<1024xf32>, %arg1: tensor<1024xf32>, %arg2: tensor<f32>) -> tensor<1024xf32> {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<1024xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<1024xf32>\n    %4 = stablehlo.floor %3 : tensor<1024xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<1024xf32>\n    %6 = stablehlo.abs %5 : tensor<1024xf32>\n    return %6 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_compound_yezg, is_scheduled=true, entry_computation_layout={(f32[1024]{0}, f32[1024]{0}, f32[])->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmp383me8_k.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"compound_yezg\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=14 end_column=19}\n6 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=13 end_column=28}\n7 {file_name_id=3 function_name_id=5 line=9 end_line=9 column=22 end_column=39}\n8 {file_name_id=3 function_name_id=5 line=9 end_line=9 column=13 end_column=39}\n9 {file_name_id=3 function_name_id=5 line=10 end_line=10 column=11 end_column=26}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=5}\n8 {file_location_id=8 parent_frame_id=5}\n9 {file_location_id=9 parent_frame_id=5}\n\n\n%fused_computation (param_0.3: f32[], param_1.3: f32[1024], param_2: f32[1024]) -> f32[1024] {\n  %param_1.3 = f32[1024]{0} parameter(1)\n  %param_2 = f32[1024]{0} parameter(2)\n  %add.2 = f32[1024]{0} add(%param_1.3, %param_2), metadata={op_name=\"jit(compound_yezg)/jit(compound_yezg)/add\" stack_frame_id=5}\n  %param_0.3 = f32[] parameter(0)\n  %mul.5 = f32[1024]{0} broadcast(%param_0.3), dimensions={}, metadata={op_name=\"jit(compound_yezg)/jit(compound_yezg)/mul\" stack_frame_id=6}\n  %mul.4 = f32[1024]{0} multiply(%add.2, %mul.5), metadata={op_name=\"jit(compound_yezg)/jit(compound_yezg)/mul\" stack_frame_id=6}\n  %floor.2 = f32[1024]{0} floor(%mul.4), metadata={op_name=\"jit(compound_yezg)/jit(compound_yezg)/floor\" stack_frame_id=7}\n  %sub.2 = f32[1024]{0} subtract(%mul.4, %floor.2), metadata={op_name=\"jit(compound_yezg)/jit(compound_yezg)/sub\" stack_frame_id=8}\n  ROOT %abs.2 = f32[1024]{0} abs(%sub.2), metadata={op_name=\"jit(compound_yezg)/jit(compound_yezg)/abs\" stack_frame_id=9}\n}\n\nENTRY %main.2 (a.1: f32[1024], b.1: f32[1024], scale.1: f32[]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  %b.1 = f32[1024]{0} parameter(1), metadata={op_name=\"b\"}\n  %scale.1 = f32[] parameter(2), metadata={op_name=\"scale\"}\n  ROOT %subtract_abs_fusion = f32[1024]{0} fusion(%scale.1, %a.1, %b.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(compound_yezg)/jit(compound_yezg)/abs\" stack_frame_id=9}\n}\n\n", "ptx": null}
{"id": 21, "kernel_name": "multi_lhxm", "python": "@jax.jit\ndef multi_lhxm(a, b):\n    temp1 = a + b\n    temp2 = jnp.sqrt(temp1)\n    return temp2 - a", "type": "generate_multi_statement_kernel", "hlo": "module @jit_multi_lhxm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>, %arg1: tensor<1024xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @multi_lhxm(%arg0, %arg1) : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @multi_lhxm(%arg0: tensor<1024xf32>, %arg1: tensor<1024xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<1024xf32>\n    %1 = stablehlo.sqrt %0 : tensor<1024xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<1024xf32>\n    return %2 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_multi_lhxm, is_scheduled=true, entry_computation_layout={(f32[1024]{0}, f32[1024]{0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmp4zfo2ht3.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"multi_lhxm\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=12 end_column=17}\n6 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=12 end_column=27}\n7 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=11 end_column=20}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[1024], param_1.2: f32[1024]) -> f32[1024] {\n  %param_0.1 = f32[1024]{0} parameter(0)\n  %param_1.2 = f32[1024]{0} parameter(1)\n  %add.2 = f32[1024]{0} add(%param_0.1, %param_1.2), metadata={op_name=\"jit(multi_lhxm)/jit(multi_lhxm)/add\" stack_frame_id=5}\n  %sqrt.2 = f32[1024]{0} sqrt(%add.2), metadata={op_name=\"jit(multi_lhxm)/jit(multi_lhxm)/sqrt\" stack_frame_id=6}\n  ROOT %sub.2 = f32[1024]{0} subtract(%sqrt.2, %param_0.1), metadata={op_name=\"jit(multi_lhxm)/jit(multi_lhxm)/sub\" stack_frame_id=7}\n}\n\nENTRY %main.2 (a.1: f32[1024], b.1: f32[1024]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  %b.1 = f32[1024]{0} parameter(1), metadata={op_name=\"b\"}\n  ROOT %sqrt_subtract_fusion = f32[1024]{0} fusion(%a.1, %b.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(multi_lhxm)/jit(multi_lhxm)/sub\" stack_frame_id=7}\n}\n\n", "ptx": null}
{"id": 22, "kernel_name": "multi_mqkx", "python": "@jax.jit\ndef multi_mqkx(a, b):\n    temp1 = a + b\n    temp2 = jnp.sin(temp1)\n    return temp2 + a", "type": "generate_multi_statement_kernel", "hlo": "module @jit_multi_mqkx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>, %arg1: tensor<1024xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @multi_mqkx(%arg0, %arg1) : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @multi_mqkx(%arg0: tensor<1024xf32>, %arg1: tensor<1024xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<1024xf32>\n    %1 = stablehlo.sine %0 : tensor<1024xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<1024xf32>\n    return %2 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_multi_mqkx, is_scheduled=true, entry_computation_layout={(f32[1024]{0}, f32[1024]{0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmpq7ku5q7u.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"multi_mqkx\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=12 end_column=17}\n6 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=12 end_column=26}\n7 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=11 end_column=20}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[1024], param_1.2: f32[1024]) -> f32[1024] {\n  %param_0.1 = f32[1024]{0} parameter(0)\n  %param_1.2 = f32[1024]{0} parameter(1)\n  %add.5 = f32[1024]{0} add(%param_0.1, %param_1.2), metadata={op_name=\"jit(multi_mqkx)/jit(multi_mqkx)/add\" stack_frame_id=5}\n  %sin.2 = f32[1024]{0} sine(%add.5), metadata={op_name=\"jit(multi_mqkx)/jit(multi_mqkx)/sin\" stack_frame_id=6}\n  ROOT %add.4 = f32[1024]{0} add(%sin.2, %param_0.1), metadata={op_name=\"jit(multi_mqkx)/jit(multi_mqkx)/add\" stack_frame_id=7}\n}\n\nENTRY %main.2 (a.1: f32[1024], b.1: f32[1024]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  %b.1 = f32[1024]{0} parameter(1), metadata={op_name=\"b\"}\n  ROOT %sine_add_fusion = f32[1024]{0} fusion(%a.1, %b.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(multi_mqkx)/jit(multi_mqkx)/add\" stack_frame_id=7}\n}\n\n", "ptx": null}
{"id": 23, "kernel_name": "vec_khvl", "python": "@jax.jit\ndef vec_khvl(a):\n    return jnp.linalg.norm(a, axis=-1)", "type": "generate_vector_kernel", "hlo": "module @jit_vec_khvl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024x3xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @vec_khvl(%arg0) : (tensor<1024x3xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @vec_khvl(%arg0: tensor<1024x3xf32>) -> tensor<1024xf32> {\n    %0 = call @norm(%arg0) : (tensor<1024x3xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @norm(%arg0: tensor<1024x3xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<1024x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<1024x3xf32>, tensor<f32>) -> tensor<1024xf32>\n    %2 = stablehlo.sqrt %1 : tensor<1024xf32>\n    return %2 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_vec_khvl, is_scheduled=true, entry_computation_layout={(f32[1024,3]{1,0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmp6o75inah.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"vec_khvl\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=11 end_column=38}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n\n\n%region_0.1 (reduce_sum.3: f32[], reduce_sum.4: f32[]) -> f32[] {\n  %reduce_sum.3 = f32[] parameter(0), metadata={op_name=\"reduce_sum\"}\n  %reduce_sum.4 = f32[] parameter(1), metadata={op_name=\"reduce_sum\"}\n  ROOT %reduce_sum.5 = f32[] add(%reduce_sum.3, %reduce_sum.4), metadata={op_name=\"reduce_sum\" stack_frame_id=5}\n}\n\n%fused_computation (param_0.2: f32[1024,3]) -> f32[1024] {\n  %param_0.2 = f32[1024,3]{1,0} parameter(0)\n  %mul.3 = f32[1024,3]{1,0} multiply(%param_0.2, %param_0.2), metadata={op_name=\"jit(vec_khvl)/jit(vec_khvl)/jit(norm)/mul\" stack_frame_id=5}\n  %constant.3 = f32[] constant(0), metadata={op_name=\"jit(vec_khvl)/jit(vec_khvl)/jit(norm)\"}\n  %reduce_sum.2 = f32[1024]{0} reduce(%mul.3, %constant.3), dimensions={1}, to_apply=%region_0.1, metadata={op_name=\"jit(vec_khvl)/jit(vec_khvl)/jit(norm)/reduce_sum\" stack_frame_id=5}\n  ROOT %sqrt.3 = f32[1024]{0} sqrt(%reduce_sum.2), metadata={op_name=\"jit(vec_khvl)/jit(vec_khvl)/jit(norm)/sqrt\" stack_frame_id=5}\n}\n\nENTRY %main.4 (a.1: f32[1024,3]) -> f32[1024] {\n  %a.1 = f32[1024,3]{1,0} parameter(0), metadata={op_name=\"a\"}\n  ROOT %reduce_sqrt_fusion = f32[1024]{0} fusion(%a.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(vec_khvl)/jit(vec_khvl)/jit(norm)/sqrt\" stack_frame_id=5}\n}\n\n", "ptx": null}
{"id": 24, "kernel_name": "scalar_arr_blgl", "python": "@jax.jit\ndef scalar_arr_blgl(alpha, x, y):\n    return alpha - x * y", "type": "generate_scalar_array_op", "hlo": "module @jit_scalar_arr_blgl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<1024xf32>, %arg2: tensor<1024xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @scalar_arr_blgl(%arg0, %arg1, %arg2) : (tensor<f32>, tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @scalar_arr_blgl(%arg0: tensor<f32>, %arg1: tensor<1024xf32>, %arg2: tensor<1024xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<1024xf32>\n    %1 = stablehlo.convert %arg0 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %3 = stablehlo.subtract %2, %0 : tensor<1024xf32>\n    return %3 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_scalar_arr_blgl, is_scheduled=true, entry_computation_layout={(f32[], f32[1024]{0}, f32[1024]{0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmpk3q7qk65.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"scalar_arr_blgl\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=19 end_column=24}\n6 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=11 end_column=24}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[1024], param_1.2: f32[1024], param_2.1: f32[]) -> f32[1024] {\n  %param_2.1 = f32[] parameter(2)\n  %sub.5 = f32[1024]{0} broadcast(%param_2.1), dimensions={}, metadata={op_name=\"jit(scalar_arr_blgl)/jit(scalar_arr_blgl)/sub\" stack_frame_id=6}\n  %param_0.1 = f32[1024]{0} parameter(0)\n  %param_1.2 = f32[1024]{0} parameter(1)\n  %mul.2 = f32[1024]{0} multiply(%param_0.1, %param_1.2), metadata={op_name=\"jit(scalar_arr_blgl)/jit(scalar_arr_blgl)/mul\" stack_frame_id=5}\n  ROOT %sub.4 = f32[1024]{0} subtract(%sub.5, %mul.2), metadata={op_name=\"jit(scalar_arr_blgl)/jit(scalar_arr_blgl)/sub\" stack_frame_id=6}\n}\n\nENTRY %main.2 (alpha.1: f32[], x.1: f32[1024], y.1: f32[1024]) -> f32[1024] {\n  %alpha.1 = f32[] parameter(0), metadata={op_name=\"alpha\"}\n  %x.1 = f32[1024]{0} parameter(1), metadata={op_name=\"x\"}\n  %y.1 = f32[1024]{0} parameter(2), metadata={op_name=\"y\"}\n  ROOT %multiply_subtract_fusion = f32[1024]{0} fusion(%x.1, %y.1, %alpha.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(scalar_arr_blgl)/jit(scalar_arr_blgl)/sub\" stack_frame_id=6}\n}\n\n", "ptx": null}
{"id": 25, "kernel_name": "scalar_arr_btzk", "python": "@jax.jit\ndef scalar_arr_btzk(alpha, x, y):\n    return alpha * x - y", "type": "generate_scalar_array_op", "hlo": "module @jit_scalar_arr_btzk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<1024xf32>, %arg2: tensor<1024xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @scalar_arr_btzk(%arg0, %arg1, %arg2) : (tensor<f32>, tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @scalar_arr_btzk(%arg0: tensor<f32>, %arg1: tensor<1024xf32>, %arg2: tensor<1024xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<1024xf32>\n    %3 = stablehlo.subtract %2, %arg2 : tensor<1024xf32>\n    return %3 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_scalar_arr_btzk, is_scheduled=true, entry_computation_layout={(f32[], f32[1024]{0}, f32[1024]{0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmp86yuewiy.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"scalar_arr_btzk\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=11 end_column=20}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[1024], param_1.2: f32[1024], param_2.1: f32[]) -> f32[1024] {\n  %param_2.1 = f32[] parameter(2)\n  %mul.5 = f32[1024]{0} broadcast(%param_2.1), dimensions={}, metadata={op_name=\"jit(scalar_arr_btzk)/jit(scalar_arr_btzk)/mul\" stack_frame_id=5}\n  %param_1.2 = f32[1024]{0} parameter(1)\n  %mul.4 = f32[1024]{0} multiply(%mul.5, %param_1.2), metadata={op_name=\"jit(scalar_arr_btzk)/jit(scalar_arr_btzk)/mul\" stack_frame_id=5}\n  %param_0.1 = f32[1024]{0} parameter(0)\n  ROOT %sub.2 = f32[1024]{0} subtract(%mul.4, %param_0.1), metadata={op_name=\"jit(scalar_arr_btzk)/jit(scalar_arr_btzk)/sub\" stack_frame_id=5}\n}\n\nENTRY %main.2 (alpha.1: f32[], x.1: f32[1024], y.1: f32[1024]) -> f32[1024] {\n  %alpha.1 = f32[] parameter(0), metadata={op_name=\"alpha\"}\n  %x.1 = f32[1024]{0} parameter(1), metadata={op_name=\"x\"}\n  %y.1 = f32[1024]{0} parameter(2), metadata={op_name=\"y\"}\n  ROOT %multiply_subtract_fusion = f32[1024]{0} fusion(%y.1, %x.1, %alpha.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(scalar_arr_btzk)/jit(scalar_arr_btzk)/sub\" stack_frame_id=5}\n}\n\n", "ptx": null}
{"id": 26, "kernel_name": "multi_tytx", "python": "@jax.jit\ndef multi_tytx(a, b):\n    temp1 = a + b\n    temp2 = jnp.abs(temp1)\n    return temp2 * a", "type": "generate_multi_statement_kernel", "hlo": "module @jit_multi_tytx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>, %arg1: tensor<1024xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @multi_tytx(%arg0, %arg1) : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @multi_tytx(%arg0: tensor<1024xf32>, %arg1: tensor<1024xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<1024xf32>\n    %1 = stablehlo.abs %0 : tensor<1024xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<1024xf32>\n    return %2 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_multi_tytx, is_scheduled=true, entry_computation_layout={(f32[1024]{0}, f32[1024]{0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmp53ioqs55.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"multi_tytx\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=12 end_column=17}\n6 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=12 end_column=26}\n7 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=11 end_column=20}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[1024], param_1.2: f32[1024]) -> f32[1024] {\n  %param_0.1 = f32[1024]{0} parameter(0)\n  %param_1.2 = f32[1024]{0} parameter(1)\n  %add.2 = f32[1024]{0} add(%param_0.1, %param_1.2), metadata={op_name=\"jit(multi_tytx)/jit(multi_tytx)/add\" stack_frame_id=5}\n  %abs.2 = f32[1024]{0} abs(%add.2), metadata={op_name=\"jit(multi_tytx)/jit(multi_tytx)/abs\" stack_frame_id=6}\n  ROOT %mul.2 = f32[1024]{0} multiply(%abs.2, %param_0.1), metadata={op_name=\"jit(multi_tytx)/jit(multi_tytx)/mul\" stack_frame_id=7}\n}\n\nENTRY %main.2 (a.1: f32[1024], b.1: f32[1024]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  %b.1 = f32[1024]{0} parameter(1), metadata={op_name=\"b\"}\n  ROOT %abs_multiply_fusion = f32[1024]{0} fusion(%a.1, %b.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(multi_tytx)/jit(multi_tytx)/mul\" stack_frame_id=7}\n}\n\n", "ptx": null}
{"id": 27, "kernel_name": "elementwise_rcep", "python": "@jax.jit\ndef elementwise_rcep(a, b):\n    return a - b", "type": "generate_simple_elementwise", "hlo": "module @jit_elementwise_rcep attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>, %arg1: tensor<1024xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @elementwise_rcep(%arg0, %arg1) : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @elementwise_rcep(%arg0: tensor<1024xf32>, %arg1: tensor<1024xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_elementwise_rcep, is_scheduled=true, entry_computation_layout={(f32[1024]{0}, f32[1024]{0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmpwngtqm_i.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"elementwise_rcep\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=11 end_column=16}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n\n\n%wrapped_subtract_computation (param_0: f32[1024], param_1: f32[1024]) -> f32[1024] {\n  %param_0 = f32[1024]{0} parameter(0)\n  %param_1 = f32[1024]{0} parameter(1)\n  ROOT %sub.2 = f32[1024]{0} subtract(%param_0, %param_1), metadata={op_name=\"jit(elementwise_rcep)/jit(elementwise_rcep)/sub\" stack_frame_id=5}\n}\n\nENTRY %main.2 (a.1: f32[1024], b.1: f32[1024]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  %b.1 = f32[1024]{0} parameter(1), metadata={op_name=\"b\"}\n  ROOT %wrapped_subtract = f32[1024]{0} fusion(%a.1, %b.1), kind=kLoop, calls=%wrapped_subtract_computation, metadata={op_name=\"jit(elementwise_rcep)/jit(elementwise_rcep)/sub\" stack_frame_id=5}\n}\n\n", "ptx": null}
{"id": 28, "kernel_name": "scalar_arr_xhld", "python": "@jax.jit\ndef scalar_arr_xhld(alpha, x, y):\n    return alpha + x + y", "type": "generate_scalar_array_op", "hlo": "module @jit_scalar_arr_xhld attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<1024xf32>, %arg2: tensor<1024xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @scalar_arr_xhld(%arg0, %arg1, %arg2) : (tensor<f32>, tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @scalar_arr_xhld(%arg0: tensor<f32>, %arg1: tensor<1024xf32>, %arg2: tensor<1024xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %2 = stablehlo.add %1, %arg1 : tensor<1024xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<1024xf32>\n    return %3 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_scalar_arr_xhld, is_scheduled=true, entry_computation_layout={(f32[], f32[1024]{0}, f32[1024]{0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmpfng4kqkx.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"scalar_arr_xhld\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=11 end_column=20}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[1024], param_1.2: f32[1024], param_2.1: f32[]) -> f32[1024] {\n  %param_2.1 = f32[] parameter(2)\n  %add.8 = f32[1024]{0} broadcast(%param_2.1), dimensions={}, metadata={op_name=\"jit(scalar_arr_xhld)/jit(scalar_arr_xhld)/add\" stack_frame_id=5}\n  %param_1.2 = f32[1024]{0} parameter(1)\n  %add.7 = f32[1024]{0} add(%add.8, %param_1.2), metadata={op_name=\"jit(scalar_arr_xhld)/jit(scalar_arr_xhld)/add\" stack_frame_id=5}\n  %param_0.1 = f32[1024]{0} parameter(0)\n  ROOT %add.6 = f32[1024]{0} add(%add.7, %param_0.1), metadata={op_name=\"jit(scalar_arr_xhld)/jit(scalar_arr_xhld)/add\" stack_frame_id=5}\n}\n\nENTRY %main.2 (alpha.1: f32[], x.1: f32[1024], y.1: f32[1024]) -> f32[1024] {\n  %alpha.1 = f32[] parameter(0), metadata={op_name=\"alpha\"}\n  %x.1 = f32[1024]{0} parameter(1), metadata={op_name=\"x\"}\n  %y.1 = f32[1024]{0} parameter(2), metadata={op_name=\"y\"}\n  ROOT %add_add_fusion = f32[1024]{0} fusion(%y.1, %x.1, %alpha.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(scalar_arr_xhld)/jit(scalar_arr_xhld)/add\" stack_frame_id=5}\n}\n\n", "ptx": null}
{"id": 29, "kernel_name": "reduce_iqaz", "python": "@jax.jit\ndef reduce_iqaz(a):\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "hlo": "module @jit_reduce_iqaz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %0 = call @reduce_iqaz(%arg0) : (tensor<1024xf32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n  func.func private @reduce_iqaz(%arg0: tensor<1024xf32>) -> tensor<f32> {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<1024xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "llvm": "HloModule jit_reduce_iqaz, is_scheduled=true, entry_computation_layout={(f32[1024]{0})->f32[]}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmpqmyvbu7j.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"reduce_iqaz\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=11 end_column=21}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n\n\n%region_0.1 (reduce_sum.3: f32[], reduce_sum.4: f32[]) -> f32[] {\n  %reduce_sum.3 = f32[] parameter(0), metadata={op_name=\"reduce_sum\"}\n  %reduce_sum.4 = f32[] parameter(1), metadata={op_name=\"reduce_sum\"}\n  ROOT %reduce_sum.5 = f32[] add(%reduce_sum.3, %reduce_sum.4), metadata={op_name=\"reduce_sum\" stack_frame_id=5}\n}\n\n%wrapped_reduce-window_computation (param_0: f32[1024], param_1: f32[]) -> f32[32] {\n  %param_0 = f32[1024]{0} parameter(0)\n  %param_1 = f32[] parameter(1)\n  ROOT %reduce-window.1 = f32[32]{0} reduce-window(%param_0, %param_1), window={size=32 stride=32}, to_apply=%region_0.1\n}\n\n%region_0.1.clone (reduce_sum.1: f32[], reduce_sum.2: f32[]) -> f32[] {\n  %reduce_sum.1 = f32[] parameter(0), metadata={op_name=\"reduce_sum\"}\n  %reduce_sum.2 = f32[] parameter(1), metadata={op_name=\"reduce_sum\"}\n  ROOT %reduce_sum.6 = f32[] add(%reduce_sum.1, %reduce_sum.2), metadata={op_name=\"reduce_sum\" stack_frame_id=5}\n}\n\n%wrapped_reduce_computation (param_0.1: f32[32], param_1.1: f32[]) -> f32[] {\n  %param_0.1 = f32[32]{0} parameter(0)\n  %param_1.1 = f32[] parameter(1)\n  ROOT %reduce_sum.8 = f32[] reduce(%param_0.1, %param_1.1), dimensions={0}, to_apply=%region_0.1.clone, metadata={op_name=\"jit(reduce_iqaz)/jit(reduce_iqaz)/reduce_sum\" stack_frame_id=5}\n}\n\nENTRY %main.3 (a.1: f32[1024]) -> f32[] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  %constant.0 = f32[] constant(0), metadata={op_name=\"jit(reduce_iqaz)/jit(reduce_iqaz)\"}\n  %wrapped_reduce-window = f32[32]{0} fusion(%a.1, %constant.0), kind=kLoop, calls=%wrapped_reduce-window_computation\n  ROOT %wrapped_reduce = f32[] fusion(%wrapped_reduce-window, %constant.0), kind=kLoop, calls=%wrapped_reduce_computation, metadata={op_name=\"jit(reduce_iqaz)/jit(reduce_iqaz)/reduce_sum\" stack_frame_id=5}\n}\n\n", "ptx": null}
{"id": 30, "kernel_name": "scalar_arr_bpzi", "python": "@jax.jit\ndef scalar_arr_bpzi(alpha, x, y):\n    return alpha * x * y", "type": "generate_scalar_array_op", "hlo": "module @jit_scalar_arr_bpzi attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<1024xf32>, %arg2: tensor<1024xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @scalar_arr_bpzi(%arg0, %arg1, %arg2) : (tensor<f32>, tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @scalar_arr_bpzi(%arg0: tensor<f32>, %arg1: tensor<1024xf32>, %arg2: tensor<1024xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<1024xf32>\n    %3 = stablehlo.multiply %2, %arg2 : tensor<1024xf32>\n    return %3 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_scalar_arr_bpzi, is_scheduled=true, entry_computation_layout={(f32[], f32[1024]{0}, f32[1024]{0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmpd02z9ud5.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"scalar_arr_bpzi\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=11 end_column=20}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[1024], param_1.2: f32[1024], param_2.1: f32[]) -> f32[1024] {\n  %param_2.1 = f32[] parameter(2)\n  %mul.8 = f32[1024]{0} broadcast(%param_2.1), dimensions={}, metadata={op_name=\"jit(scalar_arr_bpzi)/jit(scalar_arr_bpzi)/mul\" stack_frame_id=5}\n  %param_1.2 = f32[1024]{0} parameter(1)\n  %mul.7 = f32[1024]{0} multiply(%mul.8, %param_1.2), metadata={op_name=\"jit(scalar_arr_bpzi)/jit(scalar_arr_bpzi)/mul\" stack_frame_id=5}\n  %param_0.1 = f32[1024]{0} parameter(0)\n  ROOT %mul.6 = f32[1024]{0} multiply(%mul.7, %param_0.1), metadata={op_name=\"jit(scalar_arr_bpzi)/jit(scalar_arr_bpzi)/mul\" stack_frame_id=5}\n}\n\nENTRY %main.2 (alpha.1: f32[], x.1: f32[1024], y.1: f32[1024]) -> f32[1024] {\n  %alpha.1 = f32[] parameter(0), metadata={op_name=\"alpha\"}\n  %x.1 = f32[1024]{0} parameter(1), metadata={op_name=\"x\"}\n  %y.1 = f32[1024]{0} parameter(2), metadata={op_name=\"y\"}\n  ROOT %multiply_multiply_fusion = f32[1024]{0} fusion(%y.1, %x.1, %alpha.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(scalar_arr_bpzi)/jit(scalar_arr_bpzi)/mul\" stack_frame_id=5}\n}\n\n", "ptx": null}
{"id": 31, "kernel_name": "loop_hnol", "python": "@jax.jit\ndef loop_hnol(a, n):\n    def body_fun(i, val):\n        return val * a\n    # Initial value matches a's shape/type but zeroed\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "hlo": "module @jit_loop_hnol attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>, %arg1: tensor<i32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @loop_hnol(%arg0, %arg1) : (tensor<1024xf32>, tensor<i32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @loop_hnol(%arg0: tensor<1024xf32>, %arg1: tensor<i32>) -> tensor<1024xf32> {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<1024xf32>, tensor<i32>, tensor<i32>, tensor<1024xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.multiply %iterArg_2, %iterArg : tensor<1024xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<1024xf32>, tensor<i32>, tensor<i32>, tensor<1024xf32>\n    }\n    return %1#3 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_loop_hnol, is_scheduled=true, entry_computation_layout={(f32[1024]{0}, s32[])->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmpwo3ru5u4.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"loop_hnol\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=9 end_line=9 column=15 end_column=32}\n6 {file_name_id=3 function_name_id=5 line=10 end_line=10 column=11 end_column=54}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n\n\n%wrapped_add_computation (param_0.1: s32[], param_1: s32[]) -> s32[] {\n  %param_0.1 = s32[] parameter(0)\n  %param_1 = s32[] parameter(1)\n  ROOT %add.0 = s32[] add(%param_0.1, %param_1), metadata={op_name=\"jit(loop_hnol)/jit(loop_hnol)/while/body/add\" stack_frame_id=6}\n}\n\n%wrapped_multiply_computation (param_0.2: f32[1024], param_1.1: f32[1024]) -> f32[1024] {\n  %param_0.2 = f32[1024]{0} parameter(0)\n  %param_1.1 = f32[1024]{0} parameter(1)\n  ROOT %mul.0 = f32[1024]{0} multiply(%param_0.2, %param_1.1), metadata={op_name=\"jit(loop_hnol)/jit(loop_hnol)/while/body/mul\" stack_frame_id=6}\n}\n\n%region_0.1 (arg_tuple.1: (s32[], f32[1024], s32[], f32[1024])) -> (s32[], f32[1024], s32[], f32[1024]) {\n  %arg_tuple.1 = (s32[], f32[1024]{0}, s32[], f32[1024]{0}) parameter(0), metadata={op_name=\"jit(loop_hnol)/jit(loop_hnol)\"}\n  %constant.3 = s32[] constant(1), metadata={op_name=\"jit(loop_hnol)/jit(loop_hnol)\"}\n  %get-tuple-element.19 = f32[1024]{0} get-tuple-element(%arg_tuple.1), index=3, metadata={op_name=\"jit(loop_hnol)/jit(loop_hnol)\"}\n  %get-tuple-element.18 = s32[] get-tuple-element(%arg_tuple.1), index=2, metadata={op_name=\"jit(loop_hnol)/jit(loop_hnol)\"}\n  %get-tuple-element.9 = f32[1024]{0} get-tuple-element(%arg_tuple.1), index=1\n  %get-tuple-element.8 = s32[] get-tuple-element(%arg_tuple.1), index=0\n  %wrapped_multiply = f32[1024]{0} fusion(%get-tuple-element.9, %get-tuple-element.19), kind=kLoop, calls=%wrapped_multiply_computation, metadata={op_name=\"jit(loop_hnol)/jit(loop_hnol)/while/body/mul\" stack_frame_id=6}\n  %wrapped_add = s32[] fusion(%get-tuple-element.8, %constant.3), kind=kLoop, calls=%wrapped_add_computation, metadata={op_name=\"jit(loop_hnol)/jit(loop_hnol)/while/body/add\" stack_frame_id=6}\n  ROOT %tuple.3 = (s32[], f32[1024]{0}, s32[], f32[1024]{0}) tuple(%wrapped_add, %wrapped_multiply, %get-tuple-element.18, %get-tuple-element.19)\n}\n\n%wrapped_compare_computation (param_0.3: s32[], param_1.2: s32[]) -> pred[] {\n  %param_0.3 = s32[] parameter(0)\n  %param_1.2 = s32[] parameter(1)\n  ROOT %lt.0 = pred[] compare(%param_0.3, %param_1.2), direction=LT, metadata={op_name=\"jit(loop_hnol)/jit(loop_hnol)/while/cond/lt\" stack_frame_id=6}\n}\n\n%region_1.2 (arg_tuple.3: (s32[], f32[1024], s32[], f32[1024])) -> pred[] {\n  %arg_tuple.3 = (s32[], f32[1024]{0}, s32[], f32[1024]{0}) parameter(0), metadata={op_name=\"jit(loop_hnol)/jit(loop_hnol)\"}\n  %get-tuple-element.14 = s32[] get-tuple-element(%arg_tuple.3), index=2, metadata={op_name=\"jit(loop_hnol)/jit(loop_hnol)\"}\n  %get-tuple-element.12 = s32[] get-tuple-element(%arg_tuple.3), index=0, metadata={op_name=\"jit(loop_hnol)/jit(loop_hnol)\"}\n  ROOT %wrapped_compare = pred[] fusion(%get-tuple-element.12, %get-tuple-element.14), kind=kLoop, calls=%wrapped_compare_computation, metadata={op_name=\"jit(loop_hnol)/jit(loop_hnol)/while/cond/lt\" stack_frame_id=6}\n}\n\n%wrapped_broadcast_computation (param_0: f32[]) -> f32[1024] {\n  %param_0 = f32[] parameter(0)\n  ROOT %broadcast_in_dim.2 = f32[1024]{0} broadcast(%param_0), dimensions={}, metadata={op_name=\"jit(loop_hnol)/jit(loop_hnol)/broadcast_in_dim\" stack_frame_id=5}\n}\n\nENTRY %main.4 (a.1: f32[1024], n.1: s32[]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  %n.1 = s32[] parameter(1), metadata={op_name=\"n\"}\n  %constant.0 = s32[] constant(0), metadata={op_name=\"jit(loop_hnol)/jit(loop_hnol)\"}\n  %constant.1 = f32[] constant(0), metadata={op_name=\"jit(loop_hnol)/jit(loop_hnol)\"}\n  %copy.6 = s32[] copy(%constant.0)\n  %wrapped_broadcast = f32[1024]{0} fusion(%constant.1), kind=kLoop, calls=%wrapped_broadcast_computation, metadata={op_name=\"jit(loop_hnol)/jit(loop_hnol)/broadcast_in_dim\" stack_frame_id=5}\n  %tuple = (s32[], f32[1024]{0}, s32[], f32[1024]{0}) tuple(%copy.6, %wrapped_broadcast, %n.1, %a.1)\n  %while.1 = (s32[], f32[1024]{0}, s32[], f32[1024]{0}) while(%tuple), condition=%region_1.2, body=%region_0.1, metadata={op_name=\"jit(loop_hnol)/jit(loop_hnol)/while\" stack_frame_id=6}\n  ROOT %while.2 = f32[1024]{0} get-tuple-element(%while.1), index=1, metadata={op_name=\"jit(loop_hnol)/jit(loop_hnol)/while\" stack_frame_id=6}\n}\n\n", "ptx": null}
{"id": 32, "kernel_name": "compound_wncd", "python": "@jax.jit\ndef compound_wncd(a, b, scale):\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "hlo": "module @jit_compound_wncd attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>, %arg1: tensor<1024xf32>, %arg2: tensor<f32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @compound_wncd(%arg0, %arg1, %arg2) : (tensor<1024xf32>, tensor<1024xf32>, tensor<f32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @compound_wncd(%arg0: tensor<1024xf32>, %arg1: tensor<1024xf32>, %arg2: tensor<f32>) -> tensor<1024xf32> {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<1024xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<1024xf32>\n    %4 = stablehlo.floor %3 : tensor<1024xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<1024xf32>\n    %6 = stablehlo.abs %5 : tensor<1024xf32>\n    return %6 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_compound_wncd, is_scheduled=true, entry_computation_layout={(f32[1024]{0}, f32[1024]{0}, f32[])->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmpts7m4oy7.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"compound_wncd\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=14 end_column=19}\n6 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=13 end_column=28}\n7 {file_name_id=3 function_name_id=5 line=9 end_line=9 column=22 end_column=39}\n8 {file_name_id=3 function_name_id=5 line=9 end_line=9 column=13 end_column=39}\n9 {file_name_id=3 function_name_id=5 line=10 end_line=10 column=11 end_column=26}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=5}\n8 {file_location_id=8 parent_frame_id=5}\n9 {file_location_id=9 parent_frame_id=5}\n\n\n%fused_computation (param_0.3: f32[], param_1.3: f32[1024], param_2: f32[1024]) -> f32[1024] {\n  %param_1.3 = f32[1024]{0} parameter(1)\n  %param_2 = f32[1024]{0} parameter(2)\n  %add.2 = f32[1024]{0} add(%param_1.3, %param_2), metadata={op_name=\"jit(compound_wncd)/jit(compound_wncd)/add\" stack_frame_id=5}\n  %param_0.3 = f32[] parameter(0)\n  %mul.5 = f32[1024]{0} broadcast(%param_0.3), dimensions={}, metadata={op_name=\"jit(compound_wncd)/jit(compound_wncd)/mul\" stack_frame_id=6}\n  %mul.4 = f32[1024]{0} multiply(%add.2, %mul.5), metadata={op_name=\"jit(compound_wncd)/jit(compound_wncd)/mul\" stack_frame_id=6}\n  %floor.2 = f32[1024]{0} floor(%mul.4), metadata={op_name=\"jit(compound_wncd)/jit(compound_wncd)/floor\" stack_frame_id=7}\n  %sub.2 = f32[1024]{0} subtract(%mul.4, %floor.2), metadata={op_name=\"jit(compound_wncd)/jit(compound_wncd)/sub\" stack_frame_id=8}\n  ROOT %abs.2 = f32[1024]{0} abs(%sub.2), metadata={op_name=\"jit(compound_wncd)/jit(compound_wncd)/abs\" stack_frame_id=9}\n}\n\nENTRY %main.2 (a.1: f32[1024], b.1: f32[1024], scale.1: f32[]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  %b.1 = f32[1024]{0} parameter(1), metadata={op_name=\"b\"}\n  %scale.1 = f32[] parameter(2), metadata={op_name=\"scale\"}\n  ROOT %subtract_abs_fusion = f32[1024]{0} fusion(%scale.1, %a.1, %b.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(compound_wncd)/jit(compound_wncd)/abs\" stack_frame_id=9}\n}\n\n", "ptx": null}
{"id": 33, "kernel_name": "multi_llbt", "python": "@jax.jit\ndef multi_llbt(a, b):\n    temp1 = a * b\n    temp2 = jnp.cos(temp1)\n    return temp2 * a", "type": "generate_multi_statement_kernel", "hlo": "module @jit_multi_llbt attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>, %arg1: tensor<1024xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @multi_llbt(%arg0, %arg1) : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @multi_llbt(%arg0: tensor<1024xf32>, %arg1: tensor<1024xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<1024xf32>\n    %1 = stablehlo.cosine %0 : tensor<1024xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<1024xf32>\n    return %2 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_multi_llbt, is_scheduled=true, entry_computation_layout={(f32[1024]{0}, f32[1024]{0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmpsiekroej.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"multi_llbt\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=12 end_column=17}\n6 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=12 end_column=26}\n7 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=11 end_column=20}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[1024], param_1.2: f32[1024]) -> f32[1024] {\n  %param_0.1 = f32[1024]{0} parameter(0)\n  %param_1.2 = f32[1024]{0} parameter(1)\n  %mul.5 = f32[1024]{0} multiply(%param_0.1, %param_1.2), metadata={op_name=\"jit(multi_llbt)/jit(multi_llbt)/mul\" stack_frame_id=5}\n  %cos.2 = f32[1024]{0} cosine(%mul.5), metadata={op_name=\"jit(multi_llbt)/jit(multi_llbt)/cos\" stack_frame_id=6}\n  ROOT %mul.4 = f32[1024]{0} multiply(%cos.2, %param_0.1), metadata={op_name=\"jit(multi_llbt)/jit(multi_llbt)/mul\" stack_frame_id=7}\n}\n\nENTRY %main.2 (a.1: f32[1024], b.1: f32[1024]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  %b.1 = f32[1024]{0} parameter(1), metadata={op_name=\"b\"}\n  ROOT %cosine_multiply_fusion = f32[1024]{0} fusion(%a.1, %b.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(multi_llbt)/jit(multi_llbt)/mul\" stack_frame_id=7}\n}\n\n", "ptx": null}
{"id": 34, "kernel_name": "reduce_jyfh", "python": "@jax.jit\ndef reduce_jyfh(a):\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "hlo": "module @jit_reduce_jyfh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %0 = call @reduce_jyfh(%arg0) : (tensor<1024xf32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n  func.func private @reduce_jyfh(%arg0: tensor<1024xf32>) -> tensor<f32> {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<1024xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "llvm": "HloModule jit_reduce_jyfh, is_scheduled=true, entry_computation_layout={(f32[1024]{0})->f32[]}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmpxl42cpqe.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"reduce_jyfh\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=11 end_column=21}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n\n\n%region_0.1 (reduce_sum.3: f32[], reduce_sum.4: f32[]) -> f32[] {\n  %reduce_sum.3 = f32[] parameter(0), metadata={op_name=\"reduce_sum\"}\n  %reduce_sum.4 = f32[] parameter(1), metadata={op_name=\"reduce_sum\"}\n  ROOT %reduce_sum.5 = f32[] add(%reduce_sum.3, %reduce_sum.4), metadata={op_name=\"reduce_sum\" stack_frame_id=5}\n}\n\n%wrapped_reduce-window_computation (param_0: f32[1024], param_1: f32[]) -> f32[32] {\n  %param_0 = f32[1024]{0} parameter(0)\n  %param_1 = f32[] parameter(1)\n  ROOT %reduce-window.1 = f32[32]{0} reduce-window(%param_0, %param_1), window={size=32 stride=32}, to_apply=%region_0.1\n}\n\n%region_0.1.clone (reduce_sum.1: f32[], reduce_sum.2: f32[]) -> f32[] {\n  %reduce_sum.1 = f32[] parameter(0), metadata={op_name=\"reduce_sum\"}\n  %reduce_sum.2 = f32[] parameter(1), metadata={op_name=\"reduce_sum\"}\n  ROOT %reduce_sum.6 = f32[] add(%reduce_sum.1, %reduce_sum.2), metadata={op_name=\"reduce_sum\" stack_frame_id=5}\n}\n\n%wrapped_reduce_computation (param_0.1: f32[32], param_1.1: f32[]) -> f32[] {\n  %param_0.1 = f32[32]{0} parameter(0)\n  %param_1.1 = f32[] parameter(1)\n  ROOT %reduce_sum.8 = f32[] reduce(%param_0.1, %param_1.1), dimensions={0}, to_apply=%region_0.1.clone, metadata={op_name=\"jit(reduce_jyfh)/jit(reduce_jyfh)/reduce_sum\" stack_frame_id=5}\n}\n\nENTRY %main.3 (a.1: f32[1024]) -> f32[] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  %constant.0 = f32[] constant(0), metadata={op_name=\"jit(reduce_jyfh)/jit(reduce_jyfh)\"}\n  %wrapped_reduce-window = f32[32]{0} fusion(%a.1, %constant.0), kind=kLoop, calls=%wrapped_reduce-window_computation\n  ROOT %wrapped_reduce = f32[] fusion(%wrapped_reduce-window, %constant.0), kind=kLoop, calls=%wrapped_reduce_computation, metadata={op_name=\"jit(reduce_jyfh)/jit(reduce_jyfh)/reduce_sum\" stack_frame_id=5}\n}\n\n", "ptx": null}
{"id": 35, "kernel_name": "loop_uigv", "python": "@jax.jit\ndef loop_uigv(a, n):\n    def body_fun(i, val):\n        return val + a\n    # Initial value matches a's shape/type but zeroed\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "hlo": "module @jit_loop_uigv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>, %arg1: tensor<i32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @loop_uigv(%arg0, %arg1) : (tensor<1024xf32>, tensor<i32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @loop_uigv(%arg0: tensor<1024xf32>, %arg1: tensor<i32>) -> tensor<1024xf32> {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<1024xf32>, tensor<i32>, tensor<i32>, tensor<1024xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.add %iterArg_2, %iterArg : tensor<1024xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<1024xf32>, tensor<i32>, tensor<i32>, tensor<1024xf32>\n    }\n    return %1#3 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_loop_uigv, is_scheduled=true, entry_computation_layout={(f32[1024]{0}, s32[])->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmp07zlfbkh.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"loop_uigv\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=9 end_line=9 column=15 end_column=32}\n6 {file_name_id=3 function_name_id=5 line=10 end_line=10 column=11 end_column=54}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n\n\n%wrapped_add_computation (param_0.1: s32[], param_1: s32[]) -> s32[] {\n  %param_0.1 = s32[] parameter(0)\n  %param_1 = s32[] parameter(1)\n  ROOT %add.0 = s32[] add(%param_0.1, %param_1), metadata={op_name=\"jit(loop_uigv)/jit(loop_uigv)/while/body/add\" stack_frame_id=6}\n}\n\n%wrapped_add_computation.1 (param_0.2: f32[1024], param_1.1: f32[1024]) -> f32[1024] {\n  %param_0.2 = f32[1024]{0} parameter(0)\n  %param_1.1 = f32[1024]{0} parameter(1)\n  ROOT %add.1 = f32[1024]{0} add(%param_0.2, %param_1.1), metadata={op_name=\"jit(loop_uigv)/jit(loop_uigv)/while/body/add\" stack_frame_id=6}\n}\n\n%region_0.1 (arg_tuple.1: (s32[], f32[1024], s32[], f32[1024])) -> (s32[], f32[1024], s32[], f32[1024]) {\n  %arg_tuple.1 = (s32[], f32[1024]{0}, s32[], f32[1024]{0}) parameter(0), metadata={op_name=\"jit(loop_uigv)/jit(loop_uigv)\"}\n  %constant.3 = s32[] constant(1), metadata={op_name=\"jit(loop_uigv)/jit(loop_uigv)\"}\n  %get-tuple-element.19 = f32[1024]{0} get-tuple-element(%arg_tuple.1), index=3, metadata={op_name=\"jit(loop_uigv)/jit(loop_uigv)\"}\n  %get-tuple-element.18 = s32[] get-tuple-element(%arg_tuple.1), index=2, metadata={op_name=\"jit(loop_uigv)/jit(loop_uigv)\"}\n  %get-tuple-element.9 = f32[1024]{0} get-tuple-element(%arg_tuple.1), index=1\n  %get-tuple-element.8 = s32[] get-tuple-element(%arg_tuple.1), index=0\n  %wrapped_add.1 = f32[1024]{0} fusion(%get-tuple-element.9, %get-tuple-element.19), kind=kLoop, calls=%wrapped_add_computation.1, metadata={op_name=\"jit(loop_uigv)/jit(loop_uigv)/while/body/add\" stack_frame_id=6}\n  %wrapped_add = s32[] fusion(%get-tuple-element.8, %constant.3), kind=kLoop, calls=%wrapped_add_computation, metadata={op_name=\"jit(loop_uigv)/jit(loop_uigv)/while/body/add\" stack_frame_id=6}\n  ROOT %tuple.3 = (s32[], f32[1024]{0}, s32[], f32[1024]{0}) tuple(%wrapped_add, %wrapped_add.1, %get-tuple-element.18, %get-tuple-element.19)\n}\n\n%wrapped_compare_computation (param_0.3: s32[], param_1.2: s32[]) -> pred[] {\n  %param_0.3 = s32[] parameter(0)\n  %param_1.2 = s32[] parameter(1)\n  ROOT %lt.0 = pred[] compare(%param_0.3, %param_1.2), direction=LT, metadata={op_name=\"jit(loop_uigv)/jit(loop_uigv)/while/cond/lt\" stack_frame_id=6}\n}\n\n%region_1.2 (arg_tuple.3: (s32[], f32[1024], s32[], f32[1024])) -> pred[] {\n  %arg_tuple.3 = (s32[], f32[1024]{0}, s32[], f32[1024]{0}) parameter(0), metadata={op_name=\"jit(loop_uigv)/jit(loop_uigv)\"}\n  %get-tuple-element.14 = s32[] get-tuple-element(%arg_tuple.3), index=2, metadata={op_name=\"jit(loop_uigv)/jit(loop_uigv)\"}\n  %get-tuple-element.12 = s32[] get-tuple-element(%arg_tuple.3), index=0, metadata={op_name=\"jit(loop_uigv)/jit(loop_uigv)\"}\n  ROOT %wrapped_compare = pred[] fusion(%get-tuple-element.12, %get-tuple-element.14), kind=kLoop, calls=%wrapped_compare_computation, metadata={op_name=\"jit(loop_uigv)/jit(loop_uigv)/while/cond/lt\" stack_frame_id=6}\n}\n\n%wrapped_broadcast_computation (param_0: f32[]) -> f32[1024] {\n  %param_0 = f32[] parameter(0)\n  ROOT %broadcast_in_dim.2 = f32[1024]{0} broadcast(%param_0), dimensions={}, metadata={op_name=\"jit(loop_uigv)/jit(loop_uigv)/broadcast_in_dim\" stack_frame_id=5}\n}\n\nENTRY %main.4 (a.1: f32[1024], n.1: s32[]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  %n.1 = s32[] parameter(1), metadata={op_name=\"n\"}\n  %constant.0 = s32[] constant(0), metadata={op_name=\"jit(loop_uigv)/jit(loop_uigv)\"}\n  %constant.1 = f32[] constant(0), metadata={op_name=\"jit(loop_uigv)/jit(loop_uigv)\"}\n  %copy.6 = s32[] copy(%constant.0)\n  %wrapped_broadcast = f32[1024]{0} fusion(%constant.1), kind=kLoop, calls=%wrapped_broadcast_computation, metadata={op_name=\"jit(loop_uigv)/jit(loop_uigv)/broadcast_in_dim\" stack_frame_id=5}\n  %tuple = (s32[], f32[1024]{0}, s32[], f32[1024]{0}) tuple(%copy.6, %wrapped_broadcast, %n.1, %a.1)\n  %while.1 = (s32[], f32[1024]{0}, s32[], f32[1024]{0}) while(%tuple), condition=%region_1.2, body=%region_0.1, metadata={op_name=\"jit(loop_uigv)/jit(loop_uigv)/while\" stack_frame_id=6}\n  ROOT %while.2 = f32[1024]{0} get-tuple-element(%while.1), index=1, metadata={op_name=\"jit(loop_uigv)/jit(loop_uigv)/while\" stack_frame_id=6}\n}\n\n", "ptx": null}
{"id": 36, "kernel_name": "branch_vcys", "python": "@jax.jit\ndef branch_vcys(a):\n    return jnp.where(a > -0.49, a - 2.1, a + 2.7)", "type": "generate_branch_kernel", "hlo": "module @jit_branch_vcys attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @branch_vcys(%arg0) : (tensor<1024xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @branch_vcys(%arg0: tensor<1024xf32>) -> tensor<1024xf32> {\n    %cst = stablehlo.constant dense<-4.900000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xi1>\n    %cst_0 = stablehlo.constant dense<2.100000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<1024xf32>\n    %cst_1 = stablehlo.constant dense<2.700000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<1024xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<1024xi1>, tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>\n    return %6 : tensor<1024xf32>\n  }\n  func.func private @_where(%arg0: tensor<1024xi1>, %arg1: tensor<1024xf32>, %arg2: tensor<1024xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<1024xi1>, tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_branch_vcys, is_scheduled=true, entry_computation_layout={(f32[1024]{0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmp59inm45_.py\"\n4 \"/tmp/tmp7nvd0zdg.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"branch_vcys\"\n6 \"nested_odsh\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=41 end_column=48}\n6 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=32 end_column=39}\n7 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=21 end_column=30}\n8 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=11 end_column=49}\n9 {file_name_id=4 function_name_id=6 line=9 end_line=9 column=17 end_column=61}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=5}\n8 {file_location_id=8 parent_frame_id=5}\n9 {file_location_id=9 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[1024]) -> f32[1024] {\n  %param_0.1 = f32[1024]{0} parameter(0)\n  %constant.9 = f32[] constant(-0.49), metadata={op_name=\"jit(branch_vcys)/jit(branch_vcys)\"}\n  %gt.5 = f32[1024]{0} broadcast(%constant.9), dimensions={}, metadata={op_name=\"jit(branch_vcys)/jit(branch_vcys)/gt\" stack_frame_id=7}\n  %gt.4 = pred[1024]{0} compare(%param_0.1, %gt.5), direction=GT, metadata={op_name=\"jit(branch_vcys)/jit(branch_vcys)/gt\" stack_frame_id=7}\n  %constant.8 = f32[] constant(-2.1), metadata={op_name=\"jit(branch_vcys)/jit(branch_vcys)\"}\n  %broadcast.1 = f32[1024]{0} broadcast(%constant.8), dimensions={}, metadata={op_name=\"jit(branch_vcys)/jit(branch_vcys)\"}\n  %add.7 = f32[1024]{0} add(%param_0.1, %broadcast.1), metadata={op_name=\"jit(branch_vcys)/jit(branch_vcys)/sub\" stack_frame_id=6}\n  %constant.7 = f32[] constant(2.7), metadata={op_name=\"jit(branch_vcys)/jit(branch_vcys)\"}\n  %add.6 = f32[1024]{0} broadcast(%constant.7), dimensions={}, metadata={op_name=\"jit(branch_vcys)/jit(branch_vcys)/add\" stack_frame_id=5}\n  %add.5 = f32[1024]{0} add(%param_0.1, %add.6), metadata={op_name=\"jit(branch_vcys)/jit(branch_vcys)/add\" stack_frame_id=5}\n  ROOT %select_n.3 = f32[1024]{0} select(%gt.4, %add.7, %add.5), metadata={op_name=\"jit(branch_vcys)/jit(branch_vcys)/jit(_where)/select_n\" stack_frame_id=9}\n}\n\nENTRY %main.3 (a.1: f32[1024]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  ROOT %add_select_fusion = f32[1024]{0} fusion(%a.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(branch_vcys)/jit(branch_vcys)/jit(_where)/select_n\" stack_frame_id=9}\n}\n\n", "ptx": null}
{"id": 37, "kernel_name": "unary_djeg", "python": "@jax.jit\ndef unary_djeg(a):\n    return jnp.cos(a)", "type": "generate_unary_kernel", "hlo": "module @jit_unary_djeg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @unary_djeg(%arg0) : (tensor<1024xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @unary_djeg(%arg0: tensor<1024xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.cosine %arg0 : tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_unary_djeg, is_scheduled=true, entry_computation_layout={(f32[1024]{0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmp99cgn3rr.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"unary_djeg\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=11 end_column=21}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n\n\n%wrapped_cosine_computation (param_0: f32[1024]) -> f32[1024] {\n  %param_0 = f32[1024]{0} parameter(0)\n  ROOT %cos.2 = f32[1024]{0} cosine(%param_0), metadata={op_name=\"jit(unary_djeg)/jit(unary_djeg)/cos\" stack_frame_id=5}\n}\n\nENTRY %main.2 (a.1: f32[1024]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  ROOT %wrapped_cosine = f32[1024]{0} fusion(%a.1), kind=kLoop, calls=%wrapped_cosine_computation, metadata={op_name=\"jit(unary_djeg)/jit(unary_djeg)/cos\" stack_frame_id=5}\n}\n\n", "ptx": null}
{"id": 38, "kernel_name": "loop_hosy", "python": "@jax.jit\ndef loop_hosy(a, n):\n    def body_fun(i, val):\n        return val - a\n    # Initial value matches a's shape/type but zeroed\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "hlo": "module @jit_loop_hosy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>, %arg1: tensor<i32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @loop_hosy(%arg0, %arg1) : (tensor<1024xf32>, tensor<i32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @loop_hosy(%arg0: tensor<1024xf32>, %arg1: tensor<i32>) -> tensor<1024xf32> {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<1024xf32>, tensor<i32>, tensor<i32>, tensor<1024xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<1024xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<1024xf32>, tensor<i32>, tensor<i32>, tensor<1024xf32>\n    }\n    return %1#3 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_loop_hosy, is_scheduled=true, entry_computation_layout={(f32[1024]{0}, s32[])->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmpo74a0f27.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"loop_hosy\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=9 end_line=9 column=15 end_column=32}\n6 {file_name_id=3 function_name_id=5 line=10 end_line=10 column=11 end_column=54}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n\n\n%wrapped_add_computation (param_0.1: s32[], param_1: s32[]) -> s32[] {\n  %param_0.1 = s32[] parameter(0)\n  %param_1 = s32[] parameter(1)\n  ROOT %add.0 = s32[] add(%param_0.1, %param_1), metadata={op_name=\"jit(loop_hosy)/jit(loop_hosy)/while/body/add\" stack_frame_id=6}\n}\n\n%wrapped_subtract_computation (param_0.2: f32[1024], param_1.1: f32[1024]) -> f32[1024] {\n  %param_0.2 = f32[1024]{0} parameter(0)\n  %param_1.1 = f32[1024]{0} parameter(1)\n  ROOT %sub.0 = f32[1024]{0} subtract(%param_0.2, %param_1.1), metadata={op_name=\"jit(loop_hosy)/jit(loop_hosy)/while/body/sub\" stack_frame_id=6}\n}\n\n%region_0.1 (arg_tuple.1: (s32[], f32[1024], s32[], f32[1024])) -> (s32[], f32[1024], s32[], f32[1024]) {\n  %arg_tuple.1 = (s32[], f32[1024]{0}, s32[], f32[1024]{0}) parameter(0), metadata={op_name=\"jit(loop_hosy)/jit(loop_hosy)\"}\n  %constant.3 = s32[] constant(1), metadata={op_name=\"jit(loop_hosy)/jit(loop_hosy)\"}\n  %get-tuple-element.19 = f32[1024]{0} get-tuple-element(%arg_tuple.1), index=3, metadata={op_name=\"jit(loop_hosy)/jit(loop_hosy)\"}\n  %get-tuple-element.18 = s32[] get-tuple-element(%arg_tuple.1), index=2, metadata={op_name=\"jit(loop_hosy)/jit(loop_hosy)\"}\n  %get-tuple-element.9 = f32[1024]{0} get-tuple-element(%arg_tuple.1), index=1\n  %get-tuple-element.8 = s32[] get-tuple-element(%arg_tuple.1), index=0\n  %wrapped_subtract = f32[1024]{0} fusion(%get-tuple-element.9, %get-tuple-element.19), kind=kLoop, calls=%wrapped_subtract_computation, metadata={op_name=\"jit(loop_hosy)/jit(loop_hosy)/while/body/sub\" stack_frame_id=6}\n  %wrapped_add = s32[] fusion(%get-tuple-element.8, %constant.3), kind=kLoop, calls=%wrapped_add_computation, metadata={op_name=\"jit(loop_hosy)/jit(loop_hosy)/while/body/add\" stack_frame_id=6}\n  ROOT %tuple.3 = (s32[], f32[1024]{0}, s32[], f32[1024]{0}) tuple(%wrapped_add, %wrapped_subtract, %get-tuple-element.18, %get-tuple-element.19)\n}\n\n%wrapped_compare_computation (param_0.3: s32[], param_1.2: s32[]) -> pred[] {\n  %param_0.3 = s32[] parameter(0)\n  %param_1.2 = s32[] parameter(1)\n  ROOT %lt.0 = pred[] compare(%param_0.3, %param_1.2), direction=LT, metadata={op_name=\"jit(loop_hosy)/jit(loop_hosy)/while/cond/lt\" stack_frame_id=6}\n}\n\n%region_1.2 (arg_tuple.3: (s32[], f32[1024], s32[], f32[1024])) -> pred[] {\n  %arg_tuple.3 = (s32[], f32[1024]{0}, s32[], f32[1024]{0}) parameter(0), metadata={op_name=\"jit(loop_hosy)/jit(loop_hosy)\"}\n  %get-tuple-element.14 = s32[] get-tuple-element(%arg_tuple.3), index=2, metadata={op_name=\"jit(loop_hosy)/jit(loop_hosy)\"}\n  %get-tuple-element.12 = s32[] get-tuple-element(%arg_tuple.3), index=0, metadata={op_name=\"jit(loop_hosy)/jit(loop_hosy)\"}\n  ROOT %wrapped_compare = pred[] fusion(%get-tuple-element.12, %get-tuple-element.14), kind=kLoop, calls=%wrapped_compare_computation, metadata={op_name=\"jit(loop_hosy)/jit(loop_hosy)/while/cond/lt\" stack_frame_id=6}\n}\n\n%wrapped_broadcast_computation (param_0: f32[]) -> f32[1024] {\n  %param_0 = f32[] parameter(0)\n  ROOT %broadcast_in_dim.2 = f32[1024]{0} broadcast(%param_0), dimensions={}, metadata={op_name=\"jit(loop_hosy)/jit(loop_hosy)/broadcast_in_dim\" stack_frame_id=5}\n}\n\nENTRY %main.4 (a.1: f32[1024], n.1: s32[]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  %n.1 = s32[] parameter(1), metadata={op_name=\"n\"}\n  %constant.0 = s32[] constant(0), metadata={op_name=\"jit(loop_hosy)/jit(loop_hosy)\"}\n  %constant.1 = f32[] constant(0), metadata={op_name=\"jit(loop_hosy)/jit(loop_hosy)\"}\n  %copy.6 = s32[] copy(%constant.0)\n  %wrapped_broadcast = f32[1024]{0} fusion(%constant.1), kind=kLoop, calls=%wrapped_broadcast_computation, metadata={op_name=\"jit(loop_hosy)/jit(loop_hosy)/broadcast_in_dim\" stack_frame_id=5}\n  %tuple = (s32[], f32[1024]{0}, s32[], f32[1024]{0}) tuple(%copy.6, %wrapped_broadcast, %n.1, %a.1)\n  %while.1 = (s32[], f32[1024]{0}, s32[], f32[1024]{0}) while(%tuple), condition=%region_1.2, body=%region_0.1, metadata={op_name=\"jit(loop_hosy)/jit(loop_hosy)/while\" stack_frame_id=6}\n  ROOT %while.2 = f32[1024]{0} get-tuple-element(%while.1), index=1, metadata={op_name=\"jit(loop_hosy)/jit(loop_hosy)/while\" stack_frame_id=6}\n}\n\n", "ptx": null}
{"id": 39, "kernel_name": "nested_nloj", "python": "@jax.jit\ndef nested_nloj(a):\n    # Nested where: if a > t1 then (if a > t2 then ... else ...) else ...\n    inner_true = a * 3.0\n    inner_false = a * 2.0\n    outer_true = jnp.where(a > 0.66, inner_true, inner_false)\n    outer_false = a * 0.5\n    return jnp.where(a > 0.33, outer_true, outer_false)", "type": "generate_nested_branch_kernel", "hlo": "module @jit_nested_nloj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @nested_nloj(%arg0) : (tensor<1024xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @nested_nloj(%arg0: tensor<1024xf32>) -> tensor<1024xf32> {\n    %cst = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %1 = stablehlo.multiply %arg0, %0 : tensor<1024xf32>\n    %cst_0 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<1024xf32>\n    %cst_1 = stablehlo.constant dense<6.600000e-01> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %5 = stablehlo.compare  GT, %arg0, %4,  FLOAT : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xi1>\n    %6 = call @_where(%5, %1, %3) : (tensor<1024xi1>, tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>\n    %cst_2 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %8 = stablehlo.multiply %arg0, %7 : tensor<1024xf32>\n    %cst_3 = stablehlo.constant dense<3.300000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %10 = stablehlo.compare  GT, %arg0, %9,  FLOAT : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xi1>\n    %11 = call @_where(%10, %6, %8) : (tensor<1024xi1>, tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>\n    return %11 : tensor<1024xf32>\n  }\n  func.func private @_where(%arg0: tensor<1024xi1>, %arg1: tensor<1024xf32>, %arg2: tensor<1024xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<1024xi1>, tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_nested_nloj, is_scheduled=true, entry_computation_layout={(f32[1024]{0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmp14etyzgf.py\"\n4 \"/tmp/tmp7nvd0zdg.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"nested_nloj\"\n6 \"nested_odsh\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=11 end_line=11 column=21 end_column=29}\n6 {file_name_id=3 function_name_id=5 line=10 end_line=10 column=18 end_column=25}\n7 {file_name_id=3 function_name_id=5 line=9 end_line=9 column=27 end_column=35}\n8 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=18 end_column=25}\n9 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=17 end_column=24}\n10 {file_name_id=3 function_name_id=5 line=9 end_line=9 column=17 end_column=61}\n11 {file_name_id=4 function_name_id=6 line=9 end_line=9 column=17 end_column=61}\n12 {file_name_id=3 function_name_id=5 line=11 end_line=11 column=11 end_column=55}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=5}\n8 {file_location_id=8 parent_frame_id=5}\n9 {file_location_id=9 parent_frame_id=5}\n10 {file_location_id=10 parent_frame_id=5}\n11 {file_location_id=11 parent_frame_id=5}\n12 {file_location_id=12 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[1024]) -> f32[1024] {\n  %param_0.1 = f32[1024]{0} parameter(0)\n  %constant.14 = f32[] constant(0.33), metadata={op_name=\"jit(nested_nloj)/jit(nested_nloj)\"}\n  %gt.11 = f32[1024]{0} broadcast(%constant.14), dimensions={}, metadata={op_name=\"jit(nested_nloj)/jit(nested_nloj)/gt\" stack_frame_id=5}\n  %gt.10 = pred[1024]{0} compare(%param_0.1, %gt.11), direction=GT, metadata={op_name=\"jit(nested_nloj)/jit(nested_nloj)/gt\" stack_frame_id=5}\n  %constant.13 = f32[] constant(0.66), metadata={op_name=\"jit(nested_nloj)/jit(nested_nloj)\"}\n  %gt.9 = f32[1024]{0} broadcast(%constant.13), dimensions={}, metadata={op_name=\"jit(nested_nloj)/jit(nested_nloj)/gt\" stack_frame_id=7}\n  %gt.8 = pred[1024]{0} compare(%param_0.1, %gt.9), direction=GT, metadata={op_name=\"jit(nested_nloj)/jit(nested_nloj)/gt\" stack_frame_id=7}\n  %constant.12 = f32[] constant(3), metadata={op_name=\"jit(nested_nloj)/jit(nested_nloj)\"}\n  %mul.17 = f32[1024]{0} broadcast(%constant.12), dimensions={}, metadata={op_name=\"jit(nested_nloj)/jit(nested_nloj)/mul\" stack_frame_id=9}\n  %mul.16 = f32[1024]{0} multiply(%param_0.1, %mul.17), metadata={op_name=\"jit(nested_nloj)/jit(nested_nloj)/mul\" stack_frame_id=9}\n  %constant.11 = f32[] constant(2), metadata={op_name=\"jit(nested_nloj)/jit(nested_nloj)\"}\n  %mul.15 = f32[1024]{0} broadcast(%constant.11), dimensions={}, metadata={op_name=\"jit(nested_nloj)/jit(nested_nloj)/mul\" stack_frame_id=8}\n  %mul.14 = f32[1024]{0} multiply(%param_0.1, %mul.15), metadata={op_name=\"jit(nested_nloj)/jit(nested_nloj)/mul\" stack_frame_id=8}\n  %select_n.7 = f32[1024]{0} select(%gt.8, %mul.16, %mul.14), metadata={op_name=\"jit(nested_nloj)/jit(nested_nloj)/jit(_where)/select_n\" stack_frame_id=11}\n  %constant.10 = f32[] constant(0.5), metadata={op_name=\"jit(nested_nloj)/jit(nested_nloj)\"}\n  %mul.13 = f32[1024]{0} broadcast(%constant.10), dimensions={}, metadata={op_name=\"jit(nested_nloj)/jit(nested_nloj)/mul\" stack_frame_id=6}\n  %mul.12 = f32[1024]{0} multiply(%param_0.1, %mul.13), metadata={op_name=\"jit(nested_nloj)/jit(nested_nloj)/mul\" stack_frame_id=6}\n  ROOT %select_n.6 = f32[1024]{0} select(%gt.10, %select_n.7, %mul.12), metadata={op_name=\"jit(nested_nloj)/jit(nested_nloj)/jit(_where)/select_n\" stack_frame_id=11}\n}\n\nENTRY %main.3 (a.1: f32[1024]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  ROOT %multiply_select_fusion = f32[1024]{0} fusion(%a.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(nested_nloj)/jit(nested_nloj)/jit(_where)/select_n\" stack_frame_id=11}\n}\n\n", "ptx": null}
{"id": 40, "kernel_name": "unary_dmnz", "python": "@jax.jit\ndef unary_dmnz(a):\n    return jnp.abs(a)", "type": "generate_unary_kernel", "hlo": "module @jit_unary_dmnz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @unary_dmnz(%arg0) : (tensor<1024xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @unary_dmnz(%arg0: tensor<1024xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.abs %arg0 : tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_unary_dmnz, is_scheduled=true, entry_computation_layout={(f32[1024]{0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmp1i4kjb1m.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"unary_dmnz\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=11 end_column=21}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n\n\n%wrapped_abs_computation (param_0: f32[1024]) -> f32[1024] {\n  %param_0 = f32[1024]{0} parameter(0)\n  ROOT %abs.2 = f32[1024]{0} abs(%param_0), metadata={op_name=\"jit(unary_dmnz)/jit(unary_dmnz)/abs\" stack_frame_id=5}\n}\n\nENTRY %main.2 (a.1: f32[1024]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  ROOT %wrapped_abs = f32[1024]{0} fusion(%a.1), kind=kLoop, calls=%wrapped_abs_computation, metadata={op_name=\"jit(unary_dmnz)/jit(unary_dmnz)/abs\" stack_frame_id=5}\n}\n\n", "ptx": null}
{"id": 41, "kernel_name": "multi_mwcj", "python": "@jax.jit\ndef multi_mwcj(a, b):\n    temp1 = a + b\n    temp2 = jnp.abs(temp1)\n    return temp2 * a", "type": "generate_multi_statement_kernel", "hlo": "module @jit_multi_mwcj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>, %arg1: tensor<1024xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @multi_mwcj(%arg0, %arg1) : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @multi_mwcj(%arg0: tensor<1024xf32>, %arg1: tensor<1024xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<1024xf32>\n    %1 = stablehlo.abs %0 : tensor<1024xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<1024xf32>\n    return %2 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_multi_mwcj, is_scheduled=true, entry_computation_layout={(f32[1024]{0}, f32[1024]{0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmpe3_8rn_j.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"multi_mwcj\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=12 end_column=17}\n6 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=12 end_column=26}\n7 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=11 end_column=20}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[1024], param_1.2: f32[1024]) -> f32[1024] {\n  %param_0.1 = f32[1024]{0} parameter(0)\n  %param_1.2 = f32[1024]{0} parameter(1)\n  %add.2 = f32[1024]{0} add(%param_0.1, %param_1.2), metadata={op_name=\"jit(multi_mwcj)/jit(multi_mwcj)/add\" stack_frame_id=5}\n  %abs.2 = f32[1024]{0} abs(%add.2), metadata={op_name=\"jit(multi_mwcj)/jit(multi_mwcj)/abs\" stack_frame_id=6}\n  ROOT %mul.2 = f32[1024]{0} multiply(%abs.2, %param_0.1), metadata={op_name=\"jit(multi_mwcj)/jit(multi_mwcj)/mul\" stack_frame_id=7}\n}\n\nENTRY %main.2 (a.1: f32[1024], b.1: f32[1024]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  %b.1 = f32[1024]{0} parameter(1), metadata={op_name=\"b\"}\n  ROOT %abs_multiply_fusion = f32[1024]{0} fusion(%a.1, %b.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(multi_mwcj)/jit(multi_mwcj)/mul\" stack_frame_id=7}\n}\n\n", "ptx": null}
{"id": 42, "kernel_name": "loop_txaa", "python": "@jax.jit\ndef loop_txaa(a, n):\n    def body_fun(i, val):\n        return val * a\n    # Initial value matches a's shape/type but zeroed\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "hlo": "module @jit_loop_txaa attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>, %arg1: tensor<i32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @loop_txaa(%arg0, %arg1) : (tensor<1024xf32>, tensor<i32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @loop_txaa(%arg0: tensor<1024xf32>, %arg1: tensor<i32>) -> tensor<1024xf32> {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<1024xf32>, tensor<i32>, tensor<i32>, tensor<1024xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.multiply %iterArg_2, %iterArg : tensor<1024xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<1024xf32>, tensor<i32>, tensor<i32>, tensor<1024xf32>\n    }\n    return %1#3 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_loop_txaa, is_scheduled=true, entry_computation_layout={(f32[1024]{0}, s32[])->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmp54jea342.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"loop_txaa\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=9 end_line=9 column=15 end_column=32}\n6 {file_name_id=3 function_name_id=5 line=10 end_line=10 column=11 end_column=54}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n\n\n%wrapped_add_computation (param_0.1: s32[], param_1: s32[]) -> s32[] {\n  %param_0.1 = s32[] parameter(0)\n  %param_1 = s32[] parameter(1)\n  ROOT %add.0 = s32[] add(%param_0.1, %param_1), metadata={op_name=\"jit(loop_txaa)/jit(loop_txaa)/while/body/add\" stack_frame_id=6}\n}\n\n%wrapped_multiply_computation (param_0.2: f32[1024], param_1.1: f32[1024]) -> f32[1024] {\n  %param_0.2 = f32[1024]{0} parameter(0)\n  %param_1.1 = f32[1024]{0} parameter(1)\n  ROOT %mul.0 = f32[1024]{0} multiply(%param_0.2, %param_1.1), metadata={op_name=\"jit(loop_txaa)/jit(loop_txaa)/while/body/mul\" stack_frame_id=6}\n}\n\n%region_0.1 (arg_tuple.1: (s32[], f32[1024], s32[], f32[1024])) -> (s32[], f32[1024], s32[], f32[1024]) {\n  %arg_tuple.1 = (s32[], f32[1024]{0}, s32[], f32[1024]{0}) parameter(0), metadata={op_name=\"jit(loop_txaa)/jit(loop_txaa)\"}\n  %constant.3 = s32[] constant(1), metadata={op_name=\"jit(loop_txaa)/jit(loop_txaa)\"}\n  %get-tuple-element.19 = f32[1024]{0} get-tuple-element(%arg_tuple.1), index=3, metadata={op_name=\"jit(loop_txaa)/jit(loop_txaa)\"}\n  %get-tuple-element.18 = s32[] get-tuple-element(%arg_tuple.1), index=2, metadata={op_name=\"jit(loop_txaa)/jit(loop_txaa)\"}\n  %get-tuple-element.9 = f32[1024]{0} get-tuple-element(%arg_tuple.1), index=1\n  %get-tuple-element.8 = s32[] get-tuple-element(%arg_tuple.1), index=0\n  %wrapped_multiply = f32[1024]{0} fusion(%get-tuple-element.9, %get-tuple-element.19), kind=kLoop, calls=%wrapped_multiply_computation, metadata={op_name=\"jit(loop_txaa)/jit(loop_txaa)/while/body/mul\" stack_frame_id=6}\n  %wrapped_add = s32[] fusion(%get-tuple-element.8, %constant.3), kind=kLoop, calls=%wrapped_add_computation, metadata={op_name=\"jit(loop_txaa)/jit(loop_txaa)/while/body/add\" stack_frame_id=6}\n  ROOT %tuple.3 = (s32[], f32[1024]{0}, s32[], f32[1024]{0}) tuple(%wrapped_add, %wrapped_multiply, %get-tuple-element.18, %get-tuple-element.19)\n}\n\n%wrapped_compare_computation (param_0.3: s32[], param_1.2: s32[]) -> pred[] {\n  %param_0.3 = s32[] parameter(0)\n  %param_1.2 = s32[] parameter(1)\n  ROOT %lt.0 = pred[] compare(%param_0.3, %param_1.2), direction=LT, metadata={op_name=\"jit(loop_txaa)/jit(loop_txaa)/while/cond/lt\" stack_frame_id=6}\n}\n\n%region_1.2 (arg_tuple.3: (s32[], f32[1024], s32[], f32[1024])) -> pred[] {\n  %arg_tuple.3 = (s32[], f32[1024]{0}, s32[], f32[1024]{0}) parameter(0), metadata={op_name=\"jit(loop_txaa)/jit(loop_txaa)\"}\n  %get-tuple-element.14 = s32[] get-tuple-element(%arg_tuple.3), index=2, metadata={op_name=\"jit(loop_txaa)/jit(loop_txaa)\"}\n  %get-tuple-element.12 = s32[] get-tuple-element(%arg_tuple.3), index=0, metadata={op_name=\"jit(loop_txaa)/jit(loop_txaa)\"}\n  ROOT %wrapped_compare = pred[] fusion(%get-tuple-element.12, %get-tuple-element.14), kind=kLoop, calls=%wrapped_compare_computation, metadata={op_name=\"jit(loop_txaa)/jit(loop_txaa)/while/cond/lt\" stack_frame_id=6}\n}\n\n%wrapped_broadcast_computation (param_0: f32[]) -> f32[1024] {\n  %param_0 = f32[] parameter(0)\n  ROOT %broadcast_in_dim.2 = f32[1024]{0} broadcast(%param_0), dimensions={}, metadata={op_name=\"jit(loop_txaa)/jit(loop_txaa)/broadcast_in_dim\" stack_frame_id=5}\n}\n\nENTRY %main.4 (a.1: f32[1024], n.1: s32[]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  %n.1 = s32[] parameter(1), metadata={op_name=\"n\"}\n  %constant.0 = s32[] constant(0), metadata={op_name=\"jit(loop_txaa)/jit(loop_txaa)\"}\n  %constant.1 = f32[] constant(0), metadata={op_name=\"jit(loop_txaa)/jit(loop_txaa)\"}\n  %copy.6 = s32[] copy(%constant.0)\n  %wrapped_broadcast = f32[1024]{0} fusion(%constant.1), kind=kLoop, calls=%wrapped_broadcast_computation, metadata={op_name=\"jit(loop_txaa)/jit(loop_txaa)/broadcast_in_dim\" stack_frame_id=5}\n  %tuple = (s32[], f32[1024]{0}, s32[], f32[1024]{0}) tuple(%copy.6, %wrapped_broadcast, %n.1, %a.1)\n  %while.1 = (s32[], f32[1024]{0}, s32[], f32[1024]{0}) while(%tuple), condition=%region_1.2, body=%region_0.1, metadata={op_name=\"jit(loop_txaa)/jit(loop_txaa)/while\" stack_frame_id=6}\n  ROOT %while.2 = f32[1024]{0} get-tuple-element(%while.1), index=1, metadata={op_name=\"jit(loop_txaa)/jit(loop_txaa)/while\" stack_frame_id=6}\n}\n\n", "ptx": null}
{"id": 43, "kernel_name": "branch_fpcx", "python": "@jax.jit\ndef branch_fpcx(a):\n    return jnp.where(a > -0.62, a - 2.0, a - 2.2)", "type": "generate_branch_kernel", "hlo": "module @jit_branch_fpcx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @branch_fpcx(%arg0) : (tensor<1024xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @branch_fpcx(%arg0: tensor<1024xf32>) -> tensor<1024xf32> {\n    %cst = stablehlo.constant dense<-6.200000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xi1>\n    %cst_0 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<1024xf32>\n    %cst_1 = stablehlo.constant dense<2.200000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<1024xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<1024xi1>, tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>\n    return %6 : tensor<1024xf32>\n  }\n  func.func private @_where(%arg0: tensor<1024xi1>, %arg1: tensor<1024xf32>, %arg2: tensor<1024xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<1024xi1>, tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_branch_fpcx, is_scheduled=true, entry_computation_layout={(f32[1024]{0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmp2lsndul_.py\"\n4 \"/tmp/tmp7nvd0zdg.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"branch_fpcx\"\n6 \"nested_odsh\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=41 end_column=48}\n6 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=32 end_column=39}\n7 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=21 end_column=30}\n8 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=11 end_column=49}\n9 {file_name_id=4 function_name_id=6 line=9 end_line=9 column=17 end_column=61}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=5}\n8 {file_location_id=8 parent_frame_id=5}\n9 {file_location_id=9 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[1024]) -> f32[1024] {\n  %param_0.1 = f32[1024]{0} parameter(0)\n  %constant.10 = f32[] constant(-0.62), metadata={op_name=\"jit(branch_fpcx)/jit(branch_fpcx)\"}\n  %gt.5 = f32[1024]{0} broadcast(%constant.10), dimensions={}, metadata={op_name=\"jit(branch_fpcx)/jit(branch_fpcx)/gt\" stack_frame_id=7}\n  %gt.4 = pred[1024]{0} compare(%param_0.1, %gt.5), direction=GT, metadata={op_name=\"jit(branch_fpcx)/jit(branch_fpcx)/gt\" stack_frame_id=7}\n  %constant.9 = f32[] constant(-2), metadata={op_name=\"jit(branch_fpcx)/jit(branch_fpcx)\"}\n  %broadcast.3 = f32[1024]{0} broadcast(%constant.9), dimensions={}, metadata={op_name=\"jit(branch_fpcx)/jit(branch_fpcx)\"}\n  %add.3 = f32[1024]{0} add(%param_0.1, %broadcast.3), metadata={op_name=\"jit(branch_fpcx)/jit(branch_fpcx)/sub\" stack_frame_id=6}\n  %constant.8 = f32[] constant(-2.2), metadata={op_name=\"jit(branch_fpcx)/jit(branch_fpcx)\"}\n  %broadcast.2 = f32[1024]{0} broadcast(%constant.8), dimensions={}, metadata={op_name=\"jit(branch_fpcx)/jit(branch_fpcx)\"}\n  %add.2 = f32[1024]{0} add(%param_0.1, %broadcast.2), metadata={op_name=\"jit(branch_fpcx)/jit(branch_fpcx)/sub\" stack_frame_id=5}\n  ROOT %select_n.3 = f32[1024]{0} select(%gt.4, %add.3, %add.2), metadata={op_name=\"jit(branch_fpcx)/jit(branch_fpcx)/jit(_where)/select_n\" stack_frame_id=9}\n}\n\nENTRY %main.3 (a.1: f32[1024]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  ROOT %add_select_fusion = f32[1024]{0} fusion(%a.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(branch_fpcx)/jit(branch_fpcx)/jit(_where)/select_n\" stack_frame_id=9}\n}\n\n", "ptx": null}
{"id": 44, "kernel_name": "elementwise_uavi", "python": "@jax.jit\ndef elementwise_uavi(a, b):\n    return a + b", "type": "generate_simple_elementwise", "hlo": "module @jit_elementwise_uavi attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>, %arg1: tensor<1024xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @elementwise_uavi(%arg0, %arg1) : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @elementwise_uavi(%arg0: tensor<1024xf32>, %arg1: tensor<1024xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_elementwise_uavi, is_scheduled=true, entry_computation_layout={(f32[1024]{0}, f32[1024]{0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmp6gmhczlt.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"elementwise_uavi\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=11 end_column=16}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n\n\n%wrapped_add_computation (param_0: f32[1024], param_1: f32[1024]) -> f32[1024] {\n  %param_0 = f32[1024]{0} parameter(0)\n  %param_1 = f32[1024]{0} parameter(1)\n  ROOT %add.2 = f32[1024]{0} add(%param_0, %param_1), metadata={op_name=\"jit(elementwise_uavi)/jit(elementwise_uavi)/add\" stack_frame_id=5}\n}\n\nENTRY %main.2 (a.1: f32[1024], b.1: f32[1024]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  %b.1 = f32[1024]{0} parameter(1), metadata={op_name=\"b\"}\n  ROOT %wrapped_add = f32[1024]{0} fusion(%a.1, %b.1), kind=kLoop, calls=%wrapped_add_computation, metadata={op_name=\"jit(elementwise_uavi)/jit(elementwise_uavi)/add\" stack_frame_id=5}\n}\n\n", "ptx": null}
{"id": 45, "kernel_name": "unary_devo", "python": "@jax.jit\ndef unary_devo(a):\n    return jnp.sin(a)", "type": "generate_unary_kernel", "hlo": "module @jit_unary_devo attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @unary_devo(%arg0) : (tensor<1024xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @unary_devo(%arg0: tensor<1024xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.sine %arg0 : tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_unary_devo, is_scheduled=true, entry_computation_layout={(f32[1024]{0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmpgk0qpmme.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"unary_devo\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=11 end_column=21}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n\n\n%wrapped_sine_computation (param_0: f32[1024]) -> f32[1024] {\n  %param_0 = f32[1024]{0} parameter(0)\n  ROOT %sin.2 = f32[1024]{0} sine(%param_0), metadata={op_name=\"jit(unary_devo)/jit(unary_devo)/sin\" stack_frame_id=5}\n}\n\nENTRY %main.2 (a.1: f32[1024]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  ROOT %wrapped_sine = f32[1024]{0} fusion(%a.1), kind=kLoop, calls=%wrapped_sine_computation, metadata={op_name=\"jit(unary_devo)/jit(unary_devo)/sin\" stack_frame_id=5}\n}\n\n", "ptx": null}
{"id": 46, "kernel_name": "vec_kexq", "python": "@jax.jit\ndef vec_kexq(a, b):\n    # vector dot product along last axis\n    return jnp.sum(a * b, axis=-1)", "type": "generate_vector_kernel", "hlo": "module @jit_vec_kexq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024x3xf32>, %arg1: tensor<1024x3xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @vec_kexq(%arg0, %arg1) : (tensor<1024x3xf32>, tensor<1024x3xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @vec_kexq(%arg0: tensor<1024x3xf32>, %arg1: tensor<1024x3xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<1024x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<1024x3xf32>, tensor<f32>) -> tensor<1024xf32>\n    return %1 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_vec_kexq, is_scheduled=true, entry_computation_layout={(f32[1024,3]{1,0}, f32[1024,3]{1,0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmpvso2ez5l.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"vec_kexq\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=11 end_column=34}\n6 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=19 end_column=24}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n\n\n%region_0.1 (reduce_sum.3: f32[], reduce_sum.4: f32[]) -> f32[] {\n  %reduce_sum.3 = f32[] parameter(0), metadata={op_name=\"reduce_sum\"}\n  %reduce_sum.4 = f32[] parameter(1), metadata={op_name=\"reduce_sum\"}\n  ROOT %reduce_sum.5 = f32[] add(%reduce_sum.3, %reduce_sum.4), metadata={op_name=\"reduce_sum\" stack_frame_id=5}\n}\n\n%fused_computation (param_0.2: f32[1024,3], param_1.2: f32[1024,3]) -> f32[1024] {\n  %param_0.2 = f32[1024,3]{1,0} parameter(0)\n  %param_1.2 = f32[1024,3]{1,0} parameter(1)\n  %mul.2 = f32[1024,3]{1,0} multiply(%param_0.2, %param_1.2), metadata={op_name=\"jit(vec_kexq)/jit(vec_kexq)/mul\" stack_frame_id=6}\n  %constant.2 = f32[] constant(0), metadata={op_name=\"jit(vec_kexq)/jit(vec_kexq)\"}\n  ROOT %reduce_sum.1 = f32[1024]{0} reduce(%mul.2, %constant.2), dimensions={1}, to_apply=%region_0.1, metadata={op_name=\"jit(vec_kexq)/jit(vec_kexq)/reduce_sum\" stack_frame_id=5}\n}\n\nENTRY %main.3 (a.1: f32[1024,3], b.1: f32[1024,3]) -> f32[1024] {\n  %a.1 = f32[1024,3]{1,0} parameter(0), metadata={op_name=\"a\"}\n  %b.1 = f32[1024,3]{1,0} parameter(1), metadata={op_name=\"b\"}\n  ROOT %multiply_reduce_fusion = f32[1024]{0} fusion(%a.1, %b.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(vec_kexq)/jit(vec_kexq)/reduce_sum\" stack_frame_id=5}\n}\n\n", "ptx": null}
{"id": 47, "kernel_name": "scalar_arr_cpzd", "python": "@jax.jit\ndef scalar_arr_cpzd(alpha, x, y):\n    return alpha + x + y", "type": "generate_scalar_array_op", "hlo": "module @jit_scalar_arr_cpzd attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<1024xf32>, %arg2: tensor<1024xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @scalar_arr_cpzd(%arg0, %arg1, %arg2) : (tensor<f32>, tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @scalar_arr_cpzd(%arg0: tensor<f32>, %arg1: tensor<1024xf32>, %arg2: tensor<1024xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %2 = stablehlo.add %1, %arg1 : tensor<1024xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<1024xf32>\n    return %3 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_scalar_arr_cpzd, is_scheduled=true, entry_computation_layout={(f32[], f32[1024]{0}, f32[1024]{0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmpz_ni36wx.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"scalar_arr_cpzd\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=11 end_column=20}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[1024], param_1.2: f32[1024], param_2.1: f32[]) -> f32[1024] {\n  %param_2.1 = f32[] parameter(2)\n  %add.8 = f32[1024]{0} broadcast(%param_2.1), dimensions={}, metadata={op_name=\"jit(scalar_arr_cpzd)/jit(scalar_arr_cpzd)/add\" stack_frame_id=5}\n  %param_1.2 = f32[1024]{0} parameter(1)\n  %add.7 = f32[1024]{0} add(%add.8, %param_1.2), metadata={op_name=\"jit(scalar_arr_cpzd)/jit(scalar_arr_cpzd)/add\" stack_frame_id=5}\n  %param_0.1 = f32[1024]{0} parameter(0)\n  ROOT %add.6 = f32[1024]{0} add(%add.7, %param_0.1), metadata={op_name=\"jit(scalar_arr_cpzd)/jit(scalar_arr_cpzd)/add\" stack_frame_id=5}\n}\n\nENTRY %main.2 (alpha.1: f32[], x.1: f32[1024], y.1: f32[1024]) -> f32[1024] {\n  %alpha.1 = f32[] parameter(0), metadata={op_name=\"alpha\"}\n  %x.1 = f32[1024]{0} parameter(1), metadata={op_name=\"x\"}\n  %y.1 = f32[1024]{0} parameter(2), metadata={op_name=\"y\"}\n  ROOT %add_add_fusion = f32[1024]{0} fusion(%y.1, %x.1, %alpha.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(scalar_arr_cpzd)/jit(scalar_arr_cpzd)/add\" stack_frame_id=5}\n}\n\n", "ptx": null}
{"id": 48, "kernel_name": "branch_fcrp", "python": "@jax.jit\ndef branch_fcrp(a):\n    return jnp.where(a > 0.27, a - 1.9, a - 2.9)", "type": "generate_branch_kernel", "hlo": "module @jit_branch_fcrp attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @branch_fcrp(%arg0) : (tensor<1024xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @branch_fcrp(%arg0: tensor<1024xf32>) -> tensor<1024xf32> {\n    %cst = stablehlo.constant dense<2.700000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xi1>\n    %cst_0 = stablehlo.constant dense<1.900000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<1024xf32>\n    %cst_1 = stablehlo.constant dense<2.900000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<1024xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<1024xi1>, tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>\n    return %6 : tensor<1024xf32>\n  }\n  func.func private @_where(%arg0: tensor<1024xi1>, %arg1: tensor<1024xf32>, %arg2: tensor<1024xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<1024xi1>, tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_branch_fcrp, is_scheduled=true, entry_computation_layout={(f32[1024]{0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmp1t09khx3.py\"\n4 \"/tmp/tmp7nvd0zdg.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"branch_fcrp\"\n6 \"nested_odsh\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=40 end_column=47}\n6 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=31 end_column=38}\n7 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=21 end_column=29}\n8 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=11 end_column=48}\n9 {file_name_id=4 function_name_id=6 line=9 end_line=9 column=17 end_column=61}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=5}\n8 {file_location_id=8 parent_frame_id=5}\n9 {file_location_id=9 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[1024]) -> f32[1024] {\n  %param_0.1 = f32[1024]{0} parameter(0)\n  %constant.10 = f32[] constant(0.27), metadata={op_name=\"jit(branch_fcrp)/jit(branch_fcrp)\"}\n  %gt.5 = f32[1024]{0} broadcast(%constant.10), dimensions={}, metadata={op_name=\"jit(branch_fcrp)/jit(branch_fcrp)/gt\" stack_frame_id=7}\n  %gt.4 = pred[1024]{0} compare(%param_0.1, %gt.5), direction=GT, metadata={op_name=\"jit(branch_fcrp)/jit(branch_fcrp)/gt\" stack_frame_id=7}\n  %constant.9 = f32[] constant(-1.9), metadata={op_name=\"jit(branch_fcrp)/jit(branch_fcrp)\"}\n  %broadcast.3 = f32[1024]{0} broadcast(%constant.9), dimensions={}, metadata={op_name=\"jit(branch_fcrp)/jit(branch_fcrp)\"}\n  %add.3 = f32[1024]{0} add(%param_0.1, %broadcast.3), metadata={op_name=\"jit(branch_fcrp)/jit(branch_fcrp)/sub\" stack_frame_id=6}\n  %constant.8 = f32[] constant(-2.9), metadata={op_name=\"jit(branch_fcrp)/jit(branch_fcrp)\"}\n  %broadcast.2 = f32[1024]{0} broadcast(%constant.8), dimensions={}, metadata={op_name=\"jit(branch_fcrp)/jit(branch_fcrp)\"}\n  %add.2 = f32[1024]{0} add(%param_0.1, %broadcast.2), metadata={op_name=\"jit(branch_fcrp)/jit(branch_fcrp)/sub\" stack_frame_id=5}\n  ROOT %select_n.3 = f32[1024]{0} select(%gt.4, %add.3, %add.2), metadata={op_name=\"jit(branch_fcrp)/jit(branch_fcrp)/jit(_where)/select_n\" stack_frame_id=9}\n}\n\nENTRY %main.3 (a.1: f32[1024]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  ROOT %add_select_fusion = f32[1024]{0} fusion(%a.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(branch_fcrp)/jit(branch_fcrp)/jit(_where)/select_n\" stack_frame_id=9}\n}\n\n", "ptx": null}
{"id": 49, "kernel_name": "scalar_arr_cere", "python": "@jax.jit\ndef scalar_arr_cere(alpha, x, y):\n    return alpha - x * y", "type": "generate_scalar_array_op", "hlo": "module @jit_scalar_arr_cere attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<1024xf32>, %arg2: tensor<1024xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @scalar_arr_cere(%arg0, %arg1, %arg2) : (tensor<f32>, tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @scalar_arr_cere(%arg0: tensor<f32>, %arg1: tensor<1024xf32>, %arg2: tensor<1024xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<1024xf32>\n    %1 = stablehlo.convert %arg0 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %3 = stablehlo.subtract %2, %0 : tensor<1024xf32>\n    return %3 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_scalar_arr_cere, is_scheduled=true, entry_computation_layout={(f32[], f32[1024]{0}, f32[1024]{0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmpg_ig8bbv.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"scalar_arr_cere\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=19 end_column=24}\n6 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=11 end_column=24}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[1024], param_1.2: f32[1024], param_2.1: f32[]) -> f32[1024] {\n  %param_2.1 = f32[] parameter(2)\n  %sub.5 = f32[1024]{0} broadcast(%param_2.1), dimensions={}, metadata={op_name=\"jit(scalar_arr_cere)/jit(scalar_arr_cere)/sub\" stack_frame_id=6}\n  %param_0.1 = f32[1024]{0} parameter(0)\n  %param_1.2 = f32[1024]{0} parameter(1)\n  %mul.2 = f32[1024]{0} multiply(%param_0.1, %param_1.2), metadata={op_name=\"jit(scalar_arr_cere)/jit(scalar_arr_cere)/mul\" stack_frame_id=5}\n  ROOT %sub.4 = f32[1024]{0} subtract(%sub.5, %mul.2), metadata={op_name=\"jit(scalar_arr_cere)/jit(scalar_arr_cere)/sub\" stack_frame_id=6}\n}\n\nENTRY %main.2 (alpha.1: f32[], x.1: f32[1024], y.1: f32[1024]) -> f32[1024] {\n  %alpha.1 = f32[] parameter(0), metadata={op_name=\"alpha\"}\n  %x.1 = f32[1024]{0} parameter(1), metadata={op_name=\"x\"}\n  %y.1 = f32[1024]{0} parameter(2), metadata={op_name=\"y\"}\n  ROOT %multiply_subtract_fusion = f32[1024]{0} fusion(%x.1, %y.1, %alpha.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(scalar_arr_cere)/jit(scalar_arr_cere)/sub\" stack_frame_id=6}\n}\n\n", "ptx": null}
{"id": 50, "kernel_name": "vec_koop", "python": "@jax.jit\ndef vec_koop(a, b):\n    # vector dot product along last axis\n    return jnp.sum(a * b, axis=-1)", "type": "generate_vector_kernel", "hlo": "module @jit_vec_koop attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024x3xf32>, %arg1: tensor<1024x3xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @vec_koop(%arg0, %arg1) : (tensor<1024x3xf32>, tensor<1024x3xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @vec_koop(%arg0: tensor<1024x3xf32>, %arg1: tensor<1024x3xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<1024x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<1024x3xf32>, tensor<f32>) -> tensor<1024xf32>\n    return %1 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_vec_koop, is_scheduled=true, entry_computation_layout={(f32[1024,3]{1,0}, f32[1024,3]{1,0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmpoafs3trd.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"vec_koop\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=11 end_column=34}\n6 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=19 end_column=24}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n\n\n%region_0.1 (reduce_sum.3: f32[], reduce_sum.4: f32[]) -> f32[] {\n  %reduce_sum.3 = f32[] parameter(0), metadata={op_name=\"reduce_sum\"}\n  %reduce_sum.4 = f32[] parameter(1), metadata={op_name=\"reduce_sum\"}\n  ROOT %reduce_sum.5 = f32[] add(%reduce_sum.3, %reduce_sum.4), metadata={op_name=\"reduce_sum\" stack_frame_id=5}\n}\n\n%fused_computation (param_0.2: f32[1024,3], param_1.2: f32[1024,3]) -> f32[1024] {\n  %param_0.2 = f32[1024,3]{1,0} parameter(0)\n  %param_1.2 = f32[1024,3]{1,0} parameter(1)\n  %mul.2 = f32[1024,3]{1,0} multiply(%param_0.2, %param_1.2), metadata={op_name=\"jit(vec_koop)/jit(vec_koop)/mul\" stack_frame_id=6}\n  %constant.2 = f32[] constant(0), metadata={op_name=\"jit(vec_koop)/jit(vec_koop)\"}\n  ROOT %reduce_sum.1 = f32[1024]{0} reduce(%mul.2, %constant.2), dimensions={1}, to_apply=%region_0.1, metadata={op_name=\"jit(vec_koop)/jit(vec_koop)/reduce_sum\" stack_frame_id=5}\n}\n\nENTRY %main.3 (a.1: f32[1024,3], b.1: f32[1024,3]) -> f32[1024] {\n  %a.1 = f32[1024,3]{1,0} parameter(0), metadata={op_name=\"a\"}\n  %b.1 = f32[1024,3]{1,0} parameter(1), metadata={op_name=\"b\"}\n  ROOT %multiply_reduce_fusion = f32[1024]{0} fusion(%a.1, %b.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(vec_koop)/jit(vec_koop)/reduce_sum\" stack_frame_id=5}\n}\n\n", "ptx": null}
{"id": 51, "kernel_name": "multi_xmoj", "python": "@jax.jit\ndef multi_xmoj(a, b):\n    temp1 = a + b\n    temp2 = jnp.sin(temp1)\n    return temp2 + a", "type": "generate_multi_statement_kernel", "hlo": "module @jit_multi_xmoj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>, %arg1: tensor<1024xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @multi_xmoj(%arg0, %arg1) : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @multi_xmoj(%arg0: tensor<1024xf32>, %arg1: tensor<1024xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<1024xf32>\n    %1 = stablehlo.sine %0 : tensor<1024xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<1024xf32>\n    return %2 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_multi_xmoj, is_scheduled=true, entry_computation_layout={(f32[1024]{0}, f32[1024]{0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmpz8aqmgmh.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"multi_xmoj\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=12 end_column=17}\n6 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=12 end_column=26}\n7 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=11 end_column=20}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[1024], param_1.2: f32[1024]) -> f32[1024] {\n  %param_0.1 = f32[1024]{0} parameter(0)\n  %param_1.2 = f32[1024]{0} parameter(1)\n  %add.5 = f32[1024]{0} add(%param_0.1, %param_1.2), metadata={op_name=\"jit(multi_xmoj)/jit(multi_xmoj)/add\" stack_frame_id=5}\n  %sin.2 = f32[1024]{0} sine(%add.5), metadata={op_name=\"jit(multi_xmoj)/jit(multi_xmoj)/sin\" stack_frame_id=6}\n  ROOT %add.4 = f32[1024]{0} add(%sin.2, %param_0.1), metadata={op_name=\"jit(multi_xmoj)/jit(multi_xmoj)/add\" stack_frame_id=7}\n}\n\nENTRY %main.2 (a.1: f32[1024], b.1: f32[1024]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  %b.1 = f32[1024]{0} parameter(1), metadata={op_name=\"b\"}\n  ROOT %sine_add_fusion = f32[1024]{0} fusion(%a.1, %b.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(multi_xmoj)/jit(multi_xmoj)/add\" stack_frame_id=7}\n}\n\n", "ptx": null}
{"id": 52, "kernel_name": "nested_odhl", "python": "@jax.jit\ndef nested_odhl(a):\n    # Nested where: if a > t1 then (if a > t2 then ... else ...) else ...\n    inner_true = a * 3.0\n    inner_false = a * 2.0\n    outer_true = jnp.where(a > 1.0, inner_true, inner_false)\n    outer_false = a * 0.5\n    return jnp.where(a > -0.48, outer_true, outer_false)", "type": "generate_nested_branch_kernel", "hlo": "module @jit_nested_odhl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @nested_odhl(%arg0) : (tensor<1024xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @nested_odhl(%arg0: tensor<1024xf32>) -> tensor<1024xf32> {\n    %cst = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %1 = stablehlo.multiply %arg0, %0 : tensor<1024xf32>\n    %cst_0 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<1024xf32>\n    %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %5 = stablehlo.compare  GT, %arg0, %4,  FLOAT : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xi1>\n    %6 = call @_where(%5, %1, %3) : (tensor<1024xi1>, tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>\n    %cst_2 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %8 = stablehlo.multiply %arg0, %7 : tensor<1024xf32>\n    %cst_3 = stablehlo.constant dense<-4.800000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %10 = stablehlo.compare  GT, %arg0, %9,  FLOAT : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xi1>\n    %11 = call @_where(%10, %6, %8) : (tensor<1024xi1>, tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>\n    return %11 : tensor<1024xf32>\n  }\n  func.func private @_where(%arg0: tensor<1024xi1>, %arg1: tensor<1024xf32>, %arg2: tensor<1024xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<1024xi1>, tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_nested_odhl, is_scheduled=true, entry_computation_layout={(f32[1024]{0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmpy8yq8ck6.py\"\n4 \"/tmp/tmp7nvd0zdg.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"nested_odhl\"\n6 \"nested_odsh\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=11 end_line=11 column=21 end_column=30}\n6 {file_name_id=3 function_name_id=5 line=10 end_line=10 column=18 end_column=25}\n7 {file_name_id=3 function_name_id=5 line=9 end_line=9 column=27 end_column=34}\n8 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=18 end_column=25}\n9 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=17 end_column=24}\n10 {file_name_id=3 function_name_id=5 line=9 end_line=9 column=17 end_column=60}\n11 {file_name_id=4 function_name_id=6 line=9 end_line=9 column=17 end_column=61}\n12 {file_name_id=3 function_name_id=5 line=11 end_line=11 column=11 end_column=56}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=5}\n8 {file_location_id=8 parent_frame_id=5}\n9 {file_location_id=9 parent_frame_id=5}\n10 {file_location_id=10 parent_frame_id=5}\n11 {file_location_id=11 parent_frame_id=5}\n12 {file_location_id=12 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[1024]) -> f32[1024] {\n  %param_0.1 = f32[1024]{0} parameter(0)\n  %constant.14 = f32[] constant(-0.48), metadata={op_name=\"jit(nested_odhl)/jit(nested_odhl)\"}\n  %gt.11 = f32[1024]{0} broadcast(%constant.14), dimensions={}, metadata={op_name=\"jit(nested_odhl)/jit(nested_odhl)/gt\" stack_frame_id=5}\n  %gt.10 = pred[1024]{0} compare(%param_0.1, %gt.11), direction=GT, metadata={op_name=\"jit(nested_odhl)/jit(nested_odhl)/gt\" stack_frame_id=5}\n  %constant.13 = f32[] constant(1), metadata={op_name=\"jit(nested_odhl)/jit(nested_odhl)\"}\n  %gt.9 = f32[1024]{0} broadcast(%constant.13), dimensions={}, metadata={op_name=\"jit(nested_odhl)/jit(nested_odhl)/gt\" stack_frame_id=7}\n  %gt.8 = pred[1024]{0} compare(%param_0.1, %gt.9), direction=GT, metadata={op_name=\"jit(nested_odhl)/jit(nested_odhl)/gt\" stack_frame_id=7}\n  %constant.12 = f32[] constant(3), metadata={op_name=\"jit(nested_odhl)/jit(nested_odhl)\"}\n  %mul.17 = f32[1024]{0} broadcast(%constant.12), dimensions={}, metadata={op_name=\"jit(nested_odhl)/jit(nested_odhl)/mul\" stack_frame_id=9}\n  %mul.16 = f32[1024]{0} multiply(%param_0.1, %mul.17), metadata={op_name=\"jit(nested_odhl)/jit(nested_odhl)/mul\" stack_frame_id=9}\n  %constant.11 = f32[] constant(2), metadata={op_name=\"jit(nested_odhl)/jit(nested_odhl)\"}\n  %mul.15 = f32[1024]{0} broadcast(%constant.11), dimensions={}, metadata={op_name=\"jit(nested_odhl)/jit(nested_odhl)/mul\" stack_frame_id=8}\n  %mul.14 = f32[1024]{0} multiply(%param_0.1, %mul.15), metadata={op_name=\"jit(nested_odhl)/jit(nested_odhl)/mul\" stack_frame_id=8}\n  %select_n.7 = f32[1024]{0} select(%gt.8, %mul.16, %mul.14), metadata={op_name=\"jit(nested_odhl)/jit(nested_odhl)/jit(_where)/select_n\" stack_frame_id=11}\n  %constant.10 = f32[] constant(0.5), metadata={op_name=\"jit(nested_odhl)/jit(nested_odhl)\"}\n  %mul.13 = f32[1024]{0} broadcast(%constant.10), dimensions={}, metadata={op_name=\"jit(nested_odhl)/jit(nested_odhl)/mul\" stack_frame_id=6}\n  %mul.12 = f32[1024]{0} multiply(%param_0.1, %mul.13), metadata={op_name=\"jit(nested_odhl)/jit(nested_odhl)/mul\" stack_frame_id=6}\n  ROOT %select_n.6 = f32[1024]{0} select(%gt.10, %select_n.7, %mul.12), metadata={op_name=\"jit(nested_odhl)/jit(nested_odhl)/jit(_where)/select_n\" stack_frame_id=11}\n}\n\nENTRY %main.3 (a.1: f32[1024]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  ROOT %multiply_select_fusion = f32[1024]{0} fusion(%a.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(nested_odhl)/jit(nested_odhl)/jit(_where)/select_n\" stack_frame_id=11}\n}\n\n", "ptx": null}
{"id": 53, "kernel_name": "nested_tnzo", "python": "@jax.jit\ndef nested_tnzo(a):\n    # Nested where: if a > t1 then (if a > t2 then ... else ...) else ...\n    inner_true = a * 3.0\n    inner_false = a * 2.0\n    outer_true = jnp.where(a > 0.63, inner_true, inner_false)\n    outer_false = a * 0.5\n    return jnp.where(a > 0.44, outer_true, outer_false)", "type": "generate_nested_branch_kernel", "hlo": "module @jit_nested_tnzo attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @nested_tnzo(%arg0) : (tensor<1024xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @nested_tnzo(%arg0: tensor<1024xf32>) -> tensor<1024xf32> {\n    %cst = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %1 = stablehlo.multiply %arg0, %0 : tensor<1024xf32>\n    %cst_0 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<1024xf32>\n    %cst_1 = stablehlo.constant dense<6.300000e-01> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %5 = stablehlo.compare  GT, %arg0, %4,  FLOAT : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xi1>\n    %6 = call @_where(%5, %1, %3) : (tensor<1024xi1>, tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>\n    %cst_2 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %8 = stablehlo.multiply %arg0, %7 : tensor<1024xf32>\n    %cst_3 = stablehlo.constant dense<4.400000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %10 = stablehlo.compare  GT, %arg0, %9,  FLOAT : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xi1>\n    %11 = call @_where(%10, %6, %8) : (tensor<1024xi1>, tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>\n    return %11 : tensor<1024xf32>\n  }\n  func.func private @_where(%arg0: tensor<1024xi1>, %arg1: tensor<1024xf32>, %arg2: tensor<1024xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<1024xi1>, tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_nested_tnzo, is_scheduled=true, entry_computation_layout={(f32[1024]{0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmpzjbc0wl6.py\"\n4 \"/tmp/tmp7nvd0zdg.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"nested_tnzo\"\n6 \"nested_odsh\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=11 end_line=11 column=21 end_column=29}\n6 {file_name_id=3 function_name_id=5 line=10 end_line=10 column=18 end_column=25}\n7 {file_name_id=3 function_name_id=5 line=9 end_line=9 column=27 end_column=35}\n8 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=18 end_column=25}\n9 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=17 end_column=24}\n10 {file_name_id=3 function_name_id=5 line=9 end_line=9 column=17 end_column=61}\n11 {file_name_id=4 function_name_id=6 line=9 end_line=9 column=17 end_column=61}\n12 {file_name_id=3 function_name_id=5 line=11 end_line=11 column=11 end_column=55}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=5}\n8 {file_location_id=8 parent_frame_id=5}\n9 {file_location_id=9 parent_frame_id=5}\n10 {file_location_id=10 parent_frame_id=5}\n11 {file_location_id=11 parent_frame_id=5}\n12 {file_location_id=12 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[1024]) -> f32[1024] {\n  %param_0.1 = f32[1024]{0} parameter(0)\n  %constant.14 = f32[] constant(0.44), metadata={op_name=\"jit(nested_tnzo)/jit(nested_tnzo)\"}\n  %gt.11 = f32[1024]{0} broadcast(%constant.14), dimensions={}, metadata={op_name=\"jit(nested_tnzo)/jit(nested_tnzo)/gt\" stack_frame_id=5}\n  %gt.10 = pred[1024]{0} compare(%param_0.1, %gt.11), direction=GT, metadata={op_name=\"jit(nested_tnzo)/jit(nested_tnzo)/gt\" stack_frame_id=5}\n  %constant.13 = f32[] constant(0.63), metadata={op_name=\"jit(nested_tnzo)/jit(nested_tnzo)\"}\n  %gt.9 = f32[1024]{0} broadcast(%constant.13), dimensions={}, metadata={op_name=\"jit(nested_tnzo)/jit(nested_tnzo)/gt\" stack_frame_id=7}\n  %gt.8 = pred[1024]{0} compare(%param_0.1, %gt.9), direction=GT, metadata={op_name=\"jit(nested_tnzo)/jit(nested_tnzo)/gt\" stack_frame_id=7}\n  %constant.12 = f32[] constant(3), metadata={op_name=\"jit(nested_tnzo)/jit(nested_tnzo)\"}\n  %mul.17 = f32[1024]{0} broadcast(%constant.12), dimensions={}, metadata={op_name=\"jit(nested_tnzo)/jit(nested_tnzo)/mul\" stack_frame_id=9}\n  %mul.16 = f32[1024]{0} multiply(%param_0.1, %mul.17), metadata={op_name=\"jit(nested_tnzo)/jit(nested_tnzo)/mul\" stack_frame_id=9}\n  %constant.11 = f32[] constant(2), metadata={op_name=\"jit(nested_tnzo)/jit(nested_tnzo)\"}\n  %mul.15 = f32[1024]{0} broadcast(%constant.11), dimensions={}, metadata={op_name=\"jit(nested_tnzo)/jit(nested_tnzo)/mul\" stack_frame_id=8}\n  %mul.14 = f32[1024]{0} multiply(%param_0.1, %mul.15), metadata={op_name=\"jit(nested_tnzo)/jit(nested_tnzo)/mul\" stack_frame_id=8}\n  %select_n.7 = f32[1024]{0} select(%gt.8, %mul.16, %mul.14), metadata={op_name=\"jit(nested_tnzo)/jit(nested_tnzo)/jit(_where)/select_n\" stack_frame_id=11}\n  %constant.10 = f32[] constant(0.5), metadata={op_name=\"jit(nested_tnzo)/jit(nested_tnzo)\"}\n  %mul.13 = f32[1024]{0} broadcast(%constant.10), dimensions={}, metadata={op_name=\"jit(nested_tnzo)/jit(nested_tnzo)/mul\" stack_frame_id=6}\n  %mul.12 = f32[1024]{0} multiply(%param_0.1, %mul.13), metadata={op_name=\"jit(nested_tnzo)/jit(nested_tnzo)/mul\" stack_frame_id=6}\n  ROOT %select_n.6 = f32[1024]{0} select(%gt.10, %select_n.7, %mul.12), metadata={op_name=\"jit(nested_tnzo)/jit(nested_tnzo)/jit(_where)/select_n\" stack_frame_id=11}\n}\n\nENTRY %main.3 (a.1: f32[1024]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  ROOT %multiply_select_fusion = f32[1024]{0} fusion(%a.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(nested_tnzo)/jit(nested_tnzo)/jit(_where)/select_n\" stack_frame_id=11}\n}\n\n", "ptx": null}
{"id": 54, "kernel_name": "reduce_jxkf", "python": "@jax.jit\ndef reduce_jxkf(a):\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "hlo": "module @jit_reduce_jxkf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %0 = call @reduce_jxkf(%arg0) : (tensor<1024xf32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n  func.func private @reduce_jxkf(%arg0: tensor<1024xf32>) -> tensor<f32> {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<1024xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "llvm": "HloModule jit_reduce_jxkf, is_scheduled=true, entry_computation_layout={(f32[1024]{0})->f32[]}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmpfzactvnq.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"reduce_jxkf\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=11 end_column=21}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n\n\n%region_0.1 (reduce_sum.3: f32[], reduce_sum.4: f32[]) -> f32[] {\n  %reduce_sum.3 = f32[] parameter(0), metadata={op_name=\"reduce_sum\"}\n  %reduce_sum.4 = f32[] parameter(1), metadata={op_name=\"reduce_sum\"}\n  ROOT %reduce_sum.5 = f32[] add(%reduce_sum.3, %reduce_sum.4), metadata={op_name=\"reduce_sum\" stack_frame_id=5}\n}\n\n%wrapped_reduce-window_computation (param_0: f32[1024], param_1: f32[]) -> f32[32] {\n  %param_0 = f32[1024]{0} parameter(0)\n  %param_1 = f32[] parameter(1)\n  ROOT %reduce-window.1 = f32[32]{0} reduce-window(%param_0, %param_1), window={size=32 stride=32}, to_apply=%region_0.1\n}\n\n%region_0.1.clone (reduce_sum.1: f32[], reduce_sum.2: f32[]) -> f32[] {\n  %reduce_sum.1 = f32[] parameter(0), metadata={op_name=\"reduce_sum\"}\n  %reduce_sum.2 = f32[] parameter(1), metadata={op_name=\"reduce_sum\"}\n  ROOT %reduce_sum.6 = f32[] add(%reduce_sum.1, %reduce_sum.2), metadata={op_name=\"reduce_sum\" stack_frame_id=5}\n}\n\n%wrapped_reduce_computation (param_0.1: f32[32], param_1.1: f32[]) -> f32[] {\n  %param_0.1 = f32[32]{0} parameter(0)\n  %param_1.1 = f32[] parameter(1)\n  ROOT %reduce_sum.8 = f32[] reduce(%param_0.1, %param_1.1), dimensions={0}, to_apply=%region_0.1.clone, metadata={op_name=\"jit(reduce_jxkf)/jit(reduce_jxkf)/reduce_sum\" stack_frame_id=5}\n}\n\nENTRY %main.3 (a.1: f32[1024]) -> f32[] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  %constant.0 = f32[] constant(0), metadata={op_name=\"jit(reduce_jxkf)/jit(reduce_jxkf)\"}\n  %wrapped_reduce-window = f32[32]{0} fusion(%a.1, %constant.0), kind=kLoop, calls=%wrapped_reduce-window_computation\n  ROOT %wrapped_reduce = f32[] fusion(%wrapped_reduce-window, %constant.0), kind=kLoop, calls=%wrapped_reduce_computation, metadata={op_name=\"jit(reduce_jxkf)/jit(reduce_jxkf)/reduce_sum\" stack_frame_id=5}\n}\n\n", "ptx": null}
{"id": 55, "kernel_name": "branch_fujv", "python": "@jax.jit\ndef branch_fujv(a):\n    return jnp.where(a > 0.59, a * 1.9, a + 0.6)", "type": "generate_branch_kernel", "hlo": "module @jit_branch_fujv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @branch_fujv(%arg0) : (tensor<1024xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @branch_fujv(%arg0: tensor<1024xf32>) -> tensor<1024xf32> {\n    %cst = stablehlo.constant dense<5.900000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xi1>\n    %cst_0 = stablehlo.constant dense<1.900000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<1024xf32>\n    %cst_1 = stablehlo.constant dense<6.000000e-01> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<1024xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<1024xi1>, tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>\n    return %6 : tensor<1024xf32>\n  }\n  func.func private @_where(%arg0: tensor<1024xi1>, %arg1: tensor<1024xf32>, %arg2: tensor<1024xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<1024xi1>, tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_branch_fujv, is_scheduled=true, entry_computation_layout={(f32[1024]{0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmpo3p9oqxi.py\"\n4 \"/tmp/tmp7nvd0zdg.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"branch_fujv\"\n6 \"nested_odsh\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=40 end_column=47}\n6 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=31 end_column=38}\n7 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=21 end_column=29}\n8 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=11 end_column=48}\n9 {file_name_id=4 function_name_id=6 line=9 end_line=9 column=17 end_column=61}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=5}\n8 {file_location_id=8 parent_frame_id=5}\n9 {file_location_id=9 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[1024]) -> f32[1024] {\n  %param_0.1 = f32[1024]{0} parameter(0)\n  %constant.8 = f32[] constant(0.59), metadata={op_name=\"jit(branch_fujv)/jit(branch_fujv)\"}\n  %gt.5 = f32[1024]{0} broadcast(%constant.8), dimensions={}, metadata={op_name=\"jit(branch_fujv)/jit(branch_fujv)/gt\" stack_frame_id=7}\n  %gt.4 = pred[1024]{0} compare(%param_0.1, %gt.5), direction=GT, metadata={op_name=\"jit(branch_fujv)/jit(branch_fujv)/gt\" stack_frame_id=7}\n  %constant.7 = f32[] constant(1.9), metadata={op_name=\"jit(branch_fujv)/jit(branch_fujv)\"}\n  %mul.5 = f32[1024]{0} broadcast(%constant.7), dimensions={}, metadata={op_name=\"jit(branch_fujv)/jit(branch_fujv)/mul\" stack_frame_id=6}\n  %mul.4 = f32[1024]{0} multiply(%param_0.1, %mul.5), metadata={op_name=\"jit(branch_fujv)/jit(branch_fujv)/mul\" stack_frame_id=6}\n  %constant.6 = f32[] constant(0.6), metadata={op_name=\"jit(branch_fujv)/jit(branch_fujv)\"}\n  %add.5 = f32[1024]{0} broadcast(%constant.6), dimensions={}, metadata={op_name=\"jit(branch_fujv)/jit(branch_fujv)/add\" stack_frame_id=5}\n  %add.4 = f32[1024]{0} add(%param_0.1, %add.5), metadata={op_name=\"jit(branch_fujv)/jit(branch_fujv)/add\" stack_frame_id=5}\n  ROOT %select_n.3 = f32[1024]{0} select(%gt.4, %mul.4, %add.4), metadata={op_name=\"jit(branch_fujv)/jit(branch_fujv)/jit(_where)/select_n\" stack_frame_id=9}\n}\n\nENTRY %main.3 (a.1: f32[1024]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  ROOT %add_select_fusion = f32[1024]{0} fusion(%a.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(branch_fujv)/jit(branch_fujv)/jit(_where)/select_n\" stack_frame_id=9}\n}\n\n", "ptx": null}
{"id": 56, "kernel_name": "reduce_jvjb", "python": "@jax.jit\ndef reduce_jvjb(a):\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "hlo": "module @jit_reduce_jvjb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %0 = call @reduce_jvjb(%arg0) : (tensor<1024xf32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n  func.func private @reduce_jvjb(%arg0: tensor<1024xf32>) -> tensor<f32> {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<1024xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "llvm": "HloModule jit_reduce_jvjb, is_scheduled=true, entry_computation_layout={(f32[1024]{0})->f32[]}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmpl_at7mjk.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"reduce_jvjb\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=11 end_column=21}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n\n\n%region_0.1 (reduce_sum.3: f32[], reduce_sum.4: f32[]) -> f32[] {\n  %reduce_sum.3 = f32[] parameter(0), metadata={op_name=\"reduce_sum\"}\n  %reduce_sum.4 = f32[] parameter(1), metadata={op_name=\"reduce_sum\"}\n  ROOT %reduce_sum.5 = f32[] add(%reduce_sum.3, %reduce_sum.4), metadata={op_name=\"reduce_sum\" stack_frame_id=5}\n}\n\n%wrapped_reduce-window_computation (param_0: f32[1024], param_1: f32[]) -> f32[32] {\n  %param_0 = f32[1024]{0} parameter(0)\n  %param_1 = f32[] parameter(1)\n  ROOT %reduce-window.1 = f32[32]{0} reduce-window(%param_0, %param_1), window={size=32 stride=32}, to_apply=%region_0.1\n}\n\n%region_0.1.clone (reduce_sum.1: f32[], reduce_sum.2: f32[]) -> f32[] {\n  %reduce_sum.1 = f32[] parameter(0), metadata={op_name=\"reduce_sum\"}\n  %reduce_sum.2 = f32[] parameter(1), metadata={op_name=\"reduce_sum\"}\n  ROOT %reduce_sum.6 = f32[] add(%reduce_sum.1, %reduce_sum.2), metadata={op_name=\"reduce_sum\" stack_frame_id=5}\n}\n\n%wrapped_reduce_computation (param_0.1: f32[32], param_1.1: f32[]) -> f32[] {\n  %param_0.1 = f32[32]{0} parameter(0)\n  %param_1.1 = f32[] parameter(1)\n  ROOT %reduce_sum.8 = f32[] reduce(%param_0.1, %param_1.1), dimensions={0}, to_apply=%region_0.1.clone, metadata={op_name=\"jit(reduce_jvjb)/jit(reduce_jvjb)/reduce_sum\" stack_frame_id=5}\n}\n\nENTRY %main.3 (a.1: f32[1024]) -> f32[] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  %constant.0 = f32[] constant(0), metadata={op_name=\"jit(reduce_jvjb)/jit(reduce_jvjb)\"}\n  %wrapped_reduce-window = f32[32]{0} fusion(%a.1, %constant.0), kind=kLoop, calls=%wrapped_reduce-window_computation\n  ROOT %wrapped_reduce = f32[] fusion(%wrapped_reduce-window, %constant.0), kind=kLoop, calls=%wrapped_reduce_computation, metadata={op_name=\"jit(reduce_jvjb)/jit(reduce_jvjb)/reduce_sum\" stack_frame_id=5}\n}\n\n", "ptx": null}
{"id": 57, "kernel_name": "vec_kfeg", "python": "@jax.jit\ndef vec_kfeg(a, b):\n    # vector dot product along last axis\n    return jnp.sum(a * b, axis=-1)", "type": "generate_vector_kernel", "hlo": "module @jit_vec_kfeg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024x3xf32>, %arg1: tensor<1024x3xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @vec_kfeg(%arg0, %arg1) : (tensor<1024x3xf32>, tensor<1024x3xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @vec_kfeg(%arg0: tensor<1024x3xf32>, %arg1: tensor<1024x3xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<1024x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<1024x3xf32>, tensor<f32>) -> tensor<1024xf32>\n    return %1 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_vec_kfeg, is_scheduled=true, entry_computation_layout={(f32[1024,3]{1,0}, f32[1024,3]{1,0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmp_wjx850p.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"vec_kfeg\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=11 end_column=34}\n6 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=19 end_column=24}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n\n\n%region_0.1 (reduce_sum.3: f32[], reduce_sum.4: f32[]) -> f32[] {\n  %reduce_sum.3 = f32[] parameter(0), metadata={op_name=\"reduce_sum\"}\n  %reduce_sum.4 = f32[] parameter(1), metadata={op_name=\"reduce_sum\"}\n  ROOT %reduce_sum.5 = f32[] add(%reduce_sum.3, %reduce_sum.4), metadata={op_name=\"reduce_sum\" stack_frame_id=5}\n}\n\n%fused_computation (param_0.2: f32[1024,3], param_1.2: f32[1024,3]) -> f32[1024] {\n  %param_0.2 = f32[1024,3]{1,0} parameter(0)\n  %param_1.2 = f32[1024,3]{1,0} parameter(1)\n  %mul.2 = f32[1024,3]{1,0} multiply(%param_0.2, %param_1.2), metadata={op_name=\"jit(vec_kfeg)/jit(vec_kfeg)/mul\" stack_frame_id=6}\n  %constant.2 = f32[] constant(0), metadata={op_name=\"jit(vec_kfeg)/jit(vec_kfeg)\"}\n  ROOT %reduce_sum.1 = f32[1024]{0} reduce(%mul.2, %constant.2), dimensions={1}, to_apply=%region_0.1, metadata={op_name=\"jit(vec_kfeg)/jit(vec_kfeg)/reduce_sum\" stack_frame_id=5}\n}\n\nENTRY %main.3 (a.1: f32[1024,3], b.1: f32[1024,3]) -> f32[1024] {\n  %a.1 = f32[1024,3]{1,0} parameter(0), metadata={op_name=\"a\"}\n  %b.1 = f32[1024,3]{1,0} parameter(1), metadata={op_name=\"b\"}\n  ROOT %multiply_reduce_fusion = f32[1024]{0} fusion(%a.1, %b.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(vec_kfeg)/jit(vec_kfeg)/reduce_sum\" stack_frame_id=5}\n}\n\n", "ptx": null}
{"id": 58, "kernel_name": "unary_dlus", "python": "@jax.jit\ndef unary_dlus(a):\n    return jnp.sin(a)", "type": "generate_unary_kernel", "hlo": "module @jit_unary_dlus attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @unary_dlus(%arg0) : (tensor<1024xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @unary_dlus(%arg0: tensor<1024xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.sine %arg0 : tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_unary_dlus, is_scheduled=true, entry_computation_layout={(f32[1024]{0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmpi_j5ln4a.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"unary_dlus\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=11 end_column=21}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n\n\n%wrapped_sine_computation (param_0: f32[1024]) -> f32[1024] {\n  %param_0 = f32[1024]{0} parameter(0)\n  ROOT %sin.2 = f32[1024]{0} sine(%param_0), metadata={op_name=\"jit(unary_dlus)/jit(unary_dlus)/sin\" stack_frame_id=5}\n}\n\nENTRY %main.2 (a.1: f32[1024]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  ROOT %wrapped_sine = f32[1024]{0} fusion(%a.1), kind=kLoop, calls=%wrapped_sine_computation, metadata={op_name=\"jit(unary_dlus)/jit(unary_dlus)/sin\" stack_frame_id=5}\n}\n\n", "ptx": null}
{"id": 59, "kernel_name": "compound_pfzy", "python": "@jax.jit\ndef compound_pfzy(a, b, scale):\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "hlo": "module @jit_compound_pfzy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>, %arg1: tensor<1024xf32>, %arg2: tensor<f32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @compound_pfzy(%arg0, %arg1, %arg2) : (tensor<1024xf32>, tensor<1024xf32>, tensor<f32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @compound_pfzy(%arg0: tensor<1024xf32>, %arg1: tensor<1024xf32>, %arg2: tensor<f32>) -> tensor<1024xf32> {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<1024xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<1024xf32>\n    %4 = stablehlo.floor %3 : tensor<1024xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<1024xf32>\n    %6 = stablehlo.abs %5 : tensor<1024xf32>\n    return %6 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_compound_pfzy, is_scheduled=true, entry_computation_layout={(f32[1024]{0}, f32[1024]{0}, f32[])->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmpygpmyias.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"compound_pfzy\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=14 end_column=19}\n6 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=13 end_column=28}\n7 {file_name_id=3 function_name_id=5 line=9 end_line=9 column=22 end_column=39}\n8 {file_name_id=3 function_name_id=5 line=9 end_line=9 column=13 end_column=39}\n9 {file_name_id=3 function_name_id=5 line=10 end_line=10 column=11 end_column=26}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=5}\n8 {file_location_id=8 parent_frame_id=5}\n9 {file_location_id=9 parent_frame_id=5}\n\n\n%fused_computation (param_0.3: f32[], param_1.3: f32[1024], param_2: f32[1024]) -> f32[1024] {\n  %param_1.3 = f32[1024]{0} parameter(1)\n  %param_2 = f32[1024]{0} parameter(2)\n  %add.2 = f32[1024]{0} add(%param_1.3, %param_2), metadata={op_name=\"jit(compound_pfzy)/jit(compound_pfzy)/add\" stack_frame_id=5}\n  %param_0.3 = f32[] parameter(0)\n  %mul.5 = f32[1024]{0} broadcast(%param_0.3), dimensions={}, metadata={op_name=\"jit(compound_pfzy)/jit(compound_pfzy)/mul\" stack_frame_id=6}\n  %mul.4 = f32[1024]{0} multiply(%add.2, %mul.5), metadata={op_name=\"jit(compound_pfzy)/jit(compound_pfzy)/mul\" stack_frame_id=6}\n  %floor.2 = f32[1024]{0} floor(%mul.4), metadata={op_name=\"jit(compound_pfzy)/jit(compound_pfzy)/floor\" stack_frame_id=7}\n  %sub.2 = f32[1024]{0} subtract(%mul.4, %floor.2), metadata={op_name=\"jit(compound_pfzy)/jit(compound_pfzy)/sub\" stack_frame_id=8}\n  ROOT %abs.2 = f32[1024]{0} abs(%sub.2), metadata={op_name=\"jit(compound_pfzy)/jit(compound_pfzy)/abs\" stack_frame_id=9}\n}\n\nENTRY %main.2 (a.1: f32[1024], b.1: f32[1024], scale.1: f32[]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  %b.1 = f32[1024]{0} parameter(1), metadata={op_name=\"b\"}\n  %scale.1 = f32[] parameter(2), metadata={op_name=\"scale\"}\n  ROOT %subtract_abs_fusion = f32[1024]{0} fusion(%scale.1, %a.1, %b.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(compound_pfzy)/jit(compound_pfzy)/abs\" stack_frame_id=9}\n}\n\n", "ptx": null}
{"id": 60, "kernel_name": "unary_dqes", "python": "@jax.jit\ndef unary_dqes(a):\n    return jnp.abs(a)", "type": "generate_unary_kernel", "hlo": "module @jit_unary_dqes attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @unary_dqes(%arg0) : (tensor<1024xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @unary_dqes(%arg0: tensor<1024xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.abs %arg0 : tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_unary_dqes, is_scheduled=true, entry_computation_layout={(f32[1024]{0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmpnmuh0ssb.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"unary_dqes\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=11 end_column=21}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n\n\n%wrapped_abs_computation (param_0: f32[1024]) -> f32[1024] {\n  %param_0 = f32[1024]{0} parameter(0)\n  ROOT %abs.2 = f32[1024]{0} abs(%param_0), metadata={op_name=\"jit(unary_dqes)/jit(unary_dqes)/abs\" stack_frame_id=5}\n}\n\nENTRY %main.2 (a.1: f32[1024]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  ROOT %wrapped_abs = f32[1024]{0} fusion(%a.1), kind=kLoop, calls=%wrapped_abs_computation, metadata={op_name=\"jit(unary_dqes)/jit(unary_dqes)/abs\" stack_frame_id=5}\n}\n\n", "ptx": null}
{"id": 61, "kernel_name": "multi_zlst", "python": "@jax.jit\ndef multi_zlst(a, b):\n    temp1 = a + b\n    temp2 = jnp.sqrt(temp1)\n    return temp2 * a", "type": "generate_multi_statement_kernel", "hlo": "module @jit_multi_zlst attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>, %arg1: tensor<1024xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @multi_zlst(%arg0, %arg1) : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @multi_zlst(%arg0: tensor<1024xf32>, %arg1: tensor<1024xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<1024xf32>\n    %1 = stablehlo.sqrt %0 : tensor<1024xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<1024xf32>\n    return %2 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_multi_zlst, is_scheduled=true, entry_computation_layout={(f32[1024]{0}, f32[1024]{0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmpw39n6_5g.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"multi_zlst\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=12 end_column=17}\n6 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=12 end_column=27}\n7 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=11 end_column=20}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[1024], param_1.2: f32[1024]) -> f32[1024] {\n  %param_0.1 = f32[1024]{0} parameter(0)\n  %param_1.2 = f32[1024]{0} parameter(1)\n  %add.2 = f32[1024]{0} add(%param_0.1, %param_1.2), metadata={op_name=\"jit(multi_zlst)/jit(multi_zlst)/add\" stack_frame_id=5}\n  %sqrt.2 = f32[1024]{0} sqrt(%add.2), metadata={op_name=\"jit(multi_zlst)/jit(multi_zlst)/sqrt\" stack_frame_id=6}\n  ROOT %mul.2 = f32[1024]{0} multiply(%sqrt.2, %param_0.1), metadata={op_name=\"jit(multi_zlst)/jit(multi_zlst)/mul\" stack_frame_id=7}\n}\n\nENTRY %main.2 (a.1: f32[1024], b.1: f32[1024]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  %b.1 = f32[1024]{0} parameter(1), metadata={op_name=\"b\"}\n  ROOT %sqrt_multiply_fusion = f32[1024]{0} fusion(%a.1, %b.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(multi_zlst)/jit(multi_zlst)/mul\" stack_frame_id=7}\n}\n\n", "ptx": null}
{"id": 62, "kernel_name": "elementwise_ywtf", "python": "@jax.jit\ndef elementwise_ywtf(a, b):\n    return a - b", "type": "generate_simple_elementwise", "hlo": "module @jit_elementwise_ywtf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>, %arg1: tensor<1024xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @elementwise_ywtf(%arg0, %arg1) : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @elementwise_ywtf(%arg0: tensor<1024xf32>, %arg1: tensor<1024xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_elementwise_ywtf, is_scheduled=true, entry_computation_layout={(f32[1024]{0}, f32[1024]{0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmp15kyizew.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"elementwise_ywtf\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=11 end_column=16}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n\n\n%wrapped_subtract_computation (param_0: f32[1024], param_1: f32[1024]) -> f32[1024] {\n  %param_0 = f32[1024]{0} parameter(0)\n  %param_1 = f32[1024]{0} parameter(1)\n  ROOT %sub.2 = f32[1024]{0} subtract(%param_0, %param_1), metadata={op_name=\"jit(elementwise_ywtf)/jit(elementwise_ywtf)/sub\" stack_frame_id=5}\n}\n\nENTRY %main.2 (a.1: f32[1024], b.1: f32[1024]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  %b.1 = f32[1024]{0} parameter(1), metadata={op_name=\"b\"}\n  ROOT %wrapped_subtract = f32[1024]{0} fusion(%a.1, %b.1), kind=kLoop, calls=%wrapped_subtract_computation, metadata={op_name=\"jit(elementwise_ywtf)/jit(elementwise_ywtf)/sub\" stack_frame_id=5}\n}\n\n", "ptx": null}
{"id": 63, "kernel_name": "compound_wjuy", "python": "@jax.jit\ndef compound_wjuy(a, b, scale):\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "hlo": "module @jit_compound_wjuy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>, %arg1: tensor<1024xf32>, %arg2: tensor<f32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @compound_wjuy(%arg0, %arg1, %arg2) : (tensor<1024xf32>, tensor<1024xf32>, tensor<f32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @compound_wjuy(%arg0: tensor<1024xf32>, %arg1: tensor<1024xf32>, %arg2: tensor<f32>) -> tensor<1024xf32> {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<1024xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<1024xf32>\n    %4 = stablehlo.floor %3 : tensor<1024xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<1024xf32>\n    %6 = stablehlo.abs %5 : tensor<1024xf32>\n    return %6 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_compound_wjuy, is_scheduled=true, entry_computation_layout={(f32[1024]{0}, f32[1024]{0}, f32[])->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmp67v8fxsq.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"compound_wjuy\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=14 end_column=19}\n6 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=13 end_column=28}\n7 {file_name_id=3 function_name_id=5 line=9 end_line=9 column=22 end_column=39}\n8 {file_name_id=3 function_name_id=5 line=9 end_line=9 column=13 end_column=39}\n9 {file_name_id=3 function_name_id=5 line=10 end_line=10 column=11 end_column=26}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=5}\n8 {file_location_id=8 parent_frame_id=5}\n9 {file_location_id=9 parent_frame_id=5}\n\n\n%fused_computation (param_0.3: f32[], param_1.3: f32[1024], param_2: f32[1024]) -> f32[1024] {\n  %param_1.3 = f32[1024]{0} parameter(1)\n  %param_2 = f32[1024]{0} parameter(2)\n  %add.2 = f32[1024]{0} add(%param_1.3, %param_2), metadata={op_name=\"jit(compound_wjuy)/jit(compound_wjuy)/add\" stack_frame_id=5}\n  %param_0.3 = f32[] parameter(0)\n  %mul.5 = f32[1024]{0} broadcast(%param_0.3), dimensions={}, metadata={op_name=\"jit(compound_wjuy)/jit(compound_wjuy)/mul\" stack_frame_id=6}\n  %mul.4 = f32[1024]{0} multiply(%add.2, %mul.5), metadata={op_name=\"jit(compound_wjuy)/jit(compound_wjuy)/mul\" stack_frame_id=6}\n  %floor.2 = f32[1024]{0} floor(%mul.4), metadata={op_name=\"jit(compound_wjuy)/jit(compound_wjuy)/floor\" stack_frame_id=7}\n  %sub.2 = f32[1024]{0} subtract(%mul.4, %floor.2), metadata={op_name=\"jit(compound_wjuy)/jit(compound_wjuy)/sub\" stack_frame_id=8}\n  ROOT %abs.2 = f32[1024]{0} abs(%sub.2), metadata={op_name=\"jit(compound_wjuy)/jit(compound_wjuy)/abs\" stack_frame_id=9}\n}\n\nENTRY %main.2 (a.1: f32[1024], b.1: f32[1024], scale.1: f32[]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  %b.1 = f32[1024]{0} parameter(1), metadata={op_name=\"b\"}\n  %scale.1 = f32[] parameter(2), metadata={op_name=\"scale\"}\n  ROOT %subtract_abs_fusion = f32[1024]{0} fusion(%scale.1, %a.1, %b.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(compound_wjuy)/jit(compound_wjuy)/abs\" stack_frame_id=9}\n}\n\n", "ptx": null}
{"id": 64, "kernel_name": "multi_sxay", "python": "@jax.jit\ndef multi_sxay(a, b):\n    temp1 = a - b\n    temp2 = jnp.sin(temp1)\n    return temp2 + a", "type": "generate_multi_statement_kernel", "hlo": "module @jit_multi_sxay attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>, %arg1: tensor<1024xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @multi_sxay(%arg0, %arg1) : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @multi_sxay(%arg0: tensor<1024xf32>, %arg1: tensor<1024xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<1024xf32>\n    %1 = stablehlo.sine %0 : tensor<1024xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<1024xf32>\n    return %2 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_multi_sxay, is_scheduled=true, entry_computation_layout={(f32[1024]{0}, f32[1024]{0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmpx25isgpb.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"multi_sxay\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=12 end_column=17}\n6 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=12 end_column=26}\n7 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=11 end_column=20}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[1024], param_1.2: f32[1024]) -> f32[1024] {\n  %param_0.1 = f32[1024]{0} parameter(0)\n  %param_1.2 = f32[1024]{0} parameter(1)\n  %sub.2 = f32[1024]{0} subtract(%param_0.1, %param_1.2), metadata={op_name=\"jit(multi_sxay)/jit(multi_sxay)/sub\" stack_frame_id=5}\n  %sin.2 = f32[1024]{0} sine(%sub.2), metadata={op_name=\"jit(multi_sxay)/jit(multi_sxay)/sin\" stack_frame_id=6}\n  ROOT %add.2 = f32[1024]{0} add(%sin.2, %param_0.1), metadata={op_name=\"jit(multi_sxay)/jit(multi_sxay)/add\" stack_frame_id=7}\n}\n\nENTRY %main.2 (a.1: f32[1024], b.1: f32[1024]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  %b.1 = f32[1024]{0} parameter(1), metadata={op_name=\"b\"}\n  ROOT %sine_add_fusion = f32[1024]{0} fusion(%a.1, %b.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(multi_sxay)/jit(multi_sxay)/add\" stack_frame_id=7}\n}\n\n", "ptx": null}
{"id": 65, "kernel_name": "branch_gpzr", "python": "@jax.jit\ndef branch_gpzr(a):\n    return jnp.where(a > -0.03, a - 1.6, a + 1.9)", "type": "generate_branch_kernel", "hlo": "module @jit_branch_gpzr attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @branch_gpzr(%arg0) : (tensor<1024xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @branch_gpzr(%arg0: tensor<1024xf32>) -> tensor<1024xf32> {\n    %cst = stablehlo.constant dense<-3.000000e-02> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xi1>\n    %cst_0 = stablehlo.constant dense<1.600000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<1024xf32>\n    %cst_1 = stablehlo.constant dense<1.900000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<1024xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<1024xi1>, tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>\n    return %6 : tensor<1024xf32>\n  }\n  func.func private @_where(%arg0: tensor<1024xi1>, %arg1: tensor<1024xf32>, %arg2: tensor<1024xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<1024xi1>, tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_branch_gpzr, is_scheduled=true, entry_computation_layout={(f32[1024]{0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmpwot08wb0.py\"\n4 \"/tmp/tmp7nvd0zdg.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"branch_gpzr\"\n6 \"nested_odsh\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=41 end_column=48}\n6 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=32 end_column=39}\n7 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=21 end_column=30}\n8 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=11 end_column=49}\n9 {file_name_id=4 function_name_id=6 line=9 end_line=9 column=17 end_column=61}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=5}\n8 {file_location_id=8 parent_frame_id=5}\n9 {file_location_id=9 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[1024]) -> f32[1024] {\n  %param_0.1 = f32[1024]{0} parameter(0)\n  %constant.9 = f32[] constant(-0.03), metadata={op_name=\"jit(branch_gpzr)/jit(branch_gpzr)\"}\n  %gt.5 = f32[1024]{0} broadcast(%constant.9), dimensions={}, metadata={op_name=\"jit(branch_gpzr)/jit(branch_gpzr)/gt\" stack_frame_id=7}\n  %gt.4 = pred[1024]{0} compare(%param_0.1, %gt.5), direction=GT, metadata={op_name=\"jit(branch_gpzr)/jit(branch_gpzr)/gt\" stack_frame_id=7}\n  %constant.8 = f32[] constant(-1.6), metadata={op_name=\"jit(branch_gpzr)/jit(branch_gpzr)\"}\n  %broadcast.1 = f32[1024]{0} broadcast(%constant.8), dimensions={}, metadata={op_name=\"jit(branch_gpzr)/jit(branch_gpzr)\"}\n  %add.7 = f32[1024]{0} add(%param_0.1, %broadcast.1), metadata={op_name=\"jit(branch_gpzr)/jit(branch_gpzr)/sub\" stack_frame_id=6}\n  %constant.7 = f32[] constant(1.9), metadata={op_name=\"jit(branch_gpzr)/jit(branch_gpzr)\"}\n  %add.6 = f32[1024]{0} broadcast(%constant.7), dimensions={}, metadata={op_name=\"jit(branch_gpzr)/jit(branch_gpzr)/add\" stack_frame_id=5}\n  %add.5 = f32[1024]{0} add(%param_0.1, %add.6), metadata={op_name=\"jit(branch_gpzr)/jit(branch_gpzr)/add\" stack_frame_id=5}\n  ROOT %select_n.3 = f32[1024]{0} select(%gt.4, %add.7, %add.5), metadata={op_name=\"jit(branch_gpzr)/jit(branch_gpzr)/jit(_where)/select_n\" stack_frame_id=9}\n}\n\nENTRY %main.3 (a.1: f32[1024]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  ROOT %add_select_fusion = f32[1024]{0} fusion(%a.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(branch_gpzr)/jit(branch_gpzr)/jit(_where)/select_n\" stack_frame_id=9}\n}\n\n", "ptx": null}
{"id": 66, "kernel_name": "unary_dvwk", "python": "@jax.jit\ndef unary_dvwk(a):\n    return jnp.sin(a)", "type": "generate_unary_kernel", "hlo": "module @jit_unary_dvwk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @unary_dvwk(%arg0) : (tensor<1024xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @unary_dvwk(%arg0: tensor<1024xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.sine %arg0 : tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_unary_dvwk, is_scheduled=true, entry_computation_layout={(f32[1024]{0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmpf22fcohj.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"unary_dvwk\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=11 end_column=21}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n\n\n%wrapped_sine_computation (param_0: f32[1024]) -> f32[1024] {\n  %param_0 = f32[1024]{0} parameter(0)\n  ROOT %sin.2 = f32[1024]{0} sine(%param_0), metadata={op_name=\"jit(unary_dvwk)/jit(unary_dvwk)/sin\" stack_frame_id=5}\n}\n\nENTRY %main.2 (a.1: f32[1024]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  ROOT %wrapped_sine = f32[1024]{0} fusion(%a.1), kind=kLoop, calls=%wrapped_sine_computation, metadata={op_name=\"jit(unary_dvwk)/jit(unary_dvwk)/sin\" stack_frame_id=5}\n}\n\n", "ptx": null}
{"id": 67, "kernel_name": "loop_hloc", "python": "@jax.jit\ndef loop_hloc(a, n):\n    def body_fun(i, val):\n        return val * a\n    # Initial value matches a's shape/type but zeroed\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "hlo": "module @jit_loop_hloc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>, %arg1: tensor<i32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @loop_hloc(%arg0, %arg1) : (tensor<1024xf32>, tensor<i32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @loop_hloc(%arg0: tensor<1024xf32>, %arg1: tensor<i32>) -> tensor<1024xf32> {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<1024xf32>, tensor<i32>, tensor<i32>, tensor<1024xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.multiply %iterArg_2, %iterArg : tensor<1024xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<1024xf32>, tensor<i32>, tensor<i32>, tensor<1024xf32>\n    }\n    return %1#3 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_loop_hloc, is_scheduled=true, entry_computation_layout={(f32[1024]{0}, s32[])->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmpv753la1x.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"loop_hloc\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=9 end_line=9 column=15 end_column=32}\n6 {file_name_id=3 function_name_id=5 line=10 end_line=10 column=11 end_column=54}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n\n\n%wrapped_add_computation (param_0.1: s32[], param_1: s32[]) -> s32[] {\n  %param_0.1 = s32[] parameter(0)\n  %param_1 = s32[] parameter(1)\n  ROOT %add.0 = s32[] add(%param_0.1, %param_1), metadata={op_name=\"jit(loop_hloc)/jit(loop_hloc)/while/body/add\" stack_frame_id=6}\n}\n\n%wrapped_multiply_computation (param_0.2: f32[1024], param_1.1: f32[1024]) -> f32[1024] {\n  %param_0.2 = f32[1024]{0} parameter(0)\n  %param_1.1 = f32[1024]{0} parameter(1)\n  ROOT %mul.0 = f32[1024]{0} multiply(%param_0.2, %param_1.1), metadata={op_name=\"jit(loop_hloc)/jit(loop_hloc)/while/body/mul\" stack_frame_id=6}\n}\n\n%region_0.1 (arg_tuple.1: (s32[], f32[1024], s32[], f32[1024])) -> (s32[], f32[1024], s32[], f32[1024]) {\n  %arg_tuple.1 = (s32[], f32[1024]{0}, s32[], f32[1024]{0}) parameter(0), metadata={op_name=\"jit(loop_hloc)/jit(loop_hloc)\"}\n  %constant.3 = s32[] constant(1), metadata={op_name=\"jit(loop_hloc)/jit(loop_hloc)\"}\n  %get-tuple-element.19 = f32[1024]{0} get-tuple-element(%arg_tuple.1), index=3, metadata={op_name=\"jit(loop_hloc)/jit(loop_hloc)\"}\n  %get-tuple-element.18 = s32[] get-tuple-element(%arg_tuple.1), index=2, metadata={op_name=\"jit(loop_hloc)/jit(loop_hloc)\"}\n  %get-tuple-element.9 = f32[1024]{0} get-tuple-element(%arg_tuple.1), index=1\n  %get-tuple-element.8 = s32[] get-tuple-element(%arg_tuple.1), index=0\n  %wrapped_multiply = f32[1024]{0} fusion(%get-tuple-element.9, %get-tuple-element.19), kind=kLoop, calls=%wrapped_multiply_computation, metadata={op_name=\"jit(loop_hloc)/jit(loop_hloc)/while/body/mul\" stack_frame_id=6}\n  %wrapped_add = s32[] fusion(%get-tuple-element.8, %constant.3), kind=kLoop, calls=%wrapped_add_computation, metadata={op_name=\"jit(loop_hloc)/jit(loop_hloc)/while/body/add\" stack_frame_id=6}\n  ROOT %tuple.3 = (s32[], f32[1024]{0}, s32[], f32[1024]{0}) tuple(%wrapped_add, %wrapped_multiply, %get-tuple-element.18, %get-tuple-element.19)\n}\n\n%wrapped_compare_computation (param_0.3: s32[], param_1.2: s32[]) -> pred[] {\n  %param_0.3 = s32[] parameter(0)\n  %param_1.2 = s32[] parameter(1)\n  ROOT %lt.0 = pred[] compare(%param_0.3, %param_1.2), direction=LT, metadata={op_name=\"jit(loop_hloc)/jit(loop_hloc)/while/cond/lt\" stack_frame_id=6}\n}\n\n%region_1.2 (arg_tuple.3: (s32[], f32[1024], s32[], f32[1024])) -> pred[] {\n  %arg_tuple.3 = (s32[], f32[1024]{0}, s32[], f32[1024]{0}) parameter(0), metadata={op_name=\"jit(loop_hloc)/jit(loop_hloc)\"}\n  %get-tuple-element.14 = s32[] get-tuple-element(%arg_tuple.3), index=2, metadata={op_name=\"jit(loop_hloc)/jit(loop_hloc)\"}\n  %get-tuple-element.12 = s32[] get-tuple-element(%arg_tuple.3), index=0, metadata={op_name=\"jit(loop_hloc)/jit(loop_hloc)\"}\n  ROOT %wrapped_compare = pred[] fusion(%get-tuple-element.12, %get-tuple-element.14), kind=kLoop, calls=%wrapped_compare_computation, metadata={op_name=\"jit(loop_hloc)/jit(loop_hloc)/while/cond/lt\" stack_frame_id=6}\n}\n\n%wrapped_broadcast_computation (param_0: f32[]) -> f32[1024] {\n  %param_0 = f32[] parameter(0)\n  ROOT %broadcast_in_dim.2 = f32[1024]{0} broadcast(%param_0), dimensions={}, metadata={op_name=\"jit(loop_hloc)/jit(loop_hloc)/broadcast_in_dim\" stack_frame_id=5}\n}\n\nENTRY %main.4 (a.1: f32[1024], n.1: s32[]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  %n.1 = s32[] parameter(1), metadata={op_name=\"n\"}\n  %constant.0 = s32[] constant(0), metadata={op_name=\"jit(loop_hloc)/jit(loop_hloc)\"}\n  %constant.1 = f32[] constant(0), metadata={op_name=\"jit(loop_hloc)/jit(loop_hloc)\"}\n  %copy.6 = s32[] copy(%constant.0)\n  %wrapped_broadcast = f32[1024]{0} fusion(%constant.1), kind=kLoop, calls=%wrapped_broadcast_computation, metadata={op_name=\"jit(loop_hloc)/jit(loop_hloc)/broadcast_in_dim\" stack_frame_id=5}\n  %tuple = (s32[], f32[1024]{0}, s32[], f32[1024]{0}) tuple(%copy.6, %wrapped_broadcast, %n.1, %a.1)\n  %while.1 = (s32[], f32[1024]{0}, s32[], f32[1024]{0}) while(%tuple), condition=%region_1.2, body=%region_0.1, metadata={op_name=\"jit(loop_hloc)/jit(loop_hloc)/while\" stack_frame_id=6}\n  ROOT %while.2 = f32[1024]{0} get-tuple-element(%while.1), index=1, metadata={op_name=\"jit(loop_hloc)/jit(loop_hloc)/while\" stack_frame_id=6}\n}\n\n", "ptx": null}
{"id": 68, "kernel_name": "vec_ykvg", "python": "@jax.jit\ndef vec_ykvg(a):\n    return jnp.linalg.norm(a, axis=-1)", "type": "generate_vector_kernel", "hlo": "module @jit_vec_ykvg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024x3xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @vec_ykvg(%arg0) : (tensor<1024x3xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @vec_ykvg(%arg0: tensor<1024x3xf32>) -> tensor<1024xf32> {\n    %0 = call @norm(%arg0) : (tensor<1024x3xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @norm(%arg0: tensor<1024x3xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<1024x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<1024x3xf32>, tensor<f32>) -> tensor<1024xf32>\n    %2 = stablehlo.sqrt %1 : tensor<1024xf32>\n    return %2 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_vec_ykvg, is_scheduled=true, entry_computation_layout={(f32[1024,3]{1,0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmp6xu76383.py\"\n4 \"/tmp/tmp6o75inah.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"vec_ykvg\"\n6 \"vec_khvl\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=11 end_column=38}\n6 {file_name_id=4 function_name_id=6 line=6 end_line=6 column=11 end_column=38}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n\n\n%region_0.1 (reduce_sum.3: f32[], reduce_sum.4: f32[]) -> f32[] {\n  %reduce_sum.3 = f32[] parameter(0), metadata={op_name=\"reduce_sum\"}\n  %reduce_sum.4 = f32[] parameter(1), metadata={op_name=\"reduce_sum\"}\n  ROOT %reduce_sum.5 = f32[] add(%reduce_sum.3, %reduce_sum.4), metadata={op_name=\"reduce_sum\" stack_frame_id=6}\n}\n\n%fused_computation (param_0.2: f32[1024,3]) -> f32[1024] {\n  %param_0.2 = f32[1024,3]{1,0} parameter(0)\n  %mul.3 = f32[1024,3]{1,0} multiply(%param_0.2, %param_0.2), metadata={op_name=\"jit(vec_ykvg)/jit(vec_ykvg)/jit(norm)/mul\" stack_frame_id=6}\n  %constant.3 = f32[] constant(0), metadata={op_name=\"jit(vec_ykvg)/jit(vec_ykvg)/jit(norm)\"}\n  %reduce_sum.2 = f32[1024]{0} reduce(%mul.3, %constant.3), dimensions={1}, to_apply=%region_0.1, metadata={op_name=\"jit(vec_ykvg)/jit(vec_ykvg)/jit(norm)/reduce_sum\" stack_frame_id=6}\n  ROOT %sqrt.3 = f32[1024]{0} sqrt(%reduce_sum.2), metadata={op_name=\"jit(vec_ykvg)/jit(vec_ykvg)/jit(norm)/sqrt\" stack_frame_id=6}\n}\n\nENTRY %main.4 (a.1: f32[1024,3]) -> f32[1024] {\n  %a.1 = f32[1024,3]{1,0} parameter(0), metadata={op_name=\"a\"}\n  ROOT %reduce_sqrt_fusion = f32[1024]{0} fusion(%a.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(vec_ykvg)/jit(vec_ykvg)/jit(norm)/sqrt\" stack_frame_id=6}\n}\n\n", "ptx": null}
{"id": 69, "kernel_name": "branch_vfym", "python": "@jax.jit\ndef branch_vfym(a):\n    return jnp.where(a > -0.2, a * 2.1, a + 2.2)", "type": "generate_branch_kernel", "hlo": "module @jit_branch_vfym attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @branch_vfym(%arg0) : (tensor<1024xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @branch_vfym(%arg0: tensor<1024xf32>) -> tensor<1024xf32> {\n    %cst = stablehlo.constant dense<-2.000000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xi1>\n    %cst_0 = stablehlo.constant dense<2.100000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<1024xf32>\n    %cst_1 = stablehlo.constant dense<2.200000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<1024xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<1024xi1>, tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>\n    return %6 : tensor<1024xf32>\n  }\n  func.func private @_where(%arg0: tensor<1024xi1>, %arg1: tensor<1024xf32>, %arg2: tensor<1024xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<1024xi1>, tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_branch_vfym, is_scheduled=true, entry_computation_layout={(f32[1024]{0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmpvly4gtqk.py\"\n4 \"/tmp/tmp7nvd0zdg.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"branch_vfym\"\n6 \"nested_odsh\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=40 end_column=47}\n6 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=31 end_column=38}\n7 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=21 end_column=29}\n8 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=11 end_column=48}\n9 {file_name_id=4 function_name_id=6 line=9 end_line=9 column=17 end_column=61}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=5}\n8 {file_location_id=8 parent_frame_id=5}\n9 {file_location_id=9 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[1024]) -> f32[1024] {\n  %param_0.1 = f32[1024]{0} parameter(0)\n  %constant.8 = f32[] constant(-0.2), metadata={op_name=\"jit(branch_vfym)/jit(branch_vfym)\"}\n  %gt.5 = f32[1024]{0} broadcast(%constant.8), dimensions={}, metadata={op_name=\"jit(branch_vfym)/jit(branch_vfym)/gt\" stack_frame_id=7}\n  %gt.4 = pred[1024]{0} compare(%param_0.1, %gt.5), direction=GT, metadata={op_name=\"jit(branch_vfym)/jit(branch_vfym)/gt\" stack_frame_id=7}\n  %constant.7 = f32[] constant(2.1), metadata={op_name=\"jit(branch_vfym)/jit(branch_vfym)\"}\n  %mul.5 = f32[1024]{0} broadcast(%constant.7), dimensions={}, metadata={op_name=\"jit(branch_vfym)/jit(branch_vfym)/mul\" stack_frame_id=6}\n  %mul.4 = f32[1024]{0} multiply(%param_0.1, %mul.5), metadata={op_name=\"jit(branch_vfym)/jit(branch_vfym)/mul\" stack_frame_id=6}\n  %constant.6 = f32[] constant(2.2), metadata={op_name=\"jit(branch_vfym)/jit(branch_vfym)\"}\n  %add.5 = f32[1024]{0} broadcast(%constant.6), dimensions={}, metadata={op_name=\"jit(branch_vfym)/jit(branch_vfym)/add\" stack_frame_id=5}\n  %add.4 = f32[1024]{0} add(%param_0.1, %add.5), metadata={op_name=\"jit(branch_vfym)/jit(branch_vfym)/add\" stack_frame_id=5}\n  ROOT %select_n.3 = f32[1024]{0} select(%gt.4, %mul.4, %add.4), metadata={op_name=\"jit(branch_vfym)/jit(branch_vfym)/jit(_where)/select_n\" stack_frame_id=9}\n}\n\nENTRY %main.3 (a.1: f32[1024]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  ROOT %add_select_fusion = f32[1024]{0} fusion(%a.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(branch_vfym)/jit(branch_vfym)/jit(_where)/select_n\" stack_frame_id=9}\n}\n\n", "ptx": null}
{"id": 70, "kernel_name": "multi_mrqs", "python": "@jax.jit\ndef multi_mrqs(a, b):\n    temp1 = a - b\n    temp2 = jnp.abs(temp1)\n    return temp2 * a", "type": "generate_multi_statement_kernel", "hlo": "module @jit_multi_mrqs attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>, %arg1: tensor<1024xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @multi_mrqs(%arg0, %arg1) : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @multi_mrqs(%arg0: tensor<1024xf32>, %arg1: tensor<1024xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<1024xf32>\n    %1 = stablehlo.abs %0 : tensor<1024xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<1024xf32>\n    return %2 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_multi_mrqs, is_scheduled=true, entry_computation_layout={(f32[1024]{0}, f32[1024]{0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmpmod4hywo.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"multi_mrqs\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=12 end_column=17}\n6 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=12 end_column=26}\n7 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=11 end_column=20}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[1024], param_1.2: f32[1024]) -> f32[1024] {\n  %param_0.1 = f32[1024]{0} parameter(0)\n  %param_1.2 = f32[1024]{0} parameter(1)\n  %sub.2 = f32[1024]{0} subtract(%param_0.1, %param_1.2), metadata={op_name=\"jit(multi_mrqs)/jit(multi_mrqs)/sub\" stack_frame_id=5}\n  %abs.2 = f32[1024]{0} abs(%sub.2), metadata={op_name=\"jit(multi_mrqs)/jit(multi_mrqs)/abs\" stack_frame_id=6}\n  ROOT %mul.2 = f32[1024]{0} multiply(%abs.2, %param_0.1), metadata={op_name=\"jit(multi_mrqs)/jit(multi_mrqs)/mul\" stack_frame_id=7}\n}\n\nENTRY %main.2 (a.1: f32[1024], b.1: f32[1024]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  %b.1 = f32[1024]{0} parameter(1), metadata={op_name=\"b\"}\n  ROOT %abs_multiply_fusion = f32[1024]{0} fusion(%a.1, %b.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(multi_mrqs)/jit(multi_mrqs)/mul\" stack_frame_id=7}\n}\n\n", "ptx": null}
{"id": 71, "kernel_name": "elementwise_awsa", "python": "@jax.jit\ndef elementwise_awsa(a, b):\n    return a * b", "type": "generate_simple_elementwise", "hlo": "module @jit_elementwise_awsa attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>, %arg1: tensor<1024xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @elementwise_awsa(%arg0, %arg1) : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @elementwise_awsa(%arg0: tensor<1024xf32>, %arg1: tensor<1024xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_elementwise_awsa, is_scheduled=true, entry_computation_layout={(f32[1024]{0}, f32[1024]{0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmpcz65uw8h.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"elementwise_awsa\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=11 end_column=16}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n\n\n%wrapped_multiply_computation (param_0: f32[1024], param_1: f32[1024]) -> f32[1024] {\n  %param_0 = f32[1024]{0} parameter(0)\n  %param_1 = f32[1024]{0} parameter(1)\n  ROOT %mul.2 = f32[1024]{0} multiply(%param_0, %param_1), metadata={op_name=\"jit(elementwise_awsa)/jit(elementwise_awsa)/mul\" stack_frame_id=5}\n}\n\nENTRY %main.2 (a.1: f32[1024], b.1: f32[1024]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  %b.1 = f32[1024]{0} parameter(1), metadata={op_name=\"b\"}\n  ROOT %wrapped_multiply = f32[1024]{0} fusion(%a.1, %b.1), kind=kLoop, calls=%wrapped_multiply_computation, metadata={op_name=\"jit(elementwise_awsa)/jit(elementwise_awsa)/mul\" stack_frame_id=5}\n}\n\n", "ptx": null}
{"id": 72, "kernel_name": "branch_gcrg", "python": "@jax.jit\ndef branch_gcrg(a):\n    return jnp.where(a > 0.25, a + 3.0, a * 2.2)", "type": "generate_branch_kernel", "hlo": "module @jit_branch_gcrg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @branch_gcrg(%arg0) : (tensor<1024xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @branch_gcrg(%arg0: tensor<1024xf32>) -> tensor<1024xf32> {\n    %cst = stablehlo.constant dense<2.500000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<1024xf32>\n    %cst_1 = stablehlo.constant dense<2.200000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<1024xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<1024xi1>, tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>\n    return %6 : tensor<1024xf32>\n  }\n  func.func private @_where(%arg0: tensor<1024xi1>, %arg1: tensor<1024xf32>, %arg2: tensor<1024xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<1024xi1>, tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_branch_gcrg, is_scheduled=true, entry_computation_layout={(f32[1024]{0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmpgg8lod2e.py\"\n4 \"/tmp/tmp7nvd0zdg.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"branch_gcrg\"\n6 \"nested_odsh\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=40 end_column=47}\n6 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=31 end_column=38}\n7 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=21 end_column=29}\n8 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=11 end_column=48}\n9 {file_name_id=4 function_name_id=6 line=9 end_line=9 column=17 end_column=61}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=5}\n8 {file_location_id=8 parent_frame_id=5}\n9 {file_location_id=9 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[1024]) -> f32[1024] {\n  %param_0.1 = f32[1024]{0} parameter(0)\n  %constant.8 = f32[] constant(0.25), metadata={op_name=\"jit(branch_gcrg)/jit(branch_gcrg)\"}\n  %gt.5 = f32[1024]{0} broadcast(%constant.8), dimensions={}, metadata={op_name=\"jit(branch_gcrg)/jit(branch_gcrg)/gt\" stack_frame_id=7}\n  %gt.4 = pred[1024]{0} compare(%param_0.1, %gt.5), direction=GT, metadata={op_name=\"jit(branch_gcrg)/jit(branch_gcrg)/gt\" stack_frame_id=7}\n  %constant.7 = f32[] constant(3), metadata={op_name=\"jit(branch_gcrg)/jit(branch_gcrg)\"}\n  %add.5 = f32[1024]{0} broadcast(%constant.7), dimensions={}, metadata={op_name=\"jit(branch_gcrg)/jit(branch_gcrg)/add\" stack_frame_id=6}\n  %add.4 = f32[1024]{0} add(%param_0.1, %add.5), metadata={op_name=\"jit(branch_gcrg)/jit(branch_gcrg)/add\" stack_frame_id=6}\n  %constant.6 = f32[] constant(2.2), metadata={op_name=\"jit(branch_gcrg)/jit(branch_gcrg)\"}\n  %mul.5 = f32[1024]{0} broadcast(%constant.6), dimensions={}, metadata={op_name=\"jit(branch_gcrg)/jit(branch_gcrg)/mul\" stack_frame_id=5}\n  %mul.4 = f32[1024]{0} multiply(%param_0.1, %mul.5), metadata={op_name=\"jit(branch_gcrg)/jit(branch_gcrg)/mul\" stack_frame_id=5}\n  ROOT %select_n.3 = f32[1024]{0} select(%gt.4, %add.4, %mul.4), metadata={op_name=\"jit(branch_gcrg)/jit(branch_gcrg)/jit(_where)/select_n\" stack_frame_id=9}\n}\n\nENTRY %main.3 (a.1: f32[1024]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  ROOT %multiply_select_fusion = f32[1024]{0} fusion(%a.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(branch_gcrg)/jit(branch_gcrg)/jit(_where)/select_n\" stack_frame_id=9}\n}\n\n", "ptx": null}
{"id": 73, "kernel_name": "loop_xfcc", "python": "@jax.jit\ndef loop_xfcc(a, n):\n    def body_fun(i, val):\n        return val * a\n    # Initial value matches a's shape/type but zeroed\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "hlo": "module @jit_loop_xfcc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>, %arg1: tensor<i32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @loop_xfcc(%arg0, %arg1) : (tensor<1024xf32>, tensor<i32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @loop_xfcc(%arg0: tensor<1024xf32>, %arg1: tensor<i32>) -> tensor<1024xf32> {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<1024xf32>, tensor<i32>, tensor<i32>, tensor<1024xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.multiply %iterArg_2, %iterArg : tensor<1024xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<1024xf32>, tensor<i32>, tensor<i32>, tensor<1024xf32>\n    }\n    return %1#3 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_loop_xfcc, is_scheduled=true, entry_computation_layout={(f32[1024]{0}, s32[])->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmpohnv1v62.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"loop_xfcc\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=9 end_line=9 column=15 end_column=32}\n6 {file_name_id=3 function_name_id=5 line=10 end_line=10 column=11 end_column=54}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n\n\n%wrapped_add_computation (param_0.1: s32[], param_1: s32[]) -> s32[] {\n  %param_0.1 = s32[] parameter(0)\n  %param_1 = s32[] parameter(1)\n  ROOT %add.0 = s32[] add(%param_0.1, %param_1), metadata={op_name=\"jit(loop_xfcc)/jit(loop_xfcc)/while/body/add\" stack_frame_id=6}\n}\n\n%wrapped_multiply_computation (param_0.2: f32[1024], param_1.1: f32[1024]) -> f32[1024] {\n  %param_0.2 = f32[1024]{0} parameter(0)\n  %param_1.1 = f32[1024]{0} parameter(1)\n  ROOT %mul.0 = f32[1024]{0} multiply(%param_0.2, %param_1.1), metadata={op_name=\"jit(loop_xfcc)/jit(loop_xfcc)/while/body/mul\" stack_frame_id=6}\n}\n\n%region_0.1 (arg_tuple.1: (s32[], f32[1024], s32[], f32[1024])) -> (s32[], f32[1024], s32[], f32[1024]) {\n  %arg_tuple.1 = (s32[], f32[1024]{0}, s32[], f32[1024]{0}) parameter(0), metadata={op_name=\"jit(loop_xfcc)/jit(loop_xfcc)\"}\n  %constant.3 = s32[] constant(1), metadata={op_name=\"jit(loop_xfcc)/jit(loop_xfcc)\"}\n  %get-tuple-element.19 = f32[1024]{0} get-tuple-element(%arg_tuple.1), index=3, metadata={op_name=\"jit(loop_xfcc)/jit(loop_xfcc)\"}\n  %get-tuple-element.18 = s32[] get-tuple-element(%arg_tuple.1), index=2, metadata={op_name=\"jit(loop_xfcc)/jit(loop_xfcc)\"}\n  %get-tuple-element.9 = f32[1024]{0} get-tuple-element(%arg_tuple.1), index=1\n  %get-tuple-element.8 = s32[] get-tuple-element(%arg_tuple.1), index=0\n  %wrapped_multiply = f32[1024]{0} fusion(%get-tuple-element.9, %get-tuple-element.19), kind=kLoop, calls=%wrapped_multiply_computation, metadata={op_name=\"jit(loop_xfcc)/jit(loop_xfcc)/while/body/mul\" stack_frame_id=6}\n  %wrapped_add = s32[] fusion(%get-tuple-element.8, %constant.3), kind=kLoop, calls=%wrapped_add_computation, metadata={op_name=\"jit(loop_xfcc)/jit(loop_xfcc)/while/body/add\" stack_frame_id=6}\n  ROOT %tuple.3 = (s32[], f32[1024]{0}, s32[], f32[1024]{0}) tuple(%wrapped_add, %wrapped_multiply, %get-tuple-element.18, %get-tuple-element.19)\n}\n\n%wrapped_compare_computation (param_0.3: s32[], param_1.2: s32[]) -> pred[] {\n  %param_0.3 = s32[] parameter(0)\n  %param_1.2 = s32[] parameter(1)\n  ROOT %lt.0 = pred[] compare(%param_0.3, %param_1.2), direction=LT, metadata={op_name=\"jit(loop_xfcc)/jit(loop_xfcc)/while/cond/lt\" stack_frame_id=6}\n}\n\n%region_1.2 (arg_tuple.3: (s32[], f32[1024], s32[], f32[1024])) -> pred[] {\n  %arg_tuple.3 = (s32[], f32[1024]{0}, s32[], f32[1024]{0}) parameter(0), metadata={op_name=\"jit(loop_xfcc)/jit(loop_xfcc)\"}\n  %get-tuple-element.14 = s32[] get-tuple-element(%arg_tuple.3), index=2, metadata={op_name=\"jit(loop_xfcc)/jit(loop_xfcc)\"}\n  %get-tuple-element.12 = s32[] get-tuple-element(%arg_tuple.3), index=0, metadata={op_name=\"jit(loop_xfcc)/jit(loop_xfcc)\"}\n  ROOT %wrapped_compare = pred[] fusion(%get-tuple-element.12, %get-tuple-element.14), kind=kLoop, calls=%wrapped_compare_computation, metadata={op_name=\"jit(loop_xfcc)/jit(loop_xfcc)/while/cond/lt\" stack_frame_id=6}\n}\n\n%wrapped_broadcast_computation (param_0: f32[]) -> f32[1024] {\n  %param_0 = f32[] parameter(0)\n  ROOT %broadcast_in_dim.2 = f32[1024]{0} broadcast(%param_0), dimensions={}, metadata={op_name=\"jit(loop_xfcc)/jit(loop_xfcc)/broadcast_in_dim\" stack_frame_id=5}\n}\n\nENTRY %main.4 (a.1: f32[1024], n.1: s32[]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  %n.1 = s32[] parameter(1), metadata={op_name=\"n\"}\n  %constant.0 = s32[] constant(0), metadata={op_name=\"jit(loop_xfcc)/jit(loop_xfcc)\"}\n  %constant.1 = f32[] constant(0), metadata={op_name=\"jit(loop_xfcc)/jit(loop_xfcc)\"}\n  %copy.6 = s32[] copy(%constant.0)\n  %wrapped_broadcast = f32[1024]{0} fusion(%constant.1), kind=kLoop, calls=%wrapped_broadcast_computation, metadata={op_name=\"jit(loop_xfcc)/jit(loop_xfcc)/broadcast_in_dim\" stack_frame_id=5}\n  %tuple = (s32[], f32[1024]{0}, s32[], f32[1024]{0}) tuple(%copy.6, %wrapped_broadcast, %n.1, %a.1)\n  %while.1 = (s32[], f32[1024]{0}, s32[], f32[1024]{0}) while(%tuple), condition=%region_1.2, body=%region_0.1, metadata={op_name=\"jit(loop_xfcc)/jit(loop_xfcc)/while\" stack_frame_id=6}\n  ROOT %while.2 = f32[1024]{0} get-tuple-element(%while.1), index=1, metadata={op_name=\"jit(loop_xfcc)/jit(loop_xfcc)/while\" stack_frame_id=6}\n}\n\n", "ptx": null}
{"id": 74, "kernel_name": "compound_vpsa", "python": "@jax.jit\ndef compound_vpsa(a, b, scale):\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "hlo": "module @jit_compound_vpsa attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>, %arg1: tensor<1024xf32>, %arg2: tensor<f32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @compound_vpsa(%arg0, %arg1, %arg2) : (tensor<1024xf32>, tensor<1024xf32>, tensor<f32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @compound_vpsa(%arg0: tensor<1024xf32>, %arg1: tensor<1024xf32>, %arg2: tensor<f32>) -> tensor<1024xf32> {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<1024xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<1024xf32>\n    %4 = stablehlo.floor %3 : tensor<1024xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<1024xf32>\n    %6 = stablehlo.abs %5 : tensor<1024xf32>\n    return %6 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_compound_vpsa, is_scheduled=true, entry_computation_layout={(f32[1024]{0}, f32[1024]{0}, f32[])->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmpe2yykkh6.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"compound_vpsa\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=14 end_column=19}\n6 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=13 end_column=28}\n7 {file_name_id=3 function_name_id=5 line=9 end_line=9 column=22 end_column=39}\n8 {file_name_id=3 function_name_id=5 line=9 end_line=9 column=13 end_column=39}\n9 {file_name_id=3 function_name_id=5 line=10 end_line=10 column=11 end_column=26}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=5}\n8 {file_location_id=8 parent_frame_id=5}\n9 {file_location_id=9 parent_frame_id=5}\n\n\n%fused_computation (param_0.3: f32[], param_1.3: f32[1024], param_2: f32[1024]) -> f32[1024] {\n  %param_1.3 = f32[1024]{0} parameter(1)\n  %param_2 = f32[1024]{0} parameter(2)\n  %add.2 = f32[1024]{0} add(%param_1.3, %param_2), metadata={op_name=\"jit(compound_vpsa)/jit(compound_vpsa)/add\" stack_frame_id=5}\n  %param_0.3 = f32[] parameter(0)\n  %mul.5 = f32[1024]{0} broadcast(%param_0.3), dimensions={}, metadata={op_name=\"jit(compound_vpsa)/jit(compound_vpsa)/mul\" stack_frame_id=6}\n  %mul.4 = f32[1024]{0} multiply(%add.2, %mul.5), metadata={op_name=\"jit(compound_vpsa)/jit(compound_vpsa)/mul\" stack_frame_id=6}\n  %floor.2 = f32[1024]{0} floor(%mul.4), metadata={op_name=\"jit(compound_vpsa)/jit(compound_vpsa)/floor\" stack_frame_id=7}\n  %sub.2 = f32[1024]{0} subtract(%mul.4, %floor.2), metadata={op_name=\"jit(compound_vpsa)/jit(compound_vpsa)/sub\" stack_frame_id=8}\n  ROOT %abs.2 = f32[1024]{0} abs(%sub.2), metadata={op_name=\"jit(compound_vpsa)/jit(compound_vpsa)/abs\" stack_frame_id=9}\n}\n\nENTRY %main.2 (a.1: f32[1024], b.1: f32[1024], scale.1: f32[]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  %b.1 = f32[1024]{0} parameter(1), metadata={op_name=\"b\"}\n  %scale.1 = f32[] parameter(2), metadata={op_name=\"scale\"}\n  ROOT %subtract_abs_fusion = f32[1024]{0} fusion(%scale.1, %a.1, %b.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(compound_vpsa)/jit(compound_vpsa)/abs\" stack_frame_id=9}\n}\n\n", "ptx": null}
{"id": 75, "kernel_name": "branch_geke", "python": "@jax.jit\ndef branch_geke(a):\n    return jnp.where(a > -0.19, a + 1.8, a * 1.9)", "type": "generate_branch_kernel", "hlo": "module @jit_branch_geke attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @branch_geke(%arg0) : (tensor<1024xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @branch_geke(%arg0: tensor<1024xf32>) -> tensor<1024xf32> {\n    %cst = stablehlo.constant dense<-1.900000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xi1>\n    %cst_0 = stablehlo.constant dense<1.800000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<1024xf32>\n    %cst_1 = stablehlo.constant dense<1.900000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<1024xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<1024xi1>, tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>\n    return %6 : tensor<1024xf32>\n  }\n  func.func private @_where(%arg0: tensor<1024xi1>, %arg1: tensor<1024xf32>, %arg2: tensor<1024xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<1024xi1>, tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_branch_geke, is_scheduled=true, entry_computation_layout={(f32[1024]{0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmppfk4e8kq.py\"\n4 \"/tmp/tmp7nvd0zdg.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"branch_geke\"\n6 \"nested_odsh\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=41 end_column=48}\n6 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=32 end_column=39}\n7 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=21 end_column=30}\n8 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=11 end_column=49}\n9 {file_name_id=4 function_name_id=6 line=9 end_line=9 column=17 end_column=61}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=5}\n8 {file_location_id=8 parent_frame_id=5}\n9 {file_location_id=9 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[1024]) -> f32[1024] {\n  %param_0.1 = f32[1024]{0} parameter(0)\n  %constant.8 = f32[] constant(-0.19), metadata={op_name=\"jit(branch_geke)/jit(branch_geke)\"}\n  %gt.5 = f32[1024]{0} broadcast(%constant.8), dimensions={}, metadata={op_name=\"jit(branch_geke)/jit(branch_geke)/gt\" stack_frame_id=7}\n  %gt.4 = pred[1024]{0} compare(%param_0.1, %gt.5), direction=GT, metadata={op_name=\"jit(branch_geke)/jit(branch_geke)/gt\" stack_frame_id=7}\n  %constant.7 = f32[] constant(1.8), metadata={op_name=\"jit(branch_geke)/jit(branch_geke)\"}\n  %add.5 = f32[1024]{0} broadcast(%constant.7), dimensions={}, metadata={op_name=\"jit(branch_geke)/jit(branch_geke)/add\" stack_frame_id=6}\n  %add.4 = f32[1024]{0} add(%param_0.1, %add.5), metadata={op_name=\"jit(branch_geke)/jit(branch_geke)/add\" stack_frame_id=6}\n  %constant.6 = f32[] constant(1.9), metadata={op_name=\"jit(branch_geke)/jit(branch_geke)\"}\n  %mul.5 = f32[1024]{0} broadcast(%constant.6), dimensions={}, metadata={op_name=\"jit(branch_geke)/jit(branch_geke)/mul\" stack_frame_id=5}\n  %mul.4 = f32[1024]{0} multiply(%param_0.1, %mul.5), metadata={op_name=\"jit(branch_geke)/jit(branch_geke)/mul\" stack_frame_id=5}\n  ROOT %select_n.3 = f32[1024]{0} select(%gt.4, %add.4, %mul.4), metadata={op_name=\"jit(branch_geke)/jit(branch_geke)/jit(_where)/select_n\" stack_frame_id=9}\n}\n\nENTRY %main.3 (a.1: f32[1024]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  ROOT %multiply_select_fusion = f32[1024]{0} fusion(%a.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(branch_geke)/jit(branch_geke)/jit(_where)/select_n\" stack_frame_id=9}\n}\n\n", "ptx": null}
{"id": 76, "kernel_name": "unary_secj", "python": "@jax.jit\ndef unary_secj(a):\n    return jnp.sqrt(a)", "type": "generate_unary_kernel", "hlo": "module @jit_unary_secj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @unary_secj(%arg0) : (tensor<1024xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @unary_secj(%arg0: tensor<1024xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.sqrt %arg0 : tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_unary_secj, is_scheduled=true, entry_computation_layout={(f32[1024]{0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmp0fqfxczj.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"unary_secj\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=11 end_column=22}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n\n\n%wrapped_sqrt_computation (param_0: f32[1024]) -> f32[1024] {\n  %param_0 = f32[1024]{0} parameter(0)\n  ROOT %sqrt.2 = f32[1024]{0} sqrt(%param_0), metadata={op_name=\"jit(unary_secj)/jit(unary_secj)/sqrt\" stack_frame_id=5}\n}\n\nENTRY %main.2 (a.1: f32[1024]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  ROOT %wrapped_sqrt = f32[1024]{0} fusion(%a.1), kind=kLoop, calls=%wrapped_sqrt_computation, metadata={op_name=\"jit(unary_secj)/jit(unary_secj)/sqrt\" stack_frame_id=5}\n}\n\n", "ptx": null}
{"id": 77, "kernel_name": "loop_yeuo", "python": "@jax.jit\ndef loop_yeuo(a, n):\n    def body_fun(i, val):\n        return val * a\n    # Initial value matches a's shape/type but zeroed\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "hlo": "module @jit_loop_yeuo attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>, %arg1: tensor<i32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @loop_yeuo(%arg0, %arg1) : (tensor<1024xf32>, tensor<i32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @loop_yeuo(%arg0: tensor<1024xf32>, %arg1: tensor<i32>) -> tensor<1024xf32> {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<1024xf32>, tensor<i32>, tensor<i32>, tensor<1024xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.multiply %iterArg_2, %iterArg : tensor<1024xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<1024xf32>, tensor<i32>, tensor<i32>, tensor<1024xf32>\n    }\n    return %1#3 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_loop_yeuo, is_scheduled=true, entry_computation_layout={(f32[1024]{0}, s32[])->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmplhfcvine.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"loop_yeuo\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=9 end_line=9 column=15 end_column=32}\n6 {file_name_id=3 function_name_id=5 line=10 end_line=10 column=11 end_column=54}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n\n\n%wrapped_add_computation (param_0.1: s32[], param_1: s32[]) -> s32[] {\n  %param_0.1 = s32[] parameter(0)\n  %param_1 = s32[] parameter(1)\n  ROOT %add.0 = s32[] add(%param_0.1, %param_1), metadata={op_name=\"jit(loop_yeuo)/jit(loop_yeuo)/while/body/add\" stack_frame_id=6}\n}\n\n%wrapped_multiply_computation (param_0.2: f32[1024], param_1.1: f32[1024]) -> f32[1024] {\n  %param_0.2 = f32[1024]{0} parameter(0)\n  %param_1.1 = f32[1024]{0} parameter(1)\n  ROOT %mul.0 = f32[1024]{0} multiply(%param_0.2, %param_1.1), metadata={op_name=\"jit(loop_yeuo)/jit(loop_yeuo)/while/body/mul\" stack_frame_id=6}\n}\n\n%region_0.1 (arg_tuple.1: (s32[], f32[1024], s32[], f32[1024])) -> (s32[], f32[1024], s32[], f32[1024]) {\n  %arg_tuple.1 = (s32[], f32[1024]{0}, s32[], f32[1024]{0}) parameter(0), metadata={op_name=\"jit(loop_yeuo)/jit(loop_yeuo)\"}\n  %constant.3 = s32[] constant(1), metadata={op_name=\"jit(loop_yeuo)/jit(loop_yeuo)\"}\n  %get-tuple-element.19 = f32[1024]{0} get-tuple-element(%arg_tuple.1), index=3, metadata={op_name=\"jit(loop_yeuo)/jit(loop_yeuo)\"}\n  %get-tuple-element.18 = s32[] get-tuple-element(%arg_tuple.1), index=2, metadata={op_name=\"jit(loop_yeuo)/jit(loop_yeuo)\"}\n  %get-tuple-element.9 = f32[1024]{0} get-tuple-element(%arg_tuple.1), index=1\n  %get-tuple-element.8 = s32[] get-tuple-element(%arg_tuple.1), index=0\n  %wrapped_multiply = f32[1024]{0} fusion(%get-tuple-element.9, %get-tuple-element.19), kind=kLoop, calls=%wrapped_multiply_computation, metadata={op_name=\"jit(loop_yeuo)/jit(loop_yeuo)/while/body/mul\" stack_frame_id=6}\n  %wrapped_add = s32[] fusion(%get-tuple-element.8, %constant.3), kind=kLoop, calls=%wrapped_add_computation, metadata={op_name=\"jit(loop_yeuo)/jit(loop_yeuo)/while/body/add\" stack_frame_id=6}\n  ROOT %tuple.3 = (s32[], f32[1024]{0}, s32[], f32[1024]{0}) tuple(%wrapped_add, %wrapped_multiply, %get-tuple-element.18, %get-tuple-element.19)\n}\n\n%wrapped_compare_computation (param_0.3: s32[], param_1.2: s32[]) -> pred[] {\n  %param_0.3 = s32[] parameter(0)\n  %param_1.2 = s32[] parameter(1)\n  ROOT %lt.0 = pred[] compare(%param_0.3, %param_1.2), direction=LT, metadata={op_name=\"jit(loop_yeuo)/jit(loop_yeuo)/while/cond/lt\" stack_frame_id=6}\n}\n\n%region_1.2 (arg_tuple.3: (s32[], f32[1024], s32[], f32[1024])) -> pred[] {\n  %arg_tuple.3 = (s32[], f32[1024]{0}, s32[], f32[1024]{0}) parameter(0), metadata={op_name=\"jit(loop_yeuo)/jit(loop_yeuo)\"}\n  %get-tuple-element.14 = s32[] get-tuple-element(%arg_tuple.3), index=2, metadata={op_name=\"jit(loop_yeuo)/jit(loop_yeuo)\"}\n  %get-tuple-element.12 = s32[] get-tuple-element(%arg_tuple.3), index=0, metadata={op_name=\"jit(loop_yeuo)/jit(loop_yeuo)\"}\n  ROOT %wrapped_compare = pred[] fusion(%get-tuple-element.12, %get-tuple-element.14), kind=kLoop, calls=%wrapped_compare_computation, metadata={op_name=\"jit(loop_yeuo)/jit(loop_yeuo)/while/cond/lt\" stack_frame_id=6}\n}\n\n%wrapped_broadcast_computation (param_0: f32[]) -> f32[1024] {\n  %param_0 = f32[] parameter(0)\n  ROOT %broadcast_in_dim.2 = f32[1024]{0} broadcast(%param_0), dimensions={}, metadata={op_name=\"jit(loop_yeuo)/jit(loop_yeuo)/broadcast_in_dim\" stack_frame_id=5}\n}\n\nENTRY %main.4 (a.1: f32[1024], n.1: s32[]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  %n.1 = s32[] parameter(1), metadata={op_name=\"n\"}\n  %constant.0 = s32[] constant(0), metadata={op_name=\"jit(loop_yeuo)/jit(loop_yeuo)\"}\n  %constant.1 = f32[] constant(0), metadata={op_name=\"jit(loop_yeuo)/jit(loop_yeuo)\"}\n  %copy.6 = s32[] copy(%constant.0)\n  %wrapped_broadcast = f32[1024]{0} fusion(%constant.1), kind=kLoop, calls=%wrapped_broadcast_computation, metadata={op_name=\"jit(loop_yeuo)/jit(loop_yeuo)/broadcast_in_dim\" stack_frame_id=5}\n  %tuple = (s32[], f32[1024]{0}, s32[], f32[1024]{0}) tuple(%copy.6, %wrapped_broadcast, %n.1, %a.1)\n  %while.1 = (s32[], f32[1024]{0}, s32[], f32[1024]{0}) while(%tuple), condition=%region_1.2, body=%region_0.1, metadata={op_name=\"jit(loop_yeuo)/jit(loop_yeuo)/while\" stack_frame_id=6}\n  ROOT %while.2 = f32[1024]{0} get-tuple-element(%while.1), index=1, metadata={op_name=\"jit(loop_yeuo)/jit(loop_yeuo)/while\" stack_frame_id=6}\n}\n\n", "ptx": null}
{"id": 78, "kernel_name": "nested_neyq", "python": "@jax.jit\ndef nested_neyq(a):\n    # Nested where: if a > t1 then (if a > t2 then ... else ...) else ...\n    inner_true = a * 3.0\n    inner_false = a * 2.0\n    outer_true = jnp.where(a > 1.44, inner_true, inner_false)\n    outer_false = a * 0.5\n    return jnp.where(a > 0.09, outer_true, outer_false)", "type": "generate_nested_branch_kernel", "hlo": "module @jit_nested_neyq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @nested_neyq(%arg0) : (tensor<1024xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @nested_neyq(%arg0: tensor<1024xf32>) -> tensor<1024xf32> {\n    %cst = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %1 = stablehlo.multiply %arg0, %0 : tensor<1024xf32>\n    %cst_0 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<1024xf32>\n    %cst_1 = stablehlo.constant dense<1.440000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %5 = stablehlo.compare  GT, %arg0, %4,  FLOAT : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xi1>\n    %6 = call @_where(%5, %1, %3) : (tensor<1024xi1>, tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>\n    %cst_2 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %8 = stablehlo.multiply %arg0, %7 : tensor<1024xf32>\n    %cst_3 = stablehlo.constant dense<9.000000e-02> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %10 = stablehlo.compare  GT, %arg0, %9,  FLOAT : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xi1>\n    %11 = call @_where(%10, %6, %8) : (tensor<1024xi1>, tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>\n    return %11 : tensor<1024xf32>\n  }\n  func.func private @_where(%arg0: tensor<1024xi1>, %arg1: tensor<1024xf32>, %arg2: tensor<1024xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<1024xi1>, tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_nested_neyq, is_scheduled=true, entry_computation_layout={(f32[1024]{0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmpfiw7p2b9.py\"\n4 \"/tmp/tmp7nvd0zdg.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"nested_neyq\"\n6 \"nested_odsh\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=11 end_line=11 column=21 end_column=29}\n6 {file_name_id=3 function_name_id=5 line=10 end_line=10 column=18 end_column=25}\n7 {file_name_id=3 function_name_id=5 line=9 end_line=9 column=27 end_column=35}\n8 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=18 end_column=25}\n9 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=17 end_column=24}\n10 {file_name_id=3 function_name_id=5 line=9 end_line=9 column=17 end_column=61}\n11 {file_name_id=4 function_name_id=6 line=9 end_line=9 column=17 end_column=61}\n12 {file_name_id=3 function_name_id=5 line=11 end_line=11 column=11 end_column=55}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=5}\n8 {file_location_id=8 parent_frame_id=5}\n9 {file_location_id=9 parent_frame_id=5}\n10 {file_location_id=10 parent_frame_id=5}\n11 {file_location_id=11 parent_frame_id=5}\n12 {file_location_id=12 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[1024]) -> f32[1024] {\n  %param_0.1 = f32[1024]{0} parameter(0)\n  %constant.14 = f32[] constant(0.09), metadata={op_name=\"jit(nested_neyq)/jit(nested_neyq)\"}\n  %gt.11 = f32[1024]{0} broadcast(%constant.14), dimensions={}, metadata={op_name=\"jit(nested_neyq)/jit(nested_neyq)/gt\" stack_frame_id=5}\n  %gt.10 = pred[1024]{0} compare(%param_0.1, %gt.11), direction=GT, metadata={op_name=\"jit(nested_neyq)/jit(nested_neyq)/gt\" stack_frame_id=5}\n  %constant.13 = f32[] constant(1.44), metadata={op_name=\"jit(nested_neyq)/jit(nested_neyq)\"}\n  %gt.9 = f32[1024]{0} broadcast(%constant.13), dimensions={}, metadata={op_name=\"jit(nested_neyq)/jit(nested_neyq)/gt\" stack_frame_id=7}\n  %gt.8 = pred[1024]{0} compare(%param_0.1, %gt.9), direction=GT, metadata={op_name=\"jit(nested_neyq)/jit(nested_neyq)/gt\" stack_frame_id=7}\n  %constant.12 = f32[] constant(3), metadata={op_name=\"jit(nested_neyq)/jit(nested_neyq)\"}\n  %mul.17 = f32[1024]{0} broadcast(%constant.12), dimensions={}, metadata={op_name=\"jit(nested_neyq)/jit(nested_neyq)/mul\" stack_frame_id=9}\n  %mul.16 = f32[1024]{0} multiply(%param_0.1, %mul.17), metadata={op_name=\"jit(nested_neyq)/jit(nested_neyq)/mul\" stack_frame_id=9}\n  %constant.11 = f32[] constant(2), metadata={op_name=\"jit(nested_neyq)/jit(nested_neyq)\"}\n  %mul.15 = f32[1024]{0} broadcast(%constant.11), dimensions={}, metadata={op_name=\"jit(nested_neyq)/jit(nested_neyq)/mul\" stack_frame_id=8}\n  %mul.14 = f32[1024]{0} multiply(%param_0.1, %mul.15), metadata={op_name=\"jit(nested_neyq)/jit(nested_neyq)/mul\" stack_frame_id=8}\n  %select_n.7 = f32[1024]{0} select(%gt.8, %mul.16, %mul.14), metadata={op_name=\"jit(nested_neyq)/jit(nested_neyq)/jit(_where)/select_n\" stack_frame_id=11}\n  %constant.10 = f32[] constant(0.5), metadata={op_name=\"jit(nested_neyq)/jit(nested_neyq)\"}\n  %mul.13 = f32[1024]{0} broadcast(%constant.10), dimensions={}, metadata={op_name=\"jit(nested_neyq)/jit(nested_neyq)/mul\" stack_frame_id=6}\n  %mul.12 = f32[1024]{0} multiply(%param_0.1, %mul.13), metadata={op_name=\"jit(nested_neyq)/jit(nested_neyq)/mul\" stack_frame_id=6}\n  ROOT %select_n.6 = f32[1024]{0} select(%gt.10, %select_n.7, %mul.12), metadata={op_name=\"jit(nested_neyq)/jit(nested_neyq)/jit(_where)/select_n\" stack_frame_id=11}\n}\n\nENTRY %main.3 (a.1: f32[1024]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  ROOT %multiply_select_fusion = f32[1024]{0} fusion(%a.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(nested_neyq)/jit(nested_neyq)/jit(_where)/select_n\" stack_frame_id=11}\n}\n\n", "ptx": null}
{"id": 79, "kernel_name": "scalar_arr_cpkx", "python": "@jax.jit\ndef scalar_arr_cpkx(alpha, x, y):\n    return alpha * x * y", "type": "generate_scalar_array_op", "hlo": "module @jit_scalar_arr_cpkx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<1024xf32>, %arg2: tensor<1024xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @scalar_arr_cpkx(%arg0, %arg1, %arg2) : (tensor<f32>, tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @scalar_arr_cpkx(%arg0: tensor<f32>, %arg1: tensor<1024xf32>, %arg2: tensor<1024xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<1024xf32>\n    %3 = stablehlo.multiply %2, %arg2 : tensor<1024xf32>\n    return %3 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_scalar_arr_cpkx, is_scheduled=true, entry_computation_layout={(f32[], f32[1024]{0}, f32[1024]{0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmpr6wprc0r.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"scalar_arr_cpkx\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=11 end_column=20}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[1024], param_1.2: f32[1024], param_2.1: f32[]) -> f32[1024] {\n  %param_2.1 = f32[] parameter(2)\n  %mul.8 = f32[1024]{0} broadcast(%param_2.1), dimensions={}, metadata={op_name=\"jit(scalar_arr_cpkx)/jit(scalar_arr_cpkx)/mul\" stack_frame_id=5}\n  %param_1.2 = f32[1024]{0} parameter(1)\n  %mul.7 = f32[1024]{0} multiply(%mul.8, %param_1.2), metadata={op_name=\"jit(scalar_arr_cpkx)/jit(scalar_arr_cpkx)/mul\" stack_frame_id=5}\n  %param_0.1 = f32[1024]{0} parameter(0)\n  ROOT %mul.6 = f32[1024]{0} multiply(%mul.7, %param_0.1), metadata={op_name=\"jit(scalar_arr_cpkx)/jit(scalar_arr_cpkx)/mul\" stack_frame_id=5}\n}\n\nENTRY %main.2 (alpha.1: f32[], x.1: f32[1024], y.1: f32[1024]) -> f32[1024] {\n  %alpha.1 = f32[] parameter(0), metadata={op_name=\"alpha\"}\n  %x.1 = f32[1024]{0} parameter(1), metadata={op_name=\"x\"}\n  %y.1 = f32[1024]{0} parameter(2), metadata={op_name=\"y\"}\n  ROOT %multiply_multiply_fusion = f32[1024]{0} fusion(%y.1, %x.1, %alpha.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(scalar_arr_cpkx)/jit(scalar_arr_cpkx)/mul\" stack_frame_id=5}\n}\n\n", "ptx": null}
{"id": 80, "kernel_name": "nested_nphy", "python": "@jax.jit\ndef nested_nphy(a):\n    # Nested where: if a > t1 then (if a > t2 then ... else ...) else ...\n    inner_true = a * 3.0\n    inner_false = a * 2.0\n    outer_true = jnp.where(a > 1.26, inner_true, inner_false)\n    outer_false = a * 0.5\n    return jnp.where(a > 0.23, outer_true, outer_false)", "type": "generate_nested_branch_kernel", "hlo": "module @jit_nested_nphy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @nested_nphy(%arg0) : (tensor<1024xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @nested_nphy(%arg0: tensor<1024xf32>) -> tensor<1024xf32> {\n    %cst = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %1 = stablehlo.multiply %arg0, %0 : tensor<1024xf32>\n    %cst_0 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<1024xf32>\n    %cst_1 = stablehlo.constant dense<1.260000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %5 = stablehlo.compare  GT, %arg0, %4,  FLOAT : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xi1>\n    %6 = call @_where(%5, %1, %3) : (tensor<1024xi1>, tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>\n    %cst_2 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %8 = stablehlo.multiply %arg0, %7 : tensor<1024xf32>\n    %cst_3 = stablehlo.constant dense<2.300000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %10 = stablehlo.compare  GT, %arg0, %9,  FLOAT : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xi1>\n    %11 = call @_where(%10, %6, %8) : (tensor<1024xi1>, tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>\n    return %11 : tensor<1024xf32>\n  }\n  func.func private @_where(%arg0: tensor<1024xi1>, %arg1: tensor<1024xf32>, %arg2: tensor<1024xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<1024xi1>, tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_nested_nphy, is_scheduled=true, entry_computation_layout={(f32[1024]{0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmpsjv6z24a.py\"\n4 \"/tmp/tmp7nvd0zdg.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"nested_nphy\"\n6 \"nested_odsh\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=11 end_line=11 column=21 end_column=29}\n6 {file_name_id=3 function_name_id=5 line=10 end_line=10 column=18 end_column=25}\n7 {file_name_id=3 function_name_id=5 line=9 end_line=9 column=27 end_column=35}\n8 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=18 end_column=25}\n9 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=17 end_column=24}\n10 {file_name_id=3 function_name_id=5 line=9 end_line=9 column=17 end_column=61}\n11 {file_name_id=4 function_name_id=6 line=9 end_line=9 column=17 end_column=61}\n12 {file_name_id=3 function_name_id=5 line=11 end_line=11 column=11 end_column=55}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=5}\n8 {file_location_id=8 parent_frame_id=5}\n9 {file_location_id=9 parent_frame_id=5}\n10 {file_location_id=10 parent_frame_id=5}\n11 {file_location_id=11 parent_frame_id=5}\n12 {file_location_id=12 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[1024]) -> f32[1024] {\n  %param_0.1 = f32[1024]{0} parameter(0)\n  %constant.14 = f32[] constant(0.23), metadata={op_name=\"jit(nested_nphy)/jit(nested_nphy)\"}\n  %gt.11 = f32[1024]{0} broadcast(%constant.14), dimensions={}, metadata={op_name=\"jit(nested_nphy)/jit(nested_nphy)/gt\" stack_frame_id=5}\n  %gt.10 = pred[1024]{0} compare(%param_0.1, %gt.11), direction=GT, metadata={op_name=\"jit(nested_nphy)/jit(nested_nphy)/gt\" stack_frame_id=5}\n  %constant.13 = f32[] constant(1.26), metadata={op_name=\"jit(nested_nphy)/jit(nested_nphy)\"}\n  %gt.9 = f32[1024]{0} broadcast(%constant.13), dimensions={}, metadata={op_name=\"jit(nested_nphy)/jit(nested_nphy)/gt\" stack_frame_id=7}\n  %gt.8 = pred[1024]{0} compare(%param_0.1, %gt.9), direction=GT, metadata={op_name=\"jit(nested_nphy)/jit(nested_nphy)/gt\" stack_frame_id=7}\n  %constant.12 = f32[] constant(3), metadata={op_name=\"jit(nested_nphy)/jit(nested_nphy)\"}\n  %mul.17 = f32[1024]{0} broadcast(%constant.12), dimensions={}, metadata={op_name=\"jit(nested_nphy)/jit(nested_nphy)/mul\" stack_frame_id=9}\n  %mul.16 = f32[1024]{0} multiply(%param_0.1, %mul.17), metadata={op_name=\"jit(nested_nphy)/jit(nested_nphy)/mul\" stack_frame_id=9}\n  %constant.11 = f32[] constant(2), metadata={op_name=\"jit(nested_nphy)/jit(nested_nphy)\"}\n  %mul.15 = f32[1024]{0} broadcast(%constant.11), dimensions={}, metadata={op_name=\"jit(nested_nphy)/jit(nested_nphy)/mul\" stack_frame_id=8}\n  %mul.14 = f32[1024]{0} multiply(%param_0.1, %mul.15), metadata={op_name=\"jit(nested_nphy)/jit(nested_nphy)/mul\" stack_frame_id=8}\n  %select_n.7 = f32[1024]{0} select(%gt.8, %mul.16, %mul.14), metadata={op_name=\"jit(nested_nphy)/jit(nested_nphy)/jit(_where)/select_n\" stack_frame_id=11}\n  %constant.10 = f32[] constant(0.5), metadata={op_name=\"jit(nested_nphy)/jit(nested_nphy)\"}\n  %mul.13 = f32[1024]{0} broadcast(%constant.10), dimensions={}, metadata={op_name=\"jit(nested_nphy)/jit(nested_nphy)/mul\" stack_frame_id=6}\n  %mul.12 = f32[1024]{0} multiply(%param_0.1, %mul.13), metadata={op_name=\"jit(nested_nphy)/jit(nested_nphy)/mul\" stack_frame_id=6}\n  ROOT %select_n.6 = f32[1024]{0} select(%gt.10, %select_n.7, %mul.12), metadata={op_name=\"jit(nested_nphy)/jit(nested_nphy)/jit(_where)/select_n\" stack_frame_id=11}\n}\n\nENTRY %main.3 (a.1: f32[1024]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  ROOT %multiply_select_fusion = f32[1024]{0} fusion(%a.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(nested_nphy)/jit(nested_nphy)/jit(_where)/select_n\" stack_frame_id=11}\n}\n\n", "ptx": null}
{"id": 81, "kernel_name": "elementwise_bckc", "python": "@jax.jit\ndef elementwise_bckc(a, b):\n    return a + b", "type": "generate_simple_elementwise", "hlo": "module @jit_elementwise_bckc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>, %arg1: tensor<1024xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @elementwise_bckc(%arg0, %arg1) : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @elementwise_bckc(%arg0: tensor<1024xf32>, %arg1: tensor<1024xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_elementwise_bckc, is_scheduled=true, entry_computation_layout={(f32[1024]{0}, f32[1024]{0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmpkd_m122l.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"elementwise_bckc\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=11 end_column=16}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n\n\n%wrapped_add_computation (param_0: f32[1024], param_1: f32[1024]) -> f32[1024] {\n  %param_0 = f32[1024]{0} parameter(0)\n  %param_1 = f32[1024]{0} parameter(1)\n  ROOT %add.2 = f32[1024]{0} add(%param_0, %param_1), metadata={op_name=\"jit(elementwise_bckc)/jit(elementwise_bckc)/add\" stack_frame_id=5}\n}\n\nENTRY %main.2 (a.1: f32[1024], b.1: f32[1024]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  %b.1 = f32[1024]{0} parameter(1), metadata={op_name=\"b\"}\n  ROOT %wrapped_add = f32[1024]{0} fusion(%a.1, %b.1), kind=kLoop, calls=%wrapped_add_computation, metadata={op_name=\"jit(elementwise_bckc)/jit(elementwise_bckc)/add\" stack_frame_id=5}\n}\n\n", "ptx": null}
{"id": 82, "kernel_name": "loop_yoez", "python": "@jax.jit\ndef loop_yoez(a, n):\n    def body_fun(i, val):\n        return val - a\n    # Initial value matches a's shape/type but zeroed\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "hlo": "module @jit_loop_yoez attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>, %arg1: tensor<i32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @loop_yoez(%arg0, %arg1) : (tensor<1024xf32>, tensor<i32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @loop_yoez(%arg0: tensor<1024xf32>, %arg1: tensor<i32>) -> tensor<1024xf32> {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<1024xf32>, tensor<i32>, tensor<i32>, tensor<1024xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<1024xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<1024xf32>, tensor<i32>, tensor<i32>, tensor<1024xf32>\n    }\n    return %1#3 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_loop_yoez, is_scheduled=true, entry_computation_layout={(f32[1024]{0}, s32[])->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmp9rwexdvg.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"loop_yoez\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=9 end_line=9 column=15 end_column=32}\n6 {file_name_id=3 function_name_id=5 line=10 end_line=10 column=11 end_column=54}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n\n\n%wrapped_add_computation (param_0.1: s32[], param_1: s32[]) -> s32[] {\n  %param_0.1 = s32[] parameter(0)\n  %param_1 = s32[] parameter(1)\n  ROOT %add.0 = s32[] add(%param_0.1, %param_1), metadata={op_name=\"jit(loop_yoez)/jit(loop_yoez)/while/body/add\" stack_frame_id=6}\n}\n\n%wrapped_subtract_computation (param_0.2: f32[1024], param_1.1: f32[1024]) -> f32[1024] {\n  %param_0.2 = f32[1024]{0} parameter(0)\n  %param_1.1 = f32[1024]{0} parameter(1)\n  ROOT %sub.0 = f32[1024]{0} subtract(%param_0.2, %param_1.1), metadata={op_name=\"jit(loop_yoez)/jit(loop_yoez)/while/body/sub\" stack_frame_id=6}\n}\n\n%region_0.1 (arg_tuple.1: (s32[], f32[1024], s32[], f32[1024])) -> (s32[], f32[1024], s32[], f32[1024]) {\n  %arg_tuple.1 = (s32[], f32[1024]{0}, s32[], f32[1024]{0}) parameter(0), metadata={op_name=\"jit(loop_yoez)/jit(loop_yoez)\"}\n  %constant.3 = s32[] constant(1), metadata={op_name=\"jit(loop_yoez)/jit(loop_yoez)\"}\n  %get-tuple-element.19 = f32[1024]{0} get-tuple-element(%arg_tuple.1), index=3, metadata={op_name=\"jit(loop_yoez)/jit(loop_yoez)\"}\n  %get-tuple-element.18 = s32[] get-tuple-element(%arg_tuple.1), index=2, metadata={op_name=\"jit(loop_yoez)/jit(loop_yoez)\"}\n  %get-tuple-element.9 = f32[1024]{0} get-tuple-element(%arg_tuple.1), index=1\n  %get-tuple-element.8 = s32[] get-tuple-element(%arg_tuple.1), index=0\n  %wrapped_subtract = f32[1024]{0} fusion(%get-tuple-element.9, %get-tuple-element.19), kind=kLoop, calls=%wrapped_subtract_computation, metadata={op_name=\"jit(loop_yoez)/jit(loop_yoez)/while/body/sub\" stack_frame_id=6}\n  %wrapped_add = s32[] fusion(%get-tuple-element.8, %constant.3), kind=kLoop, calls=%wrapped_add_computation, metadata={op_name=\"jit(loop_yoez)/jit(loop_yoez)/while/body/add\" stack_frame_id=6}\n  ROOT %tuple.3 = (s32[], f32[1024]{0}, s32[], f32[1024]{0}) tuple(%wrapped_add, %wrapped_subtract, %get-tuple-element.18, %get-tuple-element.19)\n}\n\n%wrapped_compare_computation (param_0.3: s32[], param_1.2: s32[]) -> pred[] {\n  %param_0.3 = s32[] parameter(0)\n  %param_1.2 = s32[] parameter(1)\n  ROOT %lt.0 = pred[] compare(%param_0.3, %param_1.2), direction=LT, metadata={op_name=\"jit(loop_yoez)/jit(loop_yoez)/while/cond/lt\" stack_frame_id=6}\n}\n\n%region_1.2 (arg_tuple.3: (s32[], f32[1024], s32[], f32[1024])) -> pred[] {\n  %arg_tuple.3 = (s32[], f32[1024]{0}, s32[], f32[1024]{0}) parameter(0), metadata={op_name=\"jit(loop_yoez)/jit(loop_yoez)\"}\n  %get-tuple-element.14 = s32[] get-tuple-element(%arg_tuple.3), index=2, metadata={op_name=\"jit(loop_yoez)/jit(loop_yoez)\"}\n  %get-tuple-element.12 = s32[] get-tuple-element(%arg_tuple.3), index=0, metadata={op_name=\"jit(loop_yoez)/jit(loop_yoez)\"}\n  ROOT %wrapped_compare = pred[] fusion(%get-tuple-element.12, %get-tuple-element.14), kind=kLoop, calls=%wrapped_compare_computation, metadata={op_name=\"jit(loop_yoez)/jit(loop_yoez)/while/cond/lt\" stack_frame_id=6}\n}\n\n%wrapped_broadcast_computation (param_0: f32[]) -> f32[1024] {\n  %param_0 = f32[] parameter(0)\n  ROOT %broadcast_in_dim.2 = f32[1024]{0} broadcast(%param_0), dimensions={}, metadata={op_name=\"jit(loop_yoez)/jit(loop_yoez)/broadcast_in_dim\" stack_frame_id=5}\n}\n\nENTRY %main.4 (a.1: f32[1024], n.1: s32[]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  %n.1 = s32[] parameter(1), metadata={op_name=\"n\"}\n  %constant.0 = s32[] constant(0), metadata={op_name=\"jit(loop_yoez)/jit(loop_yoez)\"}\n  %constant.1 = f32[] constant(0), metadata={op_name=\"jit(loop_yoez)/jit(loop_yoez)\"}\n  %copy.6 = s32[] copy(%constant.0)\n  %wrapped_broadcast = f32[1024]{0} fusion(%constant.1), kind=kLoop, calls=%wrapped_broadcast_computation, metadata={op_name=\"jit(loop_yoez)/jit(loop_yoez)/broadcast_in_dim\" stack_frame_id=5}\n  %tuple = (s32[], f32[1024]{0}, s32[], f32[1024]{0}) tuple(%copy.6, %wrapped_broadcast, %n.1, %a.1)\n  %while.1 = (s32[], f32[1024]{0}, s32[], f32[1024]{0}) while(%tuple), condition=%region_1.2, body=%region_0.1, metadata={op_name=\"jit(loop_yoez)/jit(loop_yoez)/while\" stack_frame_id=6}\n  ROOT %while.2 = f32[1024]{0} get-tuple-element(%while.1), index=1, metadata={op_name=\"jit(loop_yoez)/jit(loop_yoez)/while\" stack_frame_id=6}\n}\n\n", "ptx": null}
{"id": 83, "kernel_name": "branch_xfvh", "python": "@jax.jit\ndef branch_xfvh(a):\n    return jnp.where(a > 0.47, a - 2.0, a + 2.0)", "type": "generate_branch_kernel", "hlo": "module @jit_branch_xfvh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @branch_xfvh(%arg0) : (tensor<1024xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @branch_xfvh(%arg0: tensor<1024xf32>) -> tensor<1024xf32> {\n    %cst = stablehlo.constant dense<4.700000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xi1>\n    %cst_0 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<1024xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<1024xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<1024xi1>, tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>\n    return %6 : tensor<1024xf32>\n  }\n  func.func private @_where(%arg0: tensor<1024xi1>, %arg1: tensor<1024xf32>, %arg2: tensor<1024xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<1024xi1>, tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_branch_xfvh, is_scheduled=true, entry_computation_layout={(f32[1024]{0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmp1kp3enoz.py\"\n4 \"/tmp/tmp7nvd0zdg.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"branch_xfvh\"\n6 \"nested_odsh\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=21 end_column=29}\n6 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=31 end_column=38}\n7 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=40 end_column=47}\n8 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=11 end_column=48}\n9 {file_name_id=4 function_name_id=6 line=9 end_line=9 column=17 end_column=61}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=5}\n8 {file_location_id=8 parent_frame_id=5}\n9 {file_location_id=9 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[1024]) -> f32[1024] {\n  %param_0.1 = f32[1024]{0} parameter(0)\n  %constant.7 = f32[] constant(0.47), metadata={op_name=\"jit(branch_xfvh)/jit(branch_xfvh)\"}\n  %gt.5 = f32[1024]{0} broadcast(%constant.7), dimensions={}, metadata={op_name=\"jit(branch_xfvh)/jit(branch_xfvh)/gt\" stack_frame_id=5}\n  %gt.4 = pred[1024]{0} compare(%param_0.1, %gt.5), direction=GT, metadata={op_name=\"jit(branch_xfvh)/jit(branch_xfvh)/gt\" stack_frame_id=5}\n  %constant.6 = f32[] constant(-2), metadata={op_name=\"jit(branch_xfvh)/jit(branch_xfvh)\"}\n  %broadcast.4 = f32[1024]{0} broadcast(%constant.6), dimensions={}, metadata={op_name=\"jit(branch_xfvh)/jit(branch_xfvh)\"}\n  %add.4 = f32[1024]{0} add(%param_0.1, %broadcast.4), metadata={op_name=\"jit(branch_xfvh)/jit(branch_xfvh)/sub\" stack_frame_id=6}\n  %constant.5 = f32[] constant(2), metadata={op_name=\"jit(branch_xfvh)/jit(branch_xfvh)\"}\n  %broadcast.3 = f32[1024]{0} broadcast(%constant.5), dimensions={}, metadata={op_name=\"jit(branch_xfvh)/jit(branch_xfvh)\"}\n  %add.3 = f32[1024]{0} add(%param_0.1, %broadcast.3), metadata={op_name=\"jit(branch_xfvh)/jit(branch_xfvh)/add\" stack_frame_id=7}\n  ROOT %select_n.3 = f32[1024]{0} select(%gt.4, %add.4, %add.3), metadata={op_name=\"jit(branch_xfvh)/jit(branch_xfvh)/jit(_where)/select_n\" stack_frame_id=9}\n}\n\nENTRY %main.3 (a.1: f32[1024]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  ROOT %add_select_fusion = f32[1024]{0} fusion(%a.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(branch_xfvh)/jit(branch_xfvh)/jit(_where)/select_n\" stack_frame_id=9}\n}\n\n", "ptx": null}
{"id": 84, "kernel_name": "compound_oplq", "python": "@jax.jit\ndef compound_oplq(a, b, scale):\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "hlo": "module @jit_compound_oplq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>, %arg1: tensor<1024xf32>, %arg2: tensor<f32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @compound_oplq(%arg0, %arg1, %arg2) : (tensor<1024xf32>, tensor<1024xf32>, tensor<f32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @compound_oplq(%arg0: tensor<1024xf32>, %arg1: tensor<1024xf32>, %arg2: tensor<f32>) -> tensor<1024xf32> {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<1024xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<1024xf32>\n    %4 = stablehlo.floor %3 : tensor<1024xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<1024xf32>\n    %6 = stablehlo.abs %5 : tensor<1024xf32>\n    return %6 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_compound_oplq, is_scheduled=true, entry_computation_layout={(f32[1024]{0}, f32[1024]{0}, f32[])->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmpw1b8fz50.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"compound_oplq\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=14 end_column=19}\n6 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=13 end_column=28}\n7 {file_name_id=3 function_name_id=5 line=9 end_line=9 column=22 end_column=39}\n8 {file_name_id=3 function_name_id=5 line=9 end_line=9 column=13 end_column=39}\n9 {file_name_id=3 function_name_id=5 line=10 end_line=10 column=11 end_column=26}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=5}\n8 {file_location_id=8 parent_frame_id=5}\n9 {file_location_id=9 parent_frame_id=5}\n\n\n%fused_computation (param_0.3: f32[], param_1.3: f32[1024], param_2: f32[1024]) -> f32[1024] {\n  %param_1.3 = f32[1024]{0} parameter(1)\n  %param_2 = f32[1024]{0} parameter(2)\n  %add.2 = f32[1024]{0} add(%param_1.3, %param_2), metadata={op_name=\"jit(compound_oplq)/jit(compound_oplq)/add\" stack_frame_id=5}\n  %param_0.3 = f32[] parameter(0)\n  %mul.5 = f32[1024]{0} broadcast(%param_0.3), dimensions={}, metadata={op_name=\"jit(compound_oplq)/jit(compound_oplq)/mul\" stack_frame_id=6}\n  %mul.4 = f32[1024]{0} multiply(%add.2, %mul.5), metadata={op_name=\"jit(compound_oplq)/jit(compound_oplq)/mul\" stack_frame_id=6}\n  %floor.2 = f32[1024]{0} floor(%mul.4), metadata={op_name=\"jit(compound_oplq)/jit(compound_oplq)/floor\" stack_frame_id=7}\n  %sub.2 = f32[1024]{0} subtract(%mul.4, %floor.2), metadata={op_name=\"jit(compound_oplq)/jit(compound_oplq)/sub\" stack_frame_id=8}\n  ROOT %abs.2 = f32[1024]{0} abs(%sub.2), metadata={op_name=\"jit(compound_oplq)/jit(compound_oplq)/abs\" stack_frame_id=9}\n}\n\nENTRY %main.2 (a.1: f32[1024], b.1: f32[1024], scale.1: f32[]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  %b.1 = f32[1024]{0} parameter(1), metadata={op_name=\"b\"}\n  %scale.1 = f32[] parameter(2), metadata={op_name=\"scale\"}\n  ROOT %subtract_abs_fusion = f32[1024]{0} fusion(%scale.1, %a.1, %b.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(compound_oplq)/jit(compound_oplq)/abs\" stack_frame_id=9}\n}\n\n", "ptx": null}
{"id": 85, "kernel_name": "elementwise_bqoc", "python": "@jax.jit\ndef elementwise_bqoc(a, b):\n    return a + b", "type": "generate_simple_elementwise", "hlo": "module @jit_elementwise_bqoc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>, %arg1: tensor<1024xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @elementwise_bqoc(%arg0, %arg1) : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @elementwise_bqoc(%arg0: tensor<1024xf32>, %arg1: tensor<1024xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_elementwise_bqoc, is_scheduled=true, entry_computation_layout={(f32[1024]{0}, f32[1024]{0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmphemddddk.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"elementwise_bqoc\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=11 end_column=16}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n\n\n%wrapped_add_computation (param_0: f32[1024], param_1: f32[1024]) -> f32[1024] {\n  %param_0 = f32[1024]{0} parameter(0)\n  %param_1 = f32[1024]{0} parameter(1)\n  ROOT %add.2 = f32[1024]{0} add(%param_0, %param_1), metadata={op_name=\"jit(elementwise_bqoc)/jit(elementwise_bqoc)/add\" stack_frame_id=5}\n}\n\nENTRY %main.2 (a.1: f32[1024], b.1: f32[1024]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  %b.1 = f32[1024]{0} parameter(1), metadata={op_name=\"b\"}\n  ROOT %wrapped_add = f32[1024]{0} fusion(%a.1, %b.1), kind=kLoop, calls=%wrapped_add_computation, metadata={op_name=\"jit(elementwise_bqoc)/jit(elementwise_bqoc)/add\" stack_frame_id=5}\n}\n\n", "ptx": null}
{"id": 86, "kernel_name": "branch_ygnd", "python": "@jax.jit\ndef branch_ygnd(a):\n    return jnp.where(a > -0.94, a + 2.1, a - 1.1)", "type": "generate_branch_kernel", "hlo": "module @jit_branch_ygnd attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @branch_ygnd(%arg0) : (tensor<1024xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @branch_ygnd(%arg0: tensor<1024xf32>) -> tensor<1024xf32> {\n    %cst = stablehlo.constant dense<-0.939999997> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xi1>\n    %cst_0 = stablehlo.constant dense<2.100000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<1024xf32>\n    %cst_1 = stablehlo.constant dense<1.100000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<1024xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<1024xi1>, tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>\n    return %6 : tensor<1024xf32>\n  }\n  func.func private @_where(%arg0: tensor<1024xi1>, %arg1: tensor<1024xf32>, %arg2: tensor<1024xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<1024xi1>, tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_branch_ygnd, is_scheduled=true, entry_computation_layout={(f32[1024]{0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmps6mge3xn.py\"\n4 \"/tmp/tmp7nvd0zdg.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"branch_ygnd\"\n6 \"nested_odsh\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=41 end_column=48}\n6 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=32 end_column=39}\n7 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=21 end_column=30}\n8 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=11 end_column=49}\n9 {file_name_id=4 function_name_id=6 line=9 end_line=9 column=17 end_column=61}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=5}\n8 {file_location_id=8 parent_frame_id=5}\n9 {file_location_id=9 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[1024]) -> f32[1024] {\n  %param_0.1 = f32[1024]{0} parameter(0)\n  %constant.9 = f32[] constant(-0.94), metadata={op_name=\"jit(branch_ygnd)/jit(branch_ygnd)\"}\n  %gt.5 = f32[1024]{0} broadcast(%constant.9), dimensions={}, metadata={op_name=\"jit(branch_ygnd)/jit(branch_ygnd)/gt\" stack_frame_id=7}\n  %gt.4 = pred[1024]{0} compare(%param_0.1, %gt.5), direction=GT, metadata={op_name=\"jit(branch_ygnd)/jit(branch_ygnd)/gt\" stack_frame_id=7}\n  %constant.8 = f32[] constant(2.1), metadata={op_name=\"jit(branch_ygnd)/jit(branch_ygnd)\"}\n  %add.7 = f32[1024]{0} broadcast(%constant.8), dimensions={}, metadata={op_name=\"jit(branch_ygnd)/jit(branch_ygnd)/add\" stack_frame_id=6}\n  %add.6 = f32[1024]{0} add(%param_0.1, %add.7), metadata={op_name=\"jit(branch_ygnd)/jit(branch_ygnd)/add\" stack_frame_id=6}\n  %constant.7 = f32[] constant(-1.1), metadata={op_name=\"jit(branch_ygnd)/jit(branch_ygnd)\"}\n  %broadcast.1 = f32[1024]{0} broadcast(%constant.7), dimensions={}, metadata={op_name=\"jit(branch_ygnd)/jit(branch_ygnd)\"}\n  %add.5 = f32[1024]{0} add(%param_0.1, %broadcast.1), metadata={op_name=\"jit(branch_ygnd)/jit(branch_ygnd)/sub\" stack_frame_id=5}\n  ROOT %select_n.3 = f32[1024]{0} select(%gt.4, %add.6, %add.5), metadata={op_name=\"jit(branch_ygnd)/jit(branch_ygnd)/jit(_where)/select_n\" stack_frame_id=9}\n}\n\nENTRY %main.3 (a.1: f32[1024]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  ROOT %add_select_fusion = f32[1024]{0} fusion(%a.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(branch_ygnd)/jit(branch_ygnd)/jit(_where)/select_n\" stack_frame_id=9}\n}\n\n", "ptx": null}
{"id": 87, "kernel_name": "compound_puhw", "python": "@jax.jit\ndef compound_puhw(a, b, scale):\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "hlo": "module @jit_compound_puhw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>, %arg1: tensor<1024xf32>, %arg2: tensor<f32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @compound_puhw(%arg0, %arg1, %arg2) : (tensor<1024xf32>, tensor<1024xf32>, tensor<f32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @compound_puhw(%arg0: tensor<1024xf32>, %arg1: tensor<1024xf32>, %arg2: tensor<f32>) -> tensor<1024xf32> {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<1024xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<1024xf32>\n    %4 = stablehlo.floor %3 : tensor<1024xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<1024xf32>\n    %6 = stablehlo.abs %5 : tensor<1024xf32>\n    return %6 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_compound_puhw, is_scheduled=true, entry_computation_layout={(f32[1024]{0}, f32[1024]{0}, f32[])->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmpic3l9th4.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"compound_puhw\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=14 end_column=19}\n6 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=13 end_column=28}\n7 {file_name_id=3 function_name_id=5 line=9 end_line=9 column=22 end_column=39}\n8 {file_name_id=3 function_name_id=5 line=9 end_line=9 column=13 end_column=39}\n9 {file_name_id=3 function_name_id=5 line=10 end_line=10 column=11 end_column=26}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=5}\n8 {file_location_id=8 parent_frame_id=5}\n9 {file_location_id=9 parent_frame_id=5}\n\n\n%fused_computation (param_0.3: f32[], param_1.3: f32[1024], param_2: f32[1024]) -> f32[1024] {\n  %param_1.3 = f32[1024]{0} parameter(1)\n  %param_2 = f32[1024]{0} parameter(2)\n  %add.2 = f32[1024]{0} add(%param_1.3, %param_2), metadata={op_name=\"jit(compound_puhw)/jit(compound_puhw)/add\" stack_frame_id=5}\n  %param_0.3 = f32[] parameter(0)\n  %mul.5 = f32[1024]{0} broadcast(%param_0.3), dimensions={}, metadata={op_name=\"jit(compound_puhw)/jit(compound_puhw)/mul\" stack_frame_id=6}\n  %mul.4 = f32[1024]{0} multiply(%add.2, %mul.5), metadata={op_name=\"jit(compound_puhw)/jit(compound_puhw)/mul\" stack_frame_id=6}\n  %floor.2 = f32[1024]{0} floor(%mul.4), metadata={op_name=\"jit(compound_puhw)/jit(compound_puhw)/floor\" stack_frame_id=7}\n  %sub.2 = f32[1024]{0} subtract(%mul.4, %floor.2), metadata={op_name=\"jit(compound_puhw)/jit(compound_puhw)/sub\" stack_frame_id=8}\n  ROOT %abs.2 = f32[1024]{0} abs(%sub.2), metadata={op_name=\"jit(compound_puhw)/jit(compound_puhw)/abs\" stack_frame_id=9}\n}\n\nENTRY %main.2 (a.1: f32[1024], b.1: f32[1024], scale.1: f32[]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  %b.1 = f32[1024]{0} parameter(1), metadata={op_name=\"b\"}\n  %scale.1 = f32[] parameter(2), metadata={op_name=\"scale\"}\n  ROOT %subtract_abs_fusion = f32[1024]{0} fusion(%scale.1, %a.1, %b.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(compound_puhw)/jit(compound_puhw)/abs\" stack_frame_id=9}\n}\n\n", "ptx": null}
{"id": 88, "kernel_name": "nested_nhtu", "python": "@jax.jit\ndef nested_nhtu(a):\n    # Nested where: if a > t1 then (if a > t2 then ... else ...) else ...\n    inner_true = a * 3.0\n    inner_false = a * 2.0\n    outer_true = jnp.where(a > 0.52, inner_true, inner_false)\n    outer_false = a * 0.5\n    return jnp.where(a > 0.15, outer_true, outer_false)", "type": "generate_nested_branch_kernel", "hlo": "module @jit_nested_nhtu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @nested_nhtu(%arg0) : (tensor<1024xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @nested_nhtu(%arg0: tensor<1024xf32>) -> tensor<1024xf32> {\n    %cst = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %1 = stablehlo.multiply %arg0, %0 : tensor<1024xf32>\n    %cst_0 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<1024xf32>\n    %cst_1 = stablehlo.constant dense<5.200000e-01> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %5 = stablehlo.compare  GT, %arg0, %4,  FLOAT : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xi1>\n    %6 = call @_where(%5, %1, %3) : (tensor<1024xi1>, tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>\n    %cst_2 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %8 = stablehlo.multiply %arg0, %7 : tensor<1024xf32>\n    %cst_3 = stablehlo.constant dense<1.500000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %10 = stablehlo.compare  GT, %arg0, %9,  FLOAT : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xi1>\n    %11 = call @_where(%10, %6, %8) : (tensor<1024xi1>, tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>\n    return %11 : tensor<1024xf32>\n  }\n  func.func private @_where(%arg0: tensor<1024xi1>, %arg1: tensor<1024xf32>, %arg2: tensor<1024xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<1024xi1>, tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_nested_nhtu, is_scheduled=true, entry_computation_layout={(f32[1024]{0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmpuesa0f3a.py\"\n4 \"/tmp/tmp7nvd0zdg.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"nested_nhtu\"\n6 \"nested_odsh\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=11 end_line=11 column=21 end_column=29}\n6 {file_name_id=3 function_name_id=5 line=10 end_line=10 column=18 end_column=25}\n7 {file_name_id=3 function_name_id=5 line=9 end_line=9 column=27 end_column=35}\n8 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=18 end_column=25}\n9 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=17 end_column=24}\n10 {file_name_id=3 function_name_id=5 line=9 end_line=9 column=17 end_column=61}\n11 {file_name_id=4 function_name_id=6 line=9 end_line=9 column=17 end_column=61}\n12 {file_name_id=3 function_name_id=5 line=11 end_line=11 column=11 end_column=55}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=5}\n8 {file_location_id=8 parent_frame_id=5}\n9 {file_location_id=9 parent_frame_id=5}\n10 {file_location_id=10 parent_frame_id=5}\n11 {file_location_id=11 parent_frame_id=5}\n12 {file_location_id=12 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[1024]) -> f32[1024] {\n  %param_0.1 = f32[1024]{0} parameter(0)\n  %constant.14 = f32[] constant(0.15), metadata={op_name=\"jit(nested_nhtu)/jit(nested_nhtu)\"}\n  %gt.11 = f32[1024]{0} broadcast(%constant.14), dimensions={}, metadata={op_name=\"jit(nested_nhtu)/jit(nested_nhtu)/gt\" stack_frame_id=5}\n  %gt.10 = pred[1024]{0} compare(%param_0.1, %gt.11), direction=GT, metadata={op_name=\"jit(nested_nhtu)/jit(nested_nhtu)/gt\" stack_frame_id=5}\n  %constant.13 = f32[] constant(0.52), metadata={op_name=\"jit(nested_nhtu)/jit(nested_nhtu)\"}\n  %gt.9 = f32[1024]{0} broadcast(%constant.13), dimensions={}, metadata={op_name=\"jit(nested_nhtu)/jit(nested_nhtu)/gt\" stack_frame_id=7}\n  %gt.8 = pred[1024]{0} compare(%param_0.1, %gt.9), direction=GT, metadata={op_name=\"jit(nested_nhtu)/jit(nested_nhtu)/gt\" stack_frame_id=7}\n  %constant.12 = f32[] constant(3), metadata={op_name=\"jit(nested_nhtu)/jit(nested_nhtu)\"}\n  %mul.17 = f32[1024]{0} broadcast(%constant.12), dimensions={}, metadata={op_name=\"jit(nested_nhtu)/jit(nested_nhtu)/mul\" stack_frame_id=9}\n  %mul.16 = f32[1024]{0} multiply(%param_0.1, %mul.17), metadata={op_name=\"jit(nested_nhtu)/jit(nested_nhtu)/mul\" stack_frame_id=9}\n  %constant.11 = f32[] constant(2), metadata={op_name=\"jit(nested_nhtu)/jit(nested_nhtu)\"}\n  %mul.15 = f32[1024]{0} broadcast(%constant.11), dimensions={}, metadata={op_name=\"jit(nested_nhtu)/jit(nested_nhtu)/mul\" stack_frame_id=8}\n  %mul.14 = f32[1024]{0} multiply(%param_0.1, %mul.15), metadata={op_name=\"jit(nested_nhtu)/jit(nested_nhtu)/mul\" stack_frame_id=8}\n  %select_n.7 = f32[1024]{0} select(%gt.8, %mul.16, %mul.14), metadata={op_name=\"jit(nested_nhtu)/jit(nested_nhtu)/jit(_where)/select_n\" stack_frame_id=11}\n  %constant.10 = f32[] constant(0.5), metadata={op_name=\"jit(nested_nhtu)/jit(nested_nhtu)\"}\n  %mul.13 = f32[1024]{0} broadcast(%constant.10), dimensions={}, metadata={op_name=\"jit(nested_nhtu)/jit(nested_nhtu)/mul\" stack_frame_id=6}\n  %mul.12 = f32[1024]{0} multiply(%param_0.1, %mul.13), metadata={op_name=\"jit(nested_nhtu)/jit(nested_nhtu)/mul\" stack_frame_id=6}\n  ROOT %select_n.6 = f32[1024]{0} select(%gt.10, %select_n.7, %mul.12), metadata={op_name=\"jit(nested_nhtu)/jit(nested_nhtu)/jit(_where)/select_n\" stack_frame_id=11}\n}\n\nENTRY %main.3 (a.1: f32[1024]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  ROOT %multiply_select_fusion = f32[1024]{0} fusion(%a.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(nested_nhtu)/jit(nested_nhtu)/jit(_where)/select_n\" stack_frame_id=11}\n}\n\n", "ptx": null}
{"id": 89, "kernel_name": "reduce_iuch", "python": "@jax.jit\ndef reduce_iuch(a):\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "hlo": "module @jit_reduce_iuch attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %0 = call @reduce_iuch(%arg0) : (tensor<1024xf32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n  func.func private @reduce_iuch(%arg0: tensor<1024xf32>) -> tensor<f32> {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<1024xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "llvm": "HloModule jit_reduce_iuch, is_scheduled=true, entry_computation_layout={(f32[1024]{0})->f32[]}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmp5lv0ofhb.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"reduce_iuch\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=11 end_column=21}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n\n\n%region_0.1 (reduce_sum.3: f32[], reduce_sum.4: f32[]) -> f32[] {\n  %reduce_sum.3 = f32[] parameter(0), metadata={op_name=\"reduce_sum\"}\n  %reduce_sum.4 = f32[] parameter(1), metadata={op_name=\"reduce_sum\"}\n  ROOT %reduce_sum.5 = f32[] add(%reduce_sum.3, %reduce_sum.4), metadata={op_name=\"reduce_sum\" stack_frame_id=5}\n}\n\n%wrapped_reduce-window_computation (param_0: f32[1024], param_1: f32[]) -> f32[32] {\n  %param_0 = f32[1024]{0} parameter(0)\n  %param_1 = f32[] parameter(1)\n  ROOT %reduce-window.1 = f32[32]{0} reduce-window(%param_0, %param_1), window={size=32 stride=32}, to_apply=%region_0.1\n}\n\n%region_0.1.clone (reduce_sum.1: f32[], reduce_sum.2: f32[]) -> f32[] {\n  %reduce_sum.1 = f32[] parameter(0), metadata={op_name=\"reduce_sum\"}\n  %reduce_sum.2 = f32[] parameter(1), metadata={op_name=\"reduce_sum\"}\n  ROOT %reduce_sum.6 = f32[] add(%reduce_sum.1, %reduce_sum.2), metadata={op_name=\"reduce_sum\" stack_frame_id=5}\n}\n\n%wrapped_reduce_computation (param_0.1: f32[32], param_1.1: f32[]) -> f32[] {\n  %param_0.1 = f32[32]{0} parameter(0)\n  %param_1.1 = f32[] parameter(1)\n  ROOT %reduce_sum.8 = f32[] reduce(%param_0.1, %param_1.1), dimensions={0}, to_apply=%region_0.1.clone, metadata={op_name=\"jit(reduce_iuch)/jit(reduce_iuch)/reduce_sum\" stack_frame_id=5}\n}\n\nENTRY %main.3 (a.1: f32[1024]) -> f32[] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  %constant.0 = f32[] constant(0), metadata={op_name=\"jit(reduce_iuch)/jit(reduce_iuch)\"}\n  %wrapped_reduce-window = f32[32]{0} fusion(%a.1, %constant.0), kind=kLoop, calls=%wrapped_reduce-window_computation\n  ROOT %wrapped_reduce = f32[] fusion(%wrapped_reduce-window, %constant.0), kind=kLoop, calls=%wrapped_reduce_computation, metadata={op_name=\"jit(reduce_iuch)/jit(reduce_iuch)/reduce_sum\" stack_frame_id=5}\n}\n\n", "ptx": null}
{"id": 90, "kernel_name": "vec_kkxx", "python": "@jax.jit\ndef vec_kkxx(a):\n    return jnp.linalg.norm(a, axis=-1)", "type": "generate_vector_kernel", "hlo": "module @jit_vec_kkxx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024x3xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @vec_kkxx(%arg0) : (tensor<1024x3xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @vec_kkxx(%arg0: tensor<1024x3xf32>) -> tensor<1024xf32> {\n    %0 = call @norm(%arg0) : (tensor<1024x3xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @norm(%arg0: tensor<1024x3xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<1024x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<1024x3xf32>, tensor<f32>) -> tensor<1024xf32>\n    %2 = stablehlo.sqrt %1 : tensor<1024xf32>\n    return %2 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_vec_kkxx, is_scheduled=true, entry_computation_layout={(f32[1024,3]{1,0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmpg59_a88u.py\"\n4 \"/tmp/tmp6o75inah.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"vec_kkxx\"\n6 \"vec_khvl\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=11 end_column=38}\n6 {file_name_id=4 function_name_id=6 line=6 end_line=6 column=11 end_column=38}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n\n\n%region_0.1 (reduce_sum.3: f32[], reduce_sum.4: f32[]) -> f32[] {\n  %reduce_sum.3 = f32[] parameter(0), metadata={op_name=\"reduce_sum\"}\n  %reduce_sum.4 = f32[] parameter(1), metadata={op_name=\"reduce_sum\"}\n  ROOT %reduce_sum.5 = f32[] add(%reduce_sum.3, %reduce_sum.4), metadata={op_name=\"reduce_sum\" stack_frame_id=6}\n}\n\n%fused_computation (param_0.2: f32[1024,3]) -> f32[1024] {\n  %param_0.2 = f32[1024,3]{1,0} parameter(0)\n  %mul.3 = f32[1024,3]{1,0} multiply(%param_0.2, %param_0.2), metadata={op_name=\"jit(vec_kkxx)/jit(vec_kkxx)/jit(norm)/mul\" stack_frame_id=6}\n  %constant.3 = f32[] constant(0), metadata={op_name=\"jit(vec_kkxx)/jit(vec_kkxx)/jit(norm)\"}\n  %reduce_sum.2 = f32[1024]{0} reduce(%mul.3, %constant.3), dimensions={1}, to_apply=%region_0.1, metadata={op_name=\"jit(vec_kkxx)/jit(vec_kkxx)/jit(norm)/reduce_sum\" stack_frame_id=6}\n  ROOT %sqrt.3 = f32[1024]{0} sqrt(%reduce_sum.2), metadata={op_name=\"jit(vec_kkxx)/jit(vec_kkxx)/jit(norm)/sqrt\" stack_frame_id=6}\n}\n\nENTRY %main.4 (a.1: f32[1024,3]) -> f32[1024] {\n  %a.1 = f32[1024,3]{1,0} parameter(0), metadata={op_name=\"a\"}\n  ROOT %reduce_sqrt_fusion = f32[1024]{0} fusion(%a.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(vec_kkxx)/jit(vec_kkxx)/jit(norm)/sqrt\" stack_frame_id=6}\n}\n\n", "ptx": null}
{"id": 91, "kernel_name": "multi_mmtv", "python": "@jax.jit\ndef multi_mmtv(a, b):\n    temp1 = a - b\n    temp2 = jnp.sqrt(temp1)\n    return temp2 * a", "type": "generate_multi_statement_kernel", "hlo": "module @jit_multi_mmtv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>, %arg1: tensor<1024xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @multi_mmtv(%arg0, %arg1) : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @multi_mmtv(%arg0: tensor<1024xf32>, %arg1: tensor<1024xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<1024xf32>\n    %1 = stablehlo.sqrt %0 : tensor<1024xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<1024xf32>\n    return %2 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_multi_mmtv, is_scheduled=true, entry_computation_layout={(f32[1024]{0}, f32[1024]{0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmpo91euc9_.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"multi_mmtv\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=12 end_column=17}\n6 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=12 end_column=27}\n7 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=11 end_column=20}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[1024], param_1.2: f32[1024]) -> f32[1024] {\n  %param_0.1 = f32[1024]{0} parameter(0)\n  %param_1.2 = f32[1024]{0} parameter(1)\n  %sub.2 = f32[1024]{0} subtract(%param_0.1, %param_1.2), metadata={op_name=\"jit(multi_mmtv)/jit(multi_mmtv)/sub\" stack_frame_id=5}\n  %sqrt.2 = f32[1024]{0} sqrt(%sub.2), metadata={op_name=\"jit(multi_mmtv)/jit(multi_mmtv)/sqrt\" stack_frame_id=6}\n  ROOT %mul.2 = f32[1024]{0} multiply(%sqrt.2, %param_0.1), metadata={op_name=\"jit(multi_mmtv)/jit(multi_mmtv)/mul\" stack_frame_id=7}\n}\n\nENTRY %main.2 (a.1: f32[1024], b.1: f32[1024]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  %b.1 = f32[1024]{0} parameter(1), metadata={op_name=\"b\"}\n  ROOT %sqrt_multiply_fusion = f32[1024]{0} fusion(%a.1, %b.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(multi_mmtv)/jit(multi_mmtv)/mul\" stack_frame_id=7}\n}\n\n", "ptx": null}
{"id": 92, "kernel_name": "multi_mfzx", "python": "@jax.jit\ndef multi_mfzx(a, b):\n    temp1 = a * b\n    temp2 = jnp.abs(temp1)\n    return temp2 * a", "type": "generate_multi_statement_kernel", "hlo": "module @jit_multi_mfzx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>, %arg1: tensor<1024xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @multi_mfzx(%arg0, %arg1) : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @multi_mfzx(%arg0: tensor<1024xf32>, %arg1: tensor<1024xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<1024xf32>\n    %1 = stablehlo.abs %0 : tensor<1024xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<1024xf32>\n    return %2 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_multi_mfzx, is_scheduled=true, entry_computation_layout={(f32[1024]{0}, f32[1024]{0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmp_0temiu0.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"multi_mfzx\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=12 end_column=17}\n6 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=12 end_column=26}\n7 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=11 end_column=20}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[1024], param_1.2: f32[1024]) -> f32[1024] {\n  %param_0.1 = f32[1024]{0} parameter(0)\n  %param_1.2 = f32[1024]{0} parameter(1)\n  %mul.5 = f32[1024]{0} multiply(%param_0.1, %param_1.2), metadata={op_name=\"jit(multi_mfzx)/jit(multi_mfzx)/mul\" stack_frame_id=5}\n  %abs.2 = f32[1024]{0} abs(%mul.5), metadata={op_name=\"jit(multi_mfzx)/jit(multi_mfzx)/abs\" stack_frame_id=6}\n  ROOT %mul.4 = f32[1024]{0} multiply(%abs.2, %param_0.1), metadata={op_name=\"jit(multi_mfzx)/jit(multi_mfzx)/mul\" stack_frame_id=7}\n}\n\nENTRY %main.2 (a.1: f32[1024], b.1: f32[1024]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  %b.1 = f32[1024]{0} parameter(1), metadata={op_name=\"b\"}\n  ROOT %abs_multiply_fusion = f32[1024]{0} fusion(%a.1, %b.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(multi_mfzx)/jit(multi_mfzx)/mul\" stack_frame_id=7}\n}\n\n", "ptx": null}
{"id": 93, "kernel_name": "reduce_rqpm", "python": "@jax.jit\ndef reduce_rqpm(a):\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "hlo": "module @jit_reduce_rqpm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %0 = call @reduce_rqpm(%arg0) : (tensor<1024xf32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n  func.func private @reduce_rqpm(%arg0: tensor<1024xf32>) -> tensor<f32> {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<1024xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "llvm": "HloModule jit_reduce_rqpm, is_scheduled=true, entry_computation_layout={(f32[1024]{0})->f32[]}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmp1f3eiw0_.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"reduce_rqpm\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=11 end_column=21}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n\n\n%region_0.1 (reduce_sum.3: f32[], reduce_sum.4: f32[]) -> f32[] {\n  %reduce_sum.3 = f32[] parameter(0), metadata={op_name=\"reduce_sum\"}\n  %reduce_sum.4 = f32[] parameter(1), metadata={op_name=\"reduce_sum\"}\n  ROOT %reduce_sum.5 = f32[] add(%reduce_sum.3, %reduce_sum.4), metadata={op_name=\"reduce_sum\" stack_frame_id=5}\n}\n\n%wrapped_reduce-window_computation (param_0: f32[1024], param_1: f32[]) -> f32[32] {\n  %param_0 = f32[1024]{0} parameter(0)\n  %param_1 = f32[] parameter(1)\n  ROOT %reduce-window.1 = f32[32]{0} reduce-window(%param_0, %param_1), window={size=32 stride=32}, to_apply=%region_0.1\n}\n\n%region_0.1.clone (reduce_sum.1: f32[], reduce_sum.2: f32[]) -> f32[] {\n  %reduce_sum.1 = f32[] parameter(0), metadata={op_name=\"reduce_sum\"}\n  %reduce_sum.2 = f32[] parameter(1), metadata={op_name=\"reduce_sum\"}\n  ROOT %reduce_sum.6 = f32[] add(%reduce_sum.1, %reduce_sum.2), metadata={op_name=\"reduce_sum\" stack_frame_id=5}\n}\n\n%wrapped_reduce_computation (param_0.1: f32[32], param_1.1: f32[]) -> f32[] {\n  %param_0.1 = f32[32]{0} parameter(0)\n  %param_1.1 = f32[] parameter(1)\n  ROOT %reduce_sum.8 = f32[] reduce(%param_0.1, %param_1.1), dimensions={0}, to_apply=%region_0.1.clone, metadata={op_name=\"jit(reduce_rqpm)/jit(reduce_rqpm)/reduce_sum\" stack_frame_id=5}\n}\n\nENTRY %main.3 (a.1: f32[1024]) -> f32[] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  %constant.0 = f32[] constant(0), metadata={op_name=\"jit(reduce_rqpm)/jit(reduce_rqpm)\"}\n  %wrapped_reduce-window = f32[32]{0} fusion(%a.1, %constant.0), kind=kLoop, calls=%wrapped_reduce-window_computation\n  ROOT %wrapped_reduce = f32[] fusion(%wrapped_reduce-window, %constant.0), kind=kLoop, calls=%wrapped_reduce_computation, metadata={op_name=\"jit(reduce_rqpm)/jit(reduce_rqpm)/reduce_sum\" stack_frame_id=5}\n}\n\n", "ptx": null}
{"id": 94, "kernel_name": "compound_onlt", "python": "@jax.jit\ndef compound_onlt(a, b, scale):\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "hlo": "module @jit_compound_onlt attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>, %arg1: tensor<1024xf32>, %arg2: tensor<f32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @compound_onlt(%arg0, %arg1, %arg2) : (tensor<1024xf32>, tensor<1024xf32>, tensor<f32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @compound_onlt(%arg0: tensor<1024xf32>, %arg1: tensor<1024xf32>, %arg2: tensor<f32>) -> tensor<1024xf32> {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<1024xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<1024xf32>\n    %4 = stablehlo.floor %3 : tensor<1024xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<1024xf32>\n    %6 = stablehlo.abs %5 : tensor<1024xf32>\n    return %6 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_compound_onlt, is_scheduled=true, entry_computation_layout={(f32[1024]{0}, f32[1024]{0}, f32[])->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmpif0s6qk6.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"compound_onlt\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=14 end_column=19}\n6 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=13 end_column=28}\n7 {file_name_id=3 function_name_id=5 line=9 end_line=9 column=22 end_column=39}\n8 {file_name_id=3 function_name_id=5 line=9 end_line=9 column=13 end_column=39}\n9 {file_name_id=3 function_name_id=5 line=10 end_line=10 column=11 end_column=26}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=5}\n8 {file_location_id=8 parent_frame_id=5}\n9 {file_location_id=9 parent_frame_id=5}\n\n\n%fused_computation (param_0.3: f32[], param_1.3: f32[1024], param_2: f32[1024]) -> f32[1024] {\n  %param_1.3 = f32[1024]{0} parameter(1)\n  %param_2 = f32[1024]{0} parameter(2)\n  %add.2 = f32[1024]{0} add(%param_1.3, %param_2), metadata={op_name=\"jit(compound_onlt)/jit(compound_onlt)/add\" stack_frame_id=5}\n  %param_0.3 = f32[] parameter(0)\n  %mul.5 = f32[1024]{0} broadcast(%param_0.3), dimensions={}, metadata={op_name=\"jit(compound_onlt)/jit(compound_onlt)/mul\" stack_frame_id=6}\n  %mul.4 = f32[1024]{0} multiply(%add.2, %mul.5), metadata={op_name=\"jit(compound_onlt)/jit(compound_onlt)/mul\" stack_frame_id=6}\n  %floor.2 = f32[1024]{0} floor(%mul.4), metadata={op_name=\"jit(compound_onlt)/jit(compound_onlt)/floor\" stack_frame_id=7}\n  %sub.2 = f32[1024]{0} subtract(%mul.4, %floor.2), metadata={op_name=\"jit(compound_onlt)/jit(compound_onlt)/sub\" stack_frame_id=8}\n  ROOT %abs.2 = f32[1024]{0} abs(%sub.2), metadata={op_name=\"jit(compound_onlt)/jit(compound_onlt)/abs\" stack_frame_id=9}\n}\n\nENTRY %main.2 (a.1: f32[1024], b.1: f32[1024], scale.1: f32[]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  %b.1 = f32[1024]{0} parameter(1), metadata={op_name=\"b\"}\n  %scale.1 = f32[] parameter(2), metadata={op_name=\"scale\"}\n  ROOT %subtract_abs_fusion = f32[1024]{0} fusion(%scale.1, %a.1, %b.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(compound_onlt)/jit(compound_onlt)/abs\" stack_frame_id=9}\n}\n\n", "ptx": null}
{"id": 95, "kernel_name": "scalar_arr_bkzg", "python": "@jax.jit\ndef scalar_arr_bkzg(alpha, x, y):\n    return alpha - x + y", "type": "generate_scalar_array_op", "hlo": "module @jit_scalar_arr_bkzg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<1024xf32>, %arg2: tensor<1024xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @scalar_arr_bkzg(%arg0, %arg1, %arg2) : (tensor<f32>, tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @scalar_arr_bkzg(%arg0: tensor<f32>, %arg1: tensor<1024xf32>, %arg2: tensor<1024xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %2 = stablehlo.subtract %1, %arg1 : tensor<1024xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<1024xf32>\n    return %3 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_scalar_arr_bkzg, is_scheduled=true, entry_computation_layout={(f32[], f32[1024]{0}, f32[1024]{0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmpk1zesgb1.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"scalar_arr_bkzg\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=11 end_column=20}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[1024], param_1.2: f32[1024], param_2.1: f32[]) -> f32[1024] {\n  %param_2.1 = f32[] parameter(2)\n  %sub.5 = f32[1024]{0} broadcast(%param_2.1), dimensions={}, metadata={op_name=\"jit(scalar_arr_bkzg)/jit(scalar_arr_bkzg)/sub\" stack_frame_id=5}\n  %param_1.2 = f32[1024]{0} parameter(1)\n  %sub.4 = f32[1024]{0} subtract(%sub.5, %param_1.2), metadata={op_name=\"jit(scalar_arr_bkzg)/jit(scalar_arr_bkzg)/sub\" stack_frame_id=5}\n  %param_0.1 = f32[1024]{0} parameter(0)\n  ROOT %add.2 = f32[1024]{0} add(%sub.4, %param_0.1), metadata={op_name=\"jit(scalar_arr_bkzg)/jit(scalar_arr_bkzg)/add\" stack_frame_id=5}\n}\n\nENTRY %main.2 (alpha.1: f32[], x.1: f32[1024], y.1: f32[1024]) -> f32[1024] {\n  %alpha.1 = f32[] parameter(0), metadata={op_name=\"alpha\"}\n  %x.1 = f32[1024]{0} parameter(1), metadata={op_name=\"x\"}\n  %y.1 = f32[1024]{0} parameter(2), metadata={op_name=\"y\"}\n  ROOT %subtract_add_fusion = f32[1024]{0} fusion(%y.1, %x.1, %alpha.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(scalar_arr_bkzg)/jit(scalar_arr_bkzg)/add\" stack_frame_id=5}\n}\n\n", "ptx": null}
{"id": 96, "kernel_name": "branch_flsf", "python": "@jax.jit\ndef branch_flsf(a):\n    return jnp.where(a > 0.72, a + 0.9, a + 1.6)", "type": "generate_branch_kernel", "hlo": "module @jit_branch_flsf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @branch_flsf(%arg0) : (tensor<1024xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @branch_flsf(%arg0: tensor<1024xf32>) -> tensor<1024xf32> {\n    %cst = stablehlo.constant dense<7.200000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xi1>\n    %cst_0 = stablehlo.constant dense<0.899999976> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<1024xf32>\n    %cst_1 = stablehlo.constant dense<1.600000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<1024xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<1024xi1>, tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>\n    return %6 : tensor<1024xf32>\n  }\n  func.func private @_where(%arg0: tensor<1024xi1>, %arg1: tensor<1024xf32>, %arg2: tensor<1024xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<1024xi1>, tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_branch_flsf, is_scheduled=true, entry_computation_layout={(f32[1024]{0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmp8225y27q.py\"\n4 \"/tmp/tmp7nvd0zdg.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"branch_flsf\"\n6 \"nested_odsh\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=40 end_column=47}\n6 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=31 end_column=38}\n7 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=21 end_column=29}\n8 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=11 end_column=48}\n9 {file_name_id=4 function_name_id=6 line=9 end_line=9 column=17 end_column=61}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=5}\n8 {file_location_id=8 parent_frame_id=5}\n9 {file_location_id=9 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[1024]) -> f32[1024] {\n  %param_0.1 = f32[1024]{0} parameter(0)\n  %constant.8 = f32[] constant(0.72), metadata={op_name=\"jit(branch_flsf)/jit(branch_flsf)\"}\n  %gt.5 = f32[1024]{0} broadcast(%constant.8), dimensions={}, metadata={op_name=\"jit(branch_flsf)/jit(branch_flsf)/gt\" stack_frame_id=7}\n  %gt.4 = pred[1024]{0} compare(%param_0.1, %gt.5), direction=GT, metadata={op_name=\"jit(branch_flsf)/jit(branch_flsf)/gt\" stack_frame_id=7}\n  %constant.7 = f32[] constant(0.9), metadata={op_name=\"jit(branch_flsf)/jit(branch_flsf)\"}\n  %add.11 = f32[1024]{0} broadcast(%constant.7), dimensions={}, metadata={op_name=\"jit(branch_flsf)/jit(branch_flsf)/add\" stack_frame_id=6}\n  %add.10 = f32[1024]{0} add(%param_0.1, %add.11), metadata={op_name=\"jit(branch_flsf)/jit(branch_flsf)/add\" stack_frame_id=6}\n  %constant.6 = f32[] constant(1.6), metadata={op_name=\"jit(branch_flsf)/jit(branch_flsf)\"}\n  %add.9 = f32[1024]{0} broadcast(%constant.6), dimensions={}, metadata={op_name=\"jit(branch_flsf)/jit(branch_flsf)/add\" stack_frame_id=5}\n  %add.8 = f32[1024]{0} add(%param_0.1, %add.9), metadata={op_name=\"jit(branch_flsf)/jit(branch_flsf)/add\" stack_frame_id=5}\n  ROOT %select_n.3 = f32[1024]{0} select(%gt.4, %add.10, %add.8), metadata={op_name=\"jit(branch_flsf)/jit(branch_flsf)/jit(_where)/select_n\" stack_frame_id=9}\n}\n\nENTRY %main.3 (a.1: f32[1024]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  ROOT %add_select_fusion = f32[1024]{0} fusion(%a.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(branch_flsf)/jit(branch_flsf)/jit(_where)/select_n\" stack_frame_id=9}\n}\n\n", "ptx": null}
{"id": 97, "kernel_name": "elementwise_asox", "python": "@jax.jit\ndef elementwise_asox(a, b):\n    return a * b", "type": "generate_simple_elementwise", "hlo": "module @jit_elementwise_asox attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>, %arg1: tensor<1024xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @elementwise_asox(%arg0, %arg1) : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @elementwise_asox(%arg0: tensor<1024xf32>, %arg1: tensor<1024xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_elementwise_asox, is_scheduled=true, entry_computation_layout={(f32[1024]{0}, f32[1024]{0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmpernux0al.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"elementwise_asox\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=11 end_column=16}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n\n\n%wrapped_multiply_computation (param_0: f32[1024], param_1: f32[1024]) -> f32[1024] {\n  %param_0 = f32[1024]{0} parameter(0)\n  %param_1 = f32[1024]{0} parameter(1)\n  ROOT %mul.2 = f32[1024]{0} multiply(%param_0, %param_1), metadata={op_name=\"jit(elementwise_asox)/jit(elementwise_asox)/mul\" stack_frame_id=5}\n}\n\nENTRY %main.2 (a.1: f32[1024], b.1: f32[1024]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  %b.1 = f32[1024]{0} parameter(1), metadata={op_name=\"b\"}\n  ROOT %wrapped_multiply = f32[1024]{0} fusion(%a.1, %b.1), kind=kLoop, calls=%wrapped_multiply_computation, metadata={op_name=\"jit(elementwise_asox)/jit(elementwise_asox)/mul\" stack_frame_id=5}\n}\n\n", "ptx": null}
{"id": 98, "kernel_name": "scalar_arr_ucgn", "python": "@jax.jit\ndef scalar_arr_ucgn(alpha, x, y):\n    return alpha + x * y", "type": "generate_scalar_array_op", "hlo": "module @jit_scalar_arr_ucgn attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<1024xf32>, %arg2: tensor<1024xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @scalar_arr_ucgn(%arg0, %arg1, %arg2) : (tensor<f32>, tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @scalar_arr_ucgn(%arg0: tensor<f32>, %arg1: tensor<1024xf32>, %arg2: tensor<1024xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<1024xf32>\n    %1 = stablehlo.convert %arg0 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %3 = stablehlo.add %2, %0 : tensor<1024xf32>\n    return %3 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_scalar_arr_ucgn, is_scheduled=true, entry_computation_layout={(f32[], f32[1024]{0}, f32[1024]{0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmp0vh3zr5r.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"scalar_arr_ucgn\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=19 end_column=24}\n6 {file_name_id=3 function_name_id=5 line=6 end_line=6 column=11 end_column=24}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[1024], param_1.2: f32[1024], param_2.1: f32[]) -> f32[1024] {\n  %param_2.1 = f32[] parameter(2)\n  %add.5 = f32[1024]{0} broadcast(%param_2.1), dimensions={}, metadata={op_name=\"jit(scalar_arr_ucgn)/jit(scalar_arr_ucgn)/add\" stack_frame_id=6}\n  %param_0.1 = f32[1024]{0} parameter(0)\n  %param_1.2 = f32[1024]{0} parameter(1)\n  %mul.2 = f32[1024]{0} multiply(%param_0.1, %param_1.2), metadata={op_name=\"jit(scalar_arr_ucgn)/jit(scalar_arr_ucgn)/mul\" stack_frame_id=5}\n  ROOT %add.4 = f32[1024]{0} add(%add.5, %mul.2), metadata={op_name=\"jit(scalar_arr_ucgn)/jit(scalar_arr_ucgn)/add\" stack_frame_id=6}\n}\n\nENTRY %main.2 (alpha.1: f32[], x.1: f32[1024], y.1: f32[1024]) -> f32[1024] {\n  %alpha.1 = f32[] parameter(0), metadata={op_name=\"alpha\"}\n  %x.1 = f32[1024]{0} parameter(1), metadata={op_name=\"x\"}\n  %y.1 = f32[1024]{0} parameter(2), metadata={op_name=\"y\"}\n  ROOT %multiply_add_fusion = f32[1024]{0} fusion(%x.1, %y.1, %alpha.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(scalar_arr_ucgn)/jit(scalar_arr_ucgn)/add\" stack_frame_id=6}\n}\n\n", "ptx": null}
{"id": 99, "kernel_name": "nested_naue", "python": "@jax.jit\ndef nested_naue(a):\n    # Nested where: if a > t1 then (if a > t2 then ... else ...) else ...\n    inner_true = a * 3.0\n    inner_false = a * 2.0\n    outer_true = jnp.where(a > 0.5, inner_true, inner_false)\n    outer_false = a * 0.5\n    return jnp.where(a > 0.2, outer_true, outer_false)", "type": "generate_nested_branch_kernel", "hlo": "module @jit_nested_naue attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1024xf32>) -> (tensor<1024xf32> {jax.result_info = \"result\"}) {\n    %0 = call @nested_naue(%arg0) : (tensor<1024xf32>) -> tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n  func.func private @nested_naue(%arg0: tensor<1024xf32>) -> tensor<1024xf32> {\n    %cst = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %1 = stablehlo.multiply %arg0, %0 : tensor<1024xf32>\n    %cst_0 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<1024xf32>\n    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %5 = stablehlo.compare  GT, %arg0, %4,  FLOAT : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xi1>\n    %6 = call @_where(%5, %1, %3) : (tensor<1024xi1>, tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>\n    %cst_2 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %8 = stablehlo.multiply %arg0, %7 : tensor<1024xf32>\n    %cst_3 = stablehlo.constant dense<2.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<1024xf32>\n    %10 = stablehlo.compare  GT, %arg0, %9,  FLOAT : (tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xi1>\n    %11 = call @_where(%10, %6, %8) : (tensor<1024xi1>, tensor<1024xf32>, tensor<1024xf32>) -> tensor<1024xf32>\n    return %11 : tensor<1024xf32>\n  }\n  func.func private @_where(%arg0: tensor<1024xi1>, %arg1: tensor<1024xf32>, %arg2: tensor<1024xf32>) -> tensor<1024xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<1024xi1>, tensor<1024xf32>\n    return %0 : tensor<1024xf32>\n  }\n}\n", "llvm": "HloModule jit_nested_naue, is_scheduled=true, entry_computation_layout={(f32[1024]{0})->f32[1024]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/jax_pipeline.py\"\n2 \"/workspace/jit/code/extraction/jax_ir_extractor.py\"\n3 \"/tmp/tmpvw93_pm2.py\"\n4 \"/tmp/tmp7nvd0zdg.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"nested_naue\"\n6 \"nested_odsh\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=223 end_line=223 column=8 end_column=67}\n2 {file_name_id=1 function_name_id=2 line=184 end_line=184 column=21 end_column=74}\n3 {file_name_id=1 function_name_id=3 line=83 end_line=83 column=13 end_column=44}\n4 {file_name_id=2 function_name_id=4 line=82 end_line=82 column=14 end_column=35}\n5 {file_name_id=3 function_name_id=5 line=11 end_line=11 column=21 end_column=28}\n6 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=18 end_column=25}\n7 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=17 end_column=24}\n8 {file_name_id=3 function_name_id=5 line=9 end_line=9 column=27 end_column=34}\n9 {file_name_id=3 function_name_id=5 line=9 end_line=9 column=17 end_column=60}\n10 {file_name_id=4 function_name_id=6 line=9 end_line=9 column=17 end_column=61}\n11 {file_name_id=3 function_name_id=5 line=10 end_line=10 column=18 end_column=25}\n12 {file_name_id=3 function_name_id=5 line=11 end_line=11 column=11 end_column=54}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=5}\n8 {file_location_id=8 parent_frame_id=5}\n9 {file_location_id=9 parent_frame_id=5}\n10 {file_location_id=10 parent_frame_id=5}\n11 {file_location_id=11 parent_frame_id=5}\n12 {file_location_id=12 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[1024]) -> f32[1024] {\n  %param_0.1 = f32[1024]{0} parameter(0)\n  %constant.11 = f32[] constant(0.2), metadata={op_name=\"jit(nested_naue)/jit(nested_naue)\"}\n  %gt.8 = f32[1024]{0} broadcast(%constant.11), dimensions={}, metadata={op_name=\"jit(nested_naue)/jit(nested_naue)/gt\" stack_frame_id=5}\n  %gt.7 = pred[1024]{0} compare(%param_0.1, %gt.8), direction=GT, metadata={op_name=\"jit(nested_naue)/jit(nested_naue)/gt\" stack_frame_id=5}\n  %constant.10 = f32[] constant(0.5), metadata={op_name=\"jit(nested_naue)/jit(nested_naue)\"}\n  %broadcast.2 = f32[1024]{0} broadcast(%constant.10), dimensions={}, metadata={op_name=\"jit(nested_naue)/jit(nested_naue)\"}\n  %gt.6 = pred[1024]{0} compare(%param_0.1, %broadcast.2), direction=GT, metadata={op_name=\"jit(nested_naue)/jit(nested_naue)/gt\" stack_frame_id=8}\n  %constant.9 = f32[] constant(3), metadata={op_name=\"jit(nested_naue)/jit(nested_naue)\"}\n  %mul.14 = f32[1024]{0} broadcast(%constant.9), dimensions={}, metadata={op_name=\"jit(nested_naue)/jit(nested_naue)/mul\" stack_frame_id=7}\n  %mul.13 = f32[1024]{0} multiply(%param_0.1, %mul.14), metadata={op_name=\"jit(nested_naue)/jit(nested_naue)/mul\" stack_frame_id=7}\n  %constant.8 = f32[] constant(2), metadata={op_name=\"jit(nested_naue)/jit(nested_naue)\"}\n  %mul.12 = f32[1024]{0} broadcast(%constant.8), dimensions={}, metadata={op_name=\"jit(nested_naue)/jit(nested_naue)/mul\" stack_frame_id=6}\n  %mul.11 = f32[1024]{0} multiply(%param_0.1, %mul.12), metadata={op_name=\"jit(nested_naue)/jit(nested_naue)/mul\" stack_frame_id=6}\n  %select_n.7 = f32[1024]{0} select(%gt.6, %mul.13, %mul.11), metadata={op_name=\"jit(nested_naue)/jit(nested_naue)/jit(_where)/select_n\" stack_frame_id=10}\n  %mul.10 = f32[1024]{0} multiply(%param_0.1, %broadcast.2), metadata={op_name=\"jit(nested_naue)/jit(nested_naue)/mul\" stack_frame_id=11}\n  ROOT %select_n.6 = f32[1024]{0} select(%gt.7, %select_n.7, %mul.10), metadata={op_name=\"jit(nested_naue)/jit(nested_naue)/jit(_where)/select_n\" stack_frame_id=10}\n}\n\nENTRY %main.3 (a.1: f32[1024]) -> f32[1024] {\n  %a.1 = f32[1024]{0} parameter(0), metadata={op_name=\"a\"}\n  ROOT %multiply_select_fusion = f32[1024]{0} fusion(%a.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(nested_naue)/jit(nested_naue)/jit(_where)/select_n\" stack_frame_id=10}\n}\n\n", "ptx": null}
