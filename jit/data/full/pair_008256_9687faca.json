{
  "python_source": "def f(init, xs): return lax.scan(lambda c, x: (c * 0.9 + x * 0.1, c), init, xs)",
  "jaxpr": "{ lambda ; a:f32[8] b:f32[4,8]. let\n    c:f32[8] d:f32[4,8] = scan[\n      _split_transpose=False\n      jaxpr={ lambda ; e:f32[8] f:f32[8]. let\n          g:f32[8] = mul e 0.8999999761581421:f32[]\n          h:f32[8] = mul f 0.10000000149011612:f32[]\n          i:f32[8] = add g h\n        in (i, e) }\n      length=4\n      linear=(False, False)\n      num_carry=1\n      num_consts=0\n      reverse=False\n      unroll=1\n    ] a b\n  in (c, d) }",
  "hlo_text": "module @jit_fn attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8xf32>, %arg1: tensor<4x8xf32>) -> (tensor<8xf32> {jax.result_info = \"result[0]\"}, tensor<4x8xf32> {jax.result_info = \"result[1]\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<4x8xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg1, %iterArg_0 = %c, %iterArg_1 = %arg0, %iterArg_2 = %0) : tensor<4x8xf32>, tensor<i32>, tensor<8xf32>, tensor<4x8xf32>\n    cond {\n      %c_3 = stablehlo.constant dense<4> : tensor<i32>\n      %2 = stablehlo.compare  LT, %iterArg_0, %c_3,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<0> : tensor<i32>\n      %2 = stablehlo.dynamic_slice %iterArg, %iterArg_0, %c_3, sizes = [1, 8] : (tensor<4x8xf32>, tensor<i32>, tensor<i32>) -> tensor<1x8xf32>\n      %3 = stablehlo.reshape %2 : (tensor<1x8xf32>) -> tensor<8xf32>\n      %4:2 = func.call @closed_call(%iterArg_1, %3) : (tensor<8xf32>, tensor<8xf32>) -> (tensor<8xf32>, tensor<8xf32>)\n      %5 = stablehlo.broadcast_in_dim %4#1, dims = [1] : (tensor<8xf32>) -> tensor<1x8xf32>\n      %c_4 = stablehlo.constant dense<0> : tensor<i32>\n      %6 = stablehlo.dynamic_update_slice %iterArg_2, %5, %iterArg_0, %c_4 : (tensor<4x8xf32>, tensor<1x8xf32>, tensor<i32>, tensor<i32>) -> tensor<4x8xf32>\n      %c_5 = stablehlo.constant dense<1> : tensor<i32>\n      %7 = stablehlo.add %iterArg_0, %c_5 : tensor<i32>\n      stablehlo.return %iterArg, %7, %4#0, %6 : tensor<4x8xf32>, tensor<i32>, tensor<8xf32>, tensor<4x8xf32>\n    }\n    return %1#2, %1#3 : tensor<8xf32>, tensor<4x8xf32>\n  }\n  func.func private @closed_call(%arg0: tensor<8xf32>, %arg1: tensor<8xf32>) -> (tensor<8xf32>, tensor<8xf32>) {\n    %cst = stablehlo.constant dense<0.899999976> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<8xf32>\n    %1 = stablehlo.multiply %arg0, %0 : tensor<8xf32>\n    %cst_0 = stablehlo.constant dense<1.000000e-01> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<8xf32>\n    %3 = stablehlo.multiply %arg1, %2 : tensor<8xf32>\n    %4 = stablehlo.add %1, %3 : tensor<8xf32>\n    return %4, %arg0 : tensor<8xf32>, tensor<8xf32>\n  }\n}\n",
  "input_shapes": [
    "float32[8]",
    "float32[4, 8]"
  ],
  "seed": 8256
}