{"id": 0, "kernel_name": "scalar_arr_qahf", "python": "@jax.jit\ndef scalar_arr_qahf(alpha, x, y):\n    return alpha * x + y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_qahf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_qahf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 1, "kernel_name": "elementwise_bsdm", "python": "@jax.jit\ndef elementwise_bsdm(a, b):\n    return a * b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_bsdm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_bsdm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 2, "kernel_name": "vec_kowe", "python": "@jax.jit\ndef vec_kowe(a, b):\n    # a and b are (N, 3) arrays\n    return jnp.sum(a * b, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_kowe attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_kowe attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n"}
{"id": 3, "kernel_name": "loop_hmci", "python": "@jax.jit\ndef loop_hmci(a, n):\n    # Equivalent to summing 'a' n times\n    return a * n", "type": "generate_loop_kernel", "cpp": "module @jit_loop_hmci attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg1 : (tensor<i32>) -> tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %arg0, %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_hmci attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg1 : (tensor<i32>) -> tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %arg0, %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 4, "kernel_name": "scalar_arr_xkpw", "python": "@jax.jit\ndef scalar_arr_xkpw(alpha, x, y):\n    return alpha + x * y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_xkpw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<128xf32>\n    %1 = stablehlo.convert %arg0 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %2, %0 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_xkpw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<128xf32>\n    %1 = stablehlo.convert %arg0 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %2, %0 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 5, "kernel_name": "reduce_jlli", "python": "@jax.jit\ndef reduce_jlli(a):\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_jlli attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_jlli attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 6, "kernel_name": "nested_odsh", "python": "@jax.jit\ndef nested_odsh(a):\n    cond1 = a > 0.44\n    cond2 = a > 0.69\n    \n    val_true = jnp.where(cond2, a * 3.0, a * 2.0)\n    val_false = a * 0.5\n    \n    return jnp.where(cond1, val_true, val_false)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_odsh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<4.400000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<0.689999997> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.compare  GT, %arg0, %2,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_1 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %6 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %7 = stablehlo.multiply %arg0, %6 : tensor<128xf32>\n    %8 = call @_where(%3, %5, %7) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%1, %8, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_odsh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<4.400000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<0.689999997> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.compare  GT, %arg0, %2,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_1 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %6 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %7 = stablehlo.multiply %arg0, %6 : tensor<128xf32>\n    %8 = call @_where(%3, %5, %7) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%1, %8, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 7, "kernel_name": "scalar_arr_bkct", "python": "@jax.jit\ndef scalar_arr_bkct(alpha, x, y):\n    return alpha * x + y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_bkct attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_bkct attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 8, "kernel_name": "multi_mgqg", "python": "@jax.jit\ndef multi_mgqg(a, b):\n    temp1 = a - b\n    temp2 = jnp.sqrt(temp1)\n    return temp2 - a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_mgqg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sqrt %0 : tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_mgqg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sqrt %0 : tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 9, "kernel_name": "branch_gnev", "python": "@jax.jit\ndef branch_gnev(a):\n    return jnp.where(a > -0.54, a * 2.8, a - 1.5)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_gnev attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-5.400000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.800000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.500000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_gnev attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-5.400000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.800000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.500000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
