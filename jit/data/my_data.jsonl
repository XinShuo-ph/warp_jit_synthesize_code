{"id": 0, "kernel_name": "scalar_arr_qahf", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_qahf(alpha, x, y):\n    # Scalar-array operations\n    return alpha * x + y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_qahf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_qahf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 1, "kernel_name": "elementwise_bsdm", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_bsdm(a, b):\n    # Elementwise *\n    return a * b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_bsdm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_bsdm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 2, "kernel_name": "vec_kowe", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_kowe(a, b):\n    # Vector dot product (row-wise for N x 3 arrays)\n    # a and b are expected to be shape (N, 3)\n    # contract over last axis\n    return jnp.sum(a * b, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_kowe attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_kowe attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n"}
{"id": 3, "kernel_name": "loop_hmci", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_hmci(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val + a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_hmci attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.add %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_hmci attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.add %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 4, "kernel_name": "scalar_arr_xkpw", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_xkpw(alpha, x, y):\n    # Scalar-array operations\n    return alpha + x * y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_xkpw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<128xf32>\n    %1 = stablehlo.convert %arg0 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %2, %0 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_xkpw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<128xf32>\n    %1 = stablehlo.convert %arg0 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %2, %0 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 5, "kernel_name": "reduce_jlli", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_jlli(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_jlli attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_jlli attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 6, "kernel_name": "nested_odsh", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_odsh(a):\n    # Nested branching\n    # if a > 0.44:\n    #   if a > 0.69: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 0.69, a * 3.0, a * 2.0)\n    return jnp.where(a > 0.44, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_odsh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.689999997> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<4.400000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_odsh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.689999997> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<4.400000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 7, "kernel_name": "scalar_arr_bkct", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_bkct(alpha, x, y):\n    # Scalar-array operations\n    return alpha * x + y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_bkct attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_bkct attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 8, "kernel_name": "multi_mgqg", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_mgqg(a, b):\n    # Multi-statement\n    temp1 = a - b\n    temp2 = jnp.sqrt(temp1)\n    return temp2 - a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_mgqg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sqrt %0 : tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_mgqg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sqrt %0 : tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 9, "kernel_name": "branch_gnev", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_gnev(a):\n    # Branching with where\n    # if a > -0.54: a * 2.8 else: a - 1.5\n    return jnp.where(a > -0.54, a * 2.8, a - 1.5)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_gnev attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-5.400000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.800000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.500000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_gnev attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-5.400000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.800000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.500000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 10, "kernel_name": "loop_zbsm", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_zbsm(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val - a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_zbsm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_zbsm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 11, "kernel_name": "compound_qxls", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_qxls(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_qxls attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_qxls attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 12, "kernel_name": "unary_xdom", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_xdom(a):\n    # Unary operation jnp.cos\n    return jnp.cos(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_xdom attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.cosine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_xdom attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.cosine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 13, "kernel_name": "scalar_arr_cydt", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_cydt(alpha, x, y):\n    # Scalar-array operations\n    return alpha + x * y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_cydt attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<128xf32>\n    %1 = stablehlo.convert %arg0 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %2, %0 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_cydt attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<128xf32>\n    %1 = stablehlo.convert %arg0 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %2, %0 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 14, "kernel_name": "nested_zomn", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_zomn(a):\n    # Nested branching\n    # if a > 0.27:\n    #   if a > 1.49: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 1.49, a * 3.0, a * 2.0)\n    return jnp.where(a > 0.27, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_zomn attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.490000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<2.700000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_zomn attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.490000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<2.700000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 15, "kernel_name": "elementwise_bpan", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_bpan(a, b):\n    # Elementwise -\n    return a - b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_bpan attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_bpan attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 16, "kernel_name": "compound_pfqy", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_pfqy(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_pfqy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_pfqy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 17, "kernel_name": "branch_fyya", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_fyya(a):\n    # Branching with where\n    # if a > 0.69: a * 2.2 else: a + 0.5\n    return jnp.where(a > 0.69, a * 2.2, a + 0.5)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_fyya attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.689999997> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.200000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_fyya attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.689999997> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.200000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 18, "kernel_name": "loop_ipgv", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_ipgv(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val - a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_ipgv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_ipgv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 19, "kernel_name": "multi_movi", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_movi(a, b):\n    # Multi-statement\n    temp1 = a - b\n    temp2 = jnp.sqrt(temp1)\n    return temp2 * a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_movi attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sqrt %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_movi attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sqrt %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 20, "kernel_name": "compound_yezg", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_yezg(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_yezg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_yezg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 21, "kernel_name": "multi_lhxm", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_lhxm(a, b):\n    # Multi-statement\n    temp1 = a + b\n    temp2 = jnp.sqrt(temp1)\n    return temp2 - a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_lhxm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sqrt %0 : tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_lhxm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sqrt %0 : tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 22, "kernel_name": "multi_mqkx", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_mqkx(a, b):\n    # Multi-statement\n    temp1 = a + b\n    temp2 = jnp.sin(temp1)\n    return temp2 + a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_mqkx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sine %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_mqkx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sine %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 23, "kernel_name": "vec_khvl", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_khvl(a):\n    # Vector length (row-wise for N x 3 arrays)\n    return jnp.linalg.norm(a, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_khvl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_khvl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 24, "kernel_name": "scalar_arr_blgl", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_blgl(alpha, x, y):\n    # Scalar-array operations\n    return alpha - x * y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_blgl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<128xf32>\n    %1 = stablehlo.convert %arg0 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %2, %0 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_blgl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<128xf32>\n    %1 = stablehlo.convert %arg0 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %2, %0 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 25, "kernel_name": "scalar_arr_btzk", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_btzk(alpha, x, y):\n    # Scalar-array operations\n    return alpha * x - y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_btzk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.subtract %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_btzk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.subtract %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 26, "kernel_name": "multi_tytx", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_tytx(a, b):\n    # Multi-statement\n    temp1 = a + b\n    temp2 = jnp.abs(temp1)\n    return temp2 * a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_tytx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.abs %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_tytx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.abs %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 27, "kernel_name": "elementwise_rcep", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_rcep(a, b):\n    # Elementwise -\n    return a - b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_rcep attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_rcep attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 28, "kernel_name": "scalar_arr_xhld", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_xhld(alpha, x, y):\n    # Scalar-array operations\n    return alpha + x + y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_xhld attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.add %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_xhld attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.add %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 29, "kernel_name": "reduce_iqaz", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_iqaz(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_iqaz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_iqaz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 30, "kernel_name": "scalar_arr_bpzi", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_bpzi(alpha, x, y):\n    # Scalar-array operations\n    return alpha * x * y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_bpzi attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.multiply %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_bpzi attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.multiply %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 31, "kernel_name": "loop_hnol", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_hnol(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val * a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_hnol attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.multiply %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_hnol attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.multiply %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 32, "kernel_name": "compound_wncd", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_wncd(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_wncd attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_wncd attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 33, "kernel_name": "multi_llbt", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_llbt(a, b):\n    # Multi-statement\n    temp1 = a * b\n    temp2 = jnp.cos(temp1)\n    return temp2 * a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_llbt attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.cosine %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_llbt attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.cosine %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 34, "kernel_name": "reduce_jyfh", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_jyfh(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_jyfh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_jyfh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 35, "kernel_name": "loop_uigv", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_uigv(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val + a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_uigv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.add %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_uigv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.add %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 36, "kernel_name": "branch_vcys", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_vcys(a):\n    # Branching with where\n    # if a > -0.49: a - 2.1 else: a + 2.7\n    return jnp.where(a > -0.49, a - 2.1, a + 2.7)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_vcys attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-4.900000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.100000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.700000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_vcys attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-4.900000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.100000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.700000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 37, "kernel_name": "unary_djeg", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_djeg(a):\n    # Unary operation jnp.cos\n    return jnp.cos(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_djeg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.cosine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_djeg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.cosine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 38, "kernel_name": "loop_hosy", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_hosy(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val - a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_hosy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_hosy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 39, "kernel_name": "nested_nloj", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_nloj(a):\n    # Nested branching\n    # if a > 0.33:\n    #   if a > 0.66: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 0.66, a * 3.0, a * 2.0)\n    return jnp.where(a > 0.33, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_nloj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<6.600000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<3.300000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_nloj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<6.600000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<3.300000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 40, "kernel_name": "unary_dmnz", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_dmnz(a):\n    # Unary operation jnp.abs\n    return jnp.abs(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_dmnz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.abs %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_dmnz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.abs %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 41, "kernel_name": "multi_mwcj", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_mwcj(a, b):\n    # Multi-statement\n    temp1 = a + b\n    temp2 = jnp.abs(temp1)\n    return temp2 * a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_mwcj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.abs %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_mwcj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.abs %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 42, "kernel_name": "loop_txaa", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_txaa(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val * a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_txaa attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.multiply %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_txaa attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.multiply %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 43, "kernel_name": "branch_fpcx", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_fpcx(a):\n    # Branching with where\n    # if a > -0.62: a - 2.0 else: a - 2.2\n    return jnp.where(a > -0.62, a - 2.0, a - 2.2)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_fpcx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-6.200000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.200000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_fpcx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-6.200000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.200000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 44, "kernel_name": "elementwise_uavi", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_uavi(a, b):\n    # Elementwise +\n    return a + b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_uavi attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_uavi attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 45, "kernel_name": "unary_devo", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_devo(a):\n    # Unary operation jnp.sin\n    return jnp.sin(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_devo attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_devo attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 46, "kernel_name": "vec_kexq", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_kexq(a, b):\n    # Vector dot product (row-wise for N x 3 arrays)\n    # a and b are expected to be shape (N, 3)\n    # contract over last axis\n    return jnp.sum(a * b, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_kexq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_kexq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n"}
{"id": 47, "kernel_name": "scalar_arr_cpzd", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_cpzd(alpha, x, y):\n    # Scalar-array operations\n    return alpha + x + y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_cpzd attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.add %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_cpzd attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.add %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 48, "kernel_name": "branch_fcrp", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_fcrp(a):\n    # Branching with where\n    # if a > 0.27: a - 1.9 else: a - 2.9\n    return jnp.where(a > 0.27, a - 1.9, a - 2.9)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_fcrp attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<2.700000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.900000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.900000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_fcrp attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<2.700000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.900000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.900000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 49, "kernel_name": "scalar_arr_cere", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_cere(alpha, x, y):\n    # Scalar-array operations\n    return alpha - x * y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_cere attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<128xf32>\n    %1 = stablehlo.convert %arg0 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %2, %0 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_cere attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<128xf32>\n    %1 = stablehlo.convert %arg0 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %2, %0 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 50, "kernel_name": "vec_koop", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_koop(a, b):\n    # Vector dot product (row-wise for N x 3 arrays)\n    # a and b are expected to be shape (N, 3)\n    # contract over last axis\n    return jnp.sum(a * b, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_koop attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_koop attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n"}
{"id": 51, "kernel_name": "multi_xmoj", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_xmoj(a, b):\n    # Multi-statement\n    temp1 = a + b\n    temp2 = jnp.sin(temp1)\n    return temp2 + a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_xmoj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sine %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_xmoj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sine %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 52, "kernel_name": "nested_odhl", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_odhl(a):\n    # Nested branching\n    # if a > -0.48:\n    #   if a > 1.0: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 1.0, a * 3.0, a * 2.0)\n    return jnp.where(a > -0.48, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_odhl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-4.800000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_odhl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-4.800000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 53, "kernel_name": "nested_tnzo", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_tnzo(a):\n    # Nested branching\n    # if a > 0.44:\n    #   if a > 0.63: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 0.63, a * 3.0, a * 2.0)\n    return jnp.where(a > 0.44, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_tnzo attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<6.300000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<4.400000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_tnzo attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<6.300000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<4.400000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 54, "kernel_name": "reduce_jxkf", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_jxkf(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_jxkf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_jxkf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 55, "kernel_name": "branch_fujv", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_fujv(a):\n    # Branching with where\n    # if a > 0.59: a * 1.9 else: a + 0.6\n    return jnp.where(a > 0.59, a * 1.9, a + 0.6)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_fujv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<5.900000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.900000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<6.000000e-01> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_fujv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<5.900000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.900000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<6.000000e-01> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 56, "kernel_name": "reduce_jvjb", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_jvjb(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_jvjb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_jvjb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 57, "kernel_name": "vec_kfeg", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_kfeg(a, b):\n    # Vector dot product (row-wise for N x 3 arrays)\n    # a and b are expected to be shape (N, 3)\n    # contract over last axis\n    return jnp.sum(a * b, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_kfeg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_kfeg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n"}
{"id": 58, "kernel_name": "unary_dlus", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_dlus(a):\n    # Unary operation jnp.sin\n    return jnp.sin(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_dlus attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_dlus attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 59, "kernel_name": "compound_pfzy", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_pfzy(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_pfzy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_pfzy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 60, "kernel_name": "unary_dqes", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_dqes(a):\n    # Unary operation jnp.abs\n    return jnp.abs(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_dqes attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.abs %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_dqes attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.abs %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 61, "kernel_name": "multi_zlst", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_zlst(a, b):\n    # Multi-statement\n    temp1 = a + b\n    temp2 = jnp.sqrt(temp1)\n    return temp2 * a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_zlst attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sqrt %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_zlst attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sqrt %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 62, "kernel_name": "elementwise_ywtf", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_ywtf(a, b):\n    # Elementwise -\n    return a - b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_ywtf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_ywtf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 63, "kernel_name": "compound_wjuy", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_wjuy(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_wjuy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_wjuy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 64, "kernel_name": "multi_sxay", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_sxay(a, b):\n    # Multi-statement\n    temp1 = a - b\n    temp2 = jnp.sin(temp1)\n    return temp2 + a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_sxay attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sine %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_sxay attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sine %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 65, "kernel_name": "branch_gpzr", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_gpzr(a):\n    # Branching with where\n    # if a > -0.03: a - 1.6 else: a + 1.9\n    return jnp.where(a > -0.03, a - 1.6, a + 1.9)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_gpzr attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-3.000000e-02> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.600000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.900000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_gpzr attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-3.000000e-02> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.600000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.900000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 66, "kernel_name": "unary_dvwk", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_dvwk(a):\n    # Unary operation jnp.sin\n    return jnp.sin(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_dvwk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_dvwk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 67, "kernel_name": "loop_hloc", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_hloc(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val * a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_hloc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.multiply %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_hloc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.multiply %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 68, "kernel_name": "vec_ykvg", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_ykvg(a):\n    # Vector length (row-wise for N x 3 arrays)\n    return jnp.linalg.norm(a, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_ykvg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_ykvg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 69, "kernel_name": "branch_vfym", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_vfym(a):\n    # Branching with where\n    # if a > -0.2: a * 2.1 else: a + 2.2\n    return jnp.where(a > -0.2, a * 2.1, a + 2.2)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_vfym attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-2.000000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.100000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.200000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_vfym attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-2.000000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.100000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.200000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 70, "kernel_name": "multi_mrqs", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_mrqs(a, b):\n    # Multi-statement\n    temp1 = a - b\n    temp2 = jnp.abs(temp1)\n    return temp2 * a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_mrqs attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.abs %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_mrqs attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.abs %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 71, "kernel_name": "elementwise_awsa", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_awsa(a, b):\n    # Elementwise *\n    return a * b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_awsa attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_awsa attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 72, "kernel_name": "branch_gcrg", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_gcrg(a):\n    # Branching with where\n    # if a > 0.25: a + 3.0 else: a * 2.2\n    return jnp.where(a > 0.25, a + 3.0, a * 2.2)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_gcrg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<2.500000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.200000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_gcrg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<2.500000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.200000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 73, "kernel_name": "loop_xfcc", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_xfcc(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val * a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_xfcc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.multiply %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_xfcc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.multiply %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 74, "kernel_name": "compound_vpsa", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_vpsa(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_vpsa attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_vpsa attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 75, "kernel_name": "branch_geke", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_geke(a):\n    # Branching with where\n    # if a > -0.19: a + 1.8 else: a * 1.9\n    return jnp.where(a > -0.19, a + 1.8, a * 1.9)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_geke attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-1.900000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.800000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.900000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_geke attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-1.900000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.800000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.900000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 76, "kernel_name": "unary_secj", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_secj(a):\n    # Unary operation jnp.sqrt\n    return jnp.sqrt(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_secj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sqrt %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_secj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sqrt %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 77, "kernel_name": "loop_yeuo", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_yeuo(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val * a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_yeuo attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.multiply %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_yeuo attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.multiply %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 78, "kernel_name": "nested_neyq", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_neyq(a):\n    # Nested branching\n    # if a > 0.09:\n    #   if a > 1.44: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 1.44, a * 3.0, a * 2.0)\n    return jnp.where(a > 0.09, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_neyq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.440000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<9.000000e-02> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_neyq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.440000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<9.000000e-02> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 79, "kernel_name": "scalar_arr_cpkx", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_cpkx(alpha, x, y):\n    # Scalar-array operations\n    return alpha * x * y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_cpkx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.multiply %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_cpkx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.multiply %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 80, "kernel_name": "nested_nphy", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_nphy(a):\n    # Nested branching\n    # if a > 0.23:\n    #   if a > 1.26: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 1.26, a * 3.0, a * 2.0)\n    return jnp.where(a > 0.23, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_nphy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.260000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<2.300000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_nphy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.260000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<2.300000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 81, "kernel_name": "elementwise_bckc", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_bckc(a, b):\n    # Elementwise +\n    return a + b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_bckc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_bckc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 82, "kernel_name": "loop_yoez", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_yoez(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val - a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_yoez attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_yoez attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 83, "kernel_name": "branch_xfvh", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_xfvh(a):\n    # Branching with where\n    # if a > 0.47: a - 2.0 else: a + 2.0\n    return jnp.where(a > 0.47, a - 2.0, a + 2.0)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_xfvh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<4.700000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_xfvh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<4.700000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 84, "kernel_name": "compound_oplq", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_oplq(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_oplq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_oplq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 85, "kernel_name": "elementwise_bqoc", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_bqoc(a, b):\n    # Elementwise +\n    return a + b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_bqoc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_bqoc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 86, "kernel_name": "branch_ygnd", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_ygnd(a):\n    # Branching with where\n    # if a > -0.94: a + 2.1 else: a - 1.1\n    return jnp.where(a > -0.94, a + 2.1, a - 1.1)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_ygnd attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-0.939999997> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.100000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.100000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_ygnd attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-0.939999997> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.100000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.100000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 87, "kernel_name": "compound_puhw", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_puhw(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_puhw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_puhw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 88, "kernel_name": "nested_nhtu", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_nhtu(a):\n    # Nested branching\n    # if a > 0.15:\n    #   if a > 0.52: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 0.52, a * 3.0, a * 2.0)\n    return jnp.where(a > 0.15, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_nhtu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<5.200000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<1.500000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_nhtu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<5.200000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<1.500000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 89, "kernel_name": "reduce_iuch", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_iuch(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_iuch attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_iuch attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 90, "kernel_name": "vec_kkxx", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_kkxx(a):\n    # Vector length (row-wise for N x 3 arrays)\n    return jnp.linalg.norm(a, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_kkxx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_kkxx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 91, "kernel_name": "multi_mmtv", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_mmtv(a, b):\n    # Multi-statement\n    temp1 = a - b\n    temp2 = jnp.sqrt(temp1)\n    return temp2 * a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_mmtv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sqrt %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_mmtv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sqrt %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 92, "kernel_name": "multi_mfzx", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_mfzx(a, b):\n    # Multi-statement\n    temp1 = a * b\n    temp2 = jnp.abs(temp1)\n    return temp2 * a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_mfzx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.abs %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_mfzx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.abs %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 93, "kernel_name": "reduce_rqpm", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_rqpm(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_rqpm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_rqpm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 94, "kernel_name": "compound_onlt", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_onlt(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_onlt attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_onlt attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 95, "kernel_name": "scalar_arr_bkzg", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_bkzg(alpha, x, y):\n    # Scalar-array operations\n    return alpha - x + y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_bkzg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_bkzg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 96, "kernel_name": "branch_flsf", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_flsf(a):\n    # Branching with where\n    # if a > 0.72: a + 0.9 else: a + 1.6\n    return jnp.where(a > 0.72, a + 0.9, a + 1.6)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_flsf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<7.200000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<0.899999976> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.600000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_flsf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<7.200000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<0.899999976> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.600000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 97, "kernel_name": "elementwise_asox", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_asox(a, b):\n    # Elementwise *\n    return a * b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_asox attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_asox attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 98, "kernel_name": "scalar_arr_ucgn", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_ucgn(alpha, x, y):\n    # Scalar-array operations\n    return alpha + x * y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_ucgn attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<128xf32>\n    %1 = stablehlo.convert %arg0 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %2, %0 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_ucgn attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<128xf32>\n    %1 = stablehlo.convert %arg0 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %2, %0 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 99, "kernel_name": "nested_naue", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_naue(a):\n    # Nested branching\n    # if a > 0.2:\n    #   if a > 0.5: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 0.5, a * 3.0, a * 2.0)\n    return jnp.where(a > 0.2, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_naue attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<2.000000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_naue attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<2.000000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 100, "kernel_name": "compound_prmm", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_prmm(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_prmm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_prmm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 101, "kernel_name": "unary_xwbf", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_xwbf(a):\n    # Unary operation jnp.abs\n    return jnp.abs(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_xwbf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.abs %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_xwbf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.abs %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 102, "kernel_name": "multi_llys", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_llys(a, b):\n    # Multi-statement\n    temp1 = a - b\n    temp2 = jnp.cos(temp1)\n    return temp2 + a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_llys attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.cosine %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_llys attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.cosine %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 103, "kernel_name": "vec_wuxa", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_wuxa(a, b):\n    # Vector dot product (row-wise for N x 3 arrays)\n    # a and b are expected to be shape (N, 3)\n    # contract over last axis\n    return jnp.sum(a * b, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_wuxa attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_wuxa attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n"}
{"id": 104, "kernel_name": "unary_dqub", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_dqub(a):\n    # Unary operation jnp.sin\n    return jnp.sin(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_dqub attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_dqub attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 105, "kernel_name": "compound_pdyl", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_pdyl(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_pdyl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_pdyl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 106, "kernel_name": "vec_kqzb", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_kqzb(a):\n    # Vector length (row-wise for N x 3 arrays)\n    return jnp.linalg.norm(a, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_kqzb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_kqzb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 107, "kernel_name": "scalar_arr_bgdb", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_bgdb(alpha, x, y):\n    # Scalar-array operations\n    return alpha + x * y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_bgdb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<128xf32>\n    %1 = stablehlo.convert %arg0 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %2, %0 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_bgdb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<128xf32>\n    %1 = stablehlo.convert %arg0 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %2, %0 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 108, "kernel_name": "reduce_vxeh", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_vxeh(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_vxeh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_vxeh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 109, "kernel_name": "vec_svyq", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_svyq(a, b):\n    # Vector dot product (row-wise for N x 3 arrays)\n    # a and b are expected to be shape (N, 3)\n    # contract over last axis\n    return jnp.sum(a * b, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_svyq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_svyq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n"}
{"id": 110, "kernel_name": "vec_zkdz", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_zkdz(a, b):\n    # Vector dot product (row-wise for N x 3 arrays)\n    # a and b are expected to be shape (N, 3)\n    # contract over last axis\n    return jnp.sum(a * b, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_zkdz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_zkdz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n"}
{"id": 111, "kernel_name": "multi_zbgw", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_zbgw(a, b):\n    # Multi-statement\n    temp1 = a * b\n    temp2 = jnp.sin(temp1)\n    return temp2 * a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_zbgw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sine %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_zbgw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sine %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 112, "kernel_name": "branch_fxjv", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_fxjv(a):\n    # Branching with where\n    # if a > -0.11: a + 1.1 else: a - 1.5\n    return jnp.where(a > -0.11, a + 1.1, a - 1.5)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_fxjv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-1.100000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.100000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.500000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_fxjv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-1.100000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.100000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.500000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 113, "kernel_name": "compound_poxz", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_poxz(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_poxz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_poxz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 114, "kernel_name": "multi_mxgj", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_mxgj(a, b):\n    # Multi-statement\n    temp1 = a + b\n    temp2 = jnp.abs(temp1)\n    return temp2 + a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_mxgj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.abs %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_mxgj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.abs %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 115, "kernel_name": "compound_pfaj", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_pfaj(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_pfaj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_pfaj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 116, "kernel_name": "branch_rzyc", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_rzyc(a):\n    # Branching with where\n    # if a > -0.16: a - 1.7 else: a * 0.6\n    return jnp.where(a > -0.16, a - 1.7, a * 0.6)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_rzyc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-1.600000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.700000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<6.000000e-01> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_rzyc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-1.600000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.700000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<6.000000e-01> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 117, "kernel_name": "vec_krgv", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_krgv(a, b):\n    # Vector dot product (row-wise for N x 3 arrays)\n    # a and b are expected to be shape (N, 3)\n    # contract over last axis\n    return jnp.sum(a * b, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_krgv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_krgv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n"}
{"id": 118, "kernel_name": "scalar_arr_diep", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_diep(alpha, x, y):\n    # Scalar-array operations\n    return alpha * x * y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_diep attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.multiply %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_diep attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.multiply %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 119, "kernel_name": "nested_nofx", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_nofx(a):\n    # Nested branching\n    # if a > 0.14:\n    #   if a > 0.98: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 0.98, a * 3.0, a * 2.0)\n    return jnp.where(a > 0.14, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_nofx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<9.800000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<1.400000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_nofx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<9.800000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<1.400000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 120, "kernel_name": "elementwise_bxff", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_bxff(a, b):\n    # Elementwise -\n    return a - b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_bxff attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_bxff attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 121, "kernel_name": "nested_olpm", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_olpm(a):\n    # Nested branching\n    # if a > 0.4:\n    #   if a > 0.78: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 0.78, a * 3.0, a * 2.0)\n    return jnp.where(a > 0.4, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_olpm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<7.800000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<4.000000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_olpm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<7.800000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<4.000000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 122, "kernel_name": "scalar_arr_chym", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_chym(alpha, x, y):\n    # Scalar-array operations\n    return alpha * x - y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_chym attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.subtract %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_chym attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.subtract %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 123, "kernel_name": "elementwise_azxb", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_azxb(a, b):\n    # Elementwise +\n    return a + b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_azxb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_azxb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 124, "kernel_name": "unary_epli", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_epli(a):\n    # Unary operation jnp.sqrt\n    return jnp.sqrt(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_epli attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sqrt %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_epli attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sqrt %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 125, "kernel_name": "branch_fclz", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_fclz(a):\n    # Branching with where\n    # if a > 0.54: a * 2.5 else: a + 1.8\n    return jnp.where(a > 0.54, a * 2.5, a + 1.8)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_fclz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<5.400000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.500000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.800000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_fclz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<5.400000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.500000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.800000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 126, "kernel_name": "scalar_arr_tnjf", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_tnjf(alpha, x, y):\n    # Scalar-array operations\n    return alpha + x + y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_tnjf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.add %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_tnjf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.add %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 127, "kernel_name": "loop_gqod", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_gqod(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val - a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_gqod attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_gqod attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 128, "kernel_name": "reduce_uxpi", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_uxpi(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_uxpi attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_uxpi attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 129, "kernel_name": "nested_uozd", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_uozd(a):\n    # Nested branching\n    # if a > 0.09:\n    #   if a > 1.42: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 1.42, a * 3.0, a * 2.0)\n    return jnp.where(a > 0.09, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_uozd attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.420000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<9.000000e-02> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_uozd attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.420000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<9.000000e-02> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 130, "kernel_name": "reduce_jfwt", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_jfwt(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_jfwt attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_jfwt attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 131, "kernel_name": "reduce_irus", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_irus(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_irus attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_irus attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 132, "kernel_name": "elementwise_ulaj", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_ulaj(a, b):\n    # Elementwise +\n    return a + b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_ulaj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_ulaj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 133, "kernel_name": "multi_lbou", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_lbou(a, b):\n    # Multi-statement\n    temp1 = a * b\n    temp2 = jnp.sin(temp1)\n    return temp2 + a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_lbou attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sine %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_lbou attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sine %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 134, "kernel_name": "elementwise_atfc", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_atfc(a, b):\n    # Elementwise -\n    return a - b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_atfc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_atfc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 135, "kernel_name": "branch_zmps", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_zmps(a):\n    # Branching with where\n    # if a > 0.62: a + 2.6 else: a - 2.6\n    return jnp.where(a > 0.62, a + 2.6, a - 2.6)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_zmps attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<6.200000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.600000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.600000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_zmps attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<6.200000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.600000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.600000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 136, "kernel_name": "scalar_arr_yyxq", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_yyxq(alpha, x, y):\n    # Scalar-array operations\n    return alpha - x * y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_yyxq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<128xf32>\n    %1 = stablehlo.convert %arg0 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %2, %0 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_yyxq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<128xf32>\n    %1 = stablehlo.convert %arg0 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %2, %0 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 137, "kernel_name": "branch_xfvb", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_xfvb(a):\n    # Branching with where\n    # if a > 0.03: a - 2.0 else: a + 2.5\n    return jnp.where(a > 0.03, a - 2.0, a + 2.5)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_xfvb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<3.000000e-02> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.500000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_xfvb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<3.000000e-02> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.500000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 138, "kernel_name": "unary_drqd", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_drqd(a):\n    # Unary operation jnp.cos\n    return jnp.cos(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_drqd attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.cosine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_drqd attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.cosine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 139, "kernel_name": "compound_pcii", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_pcii(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_pcii attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_pcii attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 140, "kernel_name": "multi_lgxj", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_lgxj(a, b):\n    # Multi-statement\n    temp1 = a - b\n    temp2 = jnp.sin(temp1)\n    return temp2 + a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_lgxj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sine %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_lgxj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sine %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 141, "kernel_name": "scalar_arr_ctah", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_ctah(alpha, x, y):\n    # Scalar-array operations\n    return alpha + x + y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_ctah attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.add %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_ctah attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.add %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 142, "kernel_name": "loop_hgys", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_hgys(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val - a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_hgys attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_hgys attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 143, "kernel_name": "scalar_arr_xudh", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_xudh(alpha, x, y):\n    # Scalar-array operations\n    return alpha * x - y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_xudh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.subtract %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_xudh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.subtract %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 144, "kernel_name": "reduce_ikol", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_ikol(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_ikol attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_ikol attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 145, "kernel_name": "reduce_jmpp", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_jmpp(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_jmpp attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_jmpp attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 146, "kernel_name": "branch_uvfx", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_uvfx(a):\n    # Branching with where\n    # if a > -0.52: a * 1.5 else: a - 2.8\n    return jnp.where(a > -0.52, a * 1.5, a - 2.8)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_uvfx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-5.200000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.500000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.800000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_uvfx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-5.200000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.500000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.800000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 147, "kernel_name": "multi_mtdo", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_mtdo(a, b):\n    # Multi-statement\n    temp1 = a - b\n    temp2 = jnp.sqrt(temp1)\n    return temp2 * a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_mtdo attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sqrt %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_mtdo attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sqrt %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 148, "kernel_name": "scalar_arr_btxf", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_btxf(alpha, x, y):\n    # Scalar-array operations\n    return alpha - x - y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_btxf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.subtract %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_btxf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.subtract %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 149, "kernel_name": "nested_tuwn", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_tuwn(a):\n    # Nested branching\n    # if a > -0.36:\n    #   if a > 0.9: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 0.9, a * 3.0, a * 2.0)\n    return jnp.where(a > -0.36, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_tuwn attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.899999976> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-3.600000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_tuwn attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.899999976> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-3.600000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 150, "kernel_name": "reduce_jhmy", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_jhmy(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_jhmy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_jhmy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 151, "kernel_name": "nested_oxtz", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_oxtz(a):\n    # Nested branching\n    # if a > 0.06:\n    #   if a > 1.03: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 1.03, a * 3.0, a * 2.0)\n    return jnp.where(a > 0.06, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_oxtz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.030000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<6.000000e-02> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_oxtz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.030000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<6.000000e-02> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 152, "kernel_name": "compound_vbdk", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_vbdk(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_vbdk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_vbdk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 153, "kernel_name": "compound_pbfr", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_pbfr(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_pbfr attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_pbfr attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 154, "kernel_name": "nested_qssu", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_qssu(a):\n    # Nested branching\n    # if a > -0.29:\n    #   if a > 0.92: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 0.92, a * 3.0, a * 2.0)\n    return jnp.where(a > -0.29, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_qssu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<9.200000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-2.900000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_qssu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<9.200000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-2.900000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 155, "kernel_name": "nested_odae", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_odae(a):\n    # Nested branching\n    # if a > -0.38:\n    #   if a > 1.08: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 1.08, a * 3.0, a * 2.0)\n    return jnp.where(a > -0.38, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_odae attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.080000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-3.800000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_odae attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.080000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-3.800000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 156, "kernel_name": "elementwise_bsct", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_bsct(a, b):\n    # Elementwise -\n    return a - b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_bsct attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_bsct attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 157, "kernel_name": "reduce_ikhs", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_ikhs(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_ikhs attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_ikhs attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 158, "kernel_name": "elementwise_bfsd", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_bfsd(a, b):\n    # Elementwise -\n    return a - b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_bfsd attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_bfsd attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 159, "kernel_name": "scalar_arr_biah", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_biah(alpha, x, y):\n    # Scalar-array operations\n    return alpha * x * y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_biah attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.multiply %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_biah attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.multiply %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 160, "kernel_name": "vec_tqkf", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_tqkf(a):\n    # Vector length (row-wise for N x 3 arrays)\n    return jnp.linalg.norm(a, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_tqkf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_tqkf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 161, "kernel_name": "scalar_arr_cvpf", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_cvpf(alpha, x, y):\n    # Scalar-array operations\n    return alpha * x - y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_cvpf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.subtract %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_cvpf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.subtract %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 162, "kernel_name": "vec_kcpd", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_kcpd(a):\n    # Vector length (row-wise for N x 3 arrays)\n    return jnp.linalg.norm(a, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_kcpd attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_kcpd attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 163, "kernel_name": "vec_lsnm", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_lsnm(a, b):\n    # Vector dot product (row-wise for N x 3 arrays)\n    # a and b are expected to be shape (N, 3)\n    # contract over last axis\n    return jnp.sum(a * b, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_lsnm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_lsnm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n"}
{"id": 164, "kernel_name": "elementwise_ahtv", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_ahtv(a, b):\n    # Elementwise +\n    return a + b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_ahtv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_ahtv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 165, "kernel_name": "nested_nrwf", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_nrwf(a):\n    # Nested branching\n    # if a > -0.34:\n    #   if a > 1.17: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 1.17, a * 3.0, a * 2.0)\n    return jnp.where(a > -0.34, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_nrwf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.170000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-3.400000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_nrwf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.170000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-3.400000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 166, "kernel_name": "loop_hubu", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_hubu(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val - a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_hubu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_hubu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 167, "kernel_name": "nested_oazh", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_oazh(a):\n    # Nested branching\n    # if a > 0.14:\n    #   if a > 0.68: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 0.68, a * 3.0, a * 2.0)\n    return jnp.where(a > 0.14, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_oazh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<6.800000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<1.400000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_oazh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<6.800000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<1.400000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 168, "kernel_name": "vec_krsk", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_krsk(a, b):\n    # Vector dot product (row-wise for N x 3 arrays)\n    # a and b are expected to be shape (N, 3)\n    # contract over last axis\n    return jnp.sum(a * b, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_krsk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_krsk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n"}
{"id": 169, "kernel_name": "compound_otaw", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_otaw(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_otaw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_otaw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 170, "kernel_name": "vec_kzsb", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_kzsb(a, b):\n    # Vector dot product (row-wise for N x 3 arrays)\n    # a and b are expected to be shape (N, 3)\n    # contract over last axis\n    return jnp.sum(a * b, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_kzsb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_kzsb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n"}
{"id": 171, "kernel_name": "unary_dfak", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_dfak(a):\n    # Unary operation jnp.sin\n    return jnp.sin(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_dfak attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_dfak attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 172, "kernel_name": "nested_njho", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_njho(a):\n    # Nested branching\n    # if a > -0.06:\n    #   if a > 0.72: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 0.72, a * 3.0, a * 2.0)\n    return jnp.where(a > -0.06, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_njho attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<7.200000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-6.000000e-02> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_njho attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<7.200000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-6.000000e-02> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 173, "kernel_name": "elementwise_bxch", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_bxch(a, b):\n    # Elementwise +\n    return a + b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_bxch attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_bxch attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 174, "kernel_name": "branch_wqsm", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_wqsm(a):\n    # Branching with where\n    # if a > -0.25: a * 1.2 else: a + 2.8\n    return jnp.where(a > -0.25, a * 1.2, a + 2.8)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_wqsm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-2.500000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.200000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.800000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_wqsm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-2.500000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.200000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.800000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 175, "kernel_name": "loop_yggg", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_yggg(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val - a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_yggg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_yggg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 176, "kernel_name": "loop_hcyj", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_hcyj(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val + a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_hcyj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.add %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_hcyj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.add %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 177, "kernel_name": "reduce_sibs", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_sibs(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_sibs attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_sibs attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 178, "kernel_name": "branch_tggh", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_tggh(a):\n    # Branching with where\n    # if a > -0.2: a - 2.3 else: a + 1.1\n    return jnp.where(a > -0.2, a - 2.3, a + 1.1)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_tggh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-2.000000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.300000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.100000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_tggh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-2.000000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.300000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.100000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 179, "kernel_name": "branch_gchv", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_gchv(a):\n    # Branching with where\n    # if a > -0.54: a - 1.6 else: a * 2.3\n    return jnp.where(a > -0.54, a - 1.6, a * 2.3)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_gchv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-5.400000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.600000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.300000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_gchv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-5.400000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.600000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.300000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 180, "kernel_name": "scalar_arr_ugha", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_ugha(alpha, x, y):\n    # Scalar-array operations\n    return alpha + x - y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_ugha attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.add %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.subtract %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_ugha attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.add %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.subtract %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 181, "kernel_name": "reduce_ilkn", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_ilkn(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_ilkn attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_ilkn attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 182, "kernel_name": "scalar_arr_bebn", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_bebn(alpha, x, y):\n    # Scalar-array operations\n    return alpha + x + y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_bebn attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.add %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_bebn attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.add %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 183, "kernel_name": "branch_sgsx", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_sgsx(a):\n    # Branching with where\n    # if a > 0.4: a + 2.7 else: a - 2.4\n    return jnp.where(a > 0.4, a + 2.7, a - 2.4)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_sgsx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<4.000000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.700000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.400000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_sgsx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<4.000000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.700000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.400000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 184, "kernel_name": "multi_mwop", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_mwop(a, b):\n    # Multi-statement\n    temp1 = a + b\n    temp2 = jnp.sin(temp1)\n    return temp2 + a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_mwop attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sine %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_mwop attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sine %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 185, "kernel_name": "vec_kvrc", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_kvrc(a, b):\n    # Vector dot product (row-wise for N x 3 arrays)\n    # a and b are expected to be shape (N, 3)\n    # contract over last axis\n    return jnp.sum(a * b, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_kvrc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_kvrc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n"}
{"id": 186, "kernel_name": "branch_xsff", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_xsff(a):\n    # Branching with where\n    # if a > -0.07: a * 2.9 else: a * 2.4\n    return jnp.where(a > -0.07, a * 2.9, a * 2.4)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_xsff attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-7.000000e-02> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.900000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.400000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_xsff attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-7.000000e-02> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.900000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.400000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 187, "kernel_name": "nested_nfnu", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_nfnu(a):\n    # Nested branching\n    # if a > 0.33:\n    #   if a > 1.34: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 1.34, a * 3.0, a * 2.0)\n    return jnp.where(a > 0.33, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_nfnu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.340000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<3.300000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_nfnu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.340000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<3.300000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 188, "kernel_name": "unary_shyx", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_shyx(a):\n    # Unary operation jnp.sqrt\n    return jnp.sqrt(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_shyx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sqrt %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_shyx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sqrt %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 189, "kernel_name": "elementwise_wzvw", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_wzvw(a, b):\n    # Elementwise -\n    return a - b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_wzvw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_wzvw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 190, "kernel_name": "loop_hgcw", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_hgcw(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val - a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_hgcw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_hgcw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 191, "kernel_name": "unary_rqnu", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_rqnu(a):\n    # Unary operation jnp.cos\n    return jnp.cos(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_rqnu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.cosine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_rqnu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.cosine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 192, "kernel_name": "reduce_iwwy", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_iwwy(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_iwwy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_iwwy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 193, "kernel_name": "reduce_jnab", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_jnab(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_jnab attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_jnab attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 194, "kernel_name": "reduce_rmtc", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_rmtc(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_rmtc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_rmtc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 195, "kernel_name": "compound_qxzu", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_qxzu(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_qxzu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_qxzu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 196, "kernel_name": "reduce_ucsb", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_ucsb(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_ucsb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_ucsb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 197, "kernel_name": "scalar_arr_dvdc", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_dvdc(alpha, x, y):\n    # Scalar-array operations\n    return alpha + x - y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_dvdc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.add %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.subtract %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_dvdc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.add %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.subtract %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 198, "kernel_name": "compound_wbyi", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_wbyi(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_wbyi attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_wbyi attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 199, "kernel_name": "nested_rtjs", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_rtjs(a):\n    # Nested branching\n    # if a > 0.21:\n    #   if a > 0.74: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 0.74, a * 3.0, a * 2.0)\n    return jnp.where(a > 0.21, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_rtjs attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<7.400000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<2.100000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_rtjs attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<7.400000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<2.100000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 200, "kernel_name": "reduce_imzq", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_imzq(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_imzq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_imzq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 201, "kernel_name": "nested_znll", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_znll(a):\n    # Nested branching\n    # if a > -0.19:\n    #   if a > 0.76: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 0.76, a * 3.0, a * 2.0)\n    return jnp.where(a > -0.19, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_znll attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<7.600000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-1.900000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_znll attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<7.600000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-1.900000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 202, "kernel_name": "vec_yurj", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_yurj(a):\n    # Vector length (row-wise for N x 3 arrays)\n    return jnp.linalg.norm(a, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_yurj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_yurj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 203, "kernel_name": "unary_dmlv", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_dmlv(a):\n    # Unary operation jnp.cos\n    return jnp.cos(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_dmlv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.cosine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_dmlv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.cosine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 204, "kernel_name": "elementwise_ynsb", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_ynsb(a, b):\n    # Elementwise -\n    return a - b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_ynsb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_ynsb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 205, "kernel_name": "multi_laww", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_laww(a, b):\n    # Multi-statement\n    temp1 = a + b\n    temp2 = jnp.sin(temp1)\n    return temp2 * a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_laww attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sine %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_laww attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sine %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 206, "kernel_name": "vec_vsus", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_vsus(a):\n    # Vector length (row-wise for N x 3 arrays)\n    return jnp.linalg.norm(a, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_vsus attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_vsus attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 207, "kernel_name": "loop_hwhp", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_hwhp(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val * a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_hwhp attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.multiply %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_hwhp attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.multiply %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 208, "kernel_name": "vec_kyob", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_kyob(a, b):\n    # Vector dot product (row-wise for N x 3 arrays)\n    # a and b are expected to be shape (N, 3)\n    # contract over last axis\n    return jnp.sum(a * b, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_kyob attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_kyob attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n"}
{"id": 209, "kernel_name": "multi_ljpl", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_ljpl(a, b):\n    # Multi-statement\n    temp1 = a - b\n    temp2 = jnp.cos(temp1)\n    return temp2 * a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_ljpl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.cosine %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_ljpl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.cosine %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 210, "kernel_name": "compound_uaro", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_uaro(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_uaro attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_uaro attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 211, "kernel_name": "nested_njwq", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_njwq(a):\n    # Nested branching\n    # if a > 0.48:\n    #   if a > 0.73: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 0.73, a * 3.0, a * 2.0)\n    return jnp.where(a > 0.48, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_njwq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<7.300000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<4.800000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_njwq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<7.300000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<4.800000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 212, "kernel_name": "reduce_jgrf", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_jgrf(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_jgrf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_jgrf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 213, "kernel_name": "elementwise_bqub", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_bqub(a, b):\n    # Elementwise -\n    return a - b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_bqub attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_bqub attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 214, "kernel_name": "multi_mljb", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_mljb(a, b):\n    # Multi-statement\n    temp1 = a - b\n    temp2 = jnp.abs(temp1)\n    return temp2 + a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_mljb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.abs %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_mljb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.abs %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 215, "kernel_name": "reduce_icnn", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_icnn(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_icnn attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_icnn attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 216, "kernel_name": "scalar_arr_qbyl", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_qbyl(alpha, x, y):\n    # Scalar-array operations\n    return alpha + x * y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_qbyl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<128xf32>\n    %1 = stablehlo.convert %arg0 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %2, %0 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_qbyl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<128xf32>\n    %1 = stablehlo.convert %arg0 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %2, %0 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 217, "kernel_name": "elementwise_bolx", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_bolx(a, b):\n    # Elementwise *\n    return a * b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_bolx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_bolx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 218, "kernel_name": "branch_fyom", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_fyom(a):\n    # Branching with where\n    # if a > 0.38: a * 2.4 else: a - 0.7\n    return jnp.where(a > 0.38, a * 2.4, a - 0.7)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_fyom attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<3.800000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.400000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<0.699999988> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_fyom attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<3.800000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.400000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<0.699999988> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 219, "kernel_name": "vec_lzea", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_lzea(a):\n    # Vector length (row-wise for N x 3 arrays)\n    return jnp.linalg.norm(a, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_lzea attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_lzea attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 220, "kernel_name": "reduce_inmj", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_inmj(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_inmj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_inmj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 221, "kernel_name": "vec_tvzx", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_tvzx(a):\n    # Vector length (row-wise for N x 3 arrays)\n    return jnp.linalg.norm(a, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_tvzx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_tvzx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 222, "kernel_name": "compound_qron", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_qron(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_qron attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_qron attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 223, "kernel_name": "vec_vmin", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_vmin(a):\n    # Vector length (row-wise for N x 3 arrays)\n    return jnp.linalg.norm(a, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_vmin attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_vmin attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 224, "kernel_name": "scalar_arr_zorl", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_zorl(alpha, x, y):\n    # Scalar-array operations\n    return alpha + x * y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_zorl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<128xf32>\n    %1 = stablehlo.convert %arg0 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %2, %0 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_zorl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<128xf32>\n    %1 = stablehlo.convert %arg0 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %2, %0 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 225, "kernel_name": "branch_flic", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_flic(a):\n    # Branching with where\n    # if a > 0.11: a + 1.1 else: a * 3.0\n    return jnp.where(a > 0.11, a + 1.1, a * 3.0)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_flic attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.100000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.100000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_flic attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.100000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.100000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 226, "kernel_name": "nested_tniu", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_tniu(a):\n    # Nested branching\n    # if a > 0.34:\n    #   if a > 1.23: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 1.23, a * 3.0, a * 2.0)\n    return jnp.where(a > 0.34, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_tniu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.230000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<3.400000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_tniu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.230000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<3.400000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 227, "kernel_name": "elementwise_bxtq", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_bxtq(a, b):\n    # Elementwise +\n    return a + b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_bxtq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_bxtq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 228, "kernel_name": "unary_wjcp", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_wjcp(a):\n    # Unary operation jnp.sin\n    return jnp.sin(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_wjcp attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_wjcp attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 229, "kernel_name": "multi_mjrj", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_mjrj(a, b):\n    # Multi-statement\n    temp1 = a * b\n    temp2 = jnp.cos(temp1)\n    return temp2 * a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_mjrj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.cosine %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_mjrj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.cosine %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 230, "kernel_name": "multi_zjqm", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_zjqm(a, b):\n    # Multi-statement\n    temp1 = a - b\n    temp2 = jnp.sin(temp1)\n    return temp2 * a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_zjqm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sine %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_zjqm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sine %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 231, "kernel_name": "branch_eqgt", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_eqgt(a):\n    # Branching with where\n    # if a > 0.16: a + 2.6 else: a + 1.8\n    return jnp.where(a > 0.16, a + 2.6, a + 1.8)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_eqgt attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.600000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.600000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.800000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_eqgt attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.600000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.600000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.800000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 232, "kernel_name": "unary_djbf", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_djbf(a):\n    # Unary operation jnp.sin\n    return jnp.sin(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_djbf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_djbf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 233, "kernel_name": "scalar_arr_tvoj", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_tvoj(alpha, x, y):\n    # Scalar-array operations\n    return alpha - x - y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_tvoj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.subtract %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_tvoj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.subtract %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 234, "kernel_name": "reduce_jfty", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_jfty(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_jfty attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_jfty attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 235, "kernel_name": "scalar_arr_xxfb", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_xxfb(alpha, x, y):\n    # Scalar-array operations\n    return alpha - x * y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_xxfb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<128xf32>\n    %1 = stablehlo.convert %arg0 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %2, %0 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_xxfb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<128xf32>\n    %1 = stablehlo.convert %arg0 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %2, %0 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 236, "kernel_name": "loop_xmbq", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_xmbq(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val * a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_xmbq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.multiply %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_xmbq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.multiply %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 237, "kernel_name": "vec_kube", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_kube(a, b):\n    # Vector dot product (row-wise for N x 3 arrays)\n    # a and b are expected to be shape (N, 3)\n    # contract over last axis\n    return jnp.sum(a * b, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_kube attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_kube attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n"}
{"id": 238, "kernel_name": "loop_irxq", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_irxq(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val - a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_irxq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_irxq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 239, "kernel_name": "elementwise_agly", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_agly(a, b):\n    # Elementwise +\n    return a + b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_agly attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_agly attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 240, "kernel_name": "elementwise_rymx", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_rymx(a, b):\n    # Elementwise *\n    return a * b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_rymx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_rymx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 241, "kernel_name": "elementwise_sdng", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_sdng(a, b):\n    # Elementwise -\n    return a - b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_sdng attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_sdng attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 242, "kernel_name": "elementwise_ahcl", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_ahcl(a, b):\n    # Elementwise *\n    return a * b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_ahcl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_ahcl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 243, "kernel_name": "compound_pkpc", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_pkpc(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_pkpc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_pkpc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 244, "kernel_name": "multi_mibs", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_mibs(a, b):\n    # Multi-statement\n    temp1 = a - b\n    temp2 = jnp.abs(temp1)\n    return temp2 - a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_mibs attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.abs %0 : tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_mibs attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.abs %0 : tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 245, "kernel_name": "nested_oide", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_oide(a):\n    # Nested branching\n    # if a > 0.12:\n    #   if a > 1.01: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 1.01, a * 3.0, a * 2.0)\n    return jnp.where(a > 0.12, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_oide attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.010000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<1.200000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_oide attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.010000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<1.200000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 246, "kernel_name": "loop_yhmt", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_yhmt(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val - a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_yhmt attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_yhmt attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 247, "kernel_name": "loop_whrc", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_whrc(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val + a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_whrc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.add %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_whrc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.add %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 248, "kernel_name": "nested_oima", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_oima(a):\n    # Nested branching\n    # if a > -0.25:\n    #   if a > 1.08: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 1.08, a * 3.0, a * 2.0)\n    return jnp.where(a > -0.25, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_oima attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.080000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-2.500000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_oima attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.080000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-2.500000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 249, "kernel_name": "multi_rtzv", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_rtzv(a, b):\n    # Multi-statement\n    temp1 = a - b\n    temp2 = jnp.cos(temp1)\n    return temp2 * a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_rtzv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.cosine %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_rtzv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.cosine %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 250, "kernel_name": "unary_ttdv", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_ttdv(a):\n    # Unary operation jnp.abs\n    return jnp.abs(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_ttdv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.abs %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_ttdv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.abs %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 251, "kernel_name": "multi_mcnk", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_mcnk(a, b):\n    # Multi-statement\n    temp1 = a - b\n    temp2 = jnp.abs(temp1)\n    return temp2 - a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_mcnk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.abs %0 : tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_mcnk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.abs %0 : tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 252, "kernel_name": "unary_emft", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_emft(a):\n    # Unary operation jnp.cos\n    return jnp.cos(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_emft attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.cosine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_emft attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.cosine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 253, "kernel_name": "loop_whsi", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_whsi(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val * a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_whsi attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.multiply %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_whsi attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.multiply %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 254, "kernel_name": "scalar_arr_byjr", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_byjr(alpha, x, y):\n    # Scalar-array operations\n    return alpha * x * y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_byjr attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.multiply %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_byjr attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.multiply %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 255, "kernel_name": "nested_ntzb", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_ntzb(a):\n    # Nested branching\n    # if a > -0.18:\n    #   if a > 0.61: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 0.61, a * 3.0, a * 2.0)\n    return jnp.where(a > -0.18, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_ntzb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<6.100000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-1.800000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_ntzb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<6.100000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-1.800000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 256, "kernel_name": "branch_fqee", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_fqee(a):\n    # Branching with where\n    # if a > 0.47: a - 2.7 else: a - 1.7\n    return jnp.where(a > 0.47, a - 2.7, a - 1.7)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_fqee attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<4.700000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.700000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.700000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_fqee attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<4.700000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.700000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.700000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 257, "kernel_name": "reduce_jsax", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_jsax(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_jsax attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_jsax attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 258, "kernel_name": "compound_piaj", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_piaj(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_piaj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_piaj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 259, "kernel_name": "vec_kuml", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_kuml(a, b):\n    # Vector dot product (row-wise for N x 3 arrays)\n    # a and b are expected to be shape (N, 3)\n    # contract over last axis\n    return jnp.sum(a * b, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_kuml attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_kuml attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n"}
{"id": 260, "kernel_name": "nested_zoom", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_zoom(a):\n    # Nested branching\n    # if a > -0.31:\n    #   if a > 0.54: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 0.54, a * 3.0, a * 2.0)\n    return jnp.where(a > -0.31, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_zoom attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<5.400000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-3.100000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_zoom attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<5.400000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-3.100000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 261, "kernel_name": "elementwise_azfz", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_azfz(a, b):\n    # Elementwise *\n    return a * b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_azfz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_azfz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 262, "kernel_name": "unary_dlwl", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_dlwl(a):\n    # Unary operation jnp.abs\n    return jnp.abs(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_dlwl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.abs %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_dlwl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.abs %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 263, "kernel_name": "unary_eoqh", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_eoqh(a):\n    # Unary operation jnp.abs\n    return jnp.abs(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_eoqh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.abs %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_eoqh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.abs %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 264, "kernel_name": "vec_lnet", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_lnet(a, b):\n    # Vector dot product (row-wise for N x 3 arrays)\n    # a and b are expected to be shape (N, 3)\n    # contract over last axis\n    return jnp.sum(a * b, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_lnet attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_lnet attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n"}
{"id": 265, "kernel_name": "nested_qhsn", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_qhsn(a):\n    # Nested branching\n    # if a > -0.07:\n    #   if a > 0.52: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 0.52, a * 3.0, a * 2.0)\n    return jnp.where(a > -0.07, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_qhsn attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<5.200000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-7.000000e-02> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_qhsn attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<5.200000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-7.000000e-02> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 266, "kernel_name": "unary_ezju", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_ezju(a):\n    # Unary operation jnp.sin\n    return jnp.sin(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_ezju attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_ezju attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 267, "kernel_name": "scalar_arr_bafx", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_bafx(alpha, x, y):\n    # Scalar-array operations\n    return alpha + x + y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_bafx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.add %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_bafx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.add %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 268, "kernel_name": "nested_oegh", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_oegh(a):\n    # Nested branching\n    # if a > 0.09:\n    #   if a > 1.37: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 1.37, a * 3.0, a * 2.0)\n    return jnp.where(a > 0.09, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_oegh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.370000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<9.000000e-02> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_oegh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.370000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<9.000000e-02> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 269, "kernel_name": "loop_vgbl", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_vgbl(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val + a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_vgbl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.add %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_vgbl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.add %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 270, "kernel_name": "unary_qrad", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_qrad(a):\n    # Unary operation jnp.abs\n    return jnp.abs(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_qrad attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.abs %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_qrad attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.abs %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 271, "kernel_name": "compound_ozlw", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_ozlw(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_ozlw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_ozlw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 272, "kernel_name": "branch_fcaq", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_fcaq(a):\n    # Branching with where\n    # if a > -0.53: a + 1.9 else: a - 1.6\n    return jnp.where(a > -0.53, a + 1.9, a - 1.6)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_fcaq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-5.300000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.900000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.600000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_fcaq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-5.300000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.900000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.600000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 273, "kernel_name": "vec_kkap", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_kkap(a, b):\n    # Vector dot product (row-wise for N x 3 arrays)\n    # a and b are expected to be shape (N, 3)\n    # contract over last axis\n    return jnp.sum(a * b, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_kkap attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_kkap attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n"}
{"id": 274, "kernel_name": "elementwise_ayai", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_ayai(a, b):\n    # Elementwise +\n    return a + b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_ayai attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_ayai attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 275, "kernel_name": "multi_mskf", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_mskf(a, b):\n    # Multi-statement\n    temp1 = a * b\n    temp2 = jnp.cos(temp1)\n    return temp2 - a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_mskf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.cosine %0 : tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_mskf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.cosine %0 : tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 276, "kernel_name": "elementwise_sbkk", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_sbkk(a, b):\n    # Elementwise *\n    return a * b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_sbkk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_sbkk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 277, "kernel_name": "unary_exun", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_exun(a):\n    # Unary operation jnp.cos\n    return jnp.cos(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_exun attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.cosine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_exun attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.cosine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 278, "kernel_name": "multi_lqli", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_lqli(a, b):\n    # Multi-statement\n    temp1 = a - b\n    temp2 = jnp.sqrt(temp1)\n    return temp2 - a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_lqli attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sqrt %0 : tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_lqli attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sqrt %0 : tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 279, "kernel_name": "loop_hdol", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_hdol(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val * a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_hdol attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.multiply %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_hdol attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.multiply %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 280, "kernel_name": "vec_wvkb", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_wvkb(a):\n    # Vector length (row-wise for N x 3 arrays)\n    return jnp.linalg.norm(a, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_wvkb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_wvkb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 281, "kernel_name": "scalar_arr_tsdi", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_tsdi(alpha, x, y):\n    # Scalar-array operations\n    return alpha * x * y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_tsdi attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.multiply %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_tsdi attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.multiply %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 282, "kernel_name": "compound_pszm", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_pszm(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_pszm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_pszm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 283, "kernel_name": "nested_oddx", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_oddx(a):\n    # Nested branching\n    # if a > 0.16:\n    #   if a > 0.67: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 0.67, a * 3.0, a * 2.0)\n    return jnp.where(a > 0.16, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_oddx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<6.700000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<1.600000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_oddx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<6.700000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<1.600000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 284, "kernel_name": "scalar_arr_ciuk", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_ciuk(alpha, x, y):\n    # Scalar-array operations\n    return alpha * x + y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_ciuk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_ciuk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 285, "kernel_name": "elementwise_wvlt", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_wvlt(a, b):\n    # Elementwise *\n    return a * b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_wvlt attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_wvlt attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 286, "kernel_name": "nested_qtmt", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_qtmt(a):\n    # Nested branching\n    # if a > 0.25:\n    #   if a > 1.13: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 1.13, a * 3.0, a * 2.0)\n    return jnp.where(a > 0.25, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_qtmt attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.130000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<2.500000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_qtmt attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.130000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<2.500000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 287, "kernel_name": "multi_viik", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_viik(a, b):\n    # Multi-statement\n    temp1 = a + b\n    temp2 = jnp.cos(temp1)\n    return temp2 + a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_viik attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.cosine %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_viik attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.cosine %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 288, "kernel_name": "compound_yqyk", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_yqyk(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_yqyk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_yqyk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 289, "kernel_name": "reduce_view", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_view(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_view attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_view attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 290, "kernel_name": "branch_gnps", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_gnps(a):\n    # Branching with where\n    # if a > -0.21: a + 2.3 else: a + 1.1\n    return jnp.where(a > -0.21, a + 2.3, a + 1.1)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_gnps attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-2.100000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.300000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.100000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_gnps attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-2.100000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.300000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.100000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 291, "kernel_name": "nested_ojzc", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_ojzc(a):\n    # Nested branching\n    # if a > 0.04:\n    #   if a > 0.84: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 0.84, a * 3.0, a * 2.0)\n    return jnp.where(a > 0.04, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_ojzc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.839999973> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<4.000000e-02> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_ojzc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.839999973> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<4.000000e-02> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 292, "kernel_name": "branch_yoxk", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_yoxk(a):\n    # Branching with where\n    # if a > -0.28: a - 2.5 else: a - 1.0\n    return jnp.where(a > -0.28, a - 2.5, a - 1.0)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_yoxk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-2.800000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.500000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_yoxk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-2.800000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.500000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 293, "kernel_name": "unary_dunw", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_dunw(a):\n    # Unary operation jnp.sqrt\n    return jnp.sqrt(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_dunw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sqrt %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_dunw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sqrt %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 294, "kernel_name": "elementwise_atxd", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_atxd(a, b):\n    # Elementwise -\n    return a - b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_atxd attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_atxd attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 295, "kernel_name": "branch_fvja", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_fvja(a):\n    # Branching with where\n    # if a > 0.06: a - 0.6 else: a - 2.1\n    return jnp.where(a > 0.06, a - 0.6, a - 2.1)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_fvja attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<6.000000e-02> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<6.000000e-01> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.100000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_fvja attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<6.000000e-02> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<6.000000e-01> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.100000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 296, "kernel_name": "scalar_arr_chmv", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_chmv(alpha, x, y):\n    # Scalar-array operations\n    return alpha * x + y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_chmv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_chmv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 297, "kernel_name": "unary_dzql", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_dzql(a):\n    # Unary operation jnp.sqrt\n    return jnp.sqrt(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_dzql attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sqrt %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_dzql attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sqrt %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 298, "kernel_name": "vec_kcnu", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_kcnu(a, b):\n    # Vector dot product (row-wise for N x 3 arrays)\n    # a and b are expected to be shape (N, 3)\n    # contract over last axis\n    return jnp.sum(a * b, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_kcnu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_kcnu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n"}
{"id": 299, "kernel_name": "compound_podb", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_podb(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_podb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_podb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 300, "kernel_name": "scalar_arr_dpbm", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_dpbm(alpha, x, y):\n    # Scalar-array operations\n    return alpha + x * y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_dpbm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<128xf32>\n    %1 = stablehlo.convert %arg0 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %2, %0 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_dpbm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<128xf32>\n    %1 = stablehlo.convert %arg0 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %2, %0 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 301, "kernel_name": "branch_etyl", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_etyl(a):\n    # Branching with where\n    # if a > -0.82: a * 1.5 else: a - 1.1\n    return jnp.where(a > -0.82, a * 1.5, a - 1.1)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_etyl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-0.819999992> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.500000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.100000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_etyl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-0.819999992> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.500000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.100000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 302, "kernel_name": "vec_jcxx", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_jcxx(a):\n    # Vector length (row-wise for N x 3 arrays)\n    return jnp.linalg.norm(a, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_jcxx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_jcxx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 303, "kernel_name": "loop_uazc", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_uazc(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val - a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_uazc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_uazc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 304, "kernel_name": "nested_opyd", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_opyd(a):\n    # Nested branching\n    # if a > 0.32:\n    #   if a > 1.03: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 1.03, a * 3.0, a * 2.0)\n    return jnp.where(a > 0.32, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_opyd attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.030000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<3.200000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_opyd attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.030000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<3.200000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 305, "kernel_name": "reduce_jmxm", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_jmxm(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_jmxm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_jmxm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 306, "kernel_name": "reduce_wteg", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_wteg(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_wteg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_wteg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 307, "kernel_name": "vec_klto", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_klto(a):\n    # Vector length (row-wise for N x 3 arrays)\n    return jnp.linalg.norm(a, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_klto attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_klto attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 308, "kernel_name": "reduce_xukr", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_xukr(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_xukr attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_xukr attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 309, "kernel_name": "compound_woih", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_woih(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_woih attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_woih attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 310, "kernel_name": "loop_xdbz", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_xdbz(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val + a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_xdbz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.add %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_xdbz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.add %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 311, "kernel_name": "scalar_arr_cjsp", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_cjsp(alpha, x, y):\n    # Scalar-array operations\n    return alpha - x * y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_cjsp attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<128xf32>\n    %1 = stablehlo.convert %arg0 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %2, %0 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_cjsp attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<128xf32>\n    %1 = stablehlo.convert %arg0 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %2, %0 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 312, "kernel_name": "branch_fcct", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_fcct(a):\n    # Branching with where\n    # if a > 0.27: a + 1.3 else: a + 1.1\n    return jnp.where(a > 0.27, a + 1.3, a + 1.1)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_fcct attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<2.700000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.300000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.100000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_fcct attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<2.700000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.300000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.100000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 313, "kernel_name": "unary_eqib", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_eqib(a):\n    # Unary operation jnp.sqrt\n    return jnp.sqrt(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_eqib attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sqrt %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_eqib attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sqrt %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 314, "kernel_name": "elementwise_qmgl", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_qmgl(a, b):\n    # Elementwise -\n    return a - b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_qmgl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_qmgl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 315, "kernel_name": "unary_eybx", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_eybx(a):\n    # Unary operation jnp.sin\n    return jnp.sin(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_eybx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_eybx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 316, "kernel_name": "branch_tgki", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_tgki(a):\n    # Branching with where\n    # if a > -0.62: a + 0.9 else: a + 2.2\n    return jnp.where(a > -0.62, a + 0.9, a + 2.2)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_tgki attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-6.200000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<0.899999976> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.200000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_tgki attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-6.200000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<0.899999976> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.200000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 317, "kernel_name": "scalar_arr_cnpw", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_cnpw(alpha, x, y):\n    # Scalar-array operations\n    return alpha - x - y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_cnpw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.subtract %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_cnpw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.subtract %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 318, "kernel_name": "branch_xhxc", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_xhxc(a):\n    # Branching with where\n    # if a > -0.59: a + 2.8 else: a * 0.7\n    return jnp.where(a > -0.59, a + 2.8, a * 0.7)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_xhxc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-5.900000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.800000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<0.699999988> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_xhxc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-5.900000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.800000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<0.699999988> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 319, "kernel_name": "compound_xduk", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_xduk(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_xduk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_xduk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 320, "kernel_name": "nested_tnpj", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_tnpj(a):\n    # Nested branching\n    # if a > -0.26:\n    #   if a > 1.15: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 1.15, a * 3.0, a * 2.0)\n    return jnp.where(a > -0.26, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_tnpj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.150000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-2.600000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_tnpj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.150000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-2.600000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 321, "kernel_name": "unary_uvnh", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_uvnh(a):\n    # Unary operation jnp.sqrt\n    return jnp.sqrt(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_uvnh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sqrt %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_uvnh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sqrt %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 322, "kernel_name": "elementwise_ucyx", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_ucyx(a, b):\n    # Elementwise +\n    return a + b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_ucyx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_ucyx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 323, "kernel_name": "loop_ryth", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_ryth(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val + a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_ryth attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.add %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_ryth attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.add %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 324, "kernel_name": "scalar_arr_bsdn", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_bsdn(alpha, x, y):\n    # Scalar-array operations\n    return alpha * x - y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_bsdn attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.subtract %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_bsdn attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.subtract %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 325, "kernel_name": "elementwise_anxa", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_anxa(a, b):\n    # Elementwise -\n    return a - b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_anxa attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_anxa attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 326, "kernel_name": "multi_wpay", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_wpay(a, b):\n    # Multi-statement\n    temp1 = a + b\n    temp2 = jnp.sin(temp1)\n    return temp2 - a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_wpay attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sine %0 : tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_wpay attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sine %0 : tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 327, "kernel_name": "reduce_uzxy", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_uzxy(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_uzxy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_uzxy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 328, "kernel_name": "scalar_arr_dvup", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_dvup(alpha, x, y):\n    # Scalar-array operations\n    return alpha - x * y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_dvup attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<128xf32>\n    %1 = stablehlo.convert %arg0 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %2, %0 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_dvup attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<128xf32>\n    %1 = stablehlo.convert %arg0 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %2, %0 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 329, "kernel_name": "loop_ikpz", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_ikpz(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val + a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_ikpz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.add %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_ikpz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.add %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 330, "kernel_name": "scalar_arr_dztg", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_dztg(alpha, x, y):\n    # Scalar-array operations\n    return alpha * x + y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_dztg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_dztg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 331, "kernel_name": "elementwise_awtd", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_awtd(a, b):\n    # Elementwise +\n    return a + b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_awtd attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_awtd attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 332, "kernel_name": "reduce_isud", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_isud(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_isud attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_isud attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 333, "kernel_name": "branch_gglu", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_gglu(a):\n    # Branching with where\n    # if a > -0.47: a * 2.9 else: a - 2.0\n    return jnp.where(a > -0.47, a * 2.9, a - 2.0)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_gglu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-4.700000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.900000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_gglu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-4.700000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.900000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 334, "kernel_name": "vec_rsme", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_rsme(a, b):\n    # Vector dot product (row-wise for N x 3 arrays)\n    # a and b are expected to be shape (N, 3)\n    # contract over last axis\n    return jnp.sum(a * b, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_rsme attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_rsme attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n"}
{"id": 335, "kernel_name": "branch_gczc", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_gczc(a):\n    # Branching with where\n    # if a > 0.67: a + 1.1 else: a - 1.7\n    return jnp.where(a > 0.67, a + 1.1, a - 1.7)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_gczc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<6.700000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.100000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.700000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_gczc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<6.700000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.100000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.700000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 336, "kernel_name": "elementwise_assd", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_assd(a, b):\n    # Elementwise *\n    return a * b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_assd attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_assd attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 337, "kernel_name": "elementwise_aour", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_aour(a, b):\n    # Elementwise -\n    return a - b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_aour attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_aour attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 338, "kernel_name": "unary_xioo", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_xioo(a):\n    # Unary operation jnp.cos\n    return jnp.cos(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_xioo attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.cosine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_xioo attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.cosine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 339, "kernel_name": "multi_mzax", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_mzax(a, b):\n    # Multi-statement\n    temp1 = a - b\n    temp2 = jnp.abs(temp1)\n    return temp2 * a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_mzax attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.abs %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_mzax attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.abs %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 340, "kernel_name": "elementwise_bdft", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_bdft(a, b):\n    # Elementwise -\n    return a - b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_bdft attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_bdft attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 341, "kernel_name": "nested_nndm", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_nndm(a):\n    # Nested branching\n    # if a > 0.28:\n    #   if a > 1.5: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 1.5, a * 3.0, a * 2.0)\n    return jnp.where(a > 0.28, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_nndm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.500000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<2.800000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_nndm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.500000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<2.800000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 342, "kernel_name": "vec_qjut", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_qjut(a, b):\n    # Vector dot product (row-wise for N x 3 arrays)\n    # a and b are expected to be shape (N, 3)\n    # contract over last axis\n    return jnp.sum(a * b, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_qjut attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_qjut attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n"}
{"id": 343, "kernel_name": "scalar_arr_yvki", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_yvki(alpha, x, y):\n    # Scalar-array operations\n    return alpha - x + y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_yvki attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_yvki attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 344, "kernel_name": "compound_qrnk", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_qrnk(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_qrnk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_qrnk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 345, "kernel_name": "multi_lbuu", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_lbuu(a, b):\n    # Multi-statement\n    temp1 = a + b\n    temp2 = jnp.sin(temp1)\n    return temp2 - a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_lbuu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sine %0 : tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_lbuu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sine %0 : tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 346, "kernel_name": "reduce_ujjv", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_ujjv(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_ujjv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_ujjv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 347, "kernel_name": "reduce_rqok", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_rqok(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_rqok attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_rqok attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 348, "kernel_name": "compound_ocnz", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_ocnz(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_ocnz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_ocnz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 349, "kernel_name": "multi_myqr", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_myqr(a, b):\n    # Multi-statement\n    temp1 = a + b\n    temp2 = jnp.cos(temp1)\n    return temp2 + a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_myqr attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.cosine %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_myqr attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.cosine %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 350, "kernel_name": "compound_plpx", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_plpx(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_plpx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_plpx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 351, "kernel_name": "reduce_jeat", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_jeat(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_jeat attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_jeat attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 352, "kernel_name": "branch_zgsm", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_zgsm(a):\n    # Branching with where\n    # if a > -0.34: a + 2.1 else: a + 1.5\n    return jnp.where(a > -0.34, a + 2.1, a + 1.5)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_zgsm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-3.400000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.100000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.500000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_zgsm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-3.400000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.100000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.500000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 353, "kernel_name": "reduce_jady", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_jady(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_jady attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_jady attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 354, "kernel_name": "elementwise_brod", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_brod(a, b):\n    # Elementwise -\n    return a - b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_brod attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_brod attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 355, "kernel_name": "scalar_arr_dkua", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_dkua(alpha, x, y):\n    # Scalar-array operations\n    return alpha - x * y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_dkua attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<128xf32>\n    %1 = stablehlo.convert %arg0 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %2, %0 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_dkua attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<128xf32>\n    %1 = stablehlo.convert %arg0 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %2, %0 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 356, "kernel_name": "vec_ylpj", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_ylpj(a, b):\n    # Vector dot product (row-wise for N x 3 arrays)\n    # a and b are expected to be shape (N, 3)\n    # contract over last axis\n    return jnp.sum(a * b, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_ylpj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_ylpj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n"}
{"id": 357, "kernel_name": "elementwise_asag", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_asag(a, b):\n    # Elementwise +\n    return a + b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_asag attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_asag attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 358, "kernel_name": "loop_hgvo", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_hgvo(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val - a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_hgvo attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_hgvo attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 359, "kernel_name": "multi_mlvu", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_mlvu(a, b):\n    # Multi-statement\n    temp1 = a - b\n    temp2 = jnp.sin(temp1)\n    return temp2 - a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_mlvu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sine %0 : tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_mlvu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sine %0 : tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 360, "kernel_name": "multi_vcqk", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_vcqk(a, b):\n    # Multi-statement\n    temp1 = a * b\n    temp2 = jnp.cos(temp1)\n    return temp2 - a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_vcqk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.cosine %0 : tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_vcqk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.cosine %0 : tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 361, "kernel_name": "branch_giux", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_giux(a):\n    # Branching with where\n    # if a > 0.98: a * 1.7 else: a * 3.0\n    return jnp.where(a > 0.98, a * 1.7, a * 3.0)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_giux attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<9.800000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.700000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_giux attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<9.800000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.700000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 362, "kernel_name": "scalar_arr_ckac", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_ckac(alpha, x, y):\n    # Scalar-array operations\n    return alpha * x + y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_ckac attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_ckac attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 363, "kernel_name": "elementwise_bzsv", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_bzsv(a, b):\n    # Elementwise *\n    return a * b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_bzsv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_bzsv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 364, "kernel_name": "vec_kkkh", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_kkkh(a, b):\n    # Vector dot product (row-wise for N x 3 arrays)\n    # a and b are expected to be shape (N, 3)\n    # contract over last axis\n    return jnp.sum(a * b, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_kkkh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_kkkh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n"}
{"id": 365, "kernel_name": "scalar_arr_rcvo", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_rcvo(alpha, x, y):\n    # Scalar-array operations\n    return alpha - x * y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_rcvo attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<128xf32>\n    %1 = stablehlo.convert %arg0 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %2, %0 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_rcvo attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<128xf32>\n    %1 = stablehlo.convert %arg0 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %2, %0 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 366, "kernel_name": "multi_ulyg", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_ulyg(a, b):\n    # Multi-statement\n    temp1 = a * b\n    temp2 = jnp.cos(temp1)\n    return temp2 + a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_ulyg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.cosine %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_ulyg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.cosine %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 367, "kernel_name": "nested_vodx", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_vodx(a):\n    # Nested branching\n    # if a > 0.13:\n    #   if a > 0.93: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 0.93, a * 3.0, a * 2.0)\n    return jnp.where(a > 0.13, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_vodx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<9.300000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<1.300000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_vodx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<9.300000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<1.300000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 368, "kernel_name": "unary_dyai", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_dyai(a):\n    # Unary operation jnp.sin\n    return jnp.sin(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_dyai attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_dyai attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 369, "kernel_name": "branch_fipg", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_fipg(a):\n    # Branching with where\n    # if a > 0.79: a - 2.6 else: a + 2.3\n    return jnp.where(a > 0.79, a - 2.6, a + 2.3)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_fipg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<7.900000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.600000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.300000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_fipg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<7.900000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.600000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.300000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 370, "kernel_name": "loop_hsbz", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_hsbz(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val + a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_hsbz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.add %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_hsbz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.add %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 371, "kernel_name": "multi_vqmc", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_vqmc(a, b):\n    # Multi-statement\n    temp1 = a * b\n    temp2 = jnp.sin(temp1)\n    return temp2 * a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_vqmc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sine %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_vqmc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sine %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 372, "kernel_name": "vec_kjyt", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_kjyt(a, b):\n    # Vector dot product (row-wise for N x 3 arrays)\n    # a and b are expected to be shape (N, 3)\n    # contract over last axis\n    return jnp.sum(a * b, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_kjyt attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_kjyt attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n"}
{"id": 373, "kernel_name": "reduce_ughf", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_ughf(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_ughf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_ughf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 374, "kernel_name": "loop_gapr", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_gapr(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val * a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_gapr attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.multiply %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_gapr attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.multiply %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 375, "kernel_name": "compound_yusb", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_yusb(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_yusb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_yusb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 376, "kernel_name": "reduce_icdr", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_icdr(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_icdr attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_icdr attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 377, "kernel_name": "compound_qsdh", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_qsdh(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_qsdh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_qsdh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 378, "kernel_name": "elementwise_auhc", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_auhc(a, b):\n    # Elementwise *\n    return a * b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_auhc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_auhc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 379, "kernel_name": "compound_rpwm", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_rpwm(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_rpwm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_rpwm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 380, "kernel_name": "vec_kazp", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_kazp(a, b):\n    # Vector dot product (row-wise for N x 3 arrays)\n    # a and b are expected to be shape (N, 3)\n    # contract over last axis\n    return jnp.sum(a * b, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_kazp attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_kazp attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n"}
{"id": 381, "kernel_name": "reduce_iotx", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_iotx(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_iotx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_iotx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 382, "kernel_name": "nested_ousb", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_ousb(a):\n    # Nested branching\n    # if a > 0.19:\n    #   if a > 0.91: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 0.91, a * 3.0, a * 2.0)\n    return jnp.where(a > 0.19, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_ousb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<9.100000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<1.900000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_ousb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<9.100000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<1.900000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 383, "kernel_name": "vec_kebt", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_kebt(a, b):\n    # Vector dot product (row-wise for N x 3 arrays)\n    # a and b are expected to be shape (N, 3)\n    # contract over last axis\n    return jnp.sum(a * b, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_kebt attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_kebt attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n"}
{"id": 384, "kernel_name": "unary_esjf", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_esjf(a):\n    # Unary operation jnp.sqrt\n    return jnp.sqrt(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_esjf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sqrt %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_esjf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sqrt %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 385, "kernel_name": "unary_evub", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_evub(a):\n    # Unary operation jnp.sin\n    return jnp.sin(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_evub attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_evub attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 386, "kernel_name": "loop_gblx", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_gblx(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val - a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_gblx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_gblx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 387, "kernel_name": "compound_pdaa", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_pdaa(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_pdaa attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_pdaa attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 388, "kernel_name": "loop_hdef", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_hdef(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val - a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_hdef attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_hdef attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 389, "kernel_name": "loop_zuyu", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_zuyu(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val + a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_zuyu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.add %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_zuyu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.add %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 390, "kernel_name": "compound_prbu", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_prbu(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_prbu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_prbu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 391, "kernel_name": "branch_fmbq", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_fmbq(a):\n    # Branching with where\n    # if a > 0.78: a * 1.8 else: a + 2.0\n    return jnp.where(a > 0.78, a * 1.8, a + 2.0)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_fmbq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<7.800000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.800000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_fmbq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<7.800000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.800000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 392, "kernel_name": "reduce_jnkv", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_jnkv(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_jnkv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_jnkv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 393, "kernel_name": "branch_zwnw", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_zwnw(a):\n    # Branching with where\n    # if a > -0.9: a * 2.7 else: a * 1.7\n    return jnp.where(a > -0.9, a * 2.7, a * 1.7)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_zwnw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-0.899999976> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.700000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.700000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_zwnw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-0.899999976> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.700000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.700000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 394, "kernel_name": "vec_jufv", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_jufv(a, b):\n    # Vector dot product (row-wise for N x 3 arrays)\n    # a and b are expected to be shape (N, 3)\n    # contract over last axis\n    return jnp.sum(a * b, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_jufv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_jufv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n"}
{"id": 395, "kernel_name": "nested_ncaa", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_ncaa(a):\n    # Nested branching\n    # if a > 0.49:\n    #   if a > 0.95: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 0.95, a * 3.0, a * 2.0)\n    return jnp.where(a > 0.49, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_ncaa attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.949999988> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<4.900000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_ncaa attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.949999988> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<4.900000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 396, "kernel_name": "branch_fowt", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_fowt(a):\n    # Branching with where\n    # if a > -0.43: a * 0.6 else: a - 0.5\n    return jnp.where(a > -0.43, a * 0.6, a - 0.5)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_fowt attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-4.300000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<6.000000e-01> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_fowt attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-4.300000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<6.000000e-01> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 397, "kernel_name": "vec_kihw", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_kihw(a):\n    # Vector length (row-wise for N x 3 arrays)\n    return jnp.linalg.norm(a, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_kihw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_kihw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 398, "kernel_name": "scalar_arr_dchh", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_dchh(alpha, x, y):\n    # Scalar-array operations\n    return alpha + x + y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_dchh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.add %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_dchh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.add %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 399, "kernel_name": "unary_eusp", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_eusp(a):\n    # Unary operation jnp.cos\n    return jnp.cos(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_eusp attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.cosine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_eusp attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.cosine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 400, "kernel_name": "reduce_jllk", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_jllk(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_jllk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_jllk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 401, "kernel_name": "unary_taqg", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_taqg(a):\n    # Unary operation jnp.sin\n    return jnp.sin(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_taqg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_taqg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 402, "kernel_name": "loop_iamw", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_iamw(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val * a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_iamw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.multiply %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_iamw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.multiply %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 403, "kernel_name": "nested_ogkz", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_ogkz(a):\n    # Nested branching\n    # if a > 0.39:\n    #   if a > 0.9: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 0.9, a * 3.0, a * 2.0)\n    return jnp.where(a > 0.39, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_ogkz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.899999976> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<3.900000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_ogkz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.899999976> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<3.900000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 404, "kernel_name": "multi_utrq", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_utrq(a, b):\n    # Multi-statement\n    temp1 = a + b\n    temp2 = jnp.abs(temp1)\n    return temp2 + a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_utrq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.abs %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_utrq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.abs %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 405, "kernel_name": "vec_knpk", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_knpk(a, b):\n    # Vector dot product (row-wise for N x 3 arrays)\n    # a and b are expected to be shape (N, 3)\n    # contract over last axis\n    return jnp.sum(a * b, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_knpk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_knpk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n"}
{"id": 406, "kernel_name": "loop_hdha", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_hdha(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val * a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_hdha attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.multiply %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_hdha attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.multiply %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 407, "kernel_name": "branch_eekr", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_eekr(a):\n    # Branching with where\n    # if a > 0.16: a + 0.8 else: a - 1.4\n    return jnp.where(a > 0.16, a + 0.8, a - 1.4)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_eekr attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.600000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<8.000000e-01> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.400000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_eekr attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.600000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<8.000000e-01> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.400000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 408, "kernel_name": "scalar_arr_cyvd", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_cyvd(alpha, x, y):\n    # Scalar-array operations\n    return alpha + x + y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_cyvd attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.add %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_cyvd attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.add %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 409, "kernel_name": "elementwise_bdtb", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_bdtb(a, b):\n    # Elementwise +\n    return a + b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_bdtb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_bdtb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 410, "kernel_name": "reduce_irti", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_irti(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_irti attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_irti attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 411, "kernel_name": "branch_gnrk", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_gnrk(a):\n    # Branching with where\n    # if a > 0.39: a * 1.1 else: a - 2.0\n    return jnp.where(a > 0.39, a * 1.1, a - 2.0)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_gnrk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<3.900000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.100000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_gnrk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<3.900000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.100000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 412, "kernel_name": "loop_htqi", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_htqi(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val - a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_htqi attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_htqi attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 413, "kernel_name": "elementwise_xsjo", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_xsjo(a, b):\n    # Elementwise *\n    return a * b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_xsjo attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_xsjo attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 414, "kernel_name": "multi_tzlu", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_tzlu(a, b):\n    # Multi-statement\n    temp1 = a * b\n    temp2 = jnp.sqrt(temp1)\n    return temp2 + a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_tzlu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sqrt %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_tzlu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sqrt %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 415, "kernel_name": "unary_tzol", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_tzol(a):\n    # Unary operation jnp.sin\n    return jnp.sin(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_tzol attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_tzol attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 416, "kernel_name": "scalar_arr_bnbz", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_bnbz(alpha, x, y):\n    # Scalar-array operations\n    return alpha * x * y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_bnbz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.multiply %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_bnbz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.multiply %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 417, "kernel_name": "unary_ytsk", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_ytsk(a):\n    # Unary operation jnp.sqrt\n    return jnp.sqrt(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_ytsk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sqrt %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_ytsk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sqrt %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 418, "kernel_name": "loop_httk", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_httk(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val - a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_httk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_httk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 419, "kernel_name": "reduce_stoo", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_stoo(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_stoo attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_stoo attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 420, "kernel_name": "compound_pbvk", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_pbvk(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_pbvk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_pbvk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 421, "kernel_name": "branch_fmxc", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_fmxc(a):\n    # Branching with where\n    # if a > 0.69: a * 0.9 else: a + 1.7\n    return jnp.where(a > 0.69, a * 0.9, a + 1.7)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_fmxc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.689999997> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<0.899999976> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.700000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_fmxc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.689999997> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<0.899999976> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.700000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 422, "kernel_name": "loop_hoqf", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_hoqf(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val - a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_hoqf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_hoqf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 423, "kernel_name": "branch_efva", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_efva(a):\n    # Branching with where\n    # if a > -0.55: a * 2.1 else: a * 0.9\n    return jnp.where(a > -0.55, a * 2.1, a * 0.9)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_efva attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-5.500000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.100000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<0.899999976> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_efva attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-5.500000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.100000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<0.899999976> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 424, "kernel_name": "elementwise_xdez", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_xdez(a, b):\n    # Elementwise +\n    return a + b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_xdez attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_xdez attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 425, "kernel_name": "loop_gkaw", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_gkaw(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val * a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_gkaw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.multiply %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_gkaw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.multiply %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 426, "kernel_name": "branch_zjpe", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_zjpe(a):\n    # Branching with where\n    # if a > 0.29: a - 2.3 else: a + 1.7\n    return jnp.where(a > 0.29, a - 2.3, a + 1.7)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_zjpe attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<2.900000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.300000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.700000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_zjpe attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<2.900000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.300000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.700000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 427, "kernel_name": "loop_ghgq", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_ghgq(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val * a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_ghgq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.multiply %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_ghgq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.multiply %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 428, "kernel_name": "elementwise_bidx", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_bidx(a, b):\n    # Elementwise *\n    return a * b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_bidx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_bidx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 429, "kernel_name": "scalar_arr_zybh", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_zybh(alpha, x, y):\n    # Scalar-array operations\n    return alpha - x - y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_zybh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.subtract %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_zybh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.subtract %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 430, "kernel_name": "unary_wyzl", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_wyzl(a):\n    # Unary operation jnp.abs\n    return jnp.abs(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_wyzl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.abs %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_wyzl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.abs %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 431, "kernel_name": "reduce_ibjc", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_ibjc(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_ibjc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_ibjc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 432, "kernel_name": "scalar_arr_bbzd", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_bbzd(alpha, x, y):\n    # Scalar-array operations\n    return alpha - x - y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_bbzd attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.subtract %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_bbzd attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.subtract %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 433, "kernel_name": "scalar_arr_zbuo", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_zbuo(alpha, x, y):\n    # Scalar-array operations\n    return alpha - x * y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_zbuo attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<128xf32>\n    %1 = stablehlo.convert %arg0 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %2, %0 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_zbuo attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<128xf32>\n    %1 = stablehlo.convert %arg0 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %2, %0 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 434, "kernel_name": "multi_mpzp", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_mpzp(a, b):\n    # Multi-statement\n    temp1 = a * b\n    temp2 = jnp.cos(temp1)\n    return temp2 - a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_mpzp attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.cosine %0 : tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_mpzp attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.cosine %0 : tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 435, "kernel_name": "compound_qxhe", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_qxhe(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_qxhe attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_qxhe attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 436, "kernel_name": "reduce_zvca", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_zvca(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_zvca attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_zvca attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 437, "kernel_name": "vec_xwon", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_xwon(a, b):\n    # Vector dot product (row-wise for N x 3 arrays)\n    # a and b are expected to be shape (N, 3)\n    # contract over last axis\n    return jnp.sum(a * b, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_xwon attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_xwon attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n"}
{"id": 438, "kernel_name": "vec_kwhr", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_kwhr(a):\n    # Vector length (row-wise for N x 3 arrays)\n    return jnp.linalg.norm(a, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_kwhr attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_kwhr attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 439, "kernel_name": "multi_rmld", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_rmld(a, b):\n    # Multi-statement\n    temp1 = a * b\n    temp2 = jnp.cos(temp1)\n    return temp2 - a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_rmld attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.cosine %0 : tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_rmld attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.cosine %0 : tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 440, "kernel_name": "nested_nhkv", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_nhkv(a):\n    # Nested branching\n    # if a > 0.03:\n    #   if a > 1.16: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 1.16, a * 3.0, a * 2.0)\n    return jnp.where(a > 0.03, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_nhkv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.160000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<3.000000e-02> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_nhkv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.160000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<3.000000e-02> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 441, "kernel_name": "nested_nvic", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_nvic(a):\n    # Nested branching\n    # if a > -0.46:\n    #   if a > 1.49: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 1.49, a * 3.0, a * 2.0)\n    return jnp.where(a > -0.46, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_nvic attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.490000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-4.600000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_nvic attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.490000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-4.600000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 442, "kernel_name": "compound_olbc", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_olbc(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_olbc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_olbc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 443, "kernel_name": "unary_dtmi", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_dtmi(a):\n    # Unary operation jnp.cos\n    return jnp.cos(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_dtmi attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.cosine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_dtmi attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.cosine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 444, "kernel_name": "unary_emey", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_emey(a):\n    # Unary operation jnp.sin\n    return jnp.sin(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_emey attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_emey attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 445, "kernel_name": "compound_vvpi", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_vvpi(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_vvpi attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_vvpi attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 446, "kernel_name": "scalar_arr_cswn", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_cswn(alpha, x, y):\n    # Scalar-array operations\n    return alpha * x - y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_cswn attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.subtract %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_cswn attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.subtract %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 447, "kernel_name": "unary_yelw", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_yelw(a):\n    # Unary operation jnp.cos\n    return jnp.cos(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_yelw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.cosine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_yelw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.cosine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 448, "kernel_name": "multi_xmmo", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_xmmo(a, b):\n    # Multi-statement\n    temp1 = a + b\n    temp2 = jnp.sin(temp1)\n    return temp2 * a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_xmmo attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sine %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_xmmo attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sine %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 449, "kernel_name": "multi_mkvq", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_mkvq(a, b):\n    # Multi-statement\n    temp1 = a * b\n    temp2 = jnp.sin(temp1)\n    return temp2 * a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_mkvq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sine %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_mkvq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sine %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 450, "kernel_name": "compound_uuwy", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_uuwy(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_uuwy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_uuwy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 451, "kernel_name": "loop_xrzh", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_xrzh(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val - a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_xrzh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_xrzh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 452, "kernel_name": "compound_qrqw", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_qrqw(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_qrqw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_qrqw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 453, "kernel_name": "unary_renz", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_renz(a):\n    # Unary operation jnp.abs\n    return jnp.abs(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_renz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.abs %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_renz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.abs %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 454, "kernel_name": "multi_sldu", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_sldu(a, b):\n    # Multi-statement\n    temp1 = a * b\n    temp2 = jnp.abs(temp1)\n    return temp2 + a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_sldu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.abs %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_sldu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.abs %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 455, "kernel_name": "loop_gyvi", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_gyvi(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val - a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_gyvi attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_gyvi attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 456, "kernel_name": "multi_mpgj", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_mpgj(a, b):\n    # Multi-statement\n    temp1 = a - b\n    temp2 = jnp.cos(temp1)\n    return temp2 + a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_mpgj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.cosine %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_mpgj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.cosine %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 457, "kernel_name": "scalar_arr_ctmp", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_ctmp(alpha, x, y):\n    # Scalar-array operations\n    return alpha - x * y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_ctmp attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<128xf32>\n    %1 = stablehlo.convert %arg0 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %2, %0 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_ctmp attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<128xf32>\n    %1 = stablehlo.convert %arg0 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %2, %0 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 458, "kernel_name": "multi_uypg", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_uypg(a, b):\n    # Multi-statement\n    temp1 = a * b\n    temp2 = jnp.abs(temp1)\n    return temp2 - a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_uypg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.abs %0 : tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_uypg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.abs %0 : tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 459, "kernel_name": "loop_qqgu", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_qqgu(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val - a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_qqgu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_qqgu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 460, "kernel_name": "nested_neju", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_neju(a):\n    # Nested branching\n    # if a > 0.2:\n    #   if a > 0.67: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 0.67, a * 3.0, a * 2.0)\n    return jnp.where(a > 0.2, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_neju attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<6.700000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<2.000000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_neju attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<6.700000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<2.000000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 461, "kernel_name": "unary_ejei", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_ejei(a):\n    # Unary operation jnp.sqrt\n    return jnp.sqrt(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_ejei attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sqrt %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_ejei attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sqrt %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 462, "kernel_name": "multi_mout", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_mout(a, b):\n    # Multi-statement\n    temp1 = a + b\n    temp2 = jnp.abs(temp1)\n    return temp2 + a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_mout attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.abs %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_mout attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.abs %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 463, "kernel_name": "unary_dvhs", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_dvhs(a):\n    # Unary operation jnp.sin\n    return jnp.sin(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_dvhs attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_dvhs attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 464, "kernel_name": "multi_tmjk", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_tmjk(a, b):\n    # Multi-statement\n    temp1 = a - b\n    temp2 = jnp.abs(temp1)\n    return temp2 + a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_tmjk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.abs %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_tmjk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.abs %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 465, "kernel_name": "multi_mqym", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_mqym(a, b):\n    # Multi-statement\n    temp1 = a + b\n    temp2 = jnp.sin(temp1)\n    return temp2 - a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_mqym attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sine %0 : tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_mqym attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sine %0 : tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 466, "kernel_name": "nested_ormc", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_ormc(a):\n    # Nested branching\n    # if a > -0.13:\n    #   if a > 0.58: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 0.58, a * 3.0, a * 2.0)\n    return jnp.where(a > -0.13, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_ormc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<5.800000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-1.300000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_ormc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<5.800000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-1.300000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 467, "kernel_name": "vec_sxfe", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_sxfe(a, b):\n    # Vector dot product (row-wise for N x 3 arrays)\n    # a and b are expected to be shape (N, 3)\n    # contract over last axis\n    return jnp.sum(a * b, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_sxfe attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_sxfe attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n"}
{"id": 468, "kernel_name": "nested_rotk", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_rotk(a):\n    # Nested branching\n    # if a > 0.28:\n    #   if a > 0.66: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 0.66, a * 3.0, a * 2.0)\n    return jnp.where(a > 0.28, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_rotk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<6.600000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<2.800000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_rotk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<6.600000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<2.800000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 469, "kernel_name": "scalar_arr_cdgt", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_cdgt(alpha, x, y):\n    # Scalar-array operations\n    return alpha + x * y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_cdgt attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<128xf32>\n    %1 = stablehlo.convert %arg0 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %2, %0 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_cdgt attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<128xf32>\n    %1 = stablehlo.convert %arg0 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %2, %0 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 470, "kernel_name": "elementwise_alik", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_alik(a, b):\n    # Elementwise *\n    return a * b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_alik attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_alik attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 471, "kernel_name": "elementwise_adqr", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_adqr(a, b):\n    # Elementwise *\n    return a * b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_adqr attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_adqr attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 472, "kernel_name": "reduce_jqfk", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_jqfk(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_jqfk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_jqfk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 473, "kernel_name": "unary_eyxc", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_eyxc(a):\n    # Unary operation jnp.sqrt\n    return jnp.sqrt(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_eyxc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sqrt %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_eyxc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sqrt %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 474, "kernel_name": "branch_vwot", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_vwot(a):\n    # Branching with where\n    # if a > 0.83: a * 0.5 else: a * 1.4\n    return jnp.where(a > 0.83, a * 0.5, a * 1.4)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_vwot attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.829999983> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.400000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_vwot attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.829999983> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.400000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 475, "kernel_name": "vec_vyzk", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_vyzk(a, b):\n    # Vector dot product (row-wise for N x 3 arrays)\n    # a and b are expected to be shape (N, 3)\n    # contract over last axis\n    return jnp.sum(a * b, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_vyzk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_vyzk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n"}
{"id": 476, "kernel_name": "reduce_yvng", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_yvng(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_yvng attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_yvng attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 477, "kernel_name": "compound_paid", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_paid(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_paid attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_paid attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 478, "kernel_name": "elementwise_zbwk", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_zbwk(a, b):\n    # Elementwise +\n    return a + b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_zbwk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_zbwk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 479, "kernel_name": "scalar_arr_cwgu", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_cwgu(alpha, x, y):\n    # Scalar-array operations\n    return alpha + x - y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_cwgu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.add %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.subtract %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_cwgu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.add %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.subtract %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 480, "kernel_name": "scalar_arr_bkwp", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_bkwp(alpha, x, y):\n    # Scalar-array operations\n    return alpha + x + y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_bkwp attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.add %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_bkwp attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.add %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 481, "kernel_name": "loop_zrhn", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_zrhn(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val + a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_zrhn attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.add %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_zrhn attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.add %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 482, "kernel_name": "branch_fahi", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_fahi(a):\n    # Branching with where\n    # if a > 0.93: a - 0.9 else: a + 0.7\n    return jnp.where(a > 0.93, a - 0.9, a + 0.7)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_fahi attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<9.300000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<0.899999976> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<0.699999988> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_fahi attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<9.300000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<0.899999976> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<0.699999988> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 483, "kernel_name": "nested_vzmu", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_vzmu(a):\n    # Nested branching\n    # if a > 0.18:\n    #   if a > 1.1: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 1.1, a * 3.0, a * 2.0)\n    return jnp.where(a > 0.18, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_vzmu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.100000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<1.800000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_vzmu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.100000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<1.800000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 484, "kernel_name": "unary_zfbj", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_zfbj(a):\n    # Unary operation jnp.cos\n    return jnp.cos(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_zfbj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.cosine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_zfbj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.cosine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 485, "kernel_name": "vec_zyur", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_zyur(a, b):\n    # Vector dot product (row-wise for N x 3 arrays)\n    # a and b are expected to be shape (N, 3)\n    # contract over last axis\n    return jnp.sum(a * b, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_zyur attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_zyur attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n"}
{"id": 486, "kernel_name": "elementwise_uvoc", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_uvoc(a, b):\n    # Elementwise -\n    return a - b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_uvoc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_uvoc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 487, "kernel_name": "unary_qdug", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_qdug(a):\n    # Unary operation jnp.cos\n    return jnp.cos(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_qdug attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.cosine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_qdug attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.cosine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 488, "kernel_name": "reduce_wcvv", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_wcvv(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_wcvv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_wcvv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 489, "kernel_name": "unary_xrzl", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_xrzl(a):\n    # Unary operation jnp.sin\n    return jnp.sin(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_xrzl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_xrzl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 490, "kernel_name": "branch_fqcm", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_fqcm(a):\n    # Branching with where\n    # if a > 0.43: a * 0.7 else: a * 1.7\n    return jnp.where(a > 0.43, a * 0.7, a * 1.7)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_fqcm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<4.300000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<0.699999988> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.700000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_fqcm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<4.300000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<0.699999988> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.700000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 491, "kernel_name": "vec_jrgk", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_jrgk(a, b):\n    # Vector dot product (row-wise for N x 3 arrays)\n    # a and b are expected to be shape (N, 3)\n    # contract over last axis\n    return jnp.sum(a * b, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_jrgk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_jrgk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n"}
{"id": 492, "kernel_name": "vec_kole", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_kole(a):\n    # Vector length (row-wise for N x 3 arrays)\n    return jnp.linalg.norm(a, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_kole attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_kole attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 493, "kernel_name": "unary_dkrp", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_dkrp(a):\n    # Unary operation jnp.cos\n    return jnp.cos(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_dkrp attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.cosine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_dkrp attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.cosine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 494, "kernel_name": "branch_tqou", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_tqou(a):\n    # Branching with where\n    # if a > 0.11: a - 2.1 else: a - 2.3\n    return jnp.where(a > 0.11, a - 2.1, a - 2.3)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_tqou attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.100000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.100000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.300000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_tqou attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.100000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.100000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.300000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 495, "kernel_name": "multi_szfl", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_szfl(a, b):\n    # Multi-statement\n    temp1 = a * b\n    temp2 = jnp.abs(temp1)\n    return temp2 - a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_szfl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.abs %0 : tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_szfl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.abs %0 : tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 496, "kernel_name": "loop_hbke", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_hbke(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val - a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_hbke attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_hbke attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 497, "kernel_name": "loop_htzs", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_htzs(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val + a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_htzs attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.add %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_htzs attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.add %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 498, "kernel_name": "compound_prai", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_prai(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_prai attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_prai attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 499, "kernel_name": "reduce_jmde", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_jmde(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_jmde attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_jmde attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 500, "kernel_name": "multi_meel", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_meel(a, b):\n    # Multi-statement\n    temp1 = a - b\n    temp2 = jnp.sin(temp1)\n    return temp2 + a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_meel attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sine %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_meel attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sine %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 501, "kernel_name": "unary_vzyc", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_vzyc(a):\n    # Unary operation jnp.abs\n    return jnp.abs(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_vzyc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.abs %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_vzyc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.abs %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 502, "kernel_name": "loop_rhio", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_rhio(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val * a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_rhio attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.multiply %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_rhio attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.multiply %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 503, "kernel_name": "branch_fxxq", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_fxxq(a):\n    # Branching with where\n    # if a > 1.0: a + 0.7 else: a - 1.4\n    return jnp.where(a > 1.0, a + 0.7, a - 1.4)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_fxxq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<0.699999988> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.400000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_fxxq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<0.699999988> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.400000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 504, "kernel_name": "elementwise_akxt", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_akxt(a, b):\n    # Elementwise -\n    return a - b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_akxt attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_akxt attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 505, "kernel_name": "elementwise_bsim", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_bsim(a, b):\n    # Elementwise -\n    return a - b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_bsim attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_bsim attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 506, "kernel_name": "elementwise_yund", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_yund(a, b):\n    # Elementwise +\n    return a + b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_yund attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_yund attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 507, "kernel_name": "branch_yfes", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_yfes(a):\n    # Branching with where\n    # if a > -0.03: a - 0.7 else: a - 2.6\n    return jnp.where(a > -0.03, a - 0.7, a - 2.6)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_yfes attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-3.000000e-02> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<0.699999988> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.600000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_yfes attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-3.000000e-02> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<0.699999988> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.600000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 508, "kernel_name": "multi_vmsb", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_vmsb(a, b):\n    # Multi-statement\n    temp1 = a + b\n    temp2 = jnp.sqrt(temp1)\n    return temp2 - a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_vmsb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sqrt %0 : tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_vmsb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sqrt %0 : tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 509, "kernel_name": "nested_nrva", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_nrva(a):\n    # Nested branching\n    # if a > -0.26:\n    #   if a > 1.5: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 1.5, a * 3.0, a * 2.0)\n    return jnp.where(a > -0.26, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_nrva attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.500000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-2.600000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_nrva attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.500000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-2.600000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 510, "kernel_name": "elementwise_abnt", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_abnt(a, b):\n    # Elementwise *\n    return a * b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_abnt attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_abnt attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 511, "kernel_name": "multi_mllk", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_mllk(a, b):\n    # Multi-statement\n    temp1 = a * b\n    temp2 = jnp.cos(temp1)\n    return temp2 * a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_mllk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.cosine %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_mllk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.cosine %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 512, "kernel_name": "nested_nejf", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_nejf(a):\n    # Nested branching\n    # if a > 0.38:\n    #   if a > 0.65: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 0.65, a * 3.0, a * 2.0)\n    return jnp.where(a > 0.38, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_nejf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<6.500000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<3.800000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_nejf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<6.500000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<3.800000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 513, "kernel_name": "branch_feox", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_feox(a):\n    # Branching with where\n    # if a > -0.69: a - 1.9 else: a + 2.4\n    return jnp.where(a > -0.69, a - 1.9, a + 2.4)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_feox attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-0.689999997> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.900000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.400000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_feox attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-0.689999997> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.900000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.400000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 514, "kernel_name": "multi_reqs", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_reqs(a, b):\n    # Multi-statement\n    temp1 = a - b\n    temp2 = jnp.sin(temp1)\n    return temp2 * a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_reqs attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sine %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_reqs attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sine %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 515, "kernel_name": "elementwise_acou", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_acou(a, b):\n    # Elementwise *\n    return a * b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_acou attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_acou attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 516, "kernel_name": "vec_zwon", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_zwon(a, b):\n    # Vector dot product (row-wise for N x 3 arrays)\n    # a and b are expected to be shape (N, 3)\n    # contract over last axis\n    return jnp.sum(a * b, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_zwon attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_zwon attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n"}
{"id": 517, "kernel_name": "elementwise_axuz", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_axuz(a, b):\n    # Elementwise -\n    return a - b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_axuz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_axuz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 518, "kernel_name": "elementwise_tbta", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_tbta(a, b):\n    # Elementwise *\n    return a * b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_tbta attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_tbta attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 519, "kernel_name": "compound_pfof", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_pfof(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_pfof attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_pfof attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 520, "kernel_name": "reduce_tsuv", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_tsuv(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_tsuv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_tsuv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 521, "kernel_name": "nested_nwha", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_nwha(a):\n    # Nested branching\n    # if a > 0.45:\n    #   if a > 1.44: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 1.44, a * 3.0, a * 2.0)\n    return jnp.where(a > 0.45, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_nwha attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.440000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<4.500000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_nwha attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.440000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<4.500000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 522, "kernel_name": "loop_yfii", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_yfii(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val * a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_yfii attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.multiply %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_yfii attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.multiply %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 523, "kernel_name": "multi_yumi", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_yumi(a, b):\n    # Multi-statement\n    temp1 = a - b\n    temp2 = jnp.cos(temp1)\n    return temp2 * a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_yumi attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.cosine %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_yumi attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.cosine %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 524, "kernel_name": "multi_umxi", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_umxi(a, b):\n    # Multi-statement\n    temp1 = a + b\n    temp2 = jnp.sqrt(temp1)\n    return temp2 - a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_umxi attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sqrt %0 : tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_umxi attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sqrt %0 : tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 525, "kernel_name": "unary_dmwv", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_dmwv(a):\n    # Unary operation jnp.sin\n    return jnp.sin(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_dmwv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_dmwv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 526, "kernel_name": "scalar_arr_cnvz", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_cnvz(alpha, x, y):\n    # Scalar-array operations\n    return alpha - x - y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_cnvz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.subtract %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_cnvz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.subtract %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 527, "kernel_name": "compound_qybb", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_qybb(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_qybb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_qybb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 528, "kernel_name": "scalar_arr_babz", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_babz(alpha, x, y):\n    # Scalar-array operations\n    return alpha - x - y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_babz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.subtract %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_babz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.subtract %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 529, "kernel_name": "unary_dwqk", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_dwqk(a):\n    # Unary operation jnp.abs\n    return jnp.abs(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_dwqk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.abs %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_dwqk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.abs %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 530, "kernel_name": "branch_ykgp", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_ykgp(a):\n    # Branching with where\n    # if a > -0.48: a - 0.7 else: a * 1.1\n    return jnp.where(a > -0.48, a - 0.7, a * 1.1)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_ykgp attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-4.800000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<0.699999988> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.100000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_ykgp attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-4.800000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<0.699999988> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.100000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 531, "kernel_name": "loop_sqvy", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_sqvy(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val + a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_sqvy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.add %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_sqvy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.add %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 532, "kernel_name": "loop_iocg", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_iocg(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val - a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_iocg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_iocg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 533, "kernel_name": "loop_hxuy", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_hxuy(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val + a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_hxuy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.add %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_hxuy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.add %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 534, "kernel_name": "compound_tpma", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_tpma(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_tpma attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_tpma attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 535, "kernel_name": "vec_ksik", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_ksik(a, b):\n    # Vector dot product (row-wise for N x 3 arrays)\n    # a and b are expected to be shape (N, 3)\n    # contract over last axis\n    return jnp.sum(a * b, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_ksik attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_ksik attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n"}
{"id": 536, "kernel_name": "loop_teou", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_teou(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val - a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_teou attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_teou attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 537, "kernel_name": "reduce_ikmd", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_ikmd(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_ikmd attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_ikmd attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 538, "kernel_name": "nested_nhvz", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_nhvz(a):\n    # Nested branching\n    # if a > 0.15:\n    #   if a > 1.15: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 1.15, a * 3.0, a * 2.0)\n    return jnp.where(a > 0.15, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_nhvz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.150000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<1.500000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_nhvz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.150000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<1.500000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 539, "kernel_name": "elementwise_sehk", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_sehk(a, b):\n    # Elementwise +\n    return a + b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_sehk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_sehk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 540, "kernel_name": "branch_fzvv", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_fzvv(a):\n    # Branching with where\n    # if a > 0.88: a + 0.6 else: a - 2.5\n    return jnp.where(a > 0.88, a + 0.6, a - 2.5)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_fzvv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.879999995> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<6.000000e-01> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.500000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_fzvv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.879999995> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<6.000000e-01> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.500000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 541, "kernel_name": "nested_ncso", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_ncso(a):\n    # Nested branching\n    # if a > 0.11:\n    #   if a > 0.69: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 0.69, a * 3.0, a * 2.0)\n    return jnp.where(a > 0.11, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_ncso attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.689999997> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<1.100000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_ncso attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.689999997> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<1.100000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 542, "kernel_name": "compound_pgbh", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_pgbh(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_pgbh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_pgbh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 543, "kernel_name": "elementwise_bseq", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_bseq(a, b):\n    # Elementwise -\n    return a - b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_bseq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_bseq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 544, "kernel_name": "loop_tdzj", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_tdzj(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val * a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_tdzj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.multiply %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_tdzj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.multiply %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 545, "kernel_name": "reduce_xusz", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_xusz(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_xusz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_xusz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 546, "kernel_name": "vec_sxkf", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_sxkf(a):\n    # Vector length (row-wise for N x 3 arrays)\n    return jnp.linalg.norm(a, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_sxkf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_sxkf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 547, "kernel_name": "scalar_arr_dqhs", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_dqhs(alpha, x, y):\n    # Scalar-array operations\n    return alpha - x + y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_dqhs attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_dqhs attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 548, "kernel_name": "elementwise_abml", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_abml(a, b):\n    # Elementwise *\n    return a * b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_abml attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_abml attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 549, "kernel_name": "nested_onld", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_onld(a):\n    # Nested branching\n    # if a > -0.32:\n    #   if a > 0.69: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 0.69, a * 3.0, a * 2.0)\n    return jnp.where(a > -0.32, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_onld attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.689999997> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-3.200000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_onld attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.689999997> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-3.200000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 550, "kernel_name": "scalar_arr_byed", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_byed(alpha, x, y):\n    # Scalar-array operations\n    return alpha * x + y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_byed attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_byed attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 551, "kernel_name": "reduce_yjrc", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_yjrc(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_yjrc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_yjrc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 552, "kernel_name": "unary_drfb", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_drfb(a):\n    # Unary operation jnp.cos\n    return jnp.cos(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_drfb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.cosine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_drfb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.cosine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 553, "kernel_name": "reduce_zjqt", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_zjqt(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_zjqt attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_zjqt attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 554, "kernel_name": "vec_kyue", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_kyue(a, b):\n    # Vector dot product (row-wise for N x 3 arrays)\n    # a and b are expected to be shape (N, 3)\n    # contract over last axis\n    return jnp.sum(a * b, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_kyue attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_kyue attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n"}
{"id": 555, "kernel_name": "scalar_arr_celx", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_celx(alpha, x, y):\n    # Scalar-array operations\n    return alpha * x - y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_celx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.subtract %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_celx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.subtract %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 556, "kernel_name": "scalar_arr_wctn", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_wctn(alpha, x, y):\n    # Scalar-array operations\n    return alpha * x + y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_wctn attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_wctn attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 557, "kernel_name": "elementwise_sipw", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_sipw(a, b):\n    # Elementwise *\n    return a * b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_sipw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_sipw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 558, "kernel_name": "loop_yily", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_yily(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val + a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_yily attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.add %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_yily attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.add %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 559, "kernel_name": "branch_fotc", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_fotc(a):\n    # Branching with where\n    # if a > 0.46: a - 2.7 else: a + 2.6\n    return jnp.where(a > 0.46, a - 2.7, a + 2.6)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_fotc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<4.600000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.700000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.600000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_fotc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<4.600000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.700000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.600000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 560, "kernel_name": "vec_kitm", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_kitm(a):\n    # Vector length (row-wise for N x 3 arrays)\n    return jnp.linalg.norm(a, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_kitm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_kitm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 561, "kernel_name": "unary_eolp", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_eolp(a):\n    # Unary operation jnp.sqrt\n    return jnp.sqrt(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_eolp attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sqrt %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_eolp attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sqrt %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 562, "kernel_name": "scalar_arr_bxuj", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_bxuj(alpha, x, y):\n    # Scalar-array operations\n    return alpha * x * y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_bxuj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.multiply %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_bxuj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.multiply %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 563, "kernel_name": "unary_eolm", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_eolm(a):\n    # Unary operation jnp.sqrt\n    return jnp.sqrt(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_eolm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sqrt %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_eolm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sqrt %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 564, "kernel_name": "elementwise_tbrr", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_tbrr(a, b):\n    # Elementwise +\n    return a + b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_tbrr attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_tbrr attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 565, "kernel_name": "scalar_arr_szci", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_szci(alpha, x, y):\n    # Scalar-array operations\n    return alpha + x * y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_szci attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<128xf32>\n    %1 = stablehlo.convert %arg0 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %2, %0 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_szci attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<128xf32>\n    %1 = stablehlo.convert %arg0 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %2, %0 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 566, "kernel_name": "unary_dxsm", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_dxsm(a):\n    # Unary operation jnp.cos\n    return jnp.cos(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_dxsm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.cosine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_dxsm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.cosine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 567, "kernel_name": "compound_qieq", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_qieq(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_qieq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_qieq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 568, "kernel_name": "multi_lhzj", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_lhzj(a, b):\n    # Multi-statement\n    temp1 = a + b\n    temp2 = jnp.sqrt(temp1)\n    return temp2 - a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_lhzj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sqrt %0 : tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_lhzj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sqrt %0 : tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 569, "kernel_name": "nested_ozqy", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_ozqy(a):\n    # Nested branching\n    # if a > -0.16:\n    #   if a > 0.54: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 0.54, a * 3.0, a * 2.0)\n    return jnp.where(a > -0.16, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_ozqy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<5.400000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-1.600000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_ozqy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<5.400000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-1.600000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 570, "kernel_name": "branch_vfup", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_vfup(a):\n    # Branching with where\n    # if a > 0.69: a * 2.7 else: a + 1.4\n    return jnp.where(a > 0.69, a * 2.7, a + 1.4)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_vfup attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.689999997> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.700000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.400000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_vfup attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.689999997> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.700000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.400000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 571, "kernel_name": "loop_huul", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_huul(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val - a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_huul attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_huul attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 572, "kernel_name": "loop_gpfp", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_gpfp(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val * a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_gpfp attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.multiply %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_gpfp attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.multiply %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 573, "kernel_name": "branch_gbrd", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_gbrd(a):\n    # Branching with where\n    # if a > -0.42: a - 0.6 else: a - 1.9\n    return jnp.where(a > -0.42, a - 0.6, a - 1.9)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_gbrd attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-4.200000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<6.000000e-01> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.900000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_gbrd attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-4.200000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<6.000000e-01> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.900000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 574, "kernel_name": "elementwise_alve", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_alve(a, b):\n    # Elementwise *\n    return a * b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_alve attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_alve attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 575, "kernel_name": "multi_slgv", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_slgv(a, b):\n    # Multi-statement\n    temp1 = a * b\n    temp2 = jnp.sqrt(temp1)\n    return temp2 - a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_slgv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sqrt %0 : tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_slgv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sqrt %0 : tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 576, "kernel_name": "multi_lxli", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_lxli(a, b):\n    # Multi-statement\n    temp1 = a + b\n    temp2 = jnp.sqrt(temp1)\n    return temp2 * a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_lxli attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sqrt %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_lxli attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sqrt %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 577, "kernel_name": "nested_nadq", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_nadq(a):\n    # Nested branching\n    # if a > 0.43:\n    #   if a > 1.37: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 1.37, a * 3.0, a * 2.0)\n    return jnp.where(a > 0.43, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_nadq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.370000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<4.300000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_nadq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.370000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<4.300000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 578, "kernel_name": "branch_fnpu", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_fnpu(a):\n    # Branching with where\n    # if a > -0.83: a * 1.0 else: a - 3.0\n    return jnp.where(a > -0.83, a * 1.0, a - 3.0)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_fnpu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-0.829999983> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_fnpu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-0.829999983> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 579, "kernel_name": "reduce_iqjw", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_iqjw(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_iqjw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_iqjw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 580, "kernel_name": "loop_hzmi", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_hzmi(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val * a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_hzmi attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.multiply %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_hzmi attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.multiply %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 581, "kernel_name": "elementwise_bebj", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_bebj(a, b):\n    # Elementwise +\n    return a + b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_bebj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_bebj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 582, "kernel_name": "branch_eafo", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_eafo(a):\n    # Branching with where\n    # if a > -0.45: a - 2.0 else: a * 2.1\n    return jnp.where(a > -0.45, a - 2.0, a * 2.1)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_eafo attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-4.500000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.100000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_eafo attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-4.500000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.100000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 583, "kernel_name": "vec_zgpc", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_zgpc(a):\n    # Vector length (row-wise for N x 3 arrays)\n    return jnp.linalg.norm(a, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_zgpc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_zgpc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 584, "kernel_name": "reduce_jdad", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_jdad(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_jdad attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_jdad attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 585, "kernel_name": "loop_ytcf", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_ytcf(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val + a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_ytcf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.add %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_ytcf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.add %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 586, "kernel_name": "vec_wcoc", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_wcoc(a, b):\n    # Vector dot product (row-wise for N x 3 arrays)\n    # a and b are expected to be shape (N, 3)\n    # contract over last axis\n    return jnp.sum(a * b, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_wcoc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_wcoc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n"}
{"id": 587, "kernel_name": "elementwise_bmya", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_bmya(a, b):\n    # Elementwise -\n    return a - b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_bmya attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_bmya attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 588, "kernel_name": "multi_zenc", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_zenc(a, b):\n    # Multi-statement\n    temp1 = a - b\n    temp2 = jnp.sin(temp1)\n    return temp2 + a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_zenc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sine %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_zenc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sine %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 589, "kernel_name": "loop_uifp", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_uifp(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val * a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_uifp attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.multiply %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_uifp attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.multiply %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 590, "kernel_name": "reduce_jyaw", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_jyaw(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_jyaw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_jyaw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 591, "kernel_name": "elementwise_bzvi", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_bzvi(a, b):\n    # Elementwise +\n    return a + b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_bzvi attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_bzvi attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 592, "kernel_name": "unary_dqal", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_dqal(a):\n    # Unary operation jnp.abs\n    return jnp.abs(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_dqal attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.abs %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_dqal attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.abs %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 593, "kernel_name": "reduce_iuph", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_iuph(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_iuph attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_iuph attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 594, "kernel_name": "branch_fxar", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_fxar(a):\n    # Branching with where\n    # if a > 0.05: a - 2.4 else: a - 1.5\n    return jnp.where(a > 0.05, a - 2.4, a - 1.5)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_fxar attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<5.000000e-02> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.400000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.500000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_fxar attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<5.000000e-02> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.400000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.500000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 595, "kernel_name": "multi_mgdl", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_mgdl(a, b):\n    # Multi-statement\n    temp1 = a * b\n    temp2 = jnp.abs(temp1)\n    return temp2 + a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_mgdl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.abs %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_mgdl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.abs %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 596, "kernel_name": "branch_qmys", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_qmys(a):\n    # Branching with where\n    # if a > 0.94: a * 1.3 else: a + 0.6\n    return jnp.where(a > 0.94, a * 1.3, a + 0.6)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_qmys attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.939999997> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.300000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<6.000000e-01> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_qmys attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.939999997> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.300000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<6.000000e-01> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 597, "kernel_name": "compound_spkq", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_spkq(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_spkq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_spkq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 598, "kernel_name": "elementwise_albt", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_albt(a, b):\n    # Elementwise *\n    return a * b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_albt attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_albt attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 599, "kernel_name": "vec_jabu", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_jabu(a):\n    # Vector length (row-wise for N x 3 arrays)\n    return jnp.linalg.norm(a, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_jabu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_jabu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 600, "kernel_name": "loop_rgkw", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_rgkw(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val - a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_rgkw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_rgkw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 601, "kernel_name": "nested_oknn", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_oknn(a):\n    # Nested branching\n    # if a > 0.39:\n    #   if a > 0.79: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 0.79, a * 3.0, a * 2.0)\n    return jnp.where(a > 0.39, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_oknn attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<7.900000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<3.900000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_oknn attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<7.900000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<3.900000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 602, "kernel_name": "multi_mlcl", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_mlcl(a, b):\n    # Multi-statement\n    temp1 = a * b\n    temp2 = jnp.abs(temp1)\n    return temp2 * a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_mlcl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.abs %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_mlcl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.abs %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 603, "kernel_name": "branch_fduh", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_fduh(a):\n    # Branching with where\n    # if a > -0.04: a + 1.2 else: a - 2.8\n    return jnp.where(a > -0.04, a + 1.2, a - 2.8)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_fduh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-4.000000e-02> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.200000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.800000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_fduh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-4.000000e-02> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.200000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.800000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 604, "kernel_name": "compound_opin", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_opin(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_opin attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_opin attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 605, "kernel_name": "nested_xzjb", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_xzjb(a):\n    # Nested branching\n    # if a > -0.12:\n    #   if a > 1.26: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 1.26, a * 3.0, a * 2.0)\n    return jnp.where(a > -0.12, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_xzjb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.260000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-1.200000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_xzjb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.260000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-1.200000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 606, "kernel_name": "unary_qoou", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_qoou(a):\n    # Unary operation jnp.abs\n    return jnp.abs(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_qoou attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.abs %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_qoou attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.abs %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 607, "kernel_name": "reduce_jflp", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_jflp(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_jflp attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_jflp attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 608, "kernel_name": "reduce_irwl", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_irwl(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_irwl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_irwl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 609, "kernel_name": "elementwise_bosc", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_bosc(a, b):\n    # Elementwise +\n    return a + b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_bosc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_bosc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 610, "kernel_name": "unary_dpnw", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_dpnw(a):\n    # Unary operation jnp.sin\n    return jnp.sin(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_dpnw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_dpnw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 611, "kernel_name": "multi_tmwe", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_tmwe(a, b):\n    # Multi-statement\n    temp1 = a - b\n    temp2 = jnp.cos(temp1)\n    return temp2 - a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_tmwe attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.cosine %0 : tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_tmwe attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.cosine %0 : tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 612, "kernel_name": "compound_rjiq", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_rjiq(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_rjiq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_rjiq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 613, "kernel_name": "elementwise_veaz", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_veaz(a, b):\n    # Elementwise +\n    return a + b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_veaz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_veaz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 614, "kernel_name": "multi_rrob", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_rrob(a, b):\n    # Multi-statement\n    temp1 = a - b\n    temp2 = jnp.sqrt(temp1)\n    return temp2 + a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_rrob attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sqrt %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_rrob attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sqrt %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 615, "kernel_name": "compound_puiq", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_puiq(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_puiq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_puiq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 616, "kernel_name": "unary_dbnr", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_dbnr(a):\n    # Unary operation jnp.sqrt\n    return jnp.sqrt(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_dbnr attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sqrt %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_dbnr attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sqrt %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 617, "kernel_name": "loop_stft", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_stft(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val + a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_stft attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.add %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_stft attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.add %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 618, "kernel_name": "elementwise_aqjz", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_aqjz(a, b):\n    # Elementwise +\n    return a + b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_aqjz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_aqjz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 619, "kernel_name": "unary_ywic", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_ywic(a):\n    # Unary operation jnp.abs\n    return jnp.abs(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_ywic attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.abs %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_ywic attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.abs %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 620, "kernel_name": "scalar_arr_qahu", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_qahu(alpha, x, y):\n    # Scalar-array operations\n    return alpha - x * y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_qahu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<128xf32>\n    %1 = stablehlo.convert %arg0 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %2, %0 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_qahu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<128xf32>\n    %1 = stablehlo.convert %arg0 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %2, %0 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 621, "kernel_name": "compound_xplu", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_xplu(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_xplu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_xplu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 622, "kernel_name": "nested_uxnm", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_uxnm(a):\n    # Nested branching\n    # if a > 0.13:\n    #   if a > 1.08: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 1.08, a * 3.0, a * 2.0)\n    return jnp.where(a > 0.13, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_uxnm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.080000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<1.300000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_uxnm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.080000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<1.300000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 623, "kernel_name": "nested_nflf", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_nflf(a):\n    # Nested branching\n    # if a > -0.24:\n    #   if a > 0.58: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 0.58, a * 3.0, a * 2.0)\n    return jnp.where(a > -0.24, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_nflf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<5.800000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-2.400000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_nflf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<5.800000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-2.400000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 624, "kernel_name": "multi_lxln", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_lxln(a, b):\n    # Multi-statement\n    temp1 = a * b\n    temp2 = jnp.sin(temp1)\n    return temp2 * a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_lxln attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sine %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_lxln attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sine %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 625, "kernel_name": "scalar_arr_clki", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_clki(alpha, x, y):\n    # Scalar-array operations\n    return alpha + x + y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_clki attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.add %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_clki attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.add %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 626, "kernel_name": "multi_maez", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_maez(a, b):\n    # Multi-statement\n    temp1 = a * b\n    temp2 = jnp.abs(temp1)\n    return temp2 + a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_maez attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.abs %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_maez attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.abs %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 627, "kernel_name": "loop_hnqh", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_hnqh(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val - a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_hnqh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_hnqh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 628, "kernel_name": "compound_otkj", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_otkj(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_otkj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_otkj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 629, "kernel_name": "compound_udjl", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_udjl(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_udjl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_udjl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 630, "kernel_name": "elementwise_afeo", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_afeo(a, b):\n    # Elementwise *\n    return a * b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_afeo attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_afeo attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 631, "kernel_name": "nested_nleu", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_nleu(a):\n    # Nested branching\n    # if a > -0.15:\n    #   if a > 1.19: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 1.19, a * 3.0, a * 2.0)\n    return jnp.where(a > -0.15, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_nleu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.190000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-1.500000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_nleu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.190000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-1.500000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 632, "kernel_name": "vec_kczd", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_kczd(a, b):\n    # Vector dot product (row-wise for N x 3 arrays)\n    # a and b are expected to be shape (N, 3)\n    # contract over last axis\n    return jnp.sum(a * b, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_kczd attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_kczd attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n"}
{"id": 633, "kernel_name": "branch_fith", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_fith(a):\n    # Branching with where\n    # if a > 0.26: a - 1.7 else: a - 1.4\n    return jnp.where(a > 0.26, a - 1.7, a - 1.4)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_fith attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<2.600000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.700000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.400000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_fith attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<2.600000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.700000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.400000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 634, "kernel_name": "elementwise_avqt", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_avqt(a, b):\n    # Elementwise *\n    return a * b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_avqt attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_avqt attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 635, "kernel_name": "scalar_arr_qqrm", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_qqrm(alpha, x, y):\n    # Scalar-array operations\n    return alpha - x + y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_qqrm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_qqrm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 636, "kernel_name": "scalar_arr_ciia", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_ciia(alpha, x, y):\n    # Scalar-array operations\n    return alpha - x * y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_ciia attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<128xf32>\n    %1 = stablehlo.convert %arg0 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %2, %0 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_ciia attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<128xf32>\n    %1 = stablehlo.convert %arg0 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %2, %0 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 637, "kernel_name": "compound_xpyv", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_xpyv(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_xpyv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_xpyv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 638, "kernel_name": "compound_pwdu", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_pwdu(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_pwdu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_pwdu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 639, "kernel_name": "vec_kixu", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_kixu(a, b):\n    # Vector dot product (row-wise for N x 3 arrays)\n    # a and b are expected to be shape (N, 3)\n    # contract over last axis\n    return jnp.sum(a * b, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_kixu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_kixu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n"}
{"id": 640, "kernel_name": "nested_nfbr", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_nfbr(a):\n    # Nested branching\n    # if a > 0.27:\n    #   if a > 1.35: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 1.35, a * 3.0, a * 2.0)\n    return jnp.where(a > 0.27, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_nfbr attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.350000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<2.700000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_nfbr attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.350000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<2.700000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 641, "kernel_name": "vec_kwaf", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_kwaf(a, b):\n    # Vector dot product (row-wise for N x 3 arrays)\n    # a and b are expected to be shape (N, 3)\n    # contract over last axis\n    return jnp.sum(a * b, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_kwaf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_kwaf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n"}
{"id": 642, "kernel_name": "vec_kpyo", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_kpyo(a):\n    # Vector length (row-wise for N x 3 arrays)\n    return jnp.linalg.norm(a, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_kpyo attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_kpyo attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 643, "kernel_name": "loop_hygh", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_hygh(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val + a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_hygh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.add %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_hygh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.add %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 644, "kernel_name": "nested_znhh", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_znhh(a):\n    # Nested branching\n    # if a > 0.34:\n    #   if a > 0.88: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 0.88, a * 3.0, a * 2.0)\n    return jnp.where(a > 0.34, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_znhh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.879999995> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<3.400000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_znhh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.879999995> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<3.400000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 645, "kernel_name": "vec_kdpx", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_kdpx(a):\n    # Vector length (row-wise for N x 3 arrays)\n    return jnp.linalg.norm(a, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_kdpx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_kdpx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 646, "kernel_name": "loop_uygz", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_uygz(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val + a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_uygz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.add %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_uygz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.add %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 647, "kernel_name": "compound_pwam", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_pwam(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_pwam attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_pwam attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 648, "kernel_name": "reduce_imes", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_imes(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_imes attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_imes attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 649, "kernel_name": "unary_rdlh", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_rdlh(a):\n    # Unary operation jnp.sin\n    return jnp.sin(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_rdlh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_rdlh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 650, "kernel_name": "elementwise_wqyo", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_wqyo(a, b):\n    # Elementwise *\n    return a * b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_wqyo attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_wqyo attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 651, "kernel_name": "multi_mpxe", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_mpxe(a, b):\n    # Multi-statement\n    temp1 = a - b\n    temp2 = jnp.abs(temp1)\n    return temp2 + a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_mpxe attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.abs %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_mpxe attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.abs %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 652, "kernel_name": "multi_swuc", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_swuc(a, b):\n    # Multi-statement\n    temp1 = a - b\n    temp2 = jnp.sin(temp1)\n    return temp2 - a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_swuc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sine %0 : tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_swuc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sine %0 : tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 653, "kernel_name": "reduce_isnj", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_isnj(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_isnj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_isnj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 654, "kernel_name": "unary_eglq", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_eglq(a):\n    # Unary operation jnp.abs\n    return jnp.abs(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_eglq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.abs %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_eglq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.abs %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 655, "kernel_name": "unary_dbuv", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_dbuv(a):\n    # Unary operation jnp.sqrt\n    return jnp.sqrt(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_dbuv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sqrt %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_dbuv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sqrt %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 656, "kernel_name": "reduce_wyzx", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_wyzx(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_wyzx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_wyzx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 657, "kernel_name": "reduce_jxqo", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_jxqo(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_jxqo attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_jxqo attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 658, "kernel_name": "unary_eesv", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_eesv(a):\n    # Unary operation jnp.sin\n    return jnp.sin(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_eesv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_eesv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 659, "kernel_name": "loop_tmco", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_tmco(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val + a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_tmco attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.add %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_tmco attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.add %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 660, "kernel_name": "scalar_arr_tudx", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_tudx(alpha, x, y):\n    # Scalar-array operations\n    return alpha - x + y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_tudx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_tudx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 661, "kernel_name": "elementwise_bezg", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_bezg(a, b):\n    # Elementwise +\n    return a + b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_bezg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_bezg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 662, "kernel_name": "compound_vjth", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_vjth(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_vjth attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_vjth attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 663, "kernel_name": "loop_uail", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_uail(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val - a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_uail attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_uail attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 664, "kernel_name": "compound_qwgr", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_qwgr(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_qwgr attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_qwgr attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 665, "kernel_name": "elementwise_aqit", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_aqit(a, b):\n    # Elementwise -\n    return a - b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_aqit attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_aqit attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 666, "kernel_name": "scalar_arr_dvvq", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_dvvq(alpha, x, y):\n    # Scalar-array operations\n    return alpha - x + y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_dvvq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_dvvq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 667, "kernel_name": "nested_ngge", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_ngge(a):\n    # Nested branching\n    # if a > -0.0:\n    #   if a > 1.29: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 1.29, a * 3.0, a * 2.0)\n    return jnp.where(a > -0.0, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_ngge attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.290000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-0.000000e+00> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_ngge attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.290000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-0.000000e+00> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 668, "kernel_name": "vec_jorl", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_jorl(a, b):\n    # Vector dot product (row-wise for N x 3 arrays)\n    # a and b are expected to be shape (N, 3)\n    # contract over last axis\n    return jnp.sum(a * b, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_jorl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_jorl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n"}
{"id": 669, "kernel_name": "compound_pklf", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_pklf(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_pklf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_pklf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 670, "kernel_name": "loop_hmcl", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_hmcl(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val + a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_hmcl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.add %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_hmcl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.add %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 671, "kernel_name": "vec_khmk", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_khmk(a, b):\n    # Vector dot product (row-wise for N x 3 arrays)\n    # a and b are expected to be shape (N, 3)\n    # contract over last axis\n    return jnp.sum(a * b, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_khmk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_khmk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n"}
{"id": 672, "kernel_name": "scalar_arr_ubih", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_ubih(alpha, x, y):\n    # Scalar-array operations\n    return alpha * x + y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_ubih attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_ubih attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 673, "kernel_name": "nested_nduc", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_nduc(a):\n    # Nested branching\n    # if a > 0.11:\n    #   if a > 0.85: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 0.85, a * 3.0, a * 2.0)\n    return jnp.where(a > 0.11, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_nduc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<8.500000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<1.100000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_nduc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<8.500000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<1.100000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 674, "kernel_name": "unary_etgw", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_etgw(a):\n    # Unary operation jnp.cos\n    return jnp.cos(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_etgw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.cosine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_etgw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.cosine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 675, "kernel_name": "branch_gduf", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_gduf(a):\n    # Branching with where\n    # if a > -0.02: a + 0.5 else: a + 3.0\n    return jnp.where(a > -0.02, a + 0.5, a + 3.0)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_gduf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-2.000000e-02> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_gduf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-2.000000e-02> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 676, "kernel_name": "loop_gqwl", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_gqwl(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val - a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_gqwl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_gqwl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 677, "kernel_name": "loop_hsrn", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_hsrn(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val * a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_hsrn attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.multiply %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_hsrn attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.multiply %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 678, "kernel_name": "unary_tetw", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_tetw(a):\n    # Unary operation jnp.cos\n    return jnp.cos(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_tetw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.cosine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_tetw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.cosine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 679, "kernel_name": "unary_dkti", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_dkti(a):\n    # Unary operation jnp.cos\n    return jnp.cos(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_dkti attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.cosine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_dkti attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.cosine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 680, "kernel_name": "compound_pdhn", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_pdhn(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_pdhn attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_pdhn attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 681, "kernel_name": "nested_offg", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_offg(a):\n    # Nested branching\n    # if a > 0.26:\n    #   if a > 1.08: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 1.08, a * 3.0, a * 2.0)\n    return jnp.where(a > 0.26, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_offg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.080000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<2.600000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_offg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.080000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<2.600000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 682, "kernel_name": "loop_whbc", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_whbc(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val - a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_whbc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_whbc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 683, "kernel_name": "scalar_arr_cbow", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_cbow(alpha, x, y):\n    # Scalar-array operations\n    return alpha * x + y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_cbow attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_cbow attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 684, "kernel_name": "loop_gjvb", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_gjvb(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val - a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_gjvb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_gjvb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 685, "kernel_name": "nested_wyns", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_wyns(a):\n    # Nested branching\n    # if a > 0.4:\n    #   if a > 0.66: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 0.66, a * 3.0, a * 2.0)\n    return jnp.where(a > 0.4, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_wyns attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<6.600000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<4.000000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_wyns attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<6.600000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<4.000000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 686, "kernel_name": "elementwise_xlqy", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_xlqy(a, b):\n    # Elementwise *\n    return a * b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_xlqy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_xlqy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 687, "kernel_name": "reduce_sihr", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_sihr(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_sihr attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_sihr attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 688, "kernel_name": "loop_hqzn", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_hqzn(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val - a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_hqzn attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_hqzn attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 689, "kernel_name": "scalar_arr_ssxy", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_ssxy(alpha, x, y):\n    # Scalar-array operations\n    return alpha + x - y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_ssxy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.add %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.subtract %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_ssxy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.add %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.subtract %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 690, "kernel_name": "scalar_arr_rwqq", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_rwqq(alpha, x, y):\n    # Scalar-array operations\n    return alpha + x + y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_rwqq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.add %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_rwqq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.add %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 691, "kernel_name": "vec_kqdo", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_kqdo(a):\n    # Vector length (row-wise for N x 3 arrays)\n    return jnp.linalg.norm(a, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_kqdo attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_kqdo attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 692, "kernel_name": "reduce_ruyr", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_ruyr(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_ruyr attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_ruyr attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 693, "kernel_name": "vec_ldoz", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_ldoz(a, b):\n    # Vector dot product (row-wise for N x 3 arrays)\n    # a and b are expected to be shape (N, 3)\n    # contract over last axis\n    return jnp.sum(a * b, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_ldoz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_ldoz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n"}
{"id": 694, "kernel_name": "loop_gbxc", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_gbxc(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val * a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_gbxc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.multiply %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_gbxc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.multiply %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 695, "kernel_name": "elementwise_aeee", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_aeee(a, b):\n    # Elementwise -\n    return a - b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_aeee attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_aeee attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 696, "kernel_name": "unary_wctd", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_wctd(a):\n    # Unary operation jnp.sin\n    return jnp.sin(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_wctd attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_wctd attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 697, "kernel_name": "branch_frme", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_frme(a):\n    # Branching with where\n    # if a > -0.73: a + 1.5 else: a - 1.0\n    return jnp.where(a > -0.73, a + 1.5, a - 1.0)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_frme attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-7.300000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.500000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_frme attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-7.300000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.500000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 698, "kernel_name": "compound_obvg", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_obvg(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_obvg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_obvg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 699, "kernel_name": "branch_fnyy", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_fnyy(a):\n    # Branching with where\n    # if a > -0.08: a + 1.0 else: a + 1.2\n    return jnp.where(a > -0.08, a + 1.0, a + 1.2)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_fnyy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-8.000000e-02> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.200000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_fnyy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-8.000000e-02> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.200000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 700, "kernel_name": "vec_kiyd", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_kiyd(a, b):\n    # Vector dot product (row-wise for N x 3 arrays)\n    # a and b are expected to be shape (N, 3)\n    # contract over last axis\n    return jnp.sum(a * b, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_kiyd attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_kiyd attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n"}
{"id": 701, "kernel_name": "loop_shza", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_shza(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val - a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_shza attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_shza attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 702, "kernel_name": "loop_inic", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_inic(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val + a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_inic attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.add %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_inic attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.add %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 703, "kernel_name": "branch_frjx", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_frjx(a):\n    # Branching with where\n    # if a > -0.94: a * 1.5 else: a - 1.6\n    return jnp.where(a > -0.94, a * 1.5, a - 1.6)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_frjx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-0.939999997> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.500000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.600000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_frjx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-0.939999997> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.500000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.600000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 704, "kernel_name": "vec_leuo", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_leuo(a, b):\n    # Vector dot product (row-wise for N x 3 arrays)\n    # a and b are expected to be shape (N, 3)\n    # contract over last axis\n    return jnp.sum(a * b, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_leuo attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_leuo attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n"}
{"id": 705, "kernel_name": "loop_hquh", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_hquh(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val * a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_hquh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.multiply %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_hquh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.multiply %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 706, "kernel_name": "scalar_arr_byhk", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_byhk(alpha, x, y):\n    # Scalar-array operations\n    return alpha - x - y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_byhk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.subtract %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_byhk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.subtract %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 707, "kernel_name": "multi_trlq", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_trlq(a, b):\n    # Multi-statement\n    temp1 = a - b\n    temp2 = jnp.sqrt(temp1)\n    return temp2 - a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_trlq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sqrt %0 : tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_trlq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sqrt %0 : tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 708, "kernel_name": "reduce_idgp", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_idgp(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_idgp attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_idgp attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 709, "kernel_name": "nested_tnzi", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_tnzi(a):\n    # Nested branching\n    # if a > -0.07:\n    #   if a > 1.27: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 1.27, a * 3.0, a * 2.0)\n    return jnp.where(a > -0.07, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_tnzi attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.270000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-7.000000e-02> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_tnzi attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.270000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-7.000000e-02> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 710, "kernel_name": "nested_nszi", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_nszi(a):\n    # Nested branching\n    # if a > -0.41:\n    #   if a > 1.22: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 1.22, a * 3.0, a * 2.0)\n    return jnp.where(a > -0.41, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_nszi attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.220000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-4.100000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_nszi attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.220000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-4.100000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 711, "kernel_name": "reduce_tbwr", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_tbwr(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_tbwr attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_tbwr attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 712, "kernel_name": "unary_emdw", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_emdw(a):\n    # Unary operation jnp.sqrt\n    return jnp.sqrt(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_emdw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sqrt %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_emdw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sqrt %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 713, "kernel_name": "nested_wkhh", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_wkhh(a):\n    # Nested branching\n    # if a > -0.13:\n    #   if a > 1.29: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 1.29, a * 3.0, a * 2.0)\n    return jnp.where(a > -0.13, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_wkhh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.290000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-1.300000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_wkhh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.290000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-1.300000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 714, "kernel_name": "multi_lhwj", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_lhwj(a, b):\n    # Multi-statement\n    temp1 = a + b\n    temp2 = jnp.abs(temp1)\n    return temp2 + a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_lhwj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.abs %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_lhwj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.abs %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 715, "kernel_name": "vec_keev", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_keev(a):\n    # Vector length (row-wise for N x 3 arrays)\n    return jnp.linalg.norm(a, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_keev attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_keev attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 716, "kernel_name": "reduce_ivnr", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_ivnr(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_ivnr attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_ivnr attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 717, "kernel_name": "elementwise_bnxi", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_bnxi(a, b):\n    # Elementwise +\n    return a + b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_bnxi attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_bnxi attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 718, "kernel_name": "unary_dwca", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_dwca(a):\n    # Unary operation jnp.sin\n    return jnp.sin(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_dwca attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_dwca attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 719, "kernel_name": "branch_fufg", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_fufg(a):\n    # Branching with where\n    # if a > 0.7: a + 1.3 else: a * 2.9\n    return jnp.where(a > 0.7, a + 1.3, a * 2.9)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_fufg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.699999988> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.300000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.900000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_fufg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.699999988> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.300000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.900000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 720, "kernel_name": "reduce_yjeg", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_yjeg(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_yjeg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_yjeg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 721, "kernel_name": "compound_qzdb", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_qzdb(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_qzdb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_qzdb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 722, "kernel_name": "unary_zrbx", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_zrbx(a):\n    # Unary operation jnp.cos\n    return jnp.cos(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_zrbx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.cosine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_zrbx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.cosine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 723, "kernel_name": "elementwise_aghx", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_aghx(a, b):\n    # Elementwise +\n    return a + b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_aghx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_aghx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 724, "kernel_name": "loop_huji", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_huji(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val * a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_huji attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.multiply %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_huji attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.multiply %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 725, "kernel_name": "scalar_arr_vscm", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_vscm(alpha, x, y):\n    # Scalar-array operations\n    return alpha * x - y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_vscm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.subtract %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_vscm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.subtract %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 726, "kernel_name": "scalar_arr_clxy", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_clxy(alpha, x, y):\n    # Scalar-array operations\n    return alpha + x * y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_clxy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<128xf32>\n    %1 = stablehlo.convert %arg0 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %2, %0 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_clxy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<128xf32>\n    %1 = stablehlo.convert %arg0 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %2, %0 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 727, "kernel_name": "elementwise_apco", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_apco(a, b):\n    # Elementwise +\n    return a + b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_apco attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_apco attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 728, "kernel_name": "vec_jxlx", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_jxlx(a, b):\n    # Vector dot product (row-wise for N x 3 arrays)\n    # a and b are expected to be shape (N, 3)\n    # contract over last axis\n    return jnp.sum(a * b, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_jxlx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_jxlx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n"}
{"id": 729, "kernel_name": "vec_zdmb", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_zdmb(a):\n    # Vector length (row-wise for N x 3 arrays)\n    return jnp.linalg.norm(a, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_zdmb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_zdmb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 730, "kernel_name": "scalar_arr_cvor", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_cvor(alpha, x, y):\n    # Scalar-array operations\n    return alpha + x + y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_cvor attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.add %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_cvor attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.add %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 731, "kernel_name": "unary_erso", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_erso(a):\n    # Unary operation jnp.cos\n    return jnp.cos(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_erso attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.cosine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_erso attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.cosine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 732, "kernel_name": "loop_hdoy", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_hdoy(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val * a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_hdoy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.multiply %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_hdoy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.multiply %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 733, "kernel_name": "multi_mael", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_mael(a, b):\n    # Multi-statement\n    temp1 = a - b\n    temp2 = jnp.sqrt(temp1)\n    return temp2 + a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_mael attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sqrt %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_mael attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sqrt %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 734, "kernel_name": "multi_lpsc", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_lpsc(a, b):\n    # Multi-statement\n    temp1 = a + b\n    temp2 = jnp.cos(temp1)\n    return temp2 * a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_lpsc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.cosine %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_lpsc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.cosine %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 735, "kernel_name": "branch_fljh", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_fljh(a):\n    # Branching with where\n    # if a > -0.33: a + 2.8 else: a - 2.1\n    return jnp.where(a > -0.33, a + 2.8, a - 2.1)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_fljh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-3.300000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.800000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.100000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_fljh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-3.300000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.800000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.100000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 736, "kernel_name": "elementwise_bdxv", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_bdxv(a, b):\n    # Elementwise +\n    return a + b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_bdxv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_bdxv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 737, "kernel_name": "compound_ouew", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_ouew(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_ouew attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_ouew attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 738, "kernel_name": "branch_fuvz", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_fuvz(a):\n    # Branching with where\n    # if a > -0.63: a + 1.5 else: a - 2.0\n    return jnp.where(a > -0.63, a + 1.5, a - 2.0)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_fuvz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-6.300000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.500000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_fuvz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-6.300000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.500000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 739, "kernel_name": "branch_fkxm", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_fkxm(a):\n    # Branching with where\n    # if a > -0.47: a * 2.0 else: a * 2.7\n    return jnp.where(a > -0.47, a * 2.0, a * 2.7)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_fkxm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-4.700000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.700000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_fkxm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-4.700000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.700000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 740, "kernel_name": "scalar_arr_rgnl", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_rgnl(alpha, x, y):\n    # Scalar-array operations\n    return alpha - x - y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_rgnl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.subtract %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_rgnl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.subtract %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 741, "kernel_name": "nested_owfk", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_owfk(a):\n    # Nested branching\n    # if a > 0.24:\n    #   if a > 0.61: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 0.61, a * 3.0, a * 2.0)\n    return jnp.where(a > 0.24, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_owfk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<6.100000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<2.400000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_owfk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<6.100000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<2.400000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 742, "kernel_name": "vec_lzag", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_lzag(a):\n    # Vector length (row-wise for N x 3 arrays)\n    return jnp.linalg.norm(a, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_lzag attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_lzag attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 743, "kernel_name": "nested_nxky", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_nxky(a):\n    # Nested branching\n    # if a > 0.47:\n    #   if a > 0.53: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 0.53, a * 3.0, a * 2.0)\n    return jnp.where(a > 0.47, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_nxky attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<5.300000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<4.700000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_nxky attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<5.300000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<4.700000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 744, "kernel_name": "reduce_ubhf", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_ubhf(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_ubhf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_ubhf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 745, "kernel_name": "loop_hqdp", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_hqdp(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val - a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_hqdp attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_hqdp attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 746, "kernel_name": "compound_pyaj", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_pyaj(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_pyaj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_pyaj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 747, "kernel_name": "multi_mqmk", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_mqmk(a, b):\n    # Multi-statement\n    temp1 = a - b\n    temp2 = jnp.sin(temp1)\n    return temp2 * a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_mqmk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sine %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_mqmk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sine %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 748, "kernel_name": "loop_qwht", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_qwht(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val - a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_qwht attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_qwht attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 749, "kernel_name": "branch_rpfe", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_rpfe(a):\n    # Branching with where\n    # if a > -0.18: a * 1.3 else: a + 1.1\n    return jnp.where(a > -0.18, a * 1.3, a + 1.1)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_rpfe attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-1.800000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.300000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.100000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_rpfe attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-1.800000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.300000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.100000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 750, "kernel_name": "compound_ontl", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_ontl(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_ontl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_ontl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 751, "kernel_name": "compound_utca", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_utca(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_utca attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_utca attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 752, "kernel_name": "loop_hnhx", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_hnhx(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val - a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_hnhx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_hnhx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 753, "kernel_name": "vec_kleh", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_kleh(a, b):\n    # Vector dot product (row-wise for N x 3 arrays)\n    # a and b are expected to be shape (N, 3)\n    # contract over last axis\n    return jnp.sum(a * b, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_kleh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_kleh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n"}
{"id": 754, "kernel_name": "nested_omql", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_omql(a):\n    # Nested branching\n    # if a > 0.34:\n    #   if a > 0.94: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 0.94, a * 3.0, a * 2.0)\n    return jnp.where(a > 0.34, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_omql attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.939999997> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<3.400000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_omql attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.939999997> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<3.400000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 755, "kernel_name": "compound_xydm", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_xydm(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_xydm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_xydm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 756, "kernel_name": "vec_klkx", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_klkx(a):\n    # Vector length (row-wise for N x 3 arrays)\n    return jnp.linalg.norm(a, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_klkx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_klkx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 757, "kernel_name": "branch_wwja", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_wwja(a):\n    # Branching with where\n    # if a > -0.64: a + 0.5 else: a + 2.8\n    return jnp.where(a > -0.64, a + 0.5, a + 2.8)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_wwja attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-6.400000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.800000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_wwja attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-6.400000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.800000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 758, "kernel_name": "multi_rybl", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_rybl(a, b):\n    # Multi-statement\n    temp1 = a * b\n    temp2 = jnp.sqrt(temp1)\n    return temp2 - a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_rybl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sqrt %0 : tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_rybl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sqrt %0 : tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 759, "kernel_name": "scalar_arr_cjum", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_cjum(alpha, x, y):\n    # Scalar-array operations\n    return alpha - x * y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_cjum attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<128xf32>\n    %1 = stablehlo.convert %arg0 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %2, %0 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_cjum attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<128xf32>\n    %1 = stablehlo.convert %arg0 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %2, %0 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 760, "kernel_name": "nested_ooii", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_ooii(a):\n    # Nested branching\n    # if a > -0.32:\n    #   if a > 1.4: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 1.4, a * 3.0, a * 2.0)\n    return jnp.where(a > -0.32, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_ooii attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.400000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-3.200000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_ooii attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.400000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-3.200000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 761, "kernel_name": "scalar_arr_cmkg", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_cmkg(alpha, x, y):\n    # Scalar-array operations\n    return alpha + x + y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_cmkg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.add %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_cmkg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.add %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 762, "kernel_name": "multi_mlyv", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_mlyv(a, b):\n    # Multi-statement\n    temp1 = a + b\n    temp2 = jnp.abs(temp1)\n    return temp2 + a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_mlyv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.abs %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_mlyv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.abs %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 763, "kernel_name": "multi_mwbh", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_mwbh(a, b):\n    # Multi-statement\n    temp1 = a - b\n    temp2 = jnp.sin(temp1)\n    return temp2 - a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_mwbh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sine %0 : tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_mwbh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sine %0 : tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 764, "kernel_name": "multi_yahi", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_yahi(a, b):\n    # Multi-statement\n    temp1 = a - b\n    temp2 = jnp.cos(temp1)\n    return temp2 + a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_yahi attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.cosine %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_yahi attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.cosine %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 765, "kernel_name": "reduce_jceu", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_jceu(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_jceu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_jceu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 766, "kernel_name": "reduce_jqsl", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_jqsl(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_jqsl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_jqsl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 767, "kernel_name": "elementwise_bjuv", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_bjuv(a, b):\n    # Elementwise -\n    return a - b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_bjuv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_bjuv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 768, "kernel_name": "compound_sowe", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_sowe(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_sowe attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_sowe attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 769, "kernel_name": "scalar_arr_cznl", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_cznl(alpha, x, y):\n    # Scalar-array operations\n    return alpha + x + y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_cznl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.add %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_cznl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.add %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 770, "kernel_name": "elementwise_apsj", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_apsj(a, b):\n    # Elementwise *\n    return a * b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_apsj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_apsj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 771, "kernel_name": "unary_xapp", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_xapp(a):\n    # Unary operation jnp.cos\n    return jnp.cos(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_xapp attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.cosine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_xapp attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.cosine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 772, "kernel_name": "compound_pvga", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_pvga(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_pvga attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_pvga attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 773, "kernel_name": "elementwise_xhpi", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_xhpi(a, b):\n    # Elementwise +\n    return a + b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_xhpi attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_xhpi attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 774, "kernel_name": "unary_eckc", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_eckc(a):\n    # Unary operation jnp.sin\n    return jnp.sin(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_eckc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_eckc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 775, "kernel_name": "unary_divg", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_divg(a):\n    # Unary operation jnp.sin\n    return jnp.sin(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_divg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_divg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 776, "kernel_name": "multi_minv", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_minv(a, b):\n    # Multi-statement\n    temp1 = a + b\n    temp2 = jnp.cos(temp1)\n    return temp2 * a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_minv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.cosine %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_minv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.cosine %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 777, "kernel_name": "reduce_jehd", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_jehd(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_jehd attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_jehd attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 778, "kernel_name": "branch_fxmz", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_fxmz(a):\n    # Branching with where\n    # if a > -0.62: a + 2.5 else: a - 0.7\n    return jnp.where(a > -0.62, a + 2.5, a - 0.7)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_fxmz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-6.200000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.500000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<0.699999988> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_fxmz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-6.200000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.500000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<0.699999988> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 779, "kernel_name": "compound_ozhj", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_ozhj(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_ozhj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_ozhj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 780, "kernel_name": "scalar_arr_xzke", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_xzke(alpha, x, y):\n    # Scalar-array operations\n    return alpha - x - y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_xzke attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.subtract %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_xzke attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.subtract %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 781, "kernel_name": "reduce_iwec", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_iwec(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_iwec attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_iwec attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 782, "kernel_name": "unary_qtdj", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_qtdj(a):\n    # Unary operation jnp.abs\n    return jnp.abs(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_qtdj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.abs %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_qtdj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.abs %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 783, "kernel_name": "loop_hzhu", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_hzhu(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val + a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_hzhu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.add %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_hzhu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.add %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 784, "kernel_name": "nested_oqmx", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_oqmx(a):\n    # Nested branching\n    # if a > -0.31:\n    #   if a > 0.52: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 0.52, a * 3.0, a * 2.0)\n    return jnp.where(a > -0.31, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_oqmx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<5.200000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-3.100000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_oqmx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<5.200000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-3.100000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 785, "kernel_name": "reduce_jhpn", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_jhpn(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_jhpn attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_jhpn attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 786, "kernel_name": "branch_fsgz", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_fsgz(a):\n    # Branching with where\n    # if a > 0.41: a * 1.4 else: a * 1.6\n    return jnp.where(a > 0.41, a * 1.4, a * 1.6)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_fsgz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<4.100000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.400000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.600000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_fsgz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<4.100000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.400000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.600000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 787, "kernel_name": "branch_fmey", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_fmey(a):\n    # Branching with where\n    # if a > -0.96: a * 2.5 else: a * 2.5\n    return jnp.where(a > -0.96, a * 2.5, a * 2.5)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_fmey attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-0.959999978> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.500000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.500000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_fmey attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-0.959999978> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.500000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.500000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 788, "kernel_name": "scalar_arr_cthv", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_cthv(alpha, x, y):\n    # Scalar-array operations\n    return alpha - x - y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_cthv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.subtract %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_cthv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.subtract %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 789, "kernel_name": "unary_exqa", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_exqa(a):\n    # Unary operation jnp.abs\n    return jnp.abs(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_exqa attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.abs %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_exqa attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.abs %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 790, "kernel_name": "nested_tonx", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_tonx(a):\n    # Nested branching\n    # if a > -0.12:\n    #   if a > 0.88: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 0.88, a * 3.0, a * 2.0)\n    return jnp.where(a > -0.12, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_tonx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.879999995> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-1.200000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_tonx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.879999995> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-1.200000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 791, "kernel_name": "compound_ohpc", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_ohpc(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_ohpc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_ohpc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 792, "kernel_name": "loop_hfoz", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_hfoz(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val + a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_hfoz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.add %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_hfoz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.add %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 793, "kernel_name": "compound_qyts", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_qyts(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_qyts attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_qyts attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 794, "kernel_name": "vec_koxs", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_koxs(a, b):\n    # Vector dot product (row-wise for N x 3 arrays)\n    # a and b are expected to be shape (N, 3)\n    # contract over last axis\n    return jnp.sum(a * b, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_koxs attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_koxs attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n"}
{"id": 795, "kernel_name": "nested_olug", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_olug(a):\n    # Nested branching\n    # if a > 0.33:\n    #   if a > 0.75: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 0.75, a * 3.0, a * 2.0)\n    return jnp.where(a > 0.33, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_olug attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<7.500000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<3.300000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_olug attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<7.500000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<3.300000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 796, "kernel_name": "nested_nisw", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_nisw(a):\n    # Nested branching\n    # if a > 0.48:\n    #   if a > 0.58: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 0.58, a * 3.0, a * 2.0)\n    return jnp.where(a > 0.48, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_nisw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<5.800000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<4.800000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_nisw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<5.800000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<4.800000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 797, "kernel_name": "compound_qchf", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_qchf(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_qchf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_qchf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 798, "kernel_name": "branch_fkol", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_fkol(a):\n    # Branching with where\n    # if a > -0.3: a * 2.8 else: a + 1.8\n    return jnp.where(a > -0.3, a * 2.8, a + 1.8)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_fkol attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-3.000000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.800000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.800000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_fkol attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-3.000000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.800000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.800000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 799, "kernel_name": "compound_pspf", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_pspf(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_pspf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_pspf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 800, "kernel_name": "scalar_arr_wmoi", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_wmoi(alpha, x, y):\n    # Scalar-array operations\n    return alpha * x + y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_wmoi attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_wmoi attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 801, "kernel_name": "nested_rnss", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_rnss(a):\n    # Nested branching\n    # if a > 0.38:\n    #   if a > 0.91: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 0.91, a * 3.0, a * 2.0)\n    return jnp.where(a > 0.38, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_rnss attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<9.100000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<3.800000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_rnss attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<9.100000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<3.800000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 802, "kernel_name": "nested_qtnf", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_qtnf(a):\n    # Nested branching\n    # if a > 0.26:\n    #   if a > 1.45: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 1.45, a * 3.0, a * 2.0)\n    return jnp.where(a > 0.26, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_qtnf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.450000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<2.600000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_qtnf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.450000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<2.600000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 803, "kernel_name": "elementwise_bqnd", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_bqnd(a, b):\n    # Elementwise +\n    return a + b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_bqnd attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_bqnd attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 804, "kernel_name": "scalar_arr_cmew", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_cmew(alpha, x, y):\n    # Scalar-array operations\n    return alpha + x + y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_cmew attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.add %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_cmew attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.add %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 805, "kernel_name": "vec_kwev", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_kwev(a):\n    # Vector length (row-wise for N x 3 arrays)\n    return jnp.linalg.norm(a, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_kwev attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_kwev attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 806, "kernel_name": "multi_vuux", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_vuux(a, b):\n    # Multi-statement\n    temp1 = a + b\n    temp2 = jnp.sqrt(temp1)\n    return temp2 - a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_vuux attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sqrt %0 : tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_vuux attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sqrt %0 : tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 807, "kernel_name": "elementwise_abry", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_abry(a, b):\n    # Elementwise +\n    return a + b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_abry attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_abry attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 808, "kernel_name": "multi_vmln", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_vmln(a, b):\n    # Multi-statement\n    temp1 = a - b\n    temp2 = jnp.sin(temp1)\n    return temp2 + a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_vmln attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sine %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_vmln attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sine %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 809, "kernel_name": "compound_pslk", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_pslk(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_pslk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_pslk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 810, "kernel_name": "compound_vqlm", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_vqlm(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_vqlm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_vqlm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 811, "kernel_name": "vec_kwtj", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_kwtj(a, b):\n    # Vector dot product (row-wise for N x 3 arrays)\n    # a and b are expected to be shape (N, 3)\n    # contract over last axis\n    return jnp.sum(a * b, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_kwtj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_kwtj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n"}
{"id": 812, "kernel_name": "reduce_xjsn", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_xjsn(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_xjsn attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_xjsn attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 813, "kernel_name": "branch_gyet", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_gyet(a):\n    # Branching with where\n    # if a > -0.75: a - 1.8 else: a * 2.4\n    return jnp.where(a > -0.75, a - 1.8, a * 2.4)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_gyet attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-7.500000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.800000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.400000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_gyet attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-7.500000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.800000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.400000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 814, "kernel_name": "branch_fxtb", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_fxtb(a):\n    # Branching with where\n    # if a > 0.94: a + 1.9 else: a - 1.5\n    return jnp.where(a > 0.94, a + 1.9, a - 1.5)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_fxtb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.939999997> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.900000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.500000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_fxtb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.939999997> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.900000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.500000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 815, "kernel_name": "multi_zrlv", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_zrlv(a, b):\n    # Multi-statement\n    temp1 = a + b\n    temp2 = jnp.cos(temp1)\n    return temp2 - a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_zrlv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.cosine %0 : tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_zrlv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.cosine %0 : tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 816, "kernel_name": "nested_ncew", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_ncew(a):\n    # Nested branching\n    # if a > -0.34:\n    #   if a > 0.98: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 0.98, a * 3.0, a * 2.0)\n    return jnp.where(a > -0.34, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_ncew attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<9.800000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-3.400000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_ncew attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<9.800000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-3.400000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 817, "kernel_name": "compound_ofcq", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_ofcq(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_ofcq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_ofcq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 818, "kernel_name": "unary_drcj", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_drcj(a):\n    # Unary operation jnp.sin\n    return jnp.sin(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_drcj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_drcj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 819, "kernel_name": "multi_wlry", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_wlry(a, b):\n    # Multi-statement\n    temp1 = a + b\n    temp2 = jnp.abs(temp1)\n    return temp2 + a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_wlry attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.abs %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_wlry attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.abs %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 820, "kernel_name": "compound_ofnw", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_ofnw(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_ofnw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_ofnw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 821, "kernel_name": "compound_pwoy", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_pwoy(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_pwoy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_pwoy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 822, "kernel_name": "loop_hmjf", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_hmjf(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val * a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_hmjf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.multiply %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_hmjf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.multiply %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 823, "kernel_name": "vec_kovl", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_kovl(a, b):\n    # Vector dot product (row-wise for N x 3 arrays)\n    # a and b are expected to be shape (N, 3)\n    # contract over last axis\n    return jnp.sum(a * b, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_kovl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_kovl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n"}
{"id": 824, "kernel_name": "scalar_arr_yfdx", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_yfdx(alpha, x, y):\n    # Scalar-array operations\n    return alpha - x + y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_yfdx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_yfdx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 825, "kernel_name": "unary_vtqv", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_vtqv(a):\n    # Unary operation jnp.sin\n    return jnp.sin(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_vtqv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_vtqv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 826, "kernel_name": "vec_kinb", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_kinb(a):\n    # Vector length (row-wise for N x 3 arrays)\n    return jnp.linalg.norm(a, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_kinb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_kinb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 827, "kernel_name": "vec_kkql", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_kkql(a, b):\n    # Vector dot product (row-wise for N x 3 arrays)\n    # a and b are expected to be shape (N, 3)\n    # contract over last axis\n    return jnp.sum(a * b, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_kkql attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_kkql attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n"}
{"id": 828, "kernel_name": "branch_gljp", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_gljp(a):\n    # Branching with where\n    # if a > 0.91: a + 2.9 else: a * 2.7\n    return jnp.where(a > 0.91, a + 2.9, a * 2.7)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_gljp attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<9.100000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.900000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.700000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_gljp attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<9.100000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.900000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.700000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 829, "kernel_name": "reduce_jwws", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_jwws(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_jwws attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_jwws attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 830, "kernel_name": "nested_omjm", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_omjm(a):\n    # Nested branching\n    # if a > 0.09:\n    #   if a > 1.28: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 1.28, a * 3.0, a * 2.0)\n    return jnp.where(a > 0.09, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_omjm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.280000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<9.000000e-02> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_omjm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.280000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<9.000000e-02> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 831, "kernel_name": "unary_dohy", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_dohy(a):\n    # Unary operation jnp.sin\n    return jnp.sin(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_dohy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_dohy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 832, "kernel_name": "nested_njad", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_njad(a):\n    # Nested branching\n    # if a > -0.41:\n    #   if a > 0.83: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 0.83, a * 3.0, a * 2.0)\n    return jnp.where(a > -0.41, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_njad attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.829999983> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-4.100000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_njad attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.829999983> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-4.100000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 833, "kernel_name": "nested_ntuk", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_ntuk(a):\n    # Nested branching\n    # if a > 0.12:\n    #   if a > 1.1: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 1.1, a * 3.0, a * 2.0)\n    return jnp.where(a > 0.12, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_ntuk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.100000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<1.200000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_ntuk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.100000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<1.200000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 834, "kernel_name": "elementwise_ssma", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_ssma(a, b):\n    # Elementwise +\n    return a + b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_ssma attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_ssma attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 835, "kernel_name": "branch_fwvw", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_fwvw(a):\n    # Branching with where\n    # if a > -0.04: a * 2.6 else: a * 1.9\n    return jnp.where(a > -0.04, a * 2.6, a * 1.9)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_fwvw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-4.000000e-02> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.600000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.900000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_fwvw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-4.000000e-02> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.600000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.900000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 836, "kernel_name": "nested_uoyu", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_uoyu(a):\n    # Nested branching\n    # if a > 0.3:\n    #   if a > 0.9: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 0.9, a * 3.0, a * 2.0)\n    return jnp.where(a > 0.3, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_uoyu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.899999976> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<3.000000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_uoyu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.899999976> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<3.000000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 837, "kernel_name": "scalar_arr_coea", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_coea(alpha, x, y):\n    # Scalar-array operations\n    return alpha + x - y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_coea attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.add %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.subtract %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_coea attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.add %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.subtract %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 838, "kernel_name": "scalar_arr_zcgy", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_zcgy(alpha, x, y):\n    # Scalar-array operations\n    return alpha * x + y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_zcgy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_zcgy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 839, "kernel_name": "reduce_relx", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_relx(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_relx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_relx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 840, "kernel_name": "elementwise_zswi", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_zswi(a, b):\n    # Elementwise *\n    return a * b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_zswi attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_zswi attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 841, "kernel_name": "nested_oeoy", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_oeoy(a):\n    # Nested branching\n    # if a > -0.41:\n    #   if a > 1.13: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 1.13, a * 3.0, a * 2.0)\n    return jnp.where(a > -0.41, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_oeoy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.130000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-4.100000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_oeoy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.130000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-4.100000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 842, "kernel_name": "scalar_arr_vtvm", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_vtvm(alpha, x, y):\n    # Scalar-array operations\n    return alpha + x * y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_vtvm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<128xf32>\n    %1 = stablehlo.convert %arg0 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %2, %0 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_vtvm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<128xf32>\n    %1 = stablehlo.convert %arg0 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %2, %0 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 843, "kernel_name": "reduce_jyvu", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_jyvu(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_jyvu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_jyvu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 844, "kernel_name": "branch_eodh", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_eodh(a):\n    # Branching with where\n    # if a > 0.34: a - 2.0 else: a + 0.8\n    return jnp.where(a > 0.34, a - 2.0, a + 0.8)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_eodh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<3.400000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<8.000000e-01> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_eodh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<3.400000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<8.000000e-01> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 845, "kernel_name": "multi_mmfz", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_mmfz(a, b):\n    # Multi-statement\n    temp1 = a - b\n    temp2 = jnp.sqrt(temp1)\n    return temp2 * a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_mmfz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sqrt %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_mmfz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sqrt %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 846, "kernel_name": "scalar_arr_cpxk", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_cpxk(alpha, x, y):\n    # Scalar-array operations\n    return alpha - x * y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_cpxk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<128xf32>\n    %1 = stablehlo.convert %arg0 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %2, %0 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_cpxk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<128xf32>\n    %1 = stablehlo.convert %arg0 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %2, %0 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 847, "kernel_name": "compound_qugc", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_qugc(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_qugc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_qugc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 848, "kernel_name": "elementwise_bixv", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_bixv(a, b):\n    # Elementwise *\n    return a * b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_bixv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_bixv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 849, "kernel_name": "vec_zknu", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_zknu(a, b):\n    # Vector dot product (row-wise for N x 3 arrays)\n    # a and b are expected to be shape (N, 3)\n    # contract over last axis\n    return jnp.sum(a * b, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_zknu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_zknu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n"}
{"id": 850, "kernel_name": "branch_fhsg", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_fhsg(a):\n    # Branching with where\n    # if a > 0.47: a + 2.1 else: a + 2.4\n    return jnp.where(a > 0.47, a + 2.1, a + 2.4)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_fhsg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<4.700000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.100000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.400000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_fhsg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<4.700000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.100000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.400000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 851, "kernel_name": "nested_nhxj", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_nhxj(a):\n    # Nested branching\n    # if a > 0.27:\n    #   if a > 1.21: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 1.21, a * 3.0, a * 2.0)\n    return jnp.where(a > 0.27, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_nhxj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.210000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<2.700000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_nhxj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.210000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<2.700000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 852, "kernel_name": "multi_lhrf", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_lhrf(a, b):\n    # Multi-statement\n    temp1 = a * b\n    temp2 = jnp.sqrt(temp1)\n    return temp2 - a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_lhrf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sqrt %0 : tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_lhrf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sqrt %0 : tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 853, "kernel_name": "elementwise_qago", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_qago(a, b):\n    # Elementwise +\n    return a + b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_qago attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_qago attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 854, "kernel_name": "branch_fdiq", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_fdiq(a):\n    # Branching with where\n    # if a > -0.06: a - 0.8 else: a - 1.4\n    return jnp.where(a > -0.06, a - 0.8, a - 1.4)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_fdiq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-6.000000e-02> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<8.000000e-01> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.400000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_fdiq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-6.000000e-02> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<8.000000e-01> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.400000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 855, "kernel_name": "elementwise_bbhh", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_bbhh(a, b):\n    # Elementwise -\n    return a - b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_bbhh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_bbhh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 856, "kernel_name": "vec_kcys", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_kcys(a, b):\n    # Vector dot product (row-wise for N x 3 arrays)\n    # a and b are expected to be shape (N, 3)\n    # contract over last axis\n    return jnp.sum(a * b, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_kcys attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_kcys attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n"}
{"id": 857, "kernel_name": "unary_vbog", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_vbog(a):\n    # Unary operation jnp.sqrt\n    return jnp.sqrt(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_vbog attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sqrt %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_vbog attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sqrt %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 858, "kernel_name": "compound_yftr", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_yftr(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_yftr attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_yftr attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 859, "kernel_name": "vec_lhpk", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_lhpk(a):\n    # Vector length (row-wise for N x 3 arrays)\n    return jnp.linalg.norm(a, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_lhpk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_lhpk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 860, "kernel_name": "loop_zqkb", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_zqkb(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val + a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_zqkb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.add %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_zqkb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.add %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 861, "kernel_name": "elementwise_bqhe", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_bqhe(a, b):\n    # Elementwise +\n    return a + b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_bqhe attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_bqhe attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 862, "kernel_name": "compound_sxhk", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_sxhk(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_sxhk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_sxhk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 863, "kernel_name": "reduce_rjnq", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_rjnq(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_rjnq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_rjnq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 864, "kernel_name": "elementwise_azub", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_azub(a, b):\n    # Elementwise *\n    return a * b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_azub attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_azub attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 865, "kernel_name": "vec_sbeh", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_sbeh(a):\n    # Vector length (row-wise for N x 3 arrays)\n    return jnp.linalg.norm(a, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_sbeh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_sbeh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 866, "kernel_name": "elementwise_qveb", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_qveb(a, b):\n    # Elementwise *\n    return a * b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_qveb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_qveb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 867, "kernel_name": "branch_fkvg", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_fkvg(a):\n    # Branching with where\n    # if a > 0.53: a - 2.2 else: a - 1.5\n    return jnp.where(a > 0.53, a - 2.2, a - 1.5)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_fkvg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<5.300000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.200000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.500000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_fkvg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<5.300000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.200000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.500000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 868, "kernel_name": "compound_qlqp", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_qlqp(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_qlqp attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_qlqp attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 869, "kernel_name": "multi_lxwd", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_lxwd(a, b):\n    # Multi-statement\n    temp1 = a + b\n    temp2 = jnp.abs(temp1)\n    return temp2 * a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_lxwd attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.abs %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_lxwd attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.abs %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 870, "kernel_name": "nested_nxio", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_nxio(a):\n    # Nested branching\n    # if a > 0.14:\n    #   if a > 1.03: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 1.03, a * 3.0, a * 2.0)\n    return jnp.where(a > 0.14, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_nxio attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.030000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<1.400000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_nxio attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.030000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<1.400000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 871, "kernel_name": "scalar_arr_cmtz", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_cmtz(alpha, x, y):\n    # Scalar-array operations\n    return alpha * x - y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_cmtz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.subtract %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_cmtz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.subtract %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 872, "kernel_name": "elementwise_butv", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_butv(a, b):\n    # Elementwise +\n    return a + b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_butv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_butv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 873, "kernel_name": "unary_eumv", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_eumv(a):\n    # Unary operation jnp.sin\n    return jnp.sin(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_eumv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_eumv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 874, "kernel_name": "compound_ryzu", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_ryzu(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_ryzu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_ryzu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 875, "kernel_name": "nested_wnxy", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_wnxy(a):\n    # Nested branching\n    # if a > -0.45:\n    #   if a > 1.15: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 1.15, a * 3.0, a * 2.0)\n    return jnp.where(a > -0.45, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_wnxy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.150000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-4.500000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_wnxy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.150000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-4.500000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 876, "kernel_name": "multi_wqsd", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_wqsd(a, b):\n    # Multi-statement\n    temp1 = a * b\n    temp2 = jnp.cos(temp1)\n    return temp2 + a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_wqsd attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.cosine %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_wqsd attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.cosine %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 877, "kernel_name": "nested_rtho", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_rtho(a):\n    # Nested branching\n    # if a > -0.26:\n    #   if a > 0.64: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 0.64, a * 3.0, a * 2.0)\n    return jnp.where(a > -0.26, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_rtho attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<6.400000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-2.600000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_rtho attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<6.400000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-2.600000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 878, "kernel_name": "compound_pkde", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_pkde(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_pkde attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_pkde attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 879, "kernel_name": "reduce_zjkp", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_zjkp(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_zjkp attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_zjkp attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 880, "kernel_name": "elementwise_byym", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_byym(a, b):\n    # Elementwise +\n    return a + b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_byym attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_byym attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 881, "kernel_name": "multi_vejt", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_vejt(a, b):\n    # Multi-statement\n    temp1 = a * b\n    temp2 = jnp.sqrt(temp1)\n    return temp2 * a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_vejt attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sqrt %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_vejt attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sqrt %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 882, "kernel_name": "compound_oibk", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_oibk(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_oibk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_oibk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 883, "kernel_name": "scalar_arr_zczs", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_zczs(alpha, x, y):\n    # Scalar-array operations\n    return alpha * x - y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_zczs attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.subtract %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_zczs attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.subtract %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 884, "kernel_name": "vec_qwuj", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_qwuj(a, b):\n    # Vector dot product (row-wise for N x 3 arrays)\n    # a and b are expected to be shape (N, 3)\n    # contract over last axis\n    return jnp.sum(a * b, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_qwuj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_qwuj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n"}
{"id": 885, "kernel_name": "unary_qdxe", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_qdxe(a):\n    # Unary operation jnp.sin\n    return jnp.sin(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_qdxe attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_qdxe attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 886, "kernel_name": "loop_hnuy", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_hnuy(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val - a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_hnuy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_hnuy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 887, "kernel_name": "branch_fbux", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_fbux(a):\n    # Branching with where\n    # if a > -0.87: a * 2.0 else: a - 2.1\n    return jnp.where(a > -0.87, a * 2.0, a - 2.1)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_fbux attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-8.700000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.100000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_fbux attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-8.700000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.100000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 888, "kernel_name": "nested_yumt", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_yumt(a):\n    # Nested branching\n    # if a > -0.23:\n    #   if a > 1.06: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 1.06, a * 3.0, a * 2.0)\n    return jnp.where(a > -0.23, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_yumt attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.060000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-2.300000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_yumt attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.060000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-2.300000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 889, "kernel_name": "vec_ukow", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_ukow(a):\n    # Vector length (row-wise for N x 3 arrays)\n    return jnp.linalg.norm(a, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_ukow attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_ukow attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 890, "kernel_name": "nested_nczn", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_nczn(a):\n    # Nested branching\n    # if a > 0.22:\n    #   if a > 0.91: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 0.91, a * 3.0, a * 2.0)\n    return jnp.where(a > 0.22, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_nczn attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<9.100000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<2.200000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_nczn attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<9.100000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<2.200000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 891, "kernel_name": "nested_nenr", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_nenr(a):\n    # Nested branching\n    # if a > 0.11:\n    #   if a > 0.7: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 0.7, a * 3.0, a * 2.0)\n    return jnp.where(a > 0.11, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_nenr attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.699999988> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<1.100000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_nenr attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.699999988> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<1.100000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 892, "kernel_name": "multi_labg", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_labg(a, b):\n    # Multi-statement\n    temp1 = a - b\n    temp2 = jnp.sin(temp1)\n    return temp2 * a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_labg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sine %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_labg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sine %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 893, "kernel_name": "multi_vski", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_vski(a, b):\n    # Multi-statement\n    temp1 = a * b\n    temp2 = jnp.cos(temp1)\n    return temp2 - a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_vski attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.cosine %0 : tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_vski attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.cosine %0 : tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 894, "kernel_name": "unary_eyya", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_eyya(a):\n    # Unary operation jnp.abs\n    return jnp.abs(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_eyya attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.abs %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_eyya attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.abs %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 895, "kernel_name": "nested_nqpo", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_nqpo(a):\n    # Nested branching\n    # if a > 0.03:\n    #   if a > 1.01: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 1.01, a * 3.0, a * 2.0)\n    return jnp.where(a > 0.03, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_nqpo attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.010000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<3.000000e-02> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_nqpo attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.010000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<3.000000e-02> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 896, "kernel_name": "multi_ljrq", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_ljrq(a, b):\n    # Multi-statement\n    temp1 = a * b\n    temp2 = jnp.sin(temp1)\n    return temp2 - a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_ljrq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sine %0 : tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_ljrq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sine %0 : tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 897, "kernel_name": "scalar_arr_xdgc", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_xdgc(alpha, x, y):\n    # Scalar-array operations\n    return alpha * x * y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_xdgc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.multiply %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_xdgc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.multiply %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 898, "kernel_name": "reduce_jhdi", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_jhdi(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_jhdi attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_jhdi attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 899, "kernel_name": "reduce_jntr", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_jntr(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_jntr attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_jntr attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 900, "kernel_name": "vec_zfsf", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_zfsf(a):\n    # Vector length (row-wise for N x 3 arrays)\n    return jnp.linalg.norm(a, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_zfsf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_zfsf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 901, "kernel_name": "nested_ondu", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_ondu(a):\n    # Nested branching\n    # if a > -0.32:\n    #   if a > 0.94: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 0.94, a * 3.0, a * 2.0)\n    return jnp.where(a > -0.32, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_ondu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.939999997> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-3.200000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_ondu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.939999997> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-3.200000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 902, "kernel_name": "elementwise_axnb", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_axnb(a, b):\n    # Elementwise -\n    return a - b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_axnb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_axnb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 903, "kernel_name": "vec_lsuy", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_lsuy(a, b):\n    # Vector dot product (row-wise for N x 3 arrays)\n    # a and b are expected to be shape (N, 3)\n    # contract over last axis\n    return jnp.sum(a * b, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_lsuy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_lsuy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n"}
{"id": 904, "kernel_name": "elementwise_aqzm", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_aqzm(a, b):\n    # Elementwise -\n    return a - b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_aqzm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_aqzm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 905, "kernel_name": "branch_fhjc", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_fhjc(a):\n    # Branching with where\n    # if a > 0.57: a * 2.4 else: a - 2.9\n    return jnp.where(a > 0.57, a * 2.4, a - 2.9)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_fhjc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<5.700000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.400000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.900000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_fhjc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<5.700000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.400000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.900000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 906, "kernel_name": "nested_nuap", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_nuap(a):\n    # Nested branching\n    # if a > -0.41:\n    #   if a > 1.08: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 1.08, a * 3.0, a * 2.0)\n    return jnp.where(a > -0.41, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_nuap attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.080000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-4.100000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_nuap attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.080000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-4.100000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 907, "kernel_name": "reduce_ijvg", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_ijvg(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_ijvg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_ijvg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 908, "kernel_name": "branch_fsuk", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_fsuk(a):\n    # Branching with where\n    # if a > 0.74: a + 0.6 else: a - 0.6\n    return jnp.where(a > 0.74, a + 0.6, a - 0.6)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_fsuk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<7.400000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<6.000000e-01> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<6.000000e-01> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_fsuk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<7.400000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<6.000000e-01> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<6.000000e-01> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 909, "kernel_name": "unary_tevg", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_tevg(a):\n    # Unary operation jnp.sqrt\n    return jnp.sqrt(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_tevg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sqrt %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_tevg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sqrt %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 910, "kernel_name": "scalar_arr_cbee", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_cbee(alpha, x, y):\n    # Scalar-array operations\n    return alpha * x + y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_cbee attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_cbee attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 911, "kernel_name": "elementwise_akgl", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_akgl(a, b):\n    # Elementwise +\n    return a + b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_akgl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_akgl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 912, "kernel_name": "unary_ebws", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_ebws(a):\n    # Unary operation jnp.sin\n    return jnp.sin(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_ebws attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_ebws attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 913, "kernel_name": "vec_rlpd", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_rlpd(a):\n    # Vector length (row-wise for N x 3 arrays)\n    return jnp.linalg.norm(a, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_rlpd attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_rlpd attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 914, "kernel_name": "vec_rdtl", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_rdtl(a, b):\n    # Vector dot product (row-wise for N x 3 arrays)\n    # a and b are expected to be shape (N, 3)\n    # contract over last axis\n    return jnp.sum(a * b, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_rdtl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_rdtl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n"}
{"id": 915, "kernel_name": "multi_myfj", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_myfj(a, b):\n    # Multi-statement\n    temp1 = a * b\n    temp2 = jnp.sqrt(temp1)\n    return temp2 * a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_myfj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sqrt %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_myfj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sqrt %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 916, "kernel_name": "reduce_ixym", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_ixym(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_ixym attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_ixym attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 917, "kernel_name": "multi_mckf", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_mckf(a, b):\n    # Multi-statement\n    temp1 = a * b\n    temp2 = jnp.sqrt(temp1)\n    return temp2 * a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_mckf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sqrt %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_mckf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sqrt %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 918, "kernel_name": "vec_kexh", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_kexh(a):\n    # Vector length (row-wise for N x 3 arrays)\n    return jnp.linalg.norm(a, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_kexh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_kexh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 919, "kernel_name": "nested_qncx", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_qncx(a):\n    # Nested branching\n    # if a > -0.25:\n    #   if a > 0.63: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 0.63, a * 3.0, a * 2.0)\n    return jnp.where(a > -0.25, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_qncx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<6.300000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-2.500000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_qncx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<6.300000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-2.500000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 920, "kernel_name": "loop_gqld", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_gqld(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val + a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_gqld attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.add %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_gqld attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.add %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 921, "kernel_name": "nested_zvhe", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_zvhe(a):\n    # Nested branching\n    # if a > 0.48:\n    #   if a > 0.52: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 0.52, a * 3.0, a * 2.0)\n    return jnp.where(a > 0.48, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_zvhe attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<5.200000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<4.800000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_zvhe attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<5.200000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<4.800000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 922, "kernel_name": "nested_vnoy", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_vnoy(a):\n    # Nested branching\n    # if a > -0.26:\n    #   if a > 1.4: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 1.4, a * 3.0, a * 2.0)\n    return jnp.where(a > -0.26, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_vnoy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.400000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-2.600000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_vnoy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.400000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-2.600000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 923, "kernel_name": "branch_firs", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_firs(a):\n    # Branching with where\n    # if a > 0.5: a * 1.0 else: a - 1.4\n    return jnp.where(a > 0.5, a * 1.0, a - 1.4)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_firs attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.400000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_firs attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.400000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 924, "kernel_name": "elementwise_armm", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_armm(a, b):\n    # Elementwise -\n    return a - b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_armm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_armm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 925, "kernel_name": "nested_zduw", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_zduw(a):\n    # Nested branching\n    # if a > -0.43:\n    #   if a > 0.53: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 0.53, a * 3.0, a * 2.0)\n    return jnp.where(a > -0.43, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_zduw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<5.300000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-4.300000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_zduw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<5.300000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-4.300000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 926, "kernel_name": "scalar_arr_uhzw", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_uhzw(alpha, x, y):\n    # Scalar-array operations\n    return alpha + x + y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_uhzw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.add %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_uhzw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.add %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 927, "kernel_name": "vec_jrot", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_jrot(a):\n    # Vector length (row-wise for N x 3 arrays)\n    return jnp.linalg.norm(a, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_jrot attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_jrot attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 928, "kernel_name": "loop_vsws", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_vsws(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val + a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_vsws attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.add %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_vsws attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.add %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 929, "kernel_name": "loop_gzse", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_gzse(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val + a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_gzse attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.add %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_gzse attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.add %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 930, "kernel_name": "scalar_arr_snfv", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_snfv(alpha, x, y):\n    # Scalar-array operations\n    return alpha - x - y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_snfv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.subtract %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_snfv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.subtract %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 931, "kernel_name": "unary_ebkg", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_ebkg(a):\n    # Unary operation jnp.sin\n    return jnp.sin(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_ebkg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_ebkg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 932, "kernel_name": "reduce_xipp", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_xipp(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_xipp attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_xipp attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 933, "kernel_name": "multi_msqo", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_msqo(a, b):\n    # Multi-statement\n    temp1 = a * b\n    temp2 = jnp.sin(temp1)\n    return temp2 + a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_msqo attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sine %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_msqo attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sine %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 934, "kernel_name": "reduce_vtes", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_vtes(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_vtes attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_vtes attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 935, "kernel_name": "elementwise_qbni", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_qbni(a, b):\n    # Elementwise *\n    return a * b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_qbni attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_qbni attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 936, "kernel_name": "compound_zecu", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_zecu(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_zecu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_zecu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 937, "kernel_name": "multi_mucj", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_mucj(a, b):\n    # Multi-statement\n    temp1 = a * b\n    temp2 = jnp.sqrt(temp1)\n    return temp2 + a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_mucj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sqrt %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_mucj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sqrt %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 938, "kernel_name": "vec_lzef", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_lzef(a):\n    # Vector length (row-wise for N x 3 arrays)\n    return jnp.linalg.norm(a, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_lzef attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_lzef attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 939, "kernel_name": "branch_xbjy", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_xbjy(a):\n    # Branching with where\n    # if a > 0.42: a + 1.7 else: a - 1.9\n    return jnp.where(a > 0.42, a + 1.7, a - 1.9)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_xbjy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<4.200000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.700000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.900000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_xbjy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<4.200000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.700000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.900000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 940, "kernel_name": "nested_nvcv", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_nvcv(a):\n    # Nested branching\n    # if a > 0.31:\n    #   if a > 1.42: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 1.42, a * 3.0, a * 2.0)\n    return jnp.where(a > 0.31, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_nvcv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.420000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<3.100000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_nvcv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.420000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<3.100000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 941, "kernel_name": "nested_rtjo", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_rtjo(a):\n    # Nested branching\n    # if a > 0.07:\n    #   if a > 0.51: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 0.51, a * 3.0, a * 2.0)\n    return jnp.where(a > 0.07, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_rtjo attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<5.100000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<7.000000e-02> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_rtjo attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<5.100000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<7.000000e-02> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 942, "kernel_name": "scalar_arr_uqmc", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_uqmc(alpha, x, y):\n    # Scalar-array operations\n    return alpha * x - y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_uqmc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.subtract %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_uqmc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.subtract %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 943, "kernel_name": "reduce_ifam", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_ifam(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_ifam attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_ifam attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 944, "kernel_name": "scalar_arr_dwjb", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_dwjb(alpha, x, y):\n    # Scalar-array operations\n    return alpha - x - y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_dwjb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.subtract %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_dwjb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.subtract %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 945, "kernel_name": "nested_nnqz", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_nnqz(a):\n    # Nested branching\n    # if a > -0.47:\n    #   if a > 0.78: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 0.78, a * 3.0, a * 2.0)\n    return jnp.where(a > -0.47, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_nnqz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<7.800000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-4.700000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_nnqz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<7.800000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-4.700000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 946, "kernel_name": "loop_ypog", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_ypog(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val - a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_ypog attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_ypog attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 947, "kernel_name": "scalar_arr_wuhs", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_wuhs(alpha, x, y):\n    # Scalar-array operations\n    return alpha - x * y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_wuhs attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<128xf32>\n    %1 = stablehlo.convert %arg0 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %2, %0 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_wuhs attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<128xf32>\n    %1 = stablehlo.convert %arg0 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %2, %0 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 948, "kernel_name": "nested_vbgh", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_vbgh(a):\n    # Nested branching\n    # if a > -0.24:\n    #   if a > 0.77: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 0.77, a * 3.0, a * 2.0)\n    return jnp.where(a > -0.24, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_vbgh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.76999998> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-2.400000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_vbgh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.76999998> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-2.400000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 949, "kernel_name": "unary_drhu", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_drhu(a):\n    # Unary operation jnp.abs\n    return jnp.abs(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_drhu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.abs %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_drhu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.abs %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 950, "kernel_name": "branch_zjwx", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_zjwx(a):\n    # Branching with where\n    # if a > -0.35: a * 1.4 else: a * 0.7\n    return jnp.where(a > -0.35, a * 1.4, a * 0.7)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_zjwx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-3.500000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.400000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<0.699999988> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_zjwx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-3.500000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.400000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<0.699999988> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 951, "kernel_name": "elementwise_bcop", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_bcop(a, b):\n    # Elementwise +\n    return a + b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_bcop attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_bcop attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 952, "kernel_name": "unary_ytoy", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_ytoy(a):\n    # Unary operation jnp.sin\n    return jnp.sin(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_ytoy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_ytoy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 953, "kernel_name": "unary_ehvg", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_ehvg(a):\n    # Unary operation jnp.cos\n    return jnp.cos(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_ehvg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.cosine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_ehvg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.cosine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 954, "kernel_name": "multi_xlzc", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_xlzc(a, b):\n    # Multi-statement\n    temp1 = a - b\n    temp2 = jnp.sqrt(temp1)\n    return temp2 * a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_xlzc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sqrt %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_xlzc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sqrt %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 955, "kernel_name": "multi_mhej", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_mhej(a, b):\n    # Multi-statement\n    temp1 = a + b\n    temp2 = jnp.abs(temp1)\n    return temp2 - a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_mhej attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.abs %0 : tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_mhej attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.abs %0 : tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 956, "kernel_name": "unary_eynm", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_eynm(a):\n    # Unary operation jnp.abs\n    return jnp.abs(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_eynm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.abs %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_eynm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.abs %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 957, "kernel_name": "scalar_arr_ucwo", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_ucwo(alpha, x, y):\n    # Scalar-array operations\n    return alpha - x - y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_ucwo attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.subtract %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_ucwo attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.subtract %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 958, "kernel_name": "vec_urcj", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_urcj(a):\n    # Vector length (row-wise for N x 3 arrays)\n    return jnp.linalg.norm(a, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_urcj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_urcj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 959, "kernel_name": "elementwise_ubtw", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_ubtw(a, b):\n    # Elementwise +\n    return a + b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_ubtw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_ubtw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 960, "kernel_name": "nested_nkfd", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_nkfd(a):\n    # Nested branching\n    # if a > -0.02:\n    #   if a > 1.13: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 1.13, a * 3.0, a * 2.0)\n    return jnp.where(a > -0.02, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_nkfd attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.130000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-2.000000e-02> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_nkfd attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.130000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-2.000000e-02> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 961, "kernel_name": "multi_mjsl", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_mjsl(a, b):\n    # Multi-statement\n    temp1 = a - b\n    temp2 = jnp.sqrt(temp1)\n    return temp2 - a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_mjsl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sqrt %0 : tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_mjsl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sqrt %0 : tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 962, "kernel_name": "vec_kzps", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_kzps(a, b):\n    # Vector dot product (row-wise for N x 3 arrays)\n    # a and b are expected to be shape (N, 3)\n    # contract over last axis\n    return jnp.sum(a * b, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_kzps attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_kzps attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n"}
{"id": 963, "kernel_name": "multi_mxnw", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_mxnw(a, b):\n    # Multi-statement\n    temp1 = a * b\n    temp2 = jnp.sin(temp1)\n    return temp2 * a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_mxnw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sine %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_mxnw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sine %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 964, "kernel_name": "reduce_jfay", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_jfay(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_jfay attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_jfay attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 965, "kernel_name": "elementwise_wupx", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_wupx(a, b):\n    # Elementwise *\n    return a * b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_wupx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_wupx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 966, "kernel_name": "nested_nyis", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_nyis(a):\n    # Nested branching\n    # if a > 0.43:\n    #   if a > 0.55: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 0.55, a * 3.0, a * 2.0)\n    return jnp.where(a > 0.43, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_nyis attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<5.500000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<4.300000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_nyis attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<5.500000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<4.300000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 967, "kernel_name": "unary_emcn", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_emcn(a):\n    # Unary operation jnp.abs\n    return jnp.abs(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_emcn attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.abs %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_emcn attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.abs %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 968, "kernel_name": "compound_rwgc", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_rwgc(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_rwgc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_rwgc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 969, "kernel_name": "reduce_jael", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_jael(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_jael attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_jael attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 970, "kernel_name": "reduce_idfa", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_idfa(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_idfa attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_idfa attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 971, "kernel_name": "multi_xtrs", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_xtrs(a, b):\n    # Multi-statement\n    temp1 = a * b\n    temp2 = jnp.sqrt(temp1)\n    return temp2 + a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_xtrs attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sqrt %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_xtrs attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sqrt %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 972, "kernel_name": "nested_ngal", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_ngal(a):\n    # Nested branching\n    # if a > 0.47:\n    #   if a > 1.12: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 1.12, a * 3.0, a * 2.0)\n    return jnp.where(a > 0.47, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_ngal attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.120000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<4.700000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_ngal attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.120000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<4.700000e-01> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 973, "kernel_name": "scalar_arr_chym", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_chym(alpha, x, y):\n    # Scalar-array operations\n    return alpha * x * y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_chym attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.multiply %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_chym attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<128xf32>\n    %3 = stablehlo.multiply %2, %arg2 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 974, "kernel_name": "reduce_jkjo", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef reduce_jkjo(a):\n    # Reduction (sum)\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "cpp": "module @jit_reduce_jkjo attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n", "cuda": "module @jit_reduce_jkjo attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n"}
{"id": 975, "kernel_name": "multi_mqec", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_mqec(a, b):\n    # Multi-statement\n    temp1 = a - b\n    temp2 = jnp.sin(temp1)\n    return temp2 + a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_mqec attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sine %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_mqec attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sine %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 976, "kernel_name": "vec_kusr", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_kusr(a):\n    # Vector length (row-wise for N x 3 arrays)\n    return jnp.linalg.norm(a, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_kusr attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_kusr attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 977, "kernel_name": "elementwise_aful", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_aful(a, b):\n    # Elementwise *\n    return a * b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_aful attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_aful attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 978, "kernel_name": "scalar_arr_cmke", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_cmke(alpha, x, y):\n    # Scalar-array operations\n    return alpha + x * y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_cmke attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<128xf32>\n    %1 = stablehlo.convert %arg0 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %2, %0 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_cmke attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<128xf32>\n    %1 = stablehlo.convert %arg0 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %2, %0 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 979, "kernel_name": "nested_negg", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef nested_negg(a):\n    # Nested branching\n    # if a > -0.09:\n    #   if a > 0.93: a * 3.0\n    #   else: a * 2.0\n    # else: a * 0.5\n    \n    inner_branch = jnp.where(a > 0.93, a * 3.0, a * 2.0)\n    return jnp.where(a > -0.09, inner_branch, a * 0.5)", "type": "generate_nested_branch_kernel", "cpp": "module @jit_nested_negg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<9.300000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-9.000000e-02> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_nested_negg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<9.300000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    %cst_2 = stablehlo.constant dense<-9.000000e-02> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %8 = stablehlo.compare  GT, %arg0, %7,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<128xf32>\n    %11 = call @_where(%8, %6, %10) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %11 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 980, "kernel_name": "unary_dyjv", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_dyjv(a):\n    # Unary operation jnp.sqrt\n    return jnp.sqrt(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_dyjv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sqrt %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_dyjv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sqrt %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 981, "kernel_name": "multi_mlzk", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_mlzk(a, b):\n    # Multi-statement\n    temp1 = a + b\n    temp2 = jnp.cos(temp1)\n    return temp2 * a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_mlzk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.cosine %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_mlzk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.cosine %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 982, "kernel_name": "elementwise_umkv", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_umkv(a, b):\n    # Elementwise *\n    return a * b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_umkv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_umkv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 983, "kernel_name": "branch_fbif", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_fbif(a):\n    # Branching with where\n    # if a > 0.35: a - 1.3 else: a * 2.7\n    return jnp.where(a > 0.35, a - 1.3, a * 2.7)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_fbif attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<3.500000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.300000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.700000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_fbif attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<3.500000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.300000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.700000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 984, "kernel_name": "elementwise_amkg", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_amkg(a, b):\n    # Elementwise *\n    return a * b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_amkg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_amkg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 985, "kernel_name": "elementwise_axox", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_axox(a, b):\n    # Elementwise -\n    return a - b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_axox attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_axox attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 986, "kernel_name": "multi_mkqg", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_mkqg(a, b):\n    # Multi-statement\n    temp1 = a + b\n    temp2 = jnp.sin(temp1)\n    return temp2 - a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_mkqg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sine %0 : tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_mkqg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sine %0 : tensor<128xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 987, "kernel_name": "compound_psnt", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_psnt(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_psnt attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_psnt attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 988, "kernel_name": "elementwise_ylpr", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_ylpr(a, b):\n    # Elementwise +\n    return a + b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_ylpr attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_ylpr attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 989, "kernel_name": "compound_sozw", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef compound_sozw(a, b, scale):\n    # Compound operations\n    x = a\n    y = b\n    result = (x + y) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "cpp": "module @jit_compound_sozw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_compound_sozw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>, %arg2: tensor<f32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<128xf32>\n    %4 = stablehlo.floor %3 : tensor<128xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<128xf32>\n    %6 = stablehlo.abs %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n}\n"}
{"id": 990, "kernel_name": "vec_koui", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_koui(a):\n    # Vector length (row-wise for N x 3 arrays)\n    return jnp.linalg.norm(a, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_koui attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_koui attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<128x3xf32>) -> tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n  func.func private @norm(%arg0: tensor<128x3xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.sqrt %1 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 991, "kernel_name": "branch_yfhx", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef branch_yfhx(a):\n    # Branching with where\n    # if a > -0.86: a + 2.3 else: a * 1.1\n    return jnp.where(a > -0.86, a + 2.3, a * 1.1)", "type": "generate_branch_kernel", "cpp": "module @jit_branch_yfhx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-8.600000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.300000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.100000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_branch_yfhx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-8.600000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<2.300000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<1.100000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 992, "kernel_name": "elementwise_vqck", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef elementwise_vqck(a, b):\n    # Elementwise +\n    return a + b", "type": "generate_simple_elementwise", "cpp": "module @jit_elementwise_vqck attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_elementwise_vqck attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 993, "kernel_name": "multi_txlg", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_txlg(a, b):\n    # Multi-statement\n    temp1 = a + b\n    temp2 = jnp.cos(temp1)\n    return temp2 + a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_txlg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.cosine %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_txlg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.cosine %0 : tensor<128xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
{"id": 994, "kernel_name": "vec_ktws", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef vec_ktws(a, b):\n    # Vector dot product (row-wise for N x 3 arrays)\n    # a and b are expected to be shape (N, 3)\n    # contract over last axis\n    return jnp.sum(a * b, axis=-1)", "type": "generate_vector_kernel", "cpp": "module @jit_vec_ktws attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_vec_ktws attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x3xf32>, %arg1: tensor<128x3xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<128x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<128x3xf32>, tensor<f32>) -> tensor<128xf32>\n    return %1 : tensor<128xf32>\n  }\n}\n"}
{"id": 995, "kernel_name": "scalar_arr_crrb", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_crrb(alpha, x, y):\n    # Scalar-array operations\n    return alpha - x * y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_crrb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<128xf32>\n    %1 = stablehlo.convert %arg0 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %2, %0 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_crrb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<128xf32>\n    %1 = stablehlo.convert %arg0 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %2, %0 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 996, "kernel_name": "scalar_arr_cgdm", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef scalar_arr_cgdm(alpha, x, y):\n    # Scalar-array operations\n    return alpha - x * y", "type": "generate_scalar_array_op", "cpp": "module @jit_scalar_arr_cgdm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<128xf32>\n    %1 = stablehlo.convert %arg0 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %2, %0 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_scalar_arr_cgdm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<128xf32>\n    %1 = stablehlo.convert %arg0 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.subtract %2, %0 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 997, "kernel_name": "unary_rerb", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef unary_rerb(a):\n    # Unary operation jnp.cos\n    return jnp.cos(a)", "type": "generate_unary_kernel", "cpp": "module @jit_unary_rerb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.cosine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_unary_rerb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.cosine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n"}
{"id": 998, "kernel_name": "loop_hogj", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef loop_hogj(a, n):\n    # Loop operation\n    def body_fun(i, val):\n        return val - a\n    \n    # We apply the body_fun n times starting from 0.0\n    # Since 'a' is an array, this operates elementwise across 'a'\n    init_val = jnp.zeros_like(a)\n    return jax.lax.fori_loop(0, n, body_fun, init_val)", "type": "generate_loop_kernel", "cpp": "module @jit_loop_hogj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_loop_hogj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<i32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.subtract %iterArg_2, %iterArg : tensor<128xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<128xf32>, tensor<i32>, tensor<i32>, tensor<128xf32>\n    }\n    return %1#3 : tensor<128xf32>\n  }\n}\n"}
{"id": 999, "kernel_name": "multi_llau", "python": "import jax\nimport jax.numpy as jnp\n\n@jax.jit\ndef multi_llau(a, b):\n    # Multi-statement\n    temp1 = a + b\n    temp2 = jnp.sqrt(temp1)\n    return temp2 * a", "type": "generate_multi_statement_kernel", "cpp": "module @jit_multi_llau attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sqrt %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n", "cuda": "module @jit_multi_llau attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>, %arg1: tensor<128xf32>) -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    %1 = stablehlo.sqrt %0 : tensor<128xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<128xf32>\n    return %2 : tensor<128xf32>\n  }\n}\n"}
