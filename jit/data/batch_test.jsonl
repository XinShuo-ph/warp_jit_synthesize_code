{"id": 0, "function_name": "elementwise_bfsd", "python": "def elementwise_bfsd(a, b):\n    \"\"\"Elementwise * operation.\"\"\"\n    return a * b", "jaxpr": "{ lambda ; a:f32[4] b:f32[4]. let c:f32[4] = mul a b in (c,) }\n\n# Gradient extraction failed: Gradient only defined for scalar-output functions. Output had shape: (4,).", "type": "generate_simple_elementwise", "hlo": "module @jit_elementwise_bfsd attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n"}
{"id": 1, "function_name": "scalar_arr_biah", "python": "def scalar_arr_biah(alpha, x, y):\n    \"\"\"Scalar-array operation: alpha / x * y.\"\"\"\n    return alpha / x * y", "jaxpr": "{ lambda ; a:f32[] b:f32[4] c:f32[4]. let\n    d:f32[] = convert_element_type[new_dtype=float32 weak_type=False] a\n    e:f32[4] = div d b\n    f:f32[4] = mul e c\n  in (f,) }\n\n# Gradient extraction failed: Gradient only defined for scalar-output functions. Output had shape: (4,).", "type": "generate_scalar_array_op", "hlo": "module @jit_scalar_arr_biah attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<4xf32>, %arg2: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %2 = stablehlo.divide %1, %arg1 : tensor<4xf32>\n    %3 = stablehlo.multiply %2, %arg2 : tensor<4xf32>\n    return %3 : tensor<4xf32>\n  }\n}\n"}
{"id": 2, "function_name": "vec_tqkf", "python": "def vec_tqkf(a):\n    \"\"\"Vector norm operation.\"\"\"\n    return jnp.linalg.norm(a, axis=-1)", "jaxpr": "{ lambda ; a:f32[4]. let\n    b:f32[] = jit[\n      name=norm\n      jaxpr={ lambda ; a:f32[4]. let\n          c:f32[4] = mul a a\n          d:f32[] = reduce_sum[axes=(0,) out_sharding=None] c\n          b:f32[] = sqrt d\n        in (b,) }\n    ] a\n  in (b,) }\n\n# Backward (Gradient) JAXPR:\n{ lambda ; a:f32[4]. let\n    _:f32[] b:f32[] = jit[\n      name=norm\n      jaxpr={ lambda ; a:f32[4]. let\n          c:f32[4] = mul a a\n          d:f32[] = reduce_sum[axes=(0,) out_sharding=None] c\n          _:f32[] = sqrt d\n          b:f32[] = div 0.5:f32[] _\n        in (_, b) }\n    ] a\n    e:f32[4] = jit[\n      name=norm\n      jaxpr={ lambda ; a:f32[4] b:f32[] f:f32[]. let\n          g:f32[] = mul f b\n          h:f32[4] = broadcast_in_dim[\n            broadcast_dimensions=()\n            shape=(4,)\n            sharding=None\n          ] g\n          i:f32[4] = mul a h\n          j:f32[4] = mul h a\n          e:f32[4] = add_any i j\n        in (e,) }\n    ] a b 1.0:f32[]\n  in (e,) }", "type": "generate_vector_kernel", "hlo": "module @jit_vec_tqkf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<4xf32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n  func.func private @norm(%arg0: tensor<4xf32>) -> tensor<f32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<4xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<4xf32>, tensor<f32>) -> tensor<f32>\n    %2 = stablehlo.sqrt %1 : tensor<f32>\n    return %2 : tensor<f32>\n  }\n}\n\n\n# Backward (Gradient) HLO:\nmodule @jit_vec_tqkf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<4xf32>) -> tensor<f32>\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %1 = call @norm_0(%arg0, %0, %cst) : (tensor<4xf32>, tensor<f32>, tensor<f32>) -> tensor<4xf32>\n    return %1 : tensor<4xf32>\n  }\n  func.func private @norm(%arg0: tensor<4xf32>) -> tensor<f32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<4xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<4xf32>, tensor<f32>) -> tensor<f32>\n    %2 = stablehlo.sqrt %1 : tensor<f32>\n    %cst_0 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %3 = stablehlo.divide %cst_0, %2 : tensor<f32>\n    return %3 : tensor<f32>\n  }\n  func.func private @norm_0(%arg0: tensor<4xf32>, %arg1: tensor<f32>, %arg2: tensor<f32>) -> tensor<4xf32> {\n    %0 = stablehlo.multiply %arg2, %arg1 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %2 = stablehlo.multiply %arg0, %1 : tensor<4xf32>\n    %3 = stablehlo.multiply %1, %arg0 : tensor<4xf32>\n    %4 = stablehlo.add %2, %3 : tensor<4xf32>\n    return %4 : tensor<4xf32>\n  }\n}\n"}
{"id": 3, "function_name": "scalar_arr_cvpf", "python": "def scalar_arr_cvpf(alpha, x, y):\n    \"\"\"Scalar-array operation: alpha * x - y.\"\"\"\n    return alpha * x - y", "jaxpr": "{ lambda ; a:f32[] b:f32[4] c:f32[4]. let\n    d:f32[] = convert_element_type[new_dtype=float32 weak_type=False] a\n    e:f32[4] = mul d b\n    f:f32[4] = sub e c\n  in (f,) }\n\n# Gradient extraction failed: Gradient only defined for scalar-output functions. Output had shape: (4,).", "type": "generate_scalar_array_op", "hlo": "module @jit_scalar_arr_cvpf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<4xf32>, %arg2: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<4xf32>\n    %3 = stablehlo.subtract %2, %arg2 : tensor<4xf32>\n    return %3 : tensor<4xf32>\n  }\n}\n"}
{"id": 4, "function_name": "vec_kcpd", "python": "def vec_kcpd(a):\n    \"\"\"Vector norm operation.\"\"\"\n    return jnp.linalg.norm(a, axis=-1)", "jaxpr": "{ lambda ; a:f32[4]. let\n    b:f32[] = jit[\n      name=norm\n      jaxpr={ lambda ; a:f32[4]. let\n          c:f32[4] = mul a a\n          d:f32[] = reduce_sum[axes=(0,) out_sharding=None] c\n          b:f32[] = sqrt d\n        in (b,) }\n    ] a\n  in (b,) }\n\n# Backward (Gradient) JAXPR:\n{ lambda ; a:f32[4]. let\n    _:f32[] b:f32[] = jit[\n      name=norm\n      jaxpr={ lambda ; a:f32[4]. let\n          c:f32[4] = mul a a\n          d:f32[] = reduce_sum[axes=(0,) out_sharding=None] c\n          _:f32[] = sqrt d\n          b:f32[] = div 0.5:f32[] _\n        in (_, b) }\n    ] a\n    e:f32[4] = jit[\n      name=norm\n      jaxpr={ lambda ; a:f32[4] b:f32[] f:f32[]. let\n          g:f32[] = mul f b\n          h:f32[4] = broadcast_in_dim[\n            broadcast_dimensions=()\n            shape=(4,)\n            sharding=None\n          ] g\n          i:f32[4] = mul a h\n          j:f32[4] = mul h a\n          e:f32[4] = add_any i j\n        in (e,) }\n    ] a b 1.0:f32[]\n  in (e,) }", "type": "generate_vector_kernel", "hlo": "module @jit_vec_kcpd attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<4xf32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n  func.func private @norm(%arg0: tensor<4xf32>) -> tensor<f32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<4xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<4xf32>, tensor<f32>) -> tensor<f32>\n    %2 = stablehlo.sqrt %1 : tensor<f32>\n    return %2 : tensor<f32>\n  }\n}\n\n\n# Backward (Gradient) HLO:\nmodule @jit_vec_kcpd attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<4xf32>) -> tensor<f32>\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %1 = call @norm_0(%arg0, %0, %cst) : (tensor<4xf32>, tensor<f32>, tensor<f32>) -> tensor<4xf32>\n    return %1 : tensor<4xf32>\n  }\n  func.func private @norm(%arg0: tensor<4xf32>) -> tensor<f32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<4xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<4xf32>, tensor<f32>) -> tensor<f32>\n    %2 = stablehlo.sqrt %1 : tensor<f32>\n    %cst_0 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %3 = stablehlo.divide %cst_0, %2 : tensor<f32>\n    return %3 : tensor<f32>\n  }\n  func.func private @norm_0(%arg0: tensor<4xf32>, %arg1: tensor<f32>, %arg2: tensor<f32>) -> tensor<4xf32> {\n    %0 = stablehlo.multiply %arg2, %arg1 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %2 = stablehlo.multiply %arg0, %1 : tensor<4xf32>\n    %3 = stablehlo.multiply %1, %arg0 : tensor<4xf32>\n    %4 = stablehlo.add %2, %3 : tensor<4xf32>\n    return %4 : tensor<4xf32>\n  }\n}\n"}
{"id": 5, "function_name": "vec_lsnm", "python": "def vec_lsnm(a, b):\n    \"\"\"Vector dot product operation.\"\"\"\n    return jnp.sum(a * b, axis=-1)", "jaxpr": "{ lambda ; a:f32[4] b:f32[4]. let\n    c:f32[4] = mul a b\n    d:f32[] = reduce_sum[axes=(0,) out_sharding=None] c\n  in (d,) }\n\n# Backward (Gradient) JAXPR:\n{ lambda ; a:f32[4] b:f32[4]. let\n    c:f32[4] = mul a b\n    _:f32[] = reduce_sum[axes=(0,) out_sharding=None] c\n    d:f32[4] = broadcast_in_dim[\n      broadcast_dimensions=()\n      shape=(4,)\n      sharding=None\n    ] 1.0:f32[]\n    e:f32[4] = mul d b\n  in (e,) }", "type": "generate_vector_kernel", "hlo": "module @jit_vec_lsnm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<4xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<4xf32>, tensor<f32>) -> tensor<f32>\n    return %1 : tensor<f32>\n  }\n}\n\n\n# Backward (Gradient) HLO:\nmodule @jit_vec_lsnm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %1 = stablehlo.multiply %0, %arg0 : tensor<4xf32>\n    return %1 : tensor<4xf32>\n  }\n}\n"}
{"id": 6, "function_name": "elementwise_ahtv", "python": "def elementwise_ahtv(a, b):\n    \"\"\"Elementwise - operation.\"\"\"\n    return a - b", "jaxpr": "{ lambda ; a:f32[4] b:f32[4]. let c:f32[4] = sub a b in (c,) }\n\n# Gradient extraction failed: Gradient only defined for scalar-output functions. Output had shape: (4,).", "type": "generate_simple_elementwise", "hlo": "module @jit_elementwise_ahtv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n"}
{"id": 7, "function_name": "nested_nrwf", "python": "def nested_nrwf(a):\n    \"\"\"Nested branching operation.\"\"\"\n    return jnp.where(a > -0.34,\n                     jnp.where(a > 1.17, a * 3.0, a * 2.0),\n                     a * 0.5)", "jaxpr": "let _where = { lambda ; a:bool[4] b:f32[4] c:f32[4]. let\n    d:f32[4] = select_n a c b\n  in (d,) } in\n{ lambda ; e:f32[4]. let\n    f:bool[4] = gt e -0.3400000035762787:f32[]\n    g:bool[4] = gt e 1.1699999570846558:f32[]\n    h:f32[4] = mul e 3.0:f32[]\n    i:f32[4] = mul e 2.0:f32[]\n    j:f32[4] = jit[name=_where jaxpr=_where] g h i\n    k:f32[4] = mul e 0.5:f32[]\n    l:f32[4] = jit[name=_where jaxpr=_where] f j k\n  in (l,) }\n\n# Gradient extraction failed: Gradient only defined for scalar-output functions. Output had shape: (4,).", "type": "generate_nested_branch_kernel", "hlo": "module @jit_nested_nrwf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-3.400000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xi1>\n    %cst_0 = stablehlo.constant dense<1.170000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %3 = stablehlo.compare  GT, %arg0, %2,  FLOAT : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xi1>\n    %cst_1 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<4xf32>\n    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %6 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %7 = stablehlo.multiply %arg0, %6 : tensor<4xf32>\n    %8 = call @_where(%3, %5, %7) : (tensor<4xi1>, tensor<4xf32>, tensor<4xf32>) -> tensor<4xf32>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<4xf32>\n    %11 = call @_where(%1, %8, %10) : (tensor<4xi1>, tensor<4xf32>, tensor<4xf32>) -> tensor<4xf32>\n    return %11 : tensor<4xf32>\n  }\n  func.func private @_where(%arg0: tensor<4xi1>, %arg1: tensor<4xf32>, %arg2: tensor<4xf32>) -> tensor<4xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<4xi1>, tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n"}
{"id": 8, "function_name": "loop_hubu", "python": "def loop_hubu(a):\n    \"\"\"Loop operation using jax.lax.scan.\"\"\"\n    def body_fun(carry, _):\n        return carry * a, None\n    result, _ = jax.lax.scan(body_fun, jnp.zeros_like(a), jnp.arange(10))\n    return result", "jaxpr": "{ lambda ; a:f32[4]. let\n    b:f32[4] = broadcast_in_dim[\n      broadcast_dimensions=()\n      shape=(4,)\n      sharding=None\n    ] 0.0:f32[]\n    c:i32[10] = iota[dimension=0 dtype=int32 shape=(10,) sharding=None] \n    d:f32[4] = scan[\n      _split_transpose=False\n      jaxpr={ lambda ; e:f32[4] f:f32[4] g:i32[]. let\n          h:f32[4] = mul f e\n        in (h,) }\n      length=10\n      linear=(False, False, False)\n      num_carry=1\n      num_consts=1\n      reverse=False\n      unroll=1\n    ] a b c\n  in (d,) }\n\n# Gradient extraction failed: Gradient only defined for scalar-output functions. Output had shape: (4,).", "type": "generate_loop_kernel", "hlo": "module @jit_loop_hubu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:3 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %0) : tensor<4xf32>, tensor<i32>, tensor<4xf32>\n    cond {\n      %c_2 = stablehlo.constant dense<10> : tensor<i32>\n      %2 = stablehlo.compare  LT, %iterArg_0, %c_2,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %2 = func.call @closed_call(%iterArg, %iterArg_1) : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xf32>\n      %c_2 = stablehlo.constant dense<1> : tensor<i32>\n      %3 = stablehlo.add %iterArg_0, %c_2 : tensor<i32>\n      stablehlo.return %iterArg, %3, %2 : tensor<4xf32>, tensor<i32>, tensor<4xf32>\n    }\n    return %1#2 : tensor<4xf32>\n  }\n  func.func private @closed_call(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> tensor<4xf32> {\n    %0 = stablehlo.multiply %arg1, %arg0 : tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n"}
{"id": 9, "function_name": "nested_oazh", "python": "def nested_oazh(a):\n    \"\"\"Nested branching operation.\"\"\"\n    return jnp.where(a > 0.14,\n                     jnp.where(a > 0.68, a * 3.0, a * 2.0),\n                     a * 0.5)", "jaxpr": "let _where = { lambda ; a:bool[4] b:f32[4] c:f32[4]. let\n    d:f32[4] = select_n a c b\n  in (d,) } in\n{ lambda ; e:f32[4]. let\n    f:bool[4] = gt e 0.14000000059604645:f32[]\n    g:bool[4] = gt e 0.6800000071525574:f32[]\n    h:f32[4] = mul e 3.0:f32[]\n    i:f32[4] = mul e 2.0:f32[]\n    j:f32[4] = jit[name=_where jaxpr=_where] g h i\n    k:f32[4] = mul e 0.5:f32[]\n    l:f32[4] = jit[name=_where jaxpr=_where] f j k\n  in (l,) }\n\n# Gradient extraction failed: Gradient only defined for scalar-output functions. Output had shape: (4,).", "type": "generate_nested_branch_kernel", "hlo": "module @jit_nested_oazh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.400000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xi1>\n    %cst_0 = stablehlo.constant dense<6.800000e-01> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %3 = stablehlo.compare  GT, %arg0, %2,  FLOAT : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xi1>\n    %cst_1 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<4xf32>\n    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %6 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %7 = stablehlo.multiply %arg0, %6 : tensor<4xf32>\n    %8 = call @_where(%3, %5, %7) : (tensor<4xi1>, tensor<4xf32>, tensor<4xf32>) -> tensor<4xf32>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<4xf32>\n    %11 = call @_where(%1, %8, %10) : (tensor<4xi1>, tensor<4xf32>, tensor<4xf32>) -> tensor<4xf32>\n    return %11 : tensor<4xf32>\n  }\n  func.func private @_where(%arg0: tensor<4xi1>, %arg1: tensor<4xf32>, %arg2: tensor<4xf32>) -> tensor<4xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<4xi1>, tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n"}
{"id": 10, "function_name": "vec_krsk", "python": "def vec_krsk(a, b):\n    \"\"\"Vector dot product operation.\"\"\"\n    return jnp.sum(a * b, axis=-1)", "jaxpr": "{ lambda ; a:f32[4] b:f32[4]. let\n    c:f32[4] = mul a b\n    d:f32[] = reduce_sum[axes=(0,) out_sharding=None] c\n  in (d,) }\n\n# Backward (Gradient) JAXPR:\n{ lambda ; a:f32[4] b:f32[4]. let\n    c:f32[4] = mul a b\n    _:f32[] = reduce_sum[axes=(0,) out_sharding=None] c\n    d:f32[4] = broadcast_in_dim[\n      broadcast_dimensions=()\n      shape=(4,)\n      sharding=None\n    ] 1.0:f32[]\n    e:f32[4] = mul d b\n  in (e,) }", "type": "generate_vector_kernel", "hlo": "module @jit_vec_krsk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<4xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<4xf32>, tensor<f32>) -> tensor<f32>\n    return %1 : tensor<f32>\n  }\n}\n\n\n# Backward (Gradient) HLO:\nmodule @jit_vec_krsk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %1 = stablehlo.multiply %0, %arg0 : tensor<4xf32>\n    return %1 : tensor<4xf32>\n  }\n}\n"}
{"id": 11, "function_name": "compound_otaw", "python": "def compound_otaw(a, b, scale):\n    \"\"\"Compound operation with multiple steps.\"\"\"\n    result = (a + b) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "jaxpr": "{ lambda ; a:f32[4] b:f32[4] c:f32[]. let\n    d:f32[4] = add a b\n    e:f32[] = convert_element_type[new_dtype=float32 weak_type=False] c\n    f:f32[4] = mul d e\n    g:f32[4] = floor f\n    h:f32[4] = sub f g\n    i:f32[4] = abs h\n  in (i,) }\n\n# Gradient extraction failed: Gradient only defined for scalar-output functions. Output had shape: (4,).", "type": "generate_compound_kernel", "hlo": "module @jit_compound_otaw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>, %arg2: tensor<f32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<4xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<4xf32>\n    %4 = stablehlo.floor %3 : tensor<4xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<4xf32>\n    %6 = stablehlo.abs %5 : tensor<4xf32>\n    return %6 : tensor<4xf32>\n  }\n}\n"}
{"id": 12, "function_name": "vec_kzsb", "python": "def vec_kzsb(a, b):\n    \"\"\"Vector dot product operation.\"\"\"\n    return jnp.sum(a * b, axis=-1)", "jaxpr": "{ lambda ; a:f32[4] b:f32[4]. let\n    c:f32[4] = mul a b\n    d:f32[] = reduce_sum[axes=(0,) out_sharding=None] c\n  in (d,) }\n\n# Backward (Gradient) JAXPR:\n{ lambda ; a:f32[4] b:f32[4]. let\n    c:f32[4] = mul a b\n    _:f32[] = reduce_sum[axes=(0,) out_sharding=None] c\n    d:f32[4] = broadcast_in_dim[\n      broadcast_dimensions=()\n      shape=(4,)\n      sharding=None\n    ] 1.0:f32[]\n    e:f32[4] = mul d b\n  in (e,) }", "type": "generate_vector_kernel", "hlo": "module @jit_vec_kzsb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<4xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<4xf32>, tensor<f32>) -> tensor<f32>\n    return %1 : tensor<f32>\n  }\n}\n\n\n# Backward (Gradient) HLO:\nmodule @jit_vec_kzsb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %1 = stablehlo.multiply %0, %arg0 : tensor<4xf32>\n    return %1 : tensor<4xf32>\n  }\n}\n"}
{"id": 13, "function_name": "unary_dfak", "python": "def unary_dfak(a):\n    \"\"\"Unary jnp.exp operation.\"\"\"\n    return jnp.exp(a)", "jaxpr": "{ lambda ; a:f32[4]. let b:f32[4] = exp a in (b,) }\n\n# Gradient extraction failed: Gradient only defined for scalar-output functions. Output had shape: (4,).", "type": "generate_unary_kernel", "hlo": "module @jit_unary_dfak attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.exponential %arg0 : tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n"}
{"id": 14, "function_name": "nested_njho", "python": "def nested_njho(a):\n    \"\"\"Nested branching operation.\"\"\"\n    return jnp.where(a > -0.06,\n                     jnp.where(a > 0.72, a * 3.0, a * 2.0),\n                     a * 0.5)", "jaxpr": "let _where = { lambda ; a:bool[4] b:f32[4] c:f32[4]. let\n    d:f32[4] = select_n a c b\n  in (d,) } in\n{ lambda ; e:f32[4]. let\n    f:bool[4] = gt e -0.05999999865889549:f32[]\n    g:bool[4] = gt e 0.7200000286102295:f32[]\n    h:f32[4] = mul e 3.0:f32[]\n    i:f32[4] = mul e 2.0:f32[]\n    j:f32[4] = jit[name=_where jaxpr=_where] g h i\n    k:f32[4] = mul e 0.5:f32[]\n    l:f32[4] = jit[name=_where jaxpr=_where] f j k\n  in (l,) }\n\n# Gradient extraction failed: Gradient only defined for scalar-output functions. Output had shape: (4,).", "type": "generate_nested_branch_kernel", "hlo": "module @jit_nested_njho attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-6.000000e-02> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xi1>\n    %cst_0 = stablehlo.constant dense<7.200000e-01> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %3 = stablehlo.compare  GT, %arg0, %2,  FLOAT : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xi1>\n    %cst_1 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<4xf32>\n    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %6 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %7 = stablehlo.multiply %arg0, %6 : tensor<4xf32>\n    %8 = call @_where(%3, %5, %7) : (tensor<4xi1>, tensor<4xf32>, tensor<4xf32>) -> tensor<4xf32>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<4xf32>\n    %11 = call @_where(%1, %8, %10) : (tensor<4xi1>, tensor<4xf32>, tensor<4xf32>) -> tensor<4xf32>\n    return %11 : tensor<4xf32>\n  }\n  func.func private @_where(%arg0: tensor<4xi1>, %arg1: tensor<4xf32>, %arg2: tensor<4xf32>) -> tensor<4xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<4xi1>, tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n"}
{"id": 15, "function_name": "elementwise_bxch", "python": "def elementwise_bxch(a, b):\n    \"\"\"Elementwise + operation.\"\"\"\n    return a + b", "jaxpr": "{ lambda ; a:f32[4] b:f32[4]. let c:f32[4] = add a b in (c,) }\n\n# Gradient extraction failed: Gradient only defined for scalar-output functions. Output had shape: (4,).", "type": "generate_simple_elementwise", "hlo": "module @jit_elementwise_bxch attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n"}
{"id": 16, "function_name": "branch_wqsm", "python": "def branch_wqsm(a):\n    \"\"\"Branching operation based on threshold.\"\"\"\n    return jnp.where(a > -0.25, a + 1.4, a * 2.3)", "jaxpr": "{ lambda ; a:f32[4]. let\n    b:bool[4] = gt a -0.25:f32[]\n    c:f32[4] = add a 1.399999976158142:f32[]\n    d:f32[4] = mul a 2.299999952316284:f32[]\n    e:f32[4] = jit[\n      name=_where\n      jaxpr={ lambda ; b:bool[4] c:f32[4] d:f32[4]. let\n          e:f32[4] = select_n b d c\n        in (e,) }\n    ] b c d\n  in (e,) }\n\n# Gradient extraction failed: Gradient only defined for scalar-output functions. Output had shape: (4,).", "type": "generate_branch_kernel", "hlo": "module @jit_branch_wqsm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-2.500000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xi1>\n    %cst_0 = stablehlo.constant dense<1.400000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<4xf32>\n    %cst_1 = stablehlo.constant dense<2.300000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<4xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<4xi1>, tensor<4xf32>, tensor<4xf32>) -> tensor<4xf32>\n    return %6 : tensor<4xf32>\n  }\n  func.func private @_where(%arg0: tensor<4xi1>, %arg1: tensor<4xf32>, %arg2: tensor<4xf32>) -> tensor<4xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<4xi1>, tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n"}
{"id": 17, "function_name": "loop_yggg", "python": "def loop_yggg(a):\n    \"\"\"Loop operation using jax.lax.scan.\"\"\"\n    def body_fun(carry, _):\n        return carry * a, None\n    result, _ = jax.lax.scan(body_fun, jnp.zeros_like(a), jnp.arange(5))\n    return result", "jaxpr": "{ lambda ; a:f32[4]. let\n    b:f32[4] = broadcast_in_dim[\n      broadcast_dimensions=()\n      shape=(4,)\n      sharding=None\n    ] 0.0:f32[]\n    c:i32[5] = iota[dimension=0 dtype=int32 shape=(5,) sharding=None] \n    d:f32[4] = scan[\n      _split_transpose=False\n      jaxpr={ lambda ; e:f32[4] f:f32[4] g:i32[]. let\n          h:f32[4] = mul f e\n        in (h,) }\n      length=5\n      linear=(False, False, False)\n      num_carry=1\n      num_consts=1\n      reverse=False\n      unroll=1\n    ] a b c\n  in (d,) }\n\n# Gradient extraction failed: Gradient only defined for scalar-output functions. Output had shape: (4,).", "type": "generate_loop_kernel", "hlo": "module @jit_loop_yggg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:3 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %0) : tensor<4xf32>, tensor<i32>, tensor<4xf32>\n    cond {\n      %c_2 = stablehlo.constant dense<5> : tensor<i32>\n      %2 = stablehlo.compare  LT, %iterArg_0, %c_2,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %2 = func.call @closed_call(%iterArg, %iterArg_1) : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xf32>\n      %c_2 = stablehlo.constant dense<1> : tensor<i32>\n      %3 = stablehlo.add %iterArg_0, %c_2 : tensor<i32>\n      stablehlo.return %iterArg, %3, %2 : tensor<4xf32>, tensor<i32>, tensor<4xf32>\n    }\n    return %1#2 : tensor<4xf32>\n  }\n  func.func private @closed_call(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> tensor<4xf32> {\n    %0 = stablehlo.multiply %arg1, %arg0 : tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n"}
{"id": 18, "function_name": "loop_hcyj", "python": "def loop_hcyj(a):\n    \"\"\"Loop operation using jax.lax.scan.\"\"\"\n    def body_fun(carry, _):\n        return carry - a, None\n    result, _ = jax.lax.scan(body_fun, jnp.zeros_like(a), jnp.arange(3))\n    return result", "jaxpr": "{ lambda ; a:f32[4]. let\n    b:f32[4] = broadcast_in_dim[\n      broadcast_dimensions=()\n      shape=(4,)\n      sharding=None\n    ] 0.0:f32[]\n    c:i32[3] = iota[dimension=0 dtype=int32 shape=(3,) sharding=None] \n    d:f32[4] = scan[\n      _split_transpose=False\n      jaxpr={ lambda ; e:f32[4] f:f32[4] g:i32[]. let\n          h:f32[4] = sub f e\n        in (h,) }\n      length=3\n      linear=(False, False, False)\n      num_carry=1\n      num_consts=1\n      reverse=False\n      unroll=1\n    ] a b c\n  in (d,) }\n\n# Gradient extraction failed: Gradient only defined for scalar-output functions. Output had shape: (4,).", "type": "generate_loop_kernel", "hlo": "module @jit_loop_hcyj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:3 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %0) : tensor<4xf32>, tensor<i32>, tensor<4xf32>\n    cond {\n      %c_2 = stablehlo.constant dense<3> : tensor<i32>\n      %2 = stablehlo.compare  LT, %iterArg_0, %c_2,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %2 = func.call @closed_call(%iterArg, %iterArg_1) : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xf32>\n      %c_2 = stablehlo.constant dense<1> : tensor<i32>\n      %3 = stablehlo.add %iterArg_0, %c_2 : tensor<i32>\n      stablehlo.return %iterArg, %3, %2 : tensor<4xf32>, tensor<i32>, tensor<4xf32>\n    }\n    return %1#2 : tensor<4xf32>\n  }\n  func.func private @closed_call(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> tensor<4xf32> {\n    %0 = stablehlo.subtract %arg1, %arg0 : tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n"}
{"id": 19, "function_name": "reduce_sibs", "python": "def reduce_sibs(a):\n    \"\"\"Reduction operation: jnp.sum.\"\"\"\n    return jnp.sum(a)", "jaxpr": "{ lambda ; a:f32[4]. let\n    b:f32[] = reduce_sum[axes=(0,) out_sharding=None] a\n  in (b,) }\n\n# Backward (Gradient) JAXPR:\n{ lambda ; a:f32[4]. let\n    _:f32[] = reduce_sum[axes=(0,) out_sharding=None] a\n    b:f32[4] = broadcast_in_dim[\n      broadcast_dimensions=()\n      shape=(4,)\n      sharding=None\n    ] 1.0:f32[]\n  in (b,) }", "type": "generate_reduction_kernel", "hlo": "module @jit_reduce_sibs attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<4xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n\n\n# Backward (Gradient) HLO:\nmodule @jit_reduce_sibs attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main() -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n"}
{"id": 20, "function_name": "branch_tggh", "python": "def branch_tggh(a):\n    \"\"\"Branching operation based on threshold.\"\"\"\n    return jnp.where(a > -0.2, a / 2.3, a - 1.1)", "jaxpr": "{ lambda ; a:f32[4]. let\n    b:bool[4] = gt a -0.20000000298023224:f32[]\n    c:f32[4] = div a 2.299999952316284:f32[]\n    d:f32[4] = sub a 1.100000023841858:f32[]\n    e:f32[4] = jit[\n      name=_where\n      jaxpr={ lambda ; b:bool[4] c:f32[4] d:f32[4]. let\n          e:f32[4] = select_n b d c\n        in (e,) }\n    ] b c d\n  in (e,) }\n\n# Gradient extraction failed: Gradient only defined for scalar-output functions. Output had shape: (4,).", "type": "generate_branch_kernel", "hlo": "module @jit_branch_tggh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-2.000000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xi1>\n    %cst_0 = stablehlo.constant dense<2.300000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %3 = stablehlo.divide %arg0, %2 : tensor<4xf32>\n    %cst_1 = stablehlo.constant dense<1.100000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<4xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<4xi1>, tensor<4xf32>, tensor<4xf32>) -> tensor<4xf32>\n    return %6 : tensor<4xf32>\n  }\n  func.func private @_where(%arg0: tensor<4xi1>, %arg1: tensor<4xf32>, %arg2: tensor<4xf32>) -> tensor<4xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<4xi1>, tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n"}
{"id": 21, "function_name": "branch_gchv", "python": "def branch_gchv(a):\n    \"\"\"Branching operation based on threshold.\"\"\"\n    return jnp.where(a > -0.54, a * 2.1, a / 2.0)", "jaxpr": "{ lambda ; a:f32[4]. let\n    b:bool[4] = gt a -0.5400000214576721:f32[]\n    c:f32[4] = mul a 2.0999999046325684:f32[]\n    d:f32[4] = div a 2.0:f32[]\n    e:f32[4] = jit[\n      name=_where\n      jaxpr={ lambda ; b:bool[4] c:f32[4] d:f32[4]. let\n          e:f32[4] = select_n b d c\n        in (e,) }\n    ] b c d\n  in (e,) }\n\n# Gradient extraction failed: Gradient only defined for scalar-output functions. Output had shape: (4,).", "type": "generate_branch_kernel", "hlo": "module @jit_branch_gchv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-5.400000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xi1>\n    %cst_0 = stablehlo.constant dense<2.100000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<4xf32>\n    %cst_1 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %5 = stablehlo.divide %arg0, %4 : tensor<4xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<4xi1>, tensor<4xf32>, tensor<4xf32>) -> tensor<4xf32>\n    return %6 : tensor<4xf32>\n  }\n  func.func private @_where(%arg0: tensor<4xi1>, %arg1: tensor<4xf32>, %arg2: tensor<4xf32>) -> tensor<4xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<4xi1>, tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n"}
{"id": 22, "function_name": "scalar_arr_ugha", "python": "def scalar_arr_ugha(alpha, x, y):\n    \"\"\"Scalar-array operation: alpha - x / y.\"\"\"\n    return alpha - x / y", "jaxpr": "{ lambda ; a:f32[] b:f32[4] c:f32[4]. let\n    d:f32[4] = div b c\n    e:f32[] = convert_element_type[new_dtype=float32 weak_type=False] a\n    f:f32[4] = sub e d\n  in (f,) }\n\n# Gradient extraction failed: Gradient only defined for scalar-output functions. Output had shape: (4,).", "type": "generate_scalar_array_op", "hlo": "module @jit_scalar_arr_ugha attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<4xf32>, %arg2: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.divide %arg1, %arg2 : tensor<4xf32>\n    %1 = stablehlo.convert %arg0 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %3 = stablehlo.subtract %2, %0 : tensor<4xf32>\n    return %3 : tensor<4xf32>\n  }\n}\n"}
{"id": 23, "function_name": "reduce_ilkn", "python": "def reduce_ilkn(a):\n    \"\"\"Reduction operation: jnp.sum.\"\"\"\n    return jnp.sum(a)", "jaxpr": "{ lambda ; a:f32[4]. let\n    b:f32[] = reduce_sum[axes=(0,) out_sharding=None] a\n  in (b,) }\n\n# Backward (Gradient) JAXPR:\n{ lambda ; a:f32[4]. let\n    _:f32[] = reduce_sum[axes=(0,) out_sharding=None] a\n    b:f32[4] = broadcast_in_dim[\n      broadcast_dimensions=()\n      shape=(4,)\n      sharding=None\n    ] 1.0:f32[]\n  in (b,) }", "type": "generate_reduction_kernel", "hlo": "module @jit_reduce_ilkn attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<4xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n\n\n# Backward (Gradient) HLO:\nmodule @jit_reduce_ilkn attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main() -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n"}
{"id": 24, "function_name": "scalar_arr_bebn", "python": "def scalar_arr_bebn(alpha, x, y):\n    \"\"\"Scalar-array operation: alpha - x + y.\"\"\"\n    return alpha - x + y", "jaxpr": "{ lambda ; a:f32[] b:f32[4] c:f32[4]. let\n    d:f32[] = convert_element_type[new_dtype=float32 weak_type=False] a\n    e:f32[4] = sub d b\n    f:f32[4] = add e c\n  in (f,) }\n\n# Gradient extraction failed: Gradient only defined for scalar-output functions. Output had shape: (4,).", "type": "generate_scalar_array_op", "hlo": "module @jit_scalar_arr_bebn attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<4xf32>, %arg2: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %2 = stablehlo.subtract %1, %arg1 : tensor<4xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<4xf32>\n    return %3 : tensor<4xf32>\n  }\n}\n"}
{"id": 25, "function_name": "branch_sgsx", "python": "def branch_sgsx(a):\n    \"\"\"Branching operation based on threshold.\"\"\"\n    return jnp.where(a > 0.4, a - 2.7, a * 2.4)", "jaxpr": "{ lambda ; a:f32[4]. let\n    b:bool[4] = gt a 0.4000000059604645:f32[]\n    c:f32[4] = sub a 2.700000047683716:f32[]\n    d:f32[4] = mul a 2.4000000953674316:f32[]\n    e:f32[4] = jit[\n      name=_where\n      jaxpr={ lambda ; b:bool[4] c:f32[4] d:f32[4]. let\n          e:f32[4] = select_n b d c\n        in (e,) }\n    ] b c d\n  in (e,) }\n\n# Gradient extraction failed: Gradient only defined for scalar-output functions. Output had shape: (4,).", "type": "generate_branch_kernel", "hlo": "module @jit_branch_sgsx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<4.000000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xi1>\n    %cst_0 = stablehlo.constant dense<2.700000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<4xf32>\n    %cst_1 = stablehlo.constant dense<2.400000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<4xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<4xi1>, tensor<4xf32>, tensor<4xf32>) -> tensor<4xf32>\n    return %6 : tensor<4xf32>\n  }\n  func.func private @_where(%arg0: tensor<4xi1>, %arg1: tensor<4xf32>, %arg2: tensor<4xf32>) -> tensor<4xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<4xi1>, tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n"}
{"id": 26, "function_name": "multi_mwop", "python": "def multi_mwop(a, b):\n    \"\"\"Multi-statement operation.\"\"\"\n    temp1 = a + b\n    temp2 = jnp.sin(temp1)\n    result = temp2 - a\n    return result", "jaxpr": "{ lambda ; a:f32[4] b:f32[4]. let\n    c:f32[4] = add a b\n    d:f32[4] = sin c\n    e:f32[4] = sub d a\n  in (e,) }\n\n# Gradient extraction failed: Gradient only defined for scalar-output functions. Output had shape: (4,).", "type": "generate_multi_statement_kernel", "hlo": "module @jit_multi_mwop attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<4xf32>\n    %1 = stablehlo.sine %0 : tensor<4xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<4xf32>\n    return %2 : tensor<4xf32>\n  }\n}\n"}
{"id": 27, "function_name": "vec_kvrc", "python": "def vec_kvrc(a, b):\n    \"\"\"Vector dot product operation.\"\"\"\n    return jnp.sum(a * b, axis=-1)", "jaxpr": "{ lambda ; a:f32[4] b:f32[4]. let\n    c:f32[4] = mul a b\n    d:f32[] = reduce_sum[axes=(0,) out_sharding=None] c\n  in (d,) }\n\n# Backward (Gradient) JAXPR:\n{ lambda ; a:f32[4] b:f32[4]. let\n    c:f32[4] = mul a b\n    _:f32[] = reduce_sum[axes=(0,) out_sharding=None] c\n    d:f32[4] = broadcast_in_dim[\n      broadcast_dimensions=()\n      shape=(4,)\n      sharding=None\n    ] 1.0:f32[]\n    e:f32[4] = mul d b\n  in (e,) }", "type": "generate_vector_kernel", "hlo": "module @jit_vec_kvrc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<4xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<4xf32>, tensor<f32>) -> tensor<f32>\n    return %1 : tensor<f32>\n  }\n}\n\n\n# Backward (Gradient) HLO:\nmodule @jit_vec_kvrc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %1 = stablehlo.multiply %0, %arg0 : tensor<4xf32>\n    return %1 : tensor<4xf32>\n  }\n}\n"}
{"id": 28, "function_name": "branch_xsff", "python": "def branch_xsff(a):\n    \"\"\"Branching operation based on threshold.\"\"\"\n    return jnp.where(a > -0.07, a / 2.9, a - 0.7)", "jaxpr": "{ lambda ; a:f32[4]. let\n    b:bool[4] = gt a -0.07000000029802322:f32[]\n    c:f32[4] = div a 2.9000000953674316:f32[]\n    d:f32[4] = sub a 0.699999988079071:f32[]\n    e:f32[4] = jit[\n      name=_where\n      jaxpr={ lambda ; b:bool[4] c:f32[4] d:f32[4]. let\n          e:f32[4] = select_n b d c\n        in (e,) }\n    ] b c d\n  in (e,) }\n\n# Gradient extraction failed: Gradient only defined for scalar-output functions. Output had shape: (4,).", "type": "generate_branch_kernel", "hlo": "module @jit_branch_xsff attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-7.000000e-02> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xi1>\n    %cst_0 = stablehlo.constant dense<2.900000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %3 = stablehlo.divide %arg0, %2 : tensor<4xf32>\n    %cst_1 = stablehlo.constant dense<0.699999988> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<4xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<4xi1>, tensor<4xf32>, tensor<4xf32>) -> tensor<4xf32>\n    return %6 : tensor<4xf32>\n  }\n  func.func private @_where(%arg0: tensor<4xi1>, %arg1: tensor<4xf32>, %arg2: tensor<4xf32>) -> tensor<4xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<4xi1>, tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n"}
{"id": 29, "function_name": "nested_nfnu", "python": "def nested_nfnu(a):\n    \"\"\"Nested branching operation.\"\"\"\n    return jnp.where(a > 0.33,\n                     jnp.where(a > 1.34, a * 3.0, a * 2.0),\n                     a * 0.5)", "jaxpr": "let _where = { lambda ; a:bool[4] b:f32[4] c:f32[4]. let\n    d:f32[4] = select_n a c b\n  in (d,) } in\n{ lambda ; e:f32[4]. let\n    f:bool[4] = gt e 0.33000001311302185:f32[]\n    g:bool[4] = gt e 1.340000033378601:f32[]\n    h:f32[4] = mul e 3.0:f32[]\n    i:f32[4] = mul e 2.0:f32[]\n    j:f32[4] = jit[name=_where jaxpr=_where] g h i\n    k:f32[4] = mul e 0.5:f32[]\n    l:f32[4] = jit[name=_where jaxpr=_where] f j k\n  in (l,) }\n\n# Gradient extraction failed: Gradient only defined for scalar-output functions. Output had shape: (4,).", "type": "generate_nested_branch_kernel", "hlo": "module @jit_nested_nfnu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<3.300000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xi1>\n    %cst_0 = stablehlo.constant dense<1.340000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %3 = stablehlo.compare  GT, %arg0, %2,  FLOAT : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xi1>\n    %cst_1 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<4xf32>\n    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %6 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %7 = stablehlo.multiply %arg0, %6 : tensor<4xf32>\n    %8 = call @_where(%3, %5, %7) : (tensor<4xi1>, tensor<4xf32>, tensor<4xf32>) -> tensor<4xf32>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<4xf32>\n    %11 = call @_where(%1, %8, %10) : (tensor<4xi1>, tensor<4xf32>, tensor<4xf32>) -> tensor<4xf32>\n    return %11 : tensor<4xf32>\n  }\n  func.func private @_where(%arg0: tensor<4xi1>, %arg1: tensor<4xf32>, %arg2: tensor<4xf32>) -> tensor<4xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<4xi1>, tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n"}
{"id": 30, "function_name": "unary_shyx", "python": "def unary_shyx(a):\n    \"\"\"Unary jnp.exp operation.\"\"\"\n    return jnp.exp(a)", "jaxpr": "{ lambda ; a:f32[4]. let b:f32[4] = exp a in (b,) }\n\n# Gradient extraction failed: Gradient only defined for scalar-output functions. Output had shape: (4,).", "type": "generate_unary_kernel", "hlo": "module @jit_unary_shyx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.exponential %arg0 : tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n"}
{"id": 31, "function_name": "elementwise_wzvw", "python": "def elementwise_wzvw(a, b):\n    \"\"\"Elementwise * operation.\"\"\"\n    return a * b", "jaxpr": "{ lambda ; a:f32[4] b:f32[4]. let c:f32[4] = mul a b in (c,) }\n\n# Gradient extraction failed: Gradient only defined for scalar-output functions. Output had shape: (4,).", "type": "generate_simple_elementwise", "hlo": "module @jit_elementwise_wzvw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n"}
{"id": 32, "function_name": "loop_hgcw", "python": "def loop_hgcw(a):\n    \"\"\"Loop operation using jax.lax.scan.\"\"\"\n    def body_fun(carry, _):\n        return carry * a, None\n    result, _ = jax.lax.scan(body_fun, jnp.zeros_like(a), jnp.arange(7))\n    return result", "jaxpr": "{ lambda ; a:f32[4]. let\n    b:f32[4] = broadcast_in_dim[\n      broadcast_dimensions=()\n      shape=(4,)\n      sharding=None\n    ] 0.0:f32[]\n    c:i32[7] = iota[dimension=0 dtype=int32 shape=(7,) sharding=None] \n    d:f32[4] = scan[\n      _split_transpose=False\n      jaxpr={ lambda ; e:f32[4] f:f32[4] g:i32[]. let\n          h:f32[4] = mul f e\n        in (h,) }\n      length=7\n      linear=(False, False, False)\n      num_carry=1\n      num_consts=1\n      reverse=False\n      unroll=1\n    ] a b c\n  in (d,) }\n\n# Gradient extraction failed: Gradient only defined for scalar-output functions. Output had shape: (4,).", "type": "generate_loop_kernel", "hlo": "module @jit_loop_hgcw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:3 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %0) : tensor<4xf32>, tensor<i32>, tensor<4xf32>\n    cond {\n      %c_2 = stablehlo.constant dense<7> : tensor<i32>\n      %2 = stablehlo.compare  LT, %iterArg_0, %c_2,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %2 = func.call @closed_call(%iterArg, %iterArg_1) : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xf32>\n      %c_2 = stablehlo.constant dense<1> : tensor<i32>\n      %3 = stablehlo.add %iterArg_0, %c_2 : tensor<i32>\n      stablehlo.return %iterArg, %3, %2 : tensor<4xf32>, tensor<i32>, tensor<4xf32>\n    }\n    return %1#2 : tensor<4xf32>\n  }\n  func.func private @closed_call(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> tensor<4xf32> {\n    %0 = stablehlo.multiply %arg1, %arg0 : tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n"}
{"id": 33, "function_name": "unary_rqnu", "python": "def unary_rqnu(a):\n    \"\"\"Unary jnp.cos operation.\"\"\"\n    return jnp.cos(a)", "jaxpr": "{ lambda ; a:f32[4]. let b:f32[4] = cos a in (b,) }\n\n# Gradient extraction failed: Gradient only defined for scalar-output functions. Output had shape: (4,).", "type": "generate_unary_kernel", "hlo": "module @jit_unary_rqnu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.cosine %arg0 : tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n"}
{"id": 34, "function_name": "reduce_iwwy", "python": "def reduce_iwwy(a):\n    \"\"\"Reduction operation: jnp.min.\"\"\"\n    return jnp.min(a)", "jaxpr": "{ lambda ; a:f32[4]. let b:f32[] = reduce_min[axes=(0,)] a in (b,) }\n\n# Backward (Gradient) JAXPR:\n{ lambda ; a:f32[4]. let\n    b:f32[] = reduce_min[axes=(0,)] a\n    c:f32[1] = reshape[dimensions=None new_sizes=(1,) sharding=None] b\n    d:bool[4] = eq a c\n    e:f32[4] = convert_element_type[new_dtype=float32 weak_type=False] d\n    f:f32[] = reduce_sum[axes=(0,) out_sharding=None] e\n    g:f32[] = div 1.0:f32[] f\n    h:f32[4] = broadcast_in_dim[\n      broadcast_dimensions=()\n      shape=(4,)\n      sharding=None\n    ] g\n    i:f32[4] = mul h e\n  in (i,) }", "type": "generate_reduction_kernel", "hlo": "module @jit_reduce_iwwy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0x7F800000> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.minimum across dimensions = [0] : (tensor<4xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n\n\n# Backward (Gradient) HLO:\nmodule @jit_reduce_iwwy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0x7F800000> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.minimum across dimensions = [0] : (tensor<4xf32>, tensor<f32>) -> tensor<f32>\n    %1 = stablehlo.reshape %0 : (tensor<f32>) -> tensor<1xf32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [0] : (tensor<1xf32>) -> tensor<4xf32>\n    %3 = stablehlo.compare  EQ, %arg0, %2,  FLOAT : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xi1>\n    %4 = stablehlo.convert %3 : (tensor<4xi1>) -> tensor<4xf32>\n    %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %5 = stablehlo.reduce(%4 init: %cst_0) applies stablehlo.add across dimensions = [0] : (tensor<4xf32>, tensor<f32>) -> tensor<f32>\n    %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %6 = stablehlo.divide %cst_1, %5 : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %6, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %8 = stablehlo.multiply %7, %4 : tensor<4xf32>\n    return %8 : tensor<4xf32>\n  }\n}\n"}
{"id": 35, "function_name": "reduce_jnab", "python": "def reduce_jnab(a):\n    \"\"\"Reduction operation: jnp.sum.\"\"\"\n    return jnp.sum(a)", "jaxpr": "{ lambda ; a:f32[4]. let\n    b:f32[] = reduce_sum[axes=(0,) out_sharding=None] a\n  in (b,) }\n\n# Backward (Gradient) JAXPR:\n{ lambda ; a:f32[4]. let\n    _:f32[] = reduce_sum[axes=(0,) out_sharding=None] a\n    b:f32[4] = broadcast_in_dim[\n      broadcast_dimensions=()\n      shape=(4,)\n      sharding=None\n    ] 1.0:f32[]\n  in (b,) }", "type": "generate_reduction_kernel", "hlo": "module @jit_reduce_jnab attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<4xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n\n\n# Backward (Gradient) HLO:\nmodule @jit_reduce_jnab attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main() -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n"}
{"id": 36, "function_name": "reduce_rmtc", "python": "def reduce_rmtc(a):\n    \"\"\"Reduction operation: jnp.mean.\"\"\"\n    return jnp.mean(a)", "jaxpr": "{ lambda ; a:f32[4]. let\n    b:f32[] = reduce_sum[axes=(0,) out_sharding=None] a\n    c:f32[] = div b 4.0:f32[]\n  in (c,) }\n\n# Backward (Gradient) JAXPR:\n{ lambda ; a:f32[4]. let\n    b:f32[] = reduce_sum[axes=(0,) out_sharding=None] a\n    _:f32[] = div b 4.0:f32[]\n    c:f32[] = div 1.0:f32[] 4.0:f32[]\n    d:f32[4] = broadcast_in_dim[\n      broadcast_dimensions=()\n      shape=(4,)\n      sharding=None\n    ] c\n  in (d,) }", "type": "generate_reduction_kernel", "hlo": "module @jit_reduce_rmtc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<4xf32>, tensor<f32>) -> tensor<f32>\n    %cst_0 = stablehlo.constant dense<4.000000e+00> : tensor<f32>\n    %1 = stablehlo.divide %0, %cst_0 : tensor<f32>\n    return %1 : tensor<f32>\n  }\n}\n\n\n# Backward (Gradient) HLO:\nmodule @jit_reduce_rmtc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main() -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %cst_0 = stablehlo.constant dense<4.000000e+00> : tensor<f32>\n    %0 = stablehlo.divide %cst, %cst_0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    return %1 : tensor<4xf32>\n  }\n}\n"}
{"id": 37, "function_name": "compound_qxzu", "python": "def compound_qxzu(a, b, scale):\n    \"\"\"Compound operation with multiple steps.\"\"\"\n    result = (a + b) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "jaxpr": "{ lambda ; a:f32[4] b:f32[4] c:f32[]. let\n    d:f32[4] = add a b\n    e:f32[] = convert_element_type[new_dtype=float32 weak_type=False] c\n    f:f32[4] = mul d e\n    g:f32[4] = floor f\n    h:f32[4] = sub f g\n    i:f32[4] = abs h\n  in (i,) }\n\n# Gradient extraction failed: Gradient only defined for scalar-output functions. Output had shape: (4,).", "type": "generate_compound_kernel", "hlo": "module @jit_compound_qxzu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>, %arg2: tensor<f32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<4xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<4xf32>\n    %4 = stablehlo.floor %3 : tensor<4xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<4xf32>\n    %6 = stablehlo.abs %5 : tensor<4xf32>\n    return %6 : tensor<4xf32>\n  }\n}\n"}
{"id": 38, "function_name": "reduce_ucsb", "python": "def reduce_ucsb(a):\n    \"\"\"Reduction operation: jnp.mean.\"\"\"\n    return jnp.mean(a)", "jaxpr": "{ lambda ; a:f32[4]. let\n    b:f32[] = reduce_sum[axes=(0,) out_sharding=None] a\n    c:f32[] = div b 4.0:f32[]\n  in (c,) }\n\n# Backward (Gradient) JAXPR:\n{ lambda ; a:f32[4]. let\n    b:f32[] = reduce_sum[axes=(0,) out_sharding=None] a\n    _:f32[] = div b 4.0:f32[]\n    c:f32[] = div 1.0:f32[] 4.0:f32[]\n    d:f32[4] = broadcast_in_dim[\n      broadcast_dimensions=()\n      shape=(4,)\n      sharding=None\n    ] c\n  in (d,) }", "type": "generate_reduction_kernel", "hlo": "module @jit_reduce_ucsb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<4xf32>, tensor<f32>) -> tensor<f32>\n    %cst_0 = stablehlo.constant dense<4.000000e+00> : tensor<f32>\n    %1 = stablehlo.divide %0, %cst_0 : tensor<f32>\n    return %1 : tensor<f32>\n  }\n}\n\n\n# Backward (Gradient) HLO:\nmodule @jit_reduce_ucsb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main() -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %cst_0 = stablehlo.constant dense<4.000000e+00> : tensor<f32>\n    %0 = stablehlo.divide %cst, %cst_0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    return %1 : tensor<4xf32>\n  }\n}\n"}
{"id": 39, "function_name": "scalar_arr_dvdc", "python": "def scalar_arr_dvdc(alpha, x, y):\n    \"\"\"Scalar-array operation: alpha + x * y.\"\"\"\n    return alpha + x * y", "jaxpr": "{ lambda ; a:f32[] b:f32[4] c:f32[4]. let\n    d:f32[4] = mul b c\n    e:f32[] = convert_element_type[new_dtype=float32 weak_type=False] a\n    f:f32[4] = add e d\n  in (f,) }\n\n# Gradient extraction failed: Gradient only defined for scalar-output functions. Output had shape: (4,).", "type": "generate_scalar_array_op", "hlo": "module @jit_scalar_arr_dvdc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<4xf32>, %arg2: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<4xf32>\n    %1 = stablehlo.convert %arg0 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %3 = stablehlo.add %2, %0 : tensor<4xf32>\n    return %3 : tensor<4xf32>\n  }\n}\n"}
{"id": 40, "function_name": "compound_wbyi", "python": "def compound_wbyi(a, b, scale):\n    \"\"\"Compound operation with multiple steps.\"\"\"\n    result = (a + b) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "jaxpr": "{ lambda ; a:f32[4] b:f32[4] c:f32[]. let\n    d:f32[4] = add a b\n    e:f32[] = convert_element_type[new_dtype=float32 weak_type=False] c\n    f:f32[4] = mul d e\n    g:f32[4] = floor f\n    h:f32[4] = sub f g\n    i:f32[4] = abs h\n  in (i,) }\n\n# Gradient extraction failed: Gradient only defined for scalar-output functions. Output had shape: (4,).", "type": "generate_compound_kernel", "hlo": "module @jit_compound_wbyi attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>, %arg2: tensor<f32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<4xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<4xf32>\n    %4 = stablehlo.floor %3 : tensor<4xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<4xf32>\n    %6 = stablehlo.abs %5 : tensor<4xf32>\n    return %6 : tensor<4xf32>\n  }\n}\n"}
{"id": 41, "function_name": "nested_rtjs", "python": "def nested_rtjs(a):\n    \"\"\"Nested branching operation.\"\"\"\n    return jnp.where(a > 0.21,\n                     jnp.where(a > 0.74, a * 3.0, a * 2.0),\n                     a * 0.5)", "jaxpr": "let _where = { lambda ; a:bool[4] b:f32[4] c:f32[4]. let\n    d:f32[4] = select_n a c b\n  in (d,) } in\n{ lambda ; e:f32[4]. let\n    f:bool[4] = gt e 0.20999999344348907:f32[]\n    g:bool[4] = gt e 0.7400000095367432:f32[]\n    h:f32[4] = mul e 3.0:f32[]\n    i:f32[4] = mul e 2.0:f32[]\n    j:f32[4] = jit[name=_where jaxpr=_where] g h i\n    k:f32[4] = mul e 0.5:f32[]\n    l:f32[4] = jit[name=_where jaxpr=_where] f j k\n  in (l,) }\n\n# Gradient extraction failed: Gradient only defined for scalar-output functions. Output had shape: (4,).", "type": "generate_nested_branch_kernel", "hlo": "module @jit_nested_rtjs attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<2.100000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xi1>\n    %cst_0 = stablehlo.constant dense<7.400000e-01> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %3 = stablehlo.compare  GT, %arg0, %2,  FLOAT : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xi1>\n    %cst_1 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<4xf32>\n    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %6 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %7 = stablehlo.multiply %arg0, %6 : tensor<4xf32>\n    %8 = call @_where(%3, %5, %7) : (tensor<4xi1>, tensor<4xf32>, tensor<4xf32>) -> tensor<4xf32>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<4xf32>\n    %11 = call @_where(%1, %8, %10) : (tensor<4xi1>, tensor<4xf32>, tensor<4xf32>) -> tensor<4xf32>\n    return %11 : tensor<4xf32>\n  }\n  func.func private @_where(%arg0: tensor<4xi1>, %arg1: tensor<4xf32>, %arg2: tensor<4xf32>) -> tensor<4xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<4xi1>, tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n"}
{"id": 42, "function_name": "reduce_imzq", "python": "def reduce_imzq(a):\n    \"\"\"Reduction operation: jnp.mean.\"\"\"\n    return jnp.mean(a)", "jaxpr": "{ lambda ; a:f32[4]. let\n    b:f32[] = reduce_sum[axes=(0,) out_sharding=None] a\n    c:f32[] = div b 4.0:f32[]\n  in (c,) }\n\n# Backward (Gradient) JAXPR:\n{ lambda ; a:f32[4]. let\n    b:f32[] = reduce_sum[axes=(0,) out_sharding=None] a\n    _:f32[] = div b 4.0:f32[]\n    c:f32[] = div 1.0:f32[] 4.0:f32[]\n    d:f32[4] = broadcast_in_dim[\n      broadcast_dimensions=()\n      shape=(4,)\n      sharding=None\n    ] c\n  in (d,) }", "type": "generate_reduction_kernel", "hlo": "module @jit_reduce_imzq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<4xf32>, tensor<f32>) -> tensor<f32>\n    %cst_0 = stablehlo.constant dense<4.000000e+00> : tensor<f32>\n    %1 = stablehlo.divide %0, %cst_0 : tensor<f32>\n    return %1 : tensor<f32>\n  }\n}\n\n\n# Backward (Gradient) HLO:\nmodule @jit_reduce_imzq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main() -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %cst_0 = stablehlo.constant dense<4.000000e+00> : tensor<f32>\n    %0 = stablehlo.divide %cst, %cst_0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    return %1 : tensor<4xf32>\n  }\n}\n"}
{"id": 43, "function_name": "nested_znll", "python": "def nested_znll(a):\n    \"\"\"Nested branching operation.\"\"\"\n    return jnp.where(a > -0.19,\n                     jnp.where(a > 0.76, a * 3.0, a * 2.0),\n                     a * 0.5)", "jaxpr": "let _where = { lambda ; a:bool[4] b:f32[4] c:f32[4]. let\n    d:f32[4] = select_n a c b\n  in (d,) } in\n{ lambda ; e:f32[4]. let\n    f:bool[4] = gt e -0.1899999976158142:f32[]\n    g:bool[4] = gt e 0.7599999904632568:f32[]\n    h:f32[4] = mul e 3.0:f32[]\n    i:f32[4] = mul e 2.0:f32[]\n    j:f32[4] = jit[name=_where jaxpr=_where] g h i\n    k:f32[4] = mul e 0.5:f32[]\n    l:f32[4] = jit[name=_where jaxpr=_where] f j k\n  in (l,) }\n\n# Gradient extraction failed: Gradient only defined for scalar-output functions. Output had shape: (4,).", "type": "generate_nested_branch_kernel", "hlo": "module @jit_nested_znll attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-1.900000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xi1>\n    %cst_0 = stablehlo.constant dense<7.600000e-01> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %3 = stablehlo.compare  GT, %arg0, %2,  FLOAT : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xi1>\n    %cst_1 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<4xf32>\n    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %6 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %7 = stablehlo.multiply %arg0, %6 : tensor<4xf32>\n    %8 = call @_where(%3, %5, %7) : (tensor<4xi1>, tensor<4xf32>, tensor<4xf32>) -> tensor<4xf32>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<4xf32>\n    %11 = call @_where(%1, %8, %10) : (tensor<4xi1>, tensor<4xf32>, tensor<4xf32>) -> tensor<4xf32>\n    return %11 : tensor<4xf32>\n  }\n  func.func private @_where(%arg0: tensor<4xi1>, %arg1: tensor<4xf32>, %arg2: tensor<4xf32>) -> tensor<4xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<4xi1>, tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n"}
{"id": 44, "function_name": "vec_yurj", "python": "def vec_yurj(a):\n    \"\"\"Vector norm operation.\"\"\"\n    return jnp.linalg.norm(a, axis=-1)", "jaxpr": "{ lambda ; a:f32[4]. let\n    b:f32[] = jit[\n      name=norm\n      jaxpr={ lambda ; a:f32[4]. let\n          c:f32[4] = mul a a\n          d:f32[] = reduce_sum[axes=(0,) out_sharding=None] c\n          b:f32[] = sqrt d\n        in (b,) }\n    ] a\n  in (b,) }\n\n# Backward (Gradient) JAXPR:\n{ lambda ; a:f32[4]. let\n    _:f32[] b:f32[] = jit[\n      name=norm\n      jaxpr={ lambda ; a:f32[4]. let\n          c:f32[4] = mul a a\n          d:f32[] = reduce_sum[axes=(0,) out_sharding=None] c\n          _:f32[] = sqrt d\n          b:f32[] = div 0.5:f32[] _\n        in (_, b) }\n    ] a\n    e:f32[4] = jit[\n      name=norm\n      jaxpr={ lambda ; a:f32[4] b:f32[] f:f32[]. let\n          g:f32[] = mul f b\n          h:f32[4] = broadcast_in_dim[\n            broadcast_dimensions=()\n            shape=(4,)\n            sharding=None\n          ] g\n          i:f32[4] = mul a h\n          j:f32[4] = mul h a\n          e:f32[4] = add_any i j\n        in (e,) }\n    ] a b 1.0:f32[]\n  in (e,) }", "type": "generate_vector_kernel", "hlo": "module @jit_vec_yurj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<4xf32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n  func.func private @norm(%arg0: tensor<4xf32>) -> tensor<f32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<4xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<4xf32>, tensor<f32>) -> tensor<f32>\n    %2 = stablehlo.sqrt %1 : tensor<f32>\n    return %2 : tensor<f32>\n  }\n}\n\n\n# Backward (Gradient) HLO:\nmodule @jit_vec_yurj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<4xf32>) -> tensor<f32>\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %1 = call @norm_0(%arg0, %0, %cst) : (tensor<4xf32>, tensor<f32>, tensor<f32>) -> tensor<4xf32>\n    return %1 : tensor<4xf32>\n  }\n  func.func private @norm(%arg0: tensor<4xf32>) -> tensor<f32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<4xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<4xf32>, tensor<f32>) -> tensor<f32>\n    %2 = stablehlo.sqrt %1 : tensor<f32>\n    %cst_0 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %3 = stablehlo.divide %cst_0, %2 : tensor<f32>\n    return %3 : tensor<f32>\n  }\n  func.func private @norm_0(%arg0: tensor<4xf32>, %arg1: tensor<f32>, %arg2: tensor<f32>) -> tensor<4xf32> {\n    %0 = stablehlo.multiply %arg2, %arg1 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %2 = stablehlo.multiply %arg0, %1 : tensor<4xf32>\n    %3 = stablehlo.multiply %1, %arg0 : tensor<4xf32>\n    %4 = stablehlo.add %2, %3 : tensor<4xf32>\n    return %4 : tensor<4xf32>\n  }\n}\n"}
{"id": 45, "function_name": "unary_dmlv", "python": "def unary_dmlv(a):\n    \"\"\"Unary jnp.exp operation.\"\"\"\n    return jnp.exp(a)", "jaxpr": "{ lambda ; a:f32[4]. let b:f32[4] = exp a in (b,) }\n\n# Gradient extraction failed: Gradient only defined for scalar-output functions. Output had shape: (4,).", "type": "generate_unary_kernel", "hlo": "module @jit_unary_dmlv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.exponential %arg0 : tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n"}
{"id": 46, "function_name": "elementwise_ynsb", "python": "def elementwise_ynsb(a, b):\n    \"\"\"Elementwise / operation.\"\"\"\n    return a / b", "jaxpr": "{ lambda ; a:f32[4] b:f32[4]. let c:f32[4] = div a b in (c,) }\n\n# Gradient extraction failed: Gradient only defined for scalar-output functions. Output had shape: (4,).", "type": "generate_simple_elementwise", "hlo": "module @jit_elementwise_ynsb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.divide %arg0, %arg1 : tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n"}
{"id": 47, "function_name": "multi_laww", "python": "def multi_laww(a, b):\n    \"\"\"Multi-statement operation.\"\"\"\n    temp1 = a + b\n    temp2 = jnp.abs(temp1)\n    result = temp2 * a\n    return result", "jaxpr": "{ lambda ; a:f32[4] b:f32[4]. let\n    c:f32[4] = add a b\n    d:f32[4] = abs c\n    e:f32[4] = mul d a\n  in (e,) }\n\n# Gradient extraction failed: Gradient only defined for scalar-output functions. Output had shape: (4,).", "type": "generate_multi_statement_kernel", "hlo": "module @jit_multi_laww attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<4xf32>\n    %1 = stablehlo.abs %0 : tensor<4xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<4xf32>\n    return %2 : tensor<4xf32>\n  }\n}\n"}
{"id": 48, "function_name": "vec_vsus", "python": "def vec_vsus(a):\n    \"\"\"Vector norm operation.\"\"\"\n    return jnp.linalg.norm(a, axis=-1)", "jaxpr": "{ lambda ; a:f32[4]. let\n    b:f32[] = jit[\n      name=norm\n      jaxpr={ lambda ; a:f32[4]. let\n          c:f32[4] = mul a a\n          d:f32[] = reduce_sum[axes=(0,) out_sharding=None] c\n          b:f32[] = sqrt d\n        in (b,) }\n    ] a\n  in (b,) }\n\n# Backward (Gradient) JAXPR:\n{ lambda ; a:f32[4]. let\n    _:f32[] b:f32[] = jit[\n      name=norm\n      jaxpr={ lambda ; a:f32[4]. let\n          c:f32[4] = mul a a\n          d:f32[] = reduce_sum[axes=(0,) out_sharding=None] c\n          _:f32[] = sqrt d\n          b:f32[] = div 0.5:f32[] _\n        in (_, b) }\n    ] a\n    e:f32[4] = jit[\n      name=norm\n      jaxpr={ lambda ; a:f32[4] b:f32[] f:f32[]. let\n          g:f32[] = mul f b\n          h:f32[4] = broadcast_in_dim[\n            broadcast_dimensions=()\n            shape=(4,)\n            sharding=None\n          ] g\n          i:f32[4] = mul a h\n          j:f32[4] = mul h a\n          e:f32[4] = add_any i j\n        in (e,) }\n    ] a b 1.0:f32[]\n  in (e,) }", "type": "generate_vector_kernel", "hlo": "module @jit_vec_vsus attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<4xf32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n  func.func private @norm(%arg0: tensor<4xf32>) -> tensor<f32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<4xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<4xf32>, tensor<f32>) -> tensor<f32>\n    %2 = stablehlo.sqrt %1 : tensor<f32>\n    return %2 : tensor<f32>\n  }\n}\n\n\n# Backward (Gradient) HLO:\nmodule @jit_vec_vsus attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<4xf32>) -> tensor<f32>\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %1 = call @norm_0(%arg0, %0, %cst) : (tensor<4xf32>, tensor<f32>, tensor<f32>) -> tensor<4xf32>\n    return %1 : tensor<4xf32>\n  }\n  func.func private @norm(%arg0: tensor<4xf32>) -> tensor<f32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<4xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<4xf32>, tensor<f32>) -> tensor<f32>\n    %2 = stablehlo.sqrt %1 : tensor<f32>\n    %cst_0 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %3 = stablehlo.divide %cst_0, %2 : tensor<f32>\n    return %3 : tensor<f32>\n  }\n  func.func private @norm_0(%arg0: tensor<4xf32>, %arg1: tensor<f32>, %arg2: tensor<f32>) -> tensor<4xf32> {\n    %0 = stablehlo.multiply %arg2, %arg1 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %2 = stablehlo.multiply %arg0, %1 : tensor<4xf32>\n    %3 = stablehlo.multiply %1, %arg0 : tensor<4xf32>\n    %4 = stablehlo.add %2, %3 : tensor<4xf32>\n    return %4 : tensor<4xf32>\n  }\n}\n"}
{"id": 49, "function_name": "loop_hwhp", "python": "def loop_hwhp(a):\n    \"\"\"Loop operation using jax.lax.scan.\"\"\"\n    def body_fun(carry, _):\n        return carry / a, None\n    result, _ = jax.lax.scan(body_fun, jnp.zeros_like(a), jnp.arange(5))\n    return result", "jaxpr": "{ lambda ; a:f32[4]. let\n    b:f32[4] = broadcast_in_dim[\n      broadcast_dimensions=()\n      shape=(4,)\n      sharding=None\n    ] 0.0:f32[]\n    c:i32[5] = iota[dimension=0 dtype=int32 shape=(5,) sharding=None] \n    d:f32[4] = scan[\n      _split_transpose=False\n      jaxpr={ lambda ; e:f32[4] f:f32[4] g:i32[]. let\n          h:f32[4] = div f e\n        in (h,) }\n      length=5\n      linear=(False, False, False)\n      num_carry=1\n      num_consts=1\n      reverse=False\n      unroll=1\n    ] a b c\n  in (d,) }\n\n# Gradient extraction failed: Gradient only defined for scalar-output functions. Output had shape: (4,).", "type": "generate_loop_kernel", "hlo": "module @jit_loop_hwhp attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:3 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %0) : tensor<4xf32>, tensor<i32>, tensor<4xf32>\n    cond {\n      %c_2 = stablehlo.constant dense<5> : tensor<i32>\n      %2 = stablehlo.compare  LT, %iterArg_0, %c_2,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %2 = func.call @closed_call(%iterArg, %iterArg_1) : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xf32>\n      %c_2 = stablehlo.constant dense<1> : tensor<i32>\n      %3 = stablehlo.add %iterArg_0, %c_2 : tensor<i32>\n      stablehlo.return %iterArg, %3, %2 : tensor<4xf32>, tensor<i32>, tensor<4xf32>\n    }\n    return %1#2 : tensor<4xf32>\n  }\n  func.func private @closed_call(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> tensor<4xf32> {\n    %0 = stablehlo.divide %arg1, %arg0 : tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n"}
