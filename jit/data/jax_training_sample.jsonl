{"id": 0, "kernel_name": "scalar_arr_qahf", "python": "def scalar_arr_qahf(alpha, x, y):\n    \"\"\"Scalar and array operations.\"\"\"\n    return alpha * x + y", "type": "generate_scalar_array_op", "hlo": "// ===== FORWARD PASS =====\nmodule @jit_scalar_arr_qahf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<10xf32>, %arg2: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %1 = stablehlo.multiply %0, %arg1 : tensor<10xf32>\n    %2 = stablehlo.add %1, %arg2 : tensor<10xf32>\n    return %2 : tensor<10xf32>\n  }\n}\n\n\n// ===== BACKWARD PASS =====\nmodule @jit_scalar_output_wrapper attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %1 = stablehlo.multiply %0, %arg0 : tensor<10xf32>\n    %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %2 = stablehlo.reduce(%1 init: %cst_0) applies stablehlo.add across dimensions = [0] : (tensor<10xf32>, tensor<f32>) -> tensor<f32>\n    return %2 : tensor<f32>\n  }\n}\n", "optimized_hlo": "// ===== FORWARD PASS (OPTIMIZED) =====\nHloModule jit_scalar_arr_qahf, is_scheduled=true, entry_computation_layout={(f32[], f32[10]{0}, f32[10]{0})->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmpb9nh2ojj.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"scalar_arr_qahf\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=41 end_line=41 column=14 end_column=47}\n5 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=11 end_column=20}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[10], param_1.2: f32[10], param_2.1: f32[]) -> f32[10] {\n  %param_2.1 = f32[] parameter(2)\n  %mul.1 = f32[10]{0} broadcast(%param_2.1), dimensions={}, metadata={op_name=\"jit(scalar_arr_qahf)/mul\" stack_frame_id=5}\n  %param_1.2 = f32[10]{0} parameter(1)\n  %mul.0 = f32[10]{0} multiply(%mul.1, %param_1.2), metadata={op_name=\"jit(scalar_arr_qahf)/mul\" stack_frame_id=5}\n  %param_0.1 = f32[10]{0} parameter(0)\n  ROOT %add.0 = f32[10]{0} add(%mul.0, %param_0.1), metadata={op_name=\"jit(scalar_arr_qahf)/add\" stack_frame_id=5}\n}\n\nENTRY %main.1 (alpha.1: f32[], x.1: f32[10], y.1: f32[10]) -> f32[10] {\n  %alpha.1 = f32[] parameter(0), metadata={op_name=\"alpha\"}\n  %x.1 = f32[10]{0} parameter(1), metadata={op_name=\"x\"}\n  %y.1 = f32[10]{0} parameter(2), metadata={op_name=\"y\"}\n  ROOT %multiply_add_fusion = f32[10]{0} fusion(%y.1, %x.1, %alpha.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(scalar_arr_qahf)/add\" stack_frame_id=5}\n}\n\n\n\n// ===== BACKWARD PASS (OPTIMIZED) =====\nHloModule jit_scalar_output_wrapper, is_scheduled=true, entry_computation_layout={(f32[10]{0})->f32[]}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmpb9nh2ojj.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"extract_ir.<locals>.scalar_output_wrapper\"\n6 \"scalar_arr_qahf\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=66 end_line=66 column=27 end_column=65}\n5 {file_name_id=2 function_name_id=5 line=58 end_line=58 column=21 end_column=32}\n6 {file_name_id=3 function_name_id=6 line=7 end_line=7 column=11 end_column=20}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=6}\n\n\n%region_0.1 (reduce_sum.3: f32[], reduce_sum.4: f32[]) -> f32[] {\n  %reduce_sum.3 = f32[] parameter(0), metadata={op_name=\"reduce_sum\"}\n  %reduce_sum.4 = f32[] parameter(1), metadata={op_name=\"reduce_sum\"}\n  ROOT %reduce_sum.5 = f32[] add(%reduce_sum.3, %reduce_sum.4), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/reduce_sum\" stack_frame_id=6}\n}\n\n%wrapped_reduce_computation (param_0: f32[10], param_1: f32[]) -> f32[] {\n  %param_0 = f32[10]{0} parameter(0)\n  %param_1 = f32[] parameter(1)\n  ROOT %reduce_sum.0 = f32[] reduce(%param_0, %param_1), dimensions={0}, to_apply=%region_0.1, metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/reduce_sum\" stack_frame_id=6}\n}\n\nENTRY %main.2 (args_1_.1: f32[10]) -> f32[] {\n  %args_1_.1 = f32[10]{0} parameter(0), metadata={op_name=\"args[1]\"}\n  %constant.1 = f32[] constant(0)\n  ROOT %wrapped_reduce = f32[] fusion(%args_1_.1, %constant.1), kind=kLoop, calls=%wrapped_reduce_computation, metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/reduce_sum\" stack_frame_id=6}\n}\n\n"}
{"id": 1, "kernel_name": "elementwise_bsdm", "python": "def elementwise_bsdm(a, b):\n    \"\"\"Elementwise operation on two arrays.\"\"\"\n    return a * b", "type": "generate_simple_elementwise", "hlo": "// ===== FORWARD PASS =====\nmodule @jit_elementwise_bsdm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>, %arg1: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<10xf32>\n    return %0 : tensor<10xf32>\n  }\n}\n\n\n// ===== BACKWARD PASS =====\nmodule @jit_scalar_output_wrapper attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %1 = stablehlo.multiply %0, %arg0 : tensor<10xf32>\n    return %1 : tensor<10xf32>\n  }\n}\n", "optimized_hlo": "// ===== FORWARD PASS (OPTIMIZED) =====\nHloModule jit_elementwise_bsdm, is_scheduled=true, entry_computation_layout={(f32[10]{0}, f32[10]{0})->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmp_gdz2yle.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"elementwise_bsdm\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=41 end_line=41 column=14 end_column=47}\n5 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=11 end_column=16}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n\n\n%wrapped_multiply_computation (param_0: f32[10], param_1: f32[10]) -> f32[10] {\n  %param_0 = f32[10]{0} parameter(0)\n  %param_1 = f32[10]{0} parameter(1)\n  ROOT %mul.0 = f32[10]{0} multiply(%param_0, %param_1), metadata={op_name=\"jit(elementwise_bsdm)/mul\" stack_frame_id=5}\n}\n\nENTRY %main.1 (a.1: f32[10], b.1: f32[10]) -> f32[10] {\n  %a.1 = f32[10]{0} parameter(0), metadata={op_name=\"a\"}\n  %b.1 = f32[10]{0} parameter(1), metadata={op_name=\"b\"}\n  ROOT %wrapped_multiply = f32[10]{0} fusion(%a.1, %b.1), kind=kLoop, calls=%wrapped_multiply_computation, metadata={op_name=\"jit(elementwise_bsdm)/mul\" stack_frame_id=5}\n}\n\n\n\n// ===== BACKWARD PASS (OPTIMIZED) =====\nHloModule jit_scalar_output_wrapper, is_scheduled=true, entry_computation_layout={(f32[10]{0})->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nENTRY %main.1 (args_1_.1: f32[10]) -> f32[10] {\n  %args_1_.1 = f32[10]{0} parameter(0), metadata={op_name=\"args[1]\"}\n  ROOT %copy = f32[10]{0} copy(%args_1_.1)\n}\n\n"}
{"id": 2, "kernel_name": "vec_kowe", "python": "def vec_kowe(a, b):\n    \"\"\"Dot product along last axis.\"\"\"\n    # Assuming last dimension is vector dimension\n    return jnp.sum(a * b, axis=-1)", "type": "generate_vector_kernel", "hlo": "// ===== FORWARD PASS =====\nmodule @jit_vec_kowe attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x3xf32>, %arg1: tensor<10x3xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<10x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<10x3xf32>, tensor<f32>) -> tensor<10xf32>\n    return %1 : tensor<10xf32>\n  }\n}\n\n\n// ===== BACKWARD PASS =====\nmodule @jit_scalar_output_wrapper attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x3xf32>) -> (tensor<10x3xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [0] : (tensor<10xf32>) -> tensor<10x3xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<10x3xf32>\n    return %2 : tensor<10x3xf32>\n  }\n}\n", "optimized_hlo": "// ===== FORWARD PASS (OPTIMIZED) =====\nHloModule jit_vec_kowe, is_scheduled=true, entry_computation_layout={(f32[10,3]{1,0}, f32[10,3]{1,0})->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmpe5rfzsbt.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"vec_kowe\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=41 end_line=41 column=14 end_column=47}\n5 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=11 end_column=34}\n6 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=19 end_column=24}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n\n\n%region_0.1 (reduce_sum.3: f32[], reduce_sum.4: f32[]) -> f32[] {\n  %reduce_sum.3 = f32[] parameter(0), metadata={op_name=\"reduce_sum\"}\n  %reduce_sum.4 = f32[] parameter(1), metadata={op_name=\"reduce_sum\"}\n  ROOT %reduce_sum.5 = f32[] add(%reduce_sum.3, %reduce_sum.4), metadata={op_name=\"jit(vec_kowe)/reduce_sum\" stack_frame_id=5}\n}\n\n%fused_computation (param_0.2: f32[10,3], param_1.2: f32[10,3]) -> f32[10] {\n  %param_0.2 = f32[10,3]{1,0} parameter(0)\n  %param_1.2 = f32[10,3]{1,0} parameter(1)\n  %mul.0 = f32[10,3]{1,0} multiply(%param_0.2, %param_1.2), metadata={op_name=\"jit(vec_kowe)/mul\" stack_frame_id=6}\n  %constant.0 = f32[] constant(0)\n  ROOT %reduce_sum.0 = f32[10]{0} reduce(%mul.0, %constant.0), dimensions={1}, to_apply=%region_0.1, metadata={op_name=\"jit(vec_kowe)/reduce_sum\" stack_frame_id=5}\n}\n\nENTRY %main.2 (a.1: f32[10,3], b.1: f32[10,3]) -> f32[10] {\n  %a.1 = f32[10,3]{1,0} parameter(0), metadata={op_name=\"a\"}\n  %b.1 = f32[10,3]{1,0} parameter(1), metadata={op_name=\"b\"}\n  ROOT %multiply_reduce_fusion = f32[10]{0} fusion(%a.1, %b.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(vec_kowe)/reduce_sum\" stack_frame_id=5}\n}\n\n\n\n// ===== BACKWARD PASS (OPTIMIZED) =====\nHloModule jit_scalar_output_wrapper, is_scheduled=true, entry_computation_layout={(f32[10,3]{1,0})->f32[10,3]{1,0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nENTRY %main.1 (args_1_.1: f32[10,3]) -> f32[10,3] {\n  %args_1_.1 = f32[10,3]{1,0} parameter(0), metadata={op_name=\"args[1]\"}\n  ROOT %copy = f32[10,3]{1,0} copy(%args_1_.1)\n}\n\n"}
{"id": 3, "kernel_name": "loop_hmci", "python": "def loop_hmci(a):\n    \"\"\"Reduction operation simulating loop accumulation.\"\"\"\n    # Accumulate n=5 times\n    result = a\n    for _ in range(5):\n        result = result + a\n    return result", "type": "generate_loop_kernel", "hlo": "// ===== FORWARD PASS =====\nmodule @jit_loop_hmci attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg0 : tensor<10xf32>\n    %1 = stablehlo.add %0, %arg0 : tensor<10xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<10xf32>\n    %3 = stablehlo.add %2, %arg0 : tensor<10xf32>\n    %4 = stablehlo.add %3, %arg0 : tensor<10xf32>\n    return %4 : tensor<10xf32>\n  }\n}\n\n\n// ===== BACKWARD PASS =====\nmodule @jit_scalar_output_wrapper attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main() -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %1 = stablehlo.add %0, %0 : tensor<10xf32>\n    %2 = stablehlo.add %1, %0 : tensor<10xf32>\n    %3 = stablehlo.add %2, %0 : tensor<10xf32>\n    %4 = stablehlo.add %3, %0 : tensor<10xf32>\n    %5 = stablehlo.add %4, %0 : tensor<10xf32>\n    return %5 : tensor<10xf32>\n  }\n}\n", "optimized_hlo": "// ===== FORWARD PASS (OPTIMIZED) =====\nHloModule jit_loop_hmci, is_scheduled=true, entry_computation_layout={(f32[10]{0})->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmp9lvy01xu.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"loop_hmci\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=41 end_line=41 column=14 end_column=47}\n5 {file_name_id=3 function_name_id=5 line=10 end_line=10 column=17 end_column=27}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[10]) -> f32[10] {\n  %param_0.1 = f32[10]{0} parameter(0)\n  %add.4 = f32[10]{0} add(%param_0.1, %param_0.1), metadata={op_name=\"jit(loop_hmci)/add\" stack_frame_id=5}\n  %add.3 = f32[10]{0} add(%add.4, %param_0.1), metadata={op_name=\"jit(loop_hmci)/add\" stack_frame_id=5}\n  %add.2 = f32[10]{0} add(%add.3, %param_0.1), metadata={op_name=\"jit(loop_hmci)/add\" stack_frame_id=5}\n  %add.1 = f32[10]{0} add(%add.2, %param_0.1), metadata={op_name=\"jit(loop_hmci)/add\" stack_frame_id=5}\n  ROOT %add.0 = f32[10]{0} add(%add.1, %param_0.1), metadata={op_name=\"jit(loop_hmci)/add\" stack_frame_id=5}\n}\n\nENTRY %main.1 (a.1: f32[10]) -> f32[10] {\n  %a.1 = f32[10]{0} parameter(0), metadata={op_name=\"a\"}\n  ROOT %add_add_fusion = f32[10]{0} fusion(%a.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(loop_hmci)/add\" stack_frame_id=5}\n}\n\n\n\n// ===== BACKWARD PASS (OPTIMIZED) =====\nHloModule jit_scalar_output_wrapper, is_scheduled=true, entry_computation_layout={()->f32[10]{0}}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmp9lvy01xu.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"extract_ir.<locals>.scalar_output_wrapper\"\n6 \"loop_hmci\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=66 end_line=66 column=27 end_column=65}\n5 {file_name_id=2 function_name_id=5 line=58 end_line=58 column=21 end_column=32}\n6 {file_name_id=3 function_name_id=6 line=10 end_line=10 column=17 end_column=27}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=6}\n\n\n%wrapped_broadcast_computation (param_0: f32[]) -> f32[10] {\n  %param_0 = f32[] parameter(0)\n  ROOT %broadcast.0 = f32[10]{0} broadcast(%param_0), dimensions={}\n}\n\nENTRY %main.1 () -> f32[10] {\n  %constant.1 = f32[] constant(6)\n  ROOT %wrapped_broadcast = f32[10]{0} fusion(%constant.1), kind=kLoop, calls=%wrapped_broadcast_computation\n}\n\n"}
{"id": 4, "kernel_name": "scalar_arr_xkpw", "python": "def scalar_arr_xkpw(alpha, x, y):\n    \"\"\"Scalar and array operations.\"\"\"\n    return alpha + x * y", "type": "generate_scalar_array_op", "hlo": "// ===== FORWARD PASS =====\nmodule @jit_scalar_arr_xkpw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<10xf32>, %arg2: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<10xf32>\n    %1 = stablehlo.broadcast_in_dim %arg0, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %2 = stablehlo.add %1, %0 : tensor<10xf32>\n    return %2 : tensor<10xf32>\n  }\n}\n\n\n// ===== BACKWARD PASS =====\nmodule @jit_scalar_output_wrapper attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main() -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst_0) applies stablehlo.add across dimensions = [0] : (tensor<10xf32>, tensor<f32>) -> tensor<f32>\n    return %1 : tensor<f32>\n  }\n}\n", "optimized_hlo": "// ===== FORWARD PASS (OPTIMIZED) =====\nHloModule jit_scalar_arr_xkpw, is_scheduled=true, entry_computation_layout={(f32[], f32[10]{0}, f32[10]{0})->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmp57anhilr.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"scalar_arr_xkpw\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=41 end_line=41 column=14 end_column=47}\n5 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=19 end_column=24}\n6 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=11 end_column=24}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[10], param_1.2: f32[10], param_2.1: f32[]) -> f32[10] {\n  %param_2.1 = f32[] parameter(2)\n  %add.1 = f32[10]{0} broadcast(%param_2.1), dimensions={}, metadata={op_name=\"jit(scalar_arr_xkpw)/add\" stack_frame_id=6}\n  %param_0.1 = f32[10]{0} parameter(0)\n  %param_1.2 = f32[10]{0} parameter(1)\n  %mul.0 = f32[10]{0} multiply(%param_0.1, %param_1.2), metadata={op_name=\"jit(scalar_arr_xkpw)/mul\" stack_frame_id=5}\n  ROOT %add.0 = f32[10]{0} add(%add.1, %mul.0), metadata={op_name=\"jit(scalar_arr_xkpw)/add\" stack_frame_id=6}\n}\n\nENTRY %main.1 (alpha.1: f32[], x.1: f32[10], y.1: f32[10]) -> f32[10] {\n  %alpha.1 = f32[] parameter(0), metadata={op_name=\"alpha\"}\n  %x.1 = f32[10]{0} parameter(1), metadata={op_name=\"x\"}\n  %y.1 = f32[10]{0} parameter(2), metadata={op_name=\"y\"}\n  ROOT %multiply_add_fusion = f32[10]{0} fusion(%x.1, %y.1, %alpha.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(scalar_arr_xkpw)/add\" stack_frame_id=6}\n}\n\n\n\n// ===== BACKWARD PASS (OPTIMIZED) =====\nHloModule jit_scalar_output_wrapper, is_scheduled=true, entry_computation_layout={()->f32[]}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmp57anhilr.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"extract_ir.<locals>.scalar_output_wrapper\"\n6 \"scalar_arr_xkpw\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=66 end_line=66 column=27 end_column=65}\n5 {file_name_id=2 function_name_id=5 line=58 end_line=58 column=21 end_column=32}\n6 {file_name_id=3 function_name_id=6 line=7 end_line=7 column=11 end_column=24}\n7 {file_name_id=2 function_name_id=5 line=61 end_line=61 column=23 end_column=38}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=6}\n7 {file_location_id=7 parent_frame_id=5}\n\n\nENTRY %main.2 () -> f32[] {\n  %constant = f32[] constant(10), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/reduce_sum\" stack_frame_id=6}\n  ROOT %copy = f32[] copy(%constant)\n}\n\n"}
{"id": 5, "kernel_name": "reduce_jlli", "python": "def reduce_jlli(a):\n    \"\"\"Sum reduction over array.\"\"\"\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "hlo": "// ===== FORWARD PASS =====\nmodule @jit_reduce_jlli attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<10xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n\n\n// ===== BACKWARD PASS =====\nmodule @jit_scalar_output_wrapper attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main() -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    return %0 : tensor<10xf32>\n  }\n}\n", "optimized_hlo": "// ===== FORWARD PASS (OPTIMIZED) =====\nHloModule jit_reduce_jlli, is_scheduled=true, entry_computation_layout={(f32[10]{0})->f32[]}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmpk5f6ipqy.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"reduce_jlli\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=41 end_line=41 column=14 end_column=47}\n5 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=11 end_column=21}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n\n\n%region_0.1 (reduce_sum.3: f32[], reduce_sum.4: f32[]) -> f32[] {\n  %reduce_sum.3 = f32[] parameter(0), metadata={op_name=\"reduce_sum\"}\n  %reduce_sum.4 = f32[] parameter(1), metadata={op_name=\"reduce_sum\"}\n  ROOT %reduce_sum.5 = f32[] add(%reduce_sum.3, %reduce_sum.4), metadata={op_name=\"jit(reduce_jlli)/reduce_sum\" stack_frame_id=5}\n}\n\n%wrapped_reduce_computation (param_0: f32[10], param_1: f32[]) -> f32[] {\n  %param_0 = f32[10]{0} parameter(0)\n  %param_1 = f32[] parameter(1)\n  ROOT %reduce_sum.0 = f32[] reduce(%param_0, %param_1), dimensions={0}, to_apply=%region_0.1, metadata={op_name=\"jit(reduce_jlli)/reduce_sum\" stack_frame_id=5}\n}\n\nENTRY %main.2 (a.1: f32[10]) -> f32[] {\n  %a.1 = f32[10]{0} parameter(0), metadata={op_name=\"a\"}\n  %constant.1 = f32[] constant(0)\n  ROOT %wrapped_reduce = f32[] fusion(%a.1, %constant.1), kind=kLoop, calls=%wrapped_reduce_computation, metadata={op_name=\"jit(reduce_jlli)/reduce_sum\" stack_frame_id=5}\n}\n\n\n\n// ===== BACKWARD PASS (OPTIMIZED) =====\nHloModule jit_scalar_output_wrapper, is_scheduled=true, entry_computation_layout={()->f32[10]{0}}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmpk5f6ipqy.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"extract_ir.<locals>.scalar_output_wrapper\"\n6 \"reduce_jlli\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=66 end_line=66 column=27 end_column=65}\n5 {file_name_id=2 function_name_id=5 line=58 end_line=58 column=21 end_column=32}\n6 {file_name_id=3 function_name_id=6 line=7 end_line=7 column=11 end_column=21}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=6}\n\n\n%wrapped_broadcast_computation (param_0: f32[]) -> f32[10] {\n  %param_0 = f32[] parameter(0)\n  ROOT %broadcast.0 = f32[10]{0} broadcast(%param_0), dimensions={}\n}\n\nENTRY %main.1 () -> f32[10] {\n  %constant.1 = f32[] constant(1)\n  ROOT %wrapped_broadcast = f32[10]{0} fusion(%constant.1), kind=kLoop, calls=%wrapped_broadcast_computation\n}\n\n"}
{"id": 6, "kernel_name": "nested_odsh", "python": "def nested_odsh(a):\n    \"\"\"Nested conditional using jnp.where.\"\"\"\n    return jnp.where(a > 0.44, \n                     jnp.where(a > 0.69, a * 3.0, a * 2.0),\n                     a * 0.5)", "type": "generate_nested_branch_kernel", "hlo": "// ===== FORWARD PASS =====\nmodule @jit_nested_odsh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<4.400000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<10xf32>, tensor<10xf32>) -> tensor<10xi1>\n    %cst_0 = stablehlo.constant dense<0.689999997> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %3 = stablehlo.compare  GT, %arg0, %2,  FLOAT : (tensor<10xf32>, tensor<10xf32>) -> tensor<10xi1>\n    %cst_1 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<10xf32>\n    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %6 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %7 = stablehlo.multiply %arg0, %6 : tensor<10xf32>\n    %8 = call @_where(%3, %5, %7) : (tensor<10xi1>, tensor<10xf32>, tensor<10xf32>) -> tensor<10xf32>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<10xf32>\n    %11 = call @_where(%1, %8, %10) : (tensor<10xi1>, tensor<10xf32>, tensor<10xf32>) -> tensor<10xf32>\n    return %11 : tensor<10xf32>\n  }\n  func.func private @_where(%arg0: tensor<10xi1>, %arg1: tensor<10xf32>, %arg2: tensor<10xf32>) -> tensor<10xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<10xi1>, tensor<10xf32>\n    return %0 : tensor<10xf32>\n  }\n}\n\n\n// ===== BACKWARD PASS =====\nmodule @jit_scalar_output_wrapper attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<4.400000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<10xf32>, tensor<10xf32>) -> tensor<10xi1>\n    %cst_0 = stablehlo.constant dense<0.689999997> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %3 = stablehlo.compare  GT, %arg0, %2,  FLOAT : (tensor<10xf32>, tensor<10xf32>) -> tensor<10xi1>\n    %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %5:2 = call @_where(%1, %4) : (tensor<10xi1>, tensor<10xf32>) -> (tensor<10xf32>, tensor<10xf32>)\n    %cst_2 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %6 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %7 = stablehlo.multiply %5#1, %6 : tensor<10xf32>\n    %8:2 = call @_where(%3, %5#0) : (tensor<10xi1>, tensor<10xf32>) -> (tensor<10xf32>, tensor<10xf32>)\n    %cst_3 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %10 = stablehlo.multiply %8#1, %9 : tensor<10xf32>\n    %11 = stablehlo.add %7, %10 : tensor<10xf32>\n    %cst_4 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %12 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %13 = stablehlo.multiply %8#0, %12 : tensor<10xf32>\n    %14 = stablehlo.add %11, %13 : tensor<10xf32>\n    return %14 : tensor<10xf32>\n  }\n  func.func private @_where(%arg0: tensor<10xi1>, %arg1: tensor<10xf32>) -> (tensor<10xf32>, tensor<10xf32>) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %1 = stablehlo.select %arg0, %0, %arg1 : tensor<10xi1>, tensor<10xf32>\n    %2 = stablehlo.select %arg0, %arg1, %0 : tensor<10xi1>, tensor<10xf32>\n    return %2, %1 : tensor<10xf32>, tensor<10xf32>\n  }\n}\n", "optimized_hlo": "// ===== FORWARD PASS (OPTIMIZED) =====\nHloModule jit_nested_odsh, is_scheduled=true, entry_computation_layout={(f32[10]{0})->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmphr9su7i4.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"nested_odsh\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=41 end_line=41 column=14 end_column=47}\n5 {file_name_id=3 function_name_id=5 line=9 end_line=9 column=21 end_column=28}\n6 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=50 end_column=57}\n7 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=41 end_column=48}\n8 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=31 end_column=39}\n9 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=21 end_column=29}\n10 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=21 end_column=58}\n11 {file_name_id=3 function_name_id=5 line=7 end_line=9 column=11 end_column=29}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=5}\n8 {file_location_id=8 parent_frame_id=5}\n9 {file_location_id=9 parent_frame_id=5}\n10 {file_location_id=10 parent_frame_id=5}\n11 {file_location_id=11 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[10]) -> f32[10] {\n  %param_0.1 = f32[10]{0} parameter(0)\n  %constant.4 = f32[] constant(0.44)\n  %broadcast.4 = f32[10]{0} broadcast(%constant.4), dimensions={}\n  %gt.1 = pred[10]{0} compare(%param_0.1, %broadcast.4), direction=GT, metadata={op_name=\"jit(nested_odsh)/gt\" stack_frame_id=9}\n  %constant.3 = f32[] constant(0.69)\n  %broadcast.3 = f32[10]{0} broadcast(%constant.3), dimensions={}\n  %gt.0 = pred[10]{0} compare(%param_0.1, %broadcast.3), direction=GT, metadata={op_name=\"jit(nested_odsh)/gt\" stack_frame_id=8}\n  %constant.2 = f32[] constant(3)\n  %broadcast.2 = f32[10]{0} broadcast(%constant.2), dimensions={}\n  %mul.2 = f32[10]{0} multiply(%param_0.1, %broadcast.2), metadata={op_name=\"jit(nested_odsh)/mul\" stack_frame_id=7}\n  %constant.1 = f32[] constant(2)\n  %broadcast.1 = f32[10]{0} broadcast(%constant.1), dimensions={}\n  %mul.1 = f32[10]{0} multiply(%param_0.1, %broadcast.1), metadata={op_name=\"jit(nested_odsh)/mul\" stack_frame_id=6}\n  %select_n.5 = f32[10]{0} select(%gt.0, %mul.2, %mul.1), metadata={op_name=\"jit(nested_odsh)/jit(_where)/select_n\" stack_frame_id=10}\n  %constant.0 = f32[] constant(0.5)\n  %broadcast.0 = f32[10]{0} broadcast(%constant.0), dimensions={}\n  %mul.0 = f32[10]{0} multiply(%param_0.1, %broadcast.0), metadata={op_name=\"jit(nested_odsh)/mul\" stack_frame_id=5}\n  ROOT %select_n.4 = f32[10]{0} select(%gt.1, %select_n.5, %mul.0), metadata={op_name=\"jit(nested_odsh)/jit(_where)/select_n\" stack_frame_id=10}\n}\n\nENTRY %main.2 (a.1: f32[10]) -> f32[10] {\n  %a.1 = f32[10]{0} parameter(0), metadata={op_name=\"a\"}\n  ROOT %multiply_select_fusion = f32[10]{0} fusion(%a.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(nested_odsh)/jit(_where)/select_n\" stack_frame_id=10}\n}\n\n\n\n// ===== BACKWARD PASS (OPTIMIZED) =====\nHloModule jit_scalar_output_wrapper, is_scheduled=true, entry_computation_layout={(f32[10]{0})->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmphr9su7i4.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"extract_ir.<locals>.scalar_output_wrapper\"\n6 \"nested_odsh\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=66 end_line=66 column=27 end_column=65}\n5 {file_name_id=2 function_name_id=5 line=58 end_line=58 column=21 end_column=32}\n6 {file_name_id=3 function_name_id=6 line=8 end_line=8 column=41 end_column=48}\n7 {file_name_id=3 function_name_id=6 line=8 end_line=8 column=50 end_column=57}\n8 {file_name_id=3 function_name_id=6 line=9 end_line=9 column=21 end_column=28}\n9 {file_name_id=2 function_name_id=5 line=61 end_line=61 column=23 end_column=38}\n10 {file_name_id=3 function_name_id=6 line=8 end_line=8 column=31 end_column=39}\n11 {file_name_id=3 function_name_id=6 line=7 end_line=7 column=21 end_column=29}\n12 {file_name_id=3 function_name_id=6 line=7 end_line=9 column=11 end_column=29}\n13 {file_name_id=2 function_name_id=4 line=41 end_line=41 column=14 end_column=47}\n14 {file_name_id=3 function_name_id=6 line=8 end_line=8 column=21 end_column=58}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=6}\n7 {file_location_id=7 parent_frame_id=6}\n8 {file_location_id=8 parent_frame_id=6}\n9 {file_location_id=9 parent_frame_id=5}\n10 {file_location_id=10 parent_frame_id=6}\n11 {file_location_id=11 parent_frame_id=6}\n12 {file_location_id=12 parent_frame_id=6}\n13 {file_location_id=13 parent_frame_id=4}\n14 {file_location_id=14 parent_frame_id=14}\n15 {file_location_id=14 parent_frame_id=6}\n\n\n%fused_computation (param_0.4: f32[10]) -> f32[10] {\n  %param_0.4 = f32[10]{0} parameter(0)\n  %constant.16 = f32[] constant(0.44)\n  %broadcast.16 = f32[10]{0} broadcast(%constant.16), dimensions={}\n  %gt.1 = pred[10]{0} compare(%param_0.4, %broadcast.16), direction=GT, metadata={op_name=\"jit(scalar_output_wrapper)/jvp()/gt\" stack_frame_id=11}\n  %constant.15 = f32[] constant(0), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp(jit(_where)))\"}\n  %broadcast.15 = f32[10]{0} broadcast(%constant.15), dimensions={}, metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp(jit(_where)))\"}\n  %constant.14 = f32[] constant(1)\n  %broadcast.14 = f32[10]{0} broadcast(%constant.14), dimensions={}\n  %select_n.11 = f32[10]{0} select(%gt.1, %broadcast.15, %broadcast.14), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp(jit(_where)))/select_n\" stack_frame_id=14}\n  %constant.6 = f32[] constant(0.5)\n  %broadcast.6 = f32[10]{0} broadcast(%constant.6), dimensions={}\n  %mul.2 = f32[10]{0} multiply(%select_n.11, %broadcast.6), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/mul\" stack_frame_id=8}\n  %constant.5 = f32[] constant(0.69)\n  %broadcast.5 = f32[10]{0} broadcast(%constant.5), dimensions={}\n  %gt.0 = pred[10]{0} compare(%param_0.4, %broadcast.5), direction=GT, metadata={op_name=\"jit(scalar_output_wrapper)/jvp()/gt\" stack_frame_id=10}\n  %select_n.10 = f32[10]{0} select(%gt.1, %broadcast.14, %broadcast.15), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp(jit(_where)))/select_n\" stack_frame_id=14}\n  %select_n.9 = f32[10]{0} select(%gt.0, %broadcast.15, %select_n.10), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp(jit(_where)))/select_n\" stack_frame_id=14}\n  %constant.4 = f32[] constant(2)\n  %broadcast.4 = f32[10]{0} broadcast(%constant.4), dimensions={}\n  %mul.1 = f32[10]{0} multiply(%select_n.9, %broadcast.4), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/mul\" stack_frame_id=7}\n  %add_any.1 = f32[10]{0} add(%mul.2, %mul.1), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/add_any\" stack_frame_id=7}\n  %select_n.8 = f32[10]{0} select(%gt.0, %select_n.10, %broadcast.15), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp(jit(_where)))/select_n\" stack_frame_id=14}\n  %constant.3 = f32[] constant(3)\n  %broadcast.3 = f32[10]{0} broadcast(%constant.3), dimensions={}\n  %mul.0 = f32[10]{0} multiply(%select_n.8, %broadcast.3), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/mul\" stack_frame_id=6}\n  ROOT %add_any.0 = f32[10]{0} add(%add_any.1, %mul.0), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/add_any\" stack_frame_id=6}\n}\n\nENTRY %main.2 (args_0_.1: f32[10]) -> f32[10] {\n  %args_0_.1 = f32[10]{0} parameter(0), metadata={op_name=\"args[0]\"}\n  ROOT %multiply_add_fusion = f32[10]{0} fusion(%args_0_.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/add_any\" stack_frame_id=6}\n}\n\n"}
{"id": 7, "kernel_name": "scalar_arr_bkct", "python": "def scalar_arr_bkct(alpha, x, y):\n    \"\"\"Scalar and array operations.\"\"\"\n    return alpha * x + y", "type": "generate_scalar_array_op", "hlo": "// ===== FORWARD PASS =====\nmodule @jit_scalar_arr_bkct attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<10xf32>, %arg2: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %1 = stablehlo.multiply %0, %arg1 : tensor<10xf32>\n    %2 = stablehlo.add %1, %arg2 : tensor<10xf32>\n    return %2 : tensor<10xf32>\n  }\n}\n\n\n// ===== BACKWARD PASS =====\nmodule @jit_scalar_output_wrapper attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %1 = stablehlo.multiply %0, %arg0 : tensor<10xf32>\n    %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %2 = stablehlo.reduce(%1 init: %cst_0) applies stablehlo.add across dimensions = [0] : (tensor<10xf32>, tensor<f32>) -> tensor<f32>\n    return %2 : tensor<f32>\n  }\n}\n", "optimized_hlo": "// ===== FORWARD PASS (OPTIMIZED) =====\nHloModule jit_scalar_arr_bkct, is_scheduled=true, entry_computation_layout={(f32[], f32[10]{0}, f32[10]{0})->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmpvdlya484.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"scalar_arr_bkct\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=41 end_line=41 column=14 end_column=47}\n5 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=11 end_column=20}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[10], param_1.2: f32[10], param_2.1: f32[]) -> f32[10] {\n  %param_2.1 = f32[] parameter(2)\n  %mul.1 = f32[10]{0} broadcast(%param_2.1), dimensions={}, metadata={op_name=\"jit(scalar_arr_bkct)/mul\" stack_frame_id=5}\n  %param_1.2 = f32[10]{0} parameter(1)\n  %mul.0 = f32[10]{0} multiply(%mul.1, %param_1.2), metadata={op_name=\"jit(scalar_arr_bkct)/mul\" stack_frame_id=5}\n  %param_0.1 = f32[10]{0} parameter(0)\n  ROOT %add.0 = f32[10]{0} add(%mul.0, %param_0.1), metadata={op_name=\"jit(scalar_arr_bkct)/add\" stack_frame_id=5}\n}\n\nENTRY %main.1 (alpha.1: f32[], x.1: f32[10], y.1: f32[10]) -> f32[10] {\n  %alpha.1 = f32[] parameter(0), metadata={op_name=\"alpha\"}\n  %x.1 = f32[10]{0} parameter(1), metadata={op_name=\"x\"}\n  %y.1 = f32[10]{0} parameter(2), metadata={op_name=\"y\"}\n  ROOT %multiply_add_fusion = f32[10]{0} fusion(%y.1, %x.1, %alpha.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(scalar_arr_bkct)/add\" stack_frame_id=5}\n}\n\n\n\n// ===== BACKWARD PASS (OPTIMIZED) =====\nHloModule jit_scalar_output_wrapper, is_scheduled=true, entry_computation_layout={(f32[10]{0})->f32[]}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmpvdlya484.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"extract_ir.<locals>.scalar_output_wrapper\"\n6 \"scalar_arr_bkct\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=66 end_line=66 column=27 end_column=65}\n5 {file_name_id=2 function_name_id=5 line=58 end_line=58 column=21 end_column=32}\n6 {file_name_id=3 function_name_id=6 line=7 end_line=7 column=11 end_column=20}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=6}\n\n\n%region_0.1 (reduce_sum.3: f32[], reduce_sum.4: f32[]) -> f32[] {\n  %reduce_sum.3 = f32[] parameter(0), metadata={op_name=\"reduce_sum\"}\n  %reduce_sum.4 = f32[] parameter(1), metadata={op_name=\"reduce_sum\"}\n  ROOT %reduce_sum.5 = f32[] add(%reduce_sum.3, %reduce_sum.4), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/reduce_sum\" stack_frame_id=6}\n}\n\n%wrapped_reduce_computation (param_0: f32[10], param_1: f32[]) -> f32[] {\n  %param_0 = f32[10]{0} parameter(0)\n  %param_1 = f32[] parameter(1)\n  ROOT %reduce_sum.0 = f32[] reduce(%param_0, %param_1), dimensions={0}, to_apply=%region_0.1, metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/reduce_sum\" stack_frame_id=6}\n}\n\nENTRY %main.2 (args_1_.1: f32[10]) -> f32[] {\n  %args_1_.1 = f32[10]{0} parameter(0), metadata={op_name=\"args[1]\"}\n  %constant.1 = f32[] constant(0)\n  ROOT %wrapped_reduce = f32[] fusion(%args_1_.1, %constant.1), kind=kLoop, calls=%wrapped_reduce_computation, metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/reduce_sum\" stack_frame_id=6}\n}\n\n"}
{"id": 8, "kernel_name": "multi_mgqg", "python": "def multi_mgqg(a, b):\n    \"\"\"Multi-statement computation.\"\"\"\n    temp1 = a - b\n    temp2 = jnp.sqrt(temp1)\n    result = temp2 - a\n    return result", "type": "generate_multi_statement_kernel", "hlo": "// ===== FORWARD PASS =====\nmodule @jit_multi_mgqg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>, %arg1: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<10xf32>\n    %1 = stablehlo.sqrt %0 : tensor<10xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<10xf32>\n    return %2 : tensor<10xf32>\n  }\n}\n\n\n// ===== BACKWARD PASS =====\nmodule @jit_scalar_output_wrapper attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>, %arg1: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<10xf32>\n    %1 = stablehlo.sqrt %0 : tensor<10xf32>\n    %cst = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %3 = stablehlo.divide %2, %1 : tensor<10xf32>\n    %cst_0 = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %5 = stablehlo.negate %4 : tensor<10xf32>\n    %6 = stablehlo.multiply %4, %3 : tensor<10xf32>\n    %7 = stablehlo.add %5, %6 : tensor<10xf32>\n    return %7 : tensor<10xf32>\n  }\n}\n", "optimized_hlo": "// ===== FORWARD PASS (OPTIMIZED) =====\nHloModule jit_multi_mgqg, is_scheduled=true, entry_computation_layout={(f32[10]{0}, f32[10]{0})->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmpxr1krrj1.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"multi_mgqg\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=41 end_line=41 column=14 end_column=47}\n5 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=12 end_column=17}\n6 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=12 end_column=27}\n7 {file_name_id=3 function_name_id=5 line=9 end_line=9 column=13 end_column=22}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[10], param_1.2: f32[10]) -> f32[10] {\n  %param_0.1 = f32[10]{0} parameter(0)\n  %param_1.2 = f32[10]{0} parameter(1)\n  %sub.1 = f32[10]{0} subtract(%param_0.1, %param_1.2), metadata={op_name=\"jit(multi_mgqg)/sub\" stack_frame_id=5}\n  %sqrt.0 = f32[10]{0} sqrt(%sub.1), metadata={op_name=\"jit(multi_mgqg)/sqrt\" stack_frame_id=6}\n  ROOT %sub.0 = f32[10]{0} subtract(%sqrt.0, %param_0.1), metadata={op_name=\"jit(multi_mgqg)/sub\" stack_frame_id=7}\n}\n\nENTRY %main.1 (a.1: f32[10], b.1: f32[10]) -> f32[10] {\n  %a.1 = f32[10]{0} parameter(0), metadata={op_name=\"a\"}\n  %b.1 = f32[10]{0} parameter(1), metadata={op_name=\"b\"}\n  ROOT %sqrt_subtract_fusion = f32[10]{0} fusion(%a.1, %b.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(multi_mgqg)/sub\" stack_frame_id=7}\n}\n\n\n\n// ===== BACKWARD PASS (OPTIMIZED) =====\nHloModule jit_scalar_output_wrapper, is_scheduled=true, entry_computation_layout={(f32[10]{0}, f32[10]{0})->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmpxr1krrj1.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"extract_ir.<locals>.scalar_output_wrapper\"\n6 \"multi_mgqg\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=66 end_line=66 column=27 end_column=65}\n5 {file_name_id=2 function_name_id=5 line=58 end_line=58 column=21 end_column=32}\n6 {file_name_id=3 function_name_id=6 line=9 end_line=9 column=13 end_column=22}\n7 {file_name_id=3 function_name_id=6 line=8 end_line=8 column=12 end_column=27}\n8 {file_name_id=3 function_name_id=6 line=7 end_line=7 column=12 end_column=17}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=6}\n7 {file_location_id=7 parent_frame_id=6}\n8 {file_location_id=8 parent_frame_id=6}\n\n\n%fused_computation (param_0.2: f32[10], param_1.5: f32[10]) -> f32[10] {\n  %constant.1 = f32[] constant(0.5)\n  %broadcast.1 = f32[10]{0} broadcast(%constant.1), dimensions={}\n  %param_0.2 = f32[10]{0} parameter(0)\n  %param_1.5 = f32[10]{0} parameter(1)\n  %sub.0 = f32[10]{0} subtract(%param_0.2, %param_1.5), metadata={op_name=\"jit(scalar_output_wrapper)/jvp()/sub\" stack_frame_id=8}\n  %rsqrt.1 = f32[10]{0} rsqrt(%sub.0), metadata={op_name=\"jit(scalar_output_wrapper)/jvp()/sqrt\" stack_frame_id=7}\n  %multiply.1 = f32[10]{0} multiply(%broadcast.1, %rsqrt.1), metadata={op_name=\"jit(scalar_output_wrapper)/jvp()/div\" stack_frame_id=7}\n  %constant.0 = f32[] constant(-1)\n  %broadcast.0 = f32[10]{0} broadcast(%constant.0), dimensions={}\n  ROOT %neg.0 = f32[10]{0} add(%multiply.1, %broadcast.0), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/add_any;jit(scalar_output_wrapper)/transpose(jvp())/neg\"}\n}\n\nENTRY %main.1 (args_0_.1: f32[10], args_1_.1: f32[10]) -> f32[10] {\n  %args_0_.1 = f32[10]{0} parameter(0), metadata={op_name=\"args[0]\"}\n  %args_1_.1 = f32[10]{0} parameter(1), metadata={op_name=\"args[1]\"}\n  ROOT %broadcast_add_fusion = f32[10]{0} fusion(%args_0_.1, %args_1_.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/add_any;jit(scalar_output_wrapper)/transpose(jvp())/neg\"}\n}\n\n"}
{"id": 9, "kernel_name": "branch_gnev", "python": "def branch_gnev(a):\n    \"\"\"Conditional operation using jnp.where.\"\"\"\n    return jnp.where(a > -0.54, a * 2.8, a - 1.5)", "type": "generate_branch_kernel", "hlo": "// ===== FORWARD PASS =====\nmodule @jit_branch_gnev attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-5.400000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<10xf32>, tensor<10xf32>) -> tensor<10xi1>\n    %cst_0 = stablehlo.constant dense<2.800000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<10xf32>\n    %cst_1 = stablehlo.constant dense<1.500000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<10xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<10xi1>, tensor<10xf32>, tensor<10xf32>) -> tensor<10xf32>\n    return %6 : tensor<10xf32>\n  }\n  func.func private @_where(%arg0: tensor<10xi1>, %arg1: tensor<10xf32>, %arg2: tensor<10xf32>) -> tensor<10xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<10xi1>, tensor<10xf32>\n    return %0 : tensor<10xf32>\n  }\n}\n\n\n// ===== BACKWARD PASS =====\nmodule @jit_scalar_output_wrapper attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-5.400000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<10xf32>, tensor<10xf32>) -> tensor<10xi1>\n    %cst_0 = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %3:2 = call @_where(%1, %2) : (tensor<10xi1>, tensor<10xf32>) -> (tensor<10xf32>, tensor<10xf32>)\n    %cst_1 = stablehlo.constant dense<2.800000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %5 = stablehlo.multiply %3#0, %4 : tensor<10xf32>\n    %6 = stablehlo.add %3#1, %5 : tensor<10xf32>\n    return %6 : tensor<10xf32>\n  }\n  func.func private @_where(%arg0: tensor<10xi1>, %arg1: tensor<10xf32>) -> (tensor<10xf32>, tensor<10xf32>) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %1 = stablehlo.select %arg0, %0, %arg1 : tensor<10xi1>, tensor<10xf32>\n    %2 = stablehlo.select %arg0, %arg1, %0 : tensor<10xi1>, tensor<10xf32>\n    return %2, %1 : tensor<10xf32>, tensor<10xf32>\n  }\n}\n", "optimized_hlo": "// ===== FORWARD PASS (OPTIMIZED) =====\nHloModule jit_branch_gnev, is_scheduled=true, entry_computation_layout={(f32[10]{0})->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmpfknigbbx.py\"\n4 \"/tmp/tmphr9su7i4.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"branch_gnev\"\n6 \"nested_odsh\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=41 end_line=41 column=14 end_column=47}\n5 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=41 end_column=48}\n6 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=32 end_column=39}\n7 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=21 end_column=30}\n8 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=11 end_column=49}\n9 {file_name_id=4 function_name_id=6 line=8 end_line=8 column=21 end_column=58}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=5}\n8 {file_location_id=8 parent_frame_id=5}\n9 {file_location_id=9 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[10]) -> f32[10] {\n  %param_0.1 = f32[10]{0} parameter(0)\n  %constant.6 = f32[] constant(-0.54)\n  %broadcast.6 = f32[10]{0} broadcast(%constant.6), dimensions={}\n  %gt.0 = pred[10]{0} compare(%param_0.1, %broadcast.6), direction=GT, metadata={op_name=\"jit(branch_gnev)/gt\" stack_frame_id=7}\n  %constant.2 = f32[] constant(2.8)\n  %broadcast.2 = f32[10]{0} broadcast(%constant.2), dimensions={}\n  %mul.0 = f32[10]{0} multiply(%param_0.1, %broadcast.2), metadata={op_name=\"jit(branch_gnev)/mul\" stack_frame_id=6}\n  %constant.1 = f32[] constant(-1.5)\n  %broadcast.1 = f32[10]{0} broadcast(%constant.1), dimensions={}\n  %add.1 = f32[10]{0} add(%param_0.1, %broadcast.1), metadata={op_name=\"jit(branch_gnev)/sub\" stack_frame_id=5}\n  ROOT %select_n.2 = f32[10]{0} select(%gt.0, %mul.0, %add.1), metadata={op_name=\"jit(branch_gnev)/jit(_where)/select_n\" stack_frame_id=9}\n}\n\nENTRY %main.2 (a.1: f32[10]) -> f32[10] {\n  %a.1 = f32[10]{0} parameter(0), metadata={op_name=\"a\"}\n  ROOT %add_select_fusion = f32[10]{0} fusion(%a.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(branch_gnev)/jit(_where)/select_n\" stack_frame_id=9}\n}\n\n\n\n// ===== BACKWARD PASS (OPTIMIZED) =====\nHloModule jit_scalar_output_wrapper, is_scheduled=true, entry_computation_layout={(f32[10]{0})->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmpfknigbbx.py\"\n4 \"/tmp/tmphr9su7i4.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"extract_ir.<locals>.scalar_output_wrapper\"\n6 \"branch_gnev\"\n7 \"nested_odsh\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=66 end_line=66 column=27 end_column=65}\n5 {file_name_id=2 function_name_id=5 line=58 end_line=58 column=21 end_column=32}\n6 {file_name_id=3 function_name_id=6 line=7 end_line=7 column=32 end_column=39}\n7 {file_name_id=2 function_name_id=5 line=61 end_line=61 column=23 end_column=38}\n8 {file_name_id=3 function_name_id=6 line=7 end_line=7 column=21 end_column=30}\n9 {file_name_id=3 function_name_id=6 line=7 end_line=7 column=11 end_column=49}\n10 {file_name_id=2 function_name_id=4 line=41 end_line=41 column=14 end_column=47}\n11 {file_name_id=4 function_name_id=7 line=8 end_line=8 column=21 end_column=58}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=6}\n7 {file_location_id=7 parent_frame_id=5}\n8 {file_location_id=8 parent_frame_id=6}\n9 {file_location_id=9 parent_frame_id=6}\n10 {file_location_id=10 parent_frame_id=4}\n11 {file_location_id=11 parent_frame_id=11}\n\n\n%fused_computation (param_0.2: f32[10]) -> f32[10] {\n  %param_0.2 = f32[10]{0} parameter(0)\n  %constant.8 = f32[] constant(-0.54)\n  %broadcast.8 = f32[10]{0} broadcast(%constant.8), dimensions={}\n  %gt.0 = pred[10]{0} compare(%param_0.2, %broadcast.8), direction=GT, metadata={op_name=\"jit(scalar_output_wrapper)/jvp()/gt\" stack_frame_id=8}\n  %constant.3 = f32[] constant(0), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp(jit(_where)))\"}\n  %broadcast.3 = f32[10]{0} broadcast(%constant.3), dimensions={}, metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp(jit(_where)))\"}\n  %constant.2 = f32[] constant(1)\n  %broadcast.2 = f32[10]{0} broadcast(%constant.2), dimensions={}\n  %select_n.5 = f32[10]{0} select(%gt.0, %broadcast.3, %broadcast.2), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp(jit(_where)))/select_n\" stack_frame_id=11}\n  %select_n.4 = f32[10]{0} select(%gt.0, %broadcast.2, %broadcast.3), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp(jit(_where)))/select_n\" stack_frame_id=11}\n  %constant.1 = f32[] constant(2.8)\n  %broadcast.1 = f32[10]{0} broadcast(%constant.1), dimensions={}\n  %mul.0 = f32[10]{0} multiply(%select_n.4, %broadcast.1), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/mul\" stack_frame_id=6}\n  ROOT %add_any.0 = f32[10]{0} add(%select_n.5, %mul.0), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/add_any\" stack_frame_id=6}\n}\n\nENTRY %main.2 (args_0_.1: f32[10]) -> f32[10] {\n  %args_0_.1 = f32[10]{0} parameter(0), metadata={op_name=\"args[0]\"}\n  ROOT %multiply_add_fusion = f32[10]{0} fusion(%args_0_.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/add_any\" stack_frame_id=6}\n}\n\n"}
{"id": 10, "kernel_name": "loop_zbsm", "python": "def loop_zbsm(a):\n    \"\"\"Reduction operation simulating loop accumulation.\"\"\"\n    # Accumulate n=5 times\n    result = a\n    for _ in range(5):\n        result = result - a\n    return result", "type": "generate_loop_kernel", "hlo": "// ===== FORWARD PASS =====\nmodule @jit_loop_zbsm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg0 : tensor<10xf32>\n    %1 = stablehlo.subtract %0, %arg0 : tensor<10xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<10xf32>\n    %3 = stablehlo.subtract %2, %arg0 : tensor<10xf32>\n    %4 = stablehlo.subtract %3, %arg0 : tensor<10xf32>\n    return %4 : tensor<10xf32>\n  }\n}\n\n\n// ===== BACKWARD PASS =====\nmodule @jit_scalar_output_wrapper attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main() -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %1 = stablehlo.negate %0 : tensor<10xf32>\n    %2 = stablehlo.negate %0 : tensor<10xf32>\n    %3 = stablehlo.add %1, %2 : tensor<10xf32>\n    %4 = stablehlo.negate %0 : tensor<10xf32>\n    %5 = stablehlo.add %3, %4 : tensor<10xf32>\n    %6 = stablehlo.negate %0 : tensor<10xf32>\n    %7 = stablehlo.add %5, %6 : tensor<10xf32>\n    %8 = stablehlo.negate %0 : tensor<10xf32>\n    %9 = stablehlo.add %7, %0 : tensor<10xf32>\n    %10 = stablehlo.add %9, %8 : tensor<10xf32>\n    return %10 : tensor<10xf32>\n  }\n}\n", "optimized_hlo": "// ===== FORWARD PASS (OPTIMIZED) =====\nHloModule jit_loop_zbsm, is_scheduled=true, entry_computation_layout={(f32[10]{0})->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmpfak5io52.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"loop_zbsm\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=41 end_line=41 column=14 end_column=47}\n5 {file_name_id=3 function_name_id=5 line=10 end_line=10 column=17 end_column=27}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[10]) -> f32[10] {\n  %param_0.1 = f32[10]{0} parameter(0)\n  %sub.4 = f32[10]{0} subtract(%param_0.1, %param_0.1), metadata={op_name=\"jit(loop_zbsm)/sub\" stack_frame_id=5}\n  %sub.3 = f32[10]{0} subtract(%sub.4, %param_0.1), metadata={op_name=\"jit(loop_zbsm)/sub\" stack_frame_id=5}\n  %sub.2 = f32[10]{0} subtract(%sub.3, %param_0.1), metadata={op_name=\"jit(loop_zbsm)/sub\" stack_frame_id=5}\n  %sub.1 = f32[10]{0} subtract(%sub.2, %param_0.1), metadata={op_name=\"jit(loop_zbsm)/sub\" stack_frame_id=5}\n  ROOT %sub.0 = f32[10]{0} subtract(%sub.1, %param_0.1), metadata={op_name=\"jit(loop_zbsm)/sub\" stack_frame_id=5}\n}\n\nENTRY %main.1 (a.1: f32[10]) -> f32[10] {\n  %a.1 = f32[10]{0} parameter(0), metadata={op_name=\"a\"}\n  ROOT %subtract_subtract_fusion = f32[10]{0} fusion(%a.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(loop_zbsm)/sub\" stack_frame_id=5}\n}\n\n\n\n// ===== BACKWARD PASS (OPTIMIZED) =====\nHloModule jit_scalar_output_wrapper, is_scheduled=true, entry_computation_layout={()->f32[10]{0}}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmpfak5io52.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"extract_ir.<locals>.scalar_output_wrapper\"\n6 \"loop_zbsm\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=66 end_line=66 column=27 end_column=65}\n5 {file_name_id=2 function_name_id=5 line=58 end_line=58 column=21 end_column=32}\n6 {file_name_id=3 function_name_id=6 line=10 end_line=10 column=17 end_column=27}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=6}\n\n\n%wrapped_broadcast_computation (param_0: f32[]) -> f32[10] {\n  %param_0 = f32[] parameter(0)\n  ROOT %broadcast.0 = f32[10]{0} broadcast(%param_0), dimensions={}\n}\n\nENTRY %main.1 () -> f32[10] {\n  %constant.1 = f32[] constant(-4)\n  ROOT %wrapped_broadcast = f32[10]{0} fusion(%constant.1), kind=kLoop, calls=%wrapped_broadcast_computation\n}\n\n"}
{"id": 11, "kernel_name": "compound_qxls", "python": "def compound_qxls(a, b, scale):\n    \"\"\"Compound operations with multiple steps.\"\"\"\n    result = (a + b) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "hlo": "// ===== FORWARD PASS =====\nmodule @jit_compound_qxls attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>, %arg1: tensor<10xf32>, %arg2: tensor<f32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<10xf32>\n    %1 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %2 = stablehlo.multiply %0, %1 : tensor<10xf32>\n    %3 = stablehlo.floor %2 : tensor<10xf32>\n    %4 = stablehlo.subtract %2, %3 : tensor<10xf32>\n    %5 = stablehlo.abs %4 : tensor<10xf32>\n    return %5 : tensor<10xf32>\n  }\n}\n\n\n// ===== BACKWARD PASS =====\nmodule @jit_scalar_output_wrapper attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>, %arg1: tensor<10xf32>, %arg2: tensor<f32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<10xf32>\n    %1 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %2 = stablehlo.multiply %0, %1 : tensor<10xf32>\n    %3 = stablehlo.floor %2 : tensor<10xf32>\n    %4 = stablehlo.subtract %2, %3 : tensor<10xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %5 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %6 = stablehlo.compare  GE, %4, %5,  FLOAT : (tensor<10xf32>, tensor<10xf32>) -> tensor<10xi1>\n    %cst_0 = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %8 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %9 = stablehlo.select %6, %8, %7 : tensor<10xi1>, tensor<10xf32>\n    %10 = stablehlo.select %6, %7, %8 : tensor<10xi1>, tensor<10xf32>\n    %11 = stablehlo.negate %9 : tensor<10xf32>\n    %12 = stablehlo.add %10, %11 : tensor<10xf32>\n    %13 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %14 = stablehlo.multiply %12, %13 : tensor<10xf32>\n    return %14 : tensor<10xf32>\n  }\n}\n", "optimized_hlo": "// ===== FORWARD PASS (OPTIMIZED) =====\nHloModule jit_compound_qxls, is_scheduled=true, entry_computation_layout={(f32[10]{0}, f32[10]{0}, f32[])->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmpb4_bueqa.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"compound_qxls\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=41 end_line=41 column=14 end_column=47}\n5 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=14 end_column=19}\n6 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=13 end_column=28}\n7 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=22 end_column=39}\n8 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=13 end_column=39}\n9 {file_name_id=3 function_name_id=5 line=9 end_line=9 column=11 end_column=26}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=5}\n8 {file_location_id=8 parent_frame_id=5}\n9 {file_location_id=9 parent_frame_id=5}\n\n\n%fused_computation (param_0.3: f32[], param_1.3: f32[10], param_2: f32[10]) -> f32[10] {\n  %param_1.3 = f32[10]{0} parameter(1)\n  %param_2 = f32[10]{0} parameter(2)\n  %add.0 = f32[10]{0} add(%param_1.3, %param_2), metadata={op_name=\"jit(compound_qxls)/add\" stack_frame_id=5}\n  %param_0.3 = f32[] parameter(0)\n  %mul.1 = f32[10]{0} broadcast(%param_0.3), dimensions={}, metadata={op_name=\"jit(compound_qxls)/mul\" stack_frame_id=6}\n  %mul.0 = f32[10]{0} multiply(%add.0, %mul.1), metadata={op_name=\"jit(compound_qxls)/mul\" stack_frame_id=6}\n  %floor.0 = f32[10]{0} floor(%mul.0), metadata={op_name=\"jit(compound_qxls)/floor\" stack_frame_id=7}\n  %sub.0 = f32[10]{0} subtract(%mul.0, %floor.0), metadata={op_name=\"jit(compound_qxls)/sub\" stack_frame_id=8}\n  ROOT %abs.0 = f32[10]{0} abs(%sub.0), metadata={op_name=\"jit(compound_qxls)/abs\" stack_frame_id=9}\n}\n\nENTRY %main.1 (a.1: f32[10], b.1: f32[10], scale.1: f32[]) -> f32[10] {\n  %a.1 = f32[10]{0} parameter(0), metadata={op_name=\"a\"}\n  %b.1 = f32[10]{0} parameter(1), metadata={op_name=\"b\"}\n  %scale.1 = f32[] parameter(2), metadata={op_name=\"scale\"}\n  ROOT %subtract_abs_fusion = f32[10]{0} fusion(%scale.1, %a.1, %b.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(compound_qxls)/abs\" stack_frame_id=9}\n}\n\n\n\n// ===== BACKWARD PASS (OPTIMIZED) =====\nHloModule jit_scalar_output_wrapper, is_scheduled=true, entry_computation_layout={(f32[10]{0}, f32[10]{0}, f32[])->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmpb4_bueqa.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"extract_ir.<locals>.scalar_output_wrapper\"\n6 \"compound_qxls\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=66 end_line=66 column=27 end_column=65}\n5 {file_name_id=2 function_name_id=5 line=61 end_line=61 column=23 end_column=38}\n6 {file_name_id=2 function_name_id=5 line=58 end_line=58 column=21 end_column=32}\n7 {file_name_id=3 function_name_id=6 line=7 end_line=7 column=14 end_column=19}\n8 {file_name_id=3 function_name_id=6 line=7 end_line=7 column=13 end_column=28}\n9 {file_name_id=3 function_name_id=6 line=8 end_line=8 column=22 end_column=39}\n10 {file_name_id=3 function_name_id=6 line=8 end_line=8 column=13 end_column=39}\n11 {file_name_id=3 function_name_id=6 line=9 end_line=9 column=11 end_column=26}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=7}\n8 {file_location_id=8 parent_frame_id=7}\n9 {file_location_id=9 parent_frame_id=7}\n10 {file_location_id=10 parent_frame_id=7}\n11 {file_location_id=11 parent_frame_id=7}\n\n\n%fused_computation (param_0.3: f32[], param_1.8: f32[10], param_2.7: f32[10]) -> f32[10] {\n  %param_1.8 = f32[10]{0} parameter(1)\n  %param_2.7 = f32[10]{0} parameter(2)\n  %add.0 = f32[10]{0} add(%param_1.8, %param_2.7), metadata={op_name=\"jit(scalar_output_wrapper)/jvp()/add\" stack_frame_id=7}\n  %param_0.3 = f32[] parameter(0)\n  %mul.2 = f32[10]{0} broadcast(%param_0.3), dimensions={}, metadata={op_name=\"jit(scalar_output_wrapper)/jvp()/mul\" stack_frame_id=8}\n  %mul.1 = f32[10]{0} multiply(%add.0, %mul.2), metadata={op_name=\"jit(scalar_output_wrapper)/jvp()/mul\" stack_frame_id=8}\n  %floor.0 = f32[10]{0} floor(%mul.1), metadata={op_name=\"jit(scalar_output_wrapper)/jvp()/floor\" stack_frame_id=9}\n  %sub.0 = f32[10]{0} subtract(%mul.1, %floor.0), metadata={op_name=\"jit(scalar_output_wrapper)/jvp()/sub\" stack_frame_id=10}\n  %constant.1 = f32[] constant(0)\n  %broadcast.1 = f32[10]{0} broadcast(%constant.1), dimensions={}\n  %ge.0 = pred[10]{0} compare(%sub.0, %broadcast.1), direction=GE, metadata={op_name=\"jit(scalar_output_wrapper)/jvp()/ge\" stack_frame_id=11}\n  %constant.0 = f32[] constant(1)\n  %broadcast.0 = f32[10]{0} broadcast(%constant.0), dimensions={}\n  %select_n.1 = f32[10]{0} select(%ge.0, %broadcast.0, %broadcast.1), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/select_n\" stack_frame_id=11}\n  %select_n.0 = f32[10]{0} select(%ge.0, %broadcast.1, %broadcast.0), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/select_n\" stack_frame_id=11}\n  %neg.0 = f32[10]{0} negate(%select_n.0), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/neg\" stack_frame_id=11}\n  %add_any.0 = f32[10]{0} add(%select_n.1, %neg.0), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/add_any\" stack_frame_id=11}\n  ROOT %mul.0 = f32[10]{0} multiply(%add_any.0, %mul.2), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/mul\" stack_frame_id=8}\n}\n\nENTRY %main.1 (args_0_.1: f32[10], args_1_.1: f32[10], args_2_.1: f32[]) -> f32[10] {\n  %args_0_.1 = f32[10]{0} parameter(0), metadata={op_name=\"args[0]\"}\n  %args_1_.1 = f32[10]{0} parameter(1), metadata={op_name=\"args[1]\"}\n  %args_2_.1 = f32[] parameter(2), metadata={op_name=\"args[2]\"}\n  ROOT %add_multiply_fusion = f32[10]{0} fusion(%args_2_.1, %args_0_.1, %args_1_.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/mul\" stack_frame_id=8}\n}\n\n"}
{"id": 12, "kernel_name": "unary_xdom", "python": "def unary_xdom(a):\n    \"\"\"Unary operation on array.\"\"\"\n    return jnp.cos(a)", "type": "generate_unary_kernel", "hlo": "// ===== FORWARD PASS =====\nmodule @jit_unary_xdom attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.cosine %arg0 : tensor<10xf32>\n    return %0 : tensor<10xf32>\n  }\n}\n\n\n// ===== BACKWARD PASS =====\nmodule @jit_scalar_output_wrapper attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<10xf32>\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %2 = stablehlo.negate %1 : tensor<10xf32>\n    %3 = stablehlo.multiply %2, %0 : tensor<10xf32>\n    return %3 : tensor<10xf32>\n  }\n}\n", "optimized_hlo": "// ===== FORWARD PASS (OPTIMIZED) =====\nHloModule jit_unary_xdom, is_scheduled=true, entry_computation_layout={(f32[10]{0})->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmplmoym7wr.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"unary_xdom\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=41 end_line=41 column=14 end_column=47}\n5 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=11 end_column=21}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n\n\n%wrapped_cosine_computation (param_0: f32[10]) -> f32[10] {\n  %param_0 = f32[10]{0} parameter(0)\n  ROOT %cos.0 = f32[10]{0} cosine(%param_0), metadata={op_name=\"jit(unary_xdom)/cos\" stack_frame_id=5}\n}\n\nENTRY %main.1 (a.1: f32[10]) -> f32[10] {\n  %a.1 = f32[10]{0} parameter(0), metadata={op_name=\"a\"}\n  ROOT %wrapped_cosine = f32[10]{0} fusion(%a.1), kind=kLoop, calls=%wrapped_cosine_computation, metadata={op_name=\"jit(unary_xdom)/cos\" stack_frame_id=5}\n}\n\n\n\n// ===== BACKWARD PASS (OPTIMIZED) =====\nHloModule jit_scalar_output_wrapper, is_scheduled=true, entry_computation_layout={(f32[10]{0})->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmplmoym7wr.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"extract_ir.<locals>.scalar_output_wrapper\"\n6 \"unary_xdom\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=66 end_line=66 column=27 end_column=65}\n5 {file_name_id=2 function_name_id=5 line=58 end_line=58 column=21 end_column=32}\n6 {file_name_id=3 function_name_id=6 line=7 end_line=7 column=11 end_column=21}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=6}\n\n\n%fused_computation (param_0.1: f32[10]) -> f32[10] {\n  %param_0.1 = f32[10]{0} parameter(0)\n  %sin.0 = f32[10]{0} sine(%param_0.1), metadata={op_name=\"jit(scalar_output_wrapper)/jvp()/sin\" stack_frame_id=6}\n  %constant.0 = f32[] constant(-1)\n  %broadcast.0 = f32[10]{0} broadcast(%constant.0), dimensions={}\n  ROOT %neg.0 = f32[10]{0} multiply(%sin.0, %broadcast.0), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/mul;jit(scalar_output_wrapper)/transpose(jvp())/neg\"}\n}\n\nENTRY %main.1 (args_0_.1: f32[10]) -> f32[10] {\n  %args_0_.1 = f32[10]{0} parameter(0), metadata={op_name=\"args[0]\"}\n  ROOT %broadcast_multiply_fusion = f32[10]{0} fusion(%args_0_.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/mul;jit(scalar_output_wrapper)/transpose(jvp())/neg\"}\n}\n\n"}
{"id": 13, "kernel_name": "scalar_arr_cydt", "python": "def scalar_arr_cydt(alpha, x, y):\n    \"\"\"Scalar and array operations.\"\"\"\n    return alpha + x * y", "type": "generate_scalar_array_op", "hlo": "// ===== FORWARD PASS =====\nmodule @jit_scalar_arr_cydt attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<10xf32>, %arg2: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<10xf32>\n    %1 = stablehlo.broadcast_in_dim %arg0, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %2 = stablehlo.add %1, %0 : tensor<10xf32>\n    return %2 : tensor<10xf32>\n  }\n}\n\n\n// ===== BACKWARD PASS =====\nmodule @jit_scalar_output_wrapper attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main() -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst_0) applies stablehlo.add across dimensions = [0] : (tensor<10xf32>, tensor<f32>) -> tensor<f32>\n    return %1 : tensor<f32>\n  }\n}\n", "optimized_hlo": "// ===== FORWARD PASS (OPTIMIZED) =====\nHloModule jit_scalar_arr_cydt, is_scheduled=true, entry_computation_layout={(f32[], f32[10]{0}, f32[10]{0})->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmp4ttbq9dp.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"scalar_arr_cydt\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=41 end_line=41 column=14 end_column=47}\n5 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=19 end_column=24}\n6 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=11 end_column=24}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[10], param_1.2: f32[10], param_2.1: f32[]) -> f32[10] {\n  %param_2.1 = f32[] parameter(2)\n  %add.1 = f32[10]{0} broadcast(%param_2.1), dimensions={}, metadata={op_name=\"jit(scalar_arr_cydt)/add\" stack_frame_id=6}\n  %param_0.1 = f32[10]{0} parameter(0)\n  %param_1.2 = f32[10]{0} parameter(1)\n  %mul.0 = f32[10]{0} multiply(%param_0.1, %param_1.2), metadata={op_name=\"jit(scalar_arr_cydt)/mul\" stack_frame_id=5}\n  ROOT %add.0 = f32[10]{0} add(%add.1, %mul.0), metadata={op_name=\"jit(scalar_arr_cydt)/add\" stack_frame_id=6}\n}\n\nENTRY %main.1 (alpha.1: f32[], x.1: f32[10], y.1: f32[10]) -> f32[10] {\n  %alpha.1 = f32[] parameter(0), metadata={op_name=\"alpha\"}\n  %x.1 = f32[10]{0} parameter(1), metadata={op_name=\"x\"}\n  %y.1 = f32[10]{0} parameter(2), metadata={op_name=\"y\"}\n  ROOT %multiply_add_fusion = f32[10]{0} fusion(%x.1, %y.1, %alpha.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(scalar_arr_cydt)/add\" stack_frame_id=6}\n}\n\n\n\n// ===== BACKWARD PASS (OPTIMIZED) =====\nHloModule jit_scalar_output_wrapper, is_scheduled=true, entry_computation_layout={()->f32[]}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmp4ttbq9dp.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"extract_ir.<locals>.scalar_output_wrapper\"\n6 \"scalar_arr_cydt\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=66 end_line=66 column=27 end_column=65}\n5 {file_name_id=2 function_name_id=5 line=58 end_line=58 column=21 end_column=32}\n6 {file_name_id=3 function_name_id=6 line=7 end_line=7 column=11 end_column=24}\n7 {file_name_id=2 function_name_id=5 line=61 end_line=61 column=23 end_column=38}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=6}\n7 {file_location_id=7 parent_frame_id=5}\n\n\nENTRY %main.2 () -> f32[] {\n  %constant = f32[] constant(10), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/reduce_sum\" stack_frame_id=6}\n  ROOT %copy = f32[] copy(%constant)\n}\n\n"}
{"id": 14, "kernel_name": "nested_zomn", "python": "def nested_zomn(a):\n    \"\"\"Nested conditional using jnp.where.\"\"\"\n    return jnp.where(a > 0.27, \n                     jnp.where(a > 1.49, a * 3.0, a * 2.0),\n                     a * 0.5)", "type": "generate_nested_branch_kernel", "hlo": "// ===== FORWARD PASS =====\nmodule @jit_nested_zomn attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<2.700000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<10xf32>, tensor<10xf32>) -> tensor<10xi1>\n    %cst_0 = stablehlo.constant dense<1.490000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %3 = stablehlo.compare  GT, %arg0, %2,  FLOAT : (tensor<10xf32>, tensor<10xf32>) -> tensor<10xi1>\n    %cst_1 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<10xf32>\n    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %6 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %7 = stablehlo.multiply %arg0, %6 : tensor<10xf32>\n    %8 = call @_where(%3, %5, %7) : (tensor<10xi1>, tensor<10xf32>, tensor<10xf32>) -> tensor<10xf32>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<10xf32>\n    %11 = call @_where(%1, %8, %10) : (tensor<10xi1>, tensor<10xf32>, tensor<10xf32>) -> tensor<10xf32>\n    return %11 : tensor<10xf32>\n  }\n  func.func private @_where(%arg0: tensor<10xi1>, %arg1: tensor<10xf32>, %arg2: tensor<10xf32>) -> tensor<10xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<10xi1>, tensor<10xf32>\n    return %0 : tensor<10xf32>\n  }\n}\n\n\n// ===== BACKWARD PASS =====\nmodule @jit_scalar_output_wrapper attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<2.700000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<10xf32>, tensor<10xf32>) -> tensor<10xi1>\n    %cst_0 = stablehlo.constant dense<1.490000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %3 = stablehlo.compare  GT, %arg0, %2,  FLOAT : (tensor<10xf32>, tensor<10xf32>) -> tensor<10xi1>\n    %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %5:2 = call @_where(%1, %4) : (tensor<10xi1>, tensor<10xf32>) -> (tensor<10xf32>, tensor<10xf32>)\n    %cst_2 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %6 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %7 = stablehlo.multiply %5#1, %6 : tensor<10xf32>\n    %8:2 = call @_where(%3, %5#0) : (tensor<10xi1>, tensor<10xf32>) -> (tensor<10xf32>, tensor<10xf32>)\n    %cst_3 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %10 = stablehlo.multiply %8#1, %9 : tensor<10xf32>\n    %11 = stablehlo.add %7, %10 : tensor<10xf32>\n    %cst_4 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %12 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %13 = stablehlo.multiply %8#0, %12 : tensor<10xf32>\n    %14 = stablehlo.add %11, %13 : tensor<10xf32>\n    return %14 : tensor<10xf32>\n  }\n  func.func private @_where(%arg0: tensor<10xi1>, %arg1: tensor<10xf32>) -> (tensor<10xf32>, tensor<10xf32>) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %1 = stablehlo.select %arg0, %0, %arg1 : tensor<10xi1>, tensor<10xf32>\n    %2 = stablehlo.select %arg0, %arg1, %0 : tensor<10xi1>, tensor<10xf32>\n    return %2, %1 : tensor<10xf32>, tensor<10xf32>\n  }\n}\n", "optimized_hlo": "// ===== FORWARD PASS (OPTIMIZED) =====\nHloModule jit_nested_zomn, is_scheduled=true, entry_computation_layout={(f32[10]{0})->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmpw1zwza68.py\"\n4 \"/tmp/tmphr9su7i4.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"nested_zomn\"\n6 \"nested_odsh\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=41 end_line=41 column=14 end_column=47}\n5 {file_name_id=3 function_name_id=5 line=9 end_line=9 column=21 end_column=28}\n6 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=50 end_column=57}\n7 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=41 end_column=48}\n8 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=31 end_column=39}\n9 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=21 end_column=29}\n10 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=21 end_column=58}\n11 {file_name_id=4 function_name_id=6 line=8 end_line=8 column=21 end_column=58}\n12 {file_name_id=3 function_name_id=5 line=7 end_line=9 column=11 end_column=29}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=5}\n8 {file_location_id=8 parent_frame_id=5}\n9 {file_location_id=9 parent_frame_id=5}\n10 {file_location_id=10 parent_frame_id=5}\n11 {file_location_id=11 parent_frame_id=5}\n12 {file_location_id=12 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[10]) -> f32[10] {\n  %param_0.1 = f32[10]{0} parameter(0)\n  %constant.4 = f32[] constant(0.27)\n  %broadcast.4 = f32[10]{0} broadcast(%constant.4), dimensions={}\n  %gt.1 = pred[10]{0} compare(%param_0.1, %broadcast.4), direction=GT, metadata={op_name=\"jit(nested_zomn)/gt\" stack_frame_id=9}\n  %constant.3 = f32[] constant(1.49)\n  %broadcast.3 = f32[10]{0} broadcast(%constant.3), dimensions={}\n  %gt.0 = pred[10]{0} compare(%param_0.1, %broadcast.3), direction=GT, metadata={op_name=\"jit(nested_zomn)/gt\" stack_frame_id=8}\n  %constant.2 = f32[] constant(3)\n  %broadcast.2 = f32[10]{0} broadcast(%constant.2), dimensions={}\n  %mul.2 = f32[10]{0} multiply(%param_0.1, %broadcast.2), metadata={op_name=\"jit(nested_zomn)/mul\" stack_frame_id=7}\n  %constant.1 = f32[] constant(2)\n  %broadcast.1 = f32[10]{0} broadcast(%constant.1), dimensions={}\n  %mul.1 = f32[10]{0} multiply(%param_0.1, %broadcast.1), metadata={op_name=\"jit(nested_zomn)/mul\" stack_frame_id=6}\n  %select_n.5 = f32[10]{0} select(%gt.0, %mul.2, %mul.1), metadata={op_name=\"jit(nested_zomn)/jit(_where)/select_n\" stack_frame_id=11}\n  %constant.0 = f32[] constant(0.5)\n  %broadcast.0 = f32[10]{0} broadcast(%constant.0), dimensions={}\n  %mul.0 = f32[10]{0} multiply(%param_0.1, %broadcast.0), metadata={op_name=\"jit(nested_zomn)/mul\" stack_frame_id=5}\n  ROOT %select_n.4 = f32[10]{0} select(%gt.1, %select_n.5, %mul.0), metadata={op_name=\"jit(nested_zomn)/jit(_where)/select_n\" stack_frame_id=11}\n}\n\nENTRY %main.2 (a.1: f32[10]) -> f32[10] {\n  %a.1 = f32[10]{0} parameter(0), metadata={op_name=\"a\"}\n  ROOT %multiply_select_fusion = f32[10]{0} fusion(%a.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(nested_zomn)/jit(_where)/select_n\" stack_frame_id=11}\n}\n\n\n\n// ===== BACKWARD PASS (OPTIMIZED) =====\nHloModule jit_scalar_output_wrapper, is_scheduled=true, entry_computation_layout={(f32[10]{0})->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmpw1zwza68.py\"\n4 \"/tmp/tmphr9su7i4.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"extract_ir.<locals>.scalar_output_wrapper\"\n6 \"nested_zomn\"\n7 \"nested_odsh\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=66 end_line=66 column=27 end_column=65}\n5 {file_name_id=2 function_name_id=5 line=58 end_line=58 column=21 end_column=32}\n6 {file_name_id=3 function_name_id=6 line=8 end_line=8 column=41 end_column=48}\n7 {file_name_id=3 function_name_id=6 line=8 end_line=8 column=50 end_column=57}\n8 {file_name_id=3 function_name_id=6 line=9 end_line=9 column=21 end_column=28}\n9 {file_name_id=2 function_name_id=5 line=61 end_line=61 column=23 end_column=38}\n10 {file_name_id=3 function_name_id=6 line=8 end_line=8 column=31 end_column=39}\n11 {file_name_id=3 function_name_id=6 line=7 end_line=7 column=21 end_column=29}\n12 {file_name_id=3 function_name_id=6 line=7 end_line=9 column=11 end_column=29}\n13 {file_name_id=2 function_name_id=4 line=41 end_line=41 column=14 end_column=47}\n14 {file_name_id=4 function_name_id=7 line=8 end_line=8 column=21 end_column=58}\n15 {file_name_id=3 function_name_id=6 line=8 end_line=8 column=21 end_column=58}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=6}\n7 {file_location_id=7 parent_frame_id=6}\n8 {file_location_id=8 parent_frame_id=6}\n9 {file_location_id=9 parent_frame_id=5}\n10 {file_location_id=10 parent_frame_id=6}\n11 {file_location_id=11 parent_frame_id=6}\n12 {file_location_id=12 parent_frame_id=6}\n13 {file_location_id=13 parent_frame_id=4}\n14 {file_location_id=14 parent_frame_id=14}\n15 {file_location_id=15 parent_frame_id=6}\n\n\n%fused_computation (param_0.4: f32[10]) -> f32[10] {\n  %param_0.4 = f32[10]{0} parameter(0)\n  %constant.16 = f32[] constant(0.27)\n  %broadcast.16 = f32[10]{0} broadcast(%constant.16), dimensions={}\n  %gt.1 = pred[10]{0} compare(%param_0.4, %broadcast.16), direction=GT, metadata={op_name=\"jit(scalar_output_wrapper)/jvp()/gt\" stack_frame_id=11}\n  %constant.15 = f32[] constant(0), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp(jit(_where)))\"}\n  %broadcast.15 = f32[10]{0} broadcast(%constant.15), dimensions={}, metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp(jit(_where)))\"}\n  %constant.14 = f32[] constant(1)\n  %broadcast.14 = f32[10]{0} broadcast(%constant.14), dimensions={}\n  %select_n.11 = f32[10]{0} select(%gt.1, %broadcast.15, %broadcast.14), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp(jit(_where)))/select_n\" stack_frame_id=14}\n  %constant.6 = f32[] constant(0.5)\n  %broadcast.6 = f32[10]{0} broadcast(%constant.6), dimensions={}\n  %mul.2 = f32[10]{0} multiply(%select_n.11, %broadcast.6), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/mul\" stack_frame_id=8}\n  %constant.5 = f32[] constant(1.49)\n  %broadcast.5 = f32[10]{0} broadcast(%constant.5), dimensions={}\n  %gt.0 = pred[10]{0} compare(%param_0.4, %broadcast.5), direction=GT, metadata={op_name=\"jit(scalar_output_wrapper)/jvp()/gt\" stack_frame_id=10}\n  %select_n.10 = f32[10]{0} select(%gt.1, %broadcast.14, %broadcast.15), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp(jit(_where)))/select_n\" stack_frame_id=14}\n  %select_n.9 = f32[10]{0} select(%gt.0, %broadcast.15, %select_n.10), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp(jit(_where)))/select_n\" stack_frame_id=14}\n  %constant.4 = f32[] constant(2)\n  %broadcast.4 = f32[10]{0} broadcast(%constant.4), dimensions={}\n  %mul.1 = f32[10]{0} multiply(%select_n.9, %broadcast.4), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/mul\" stack_frame_id=7}\n  %add_any.1 = f32[10]{0} add(%mul.2, %mul.1), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/add_any\" stack_frame_id=7}\n  %select_n.8 = f32[10]{0} select(%gt.0, %select_n.10, %broadcast.15), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp(jit(_where)))/select_n\" stack_frame_id=14}\n  %constant.3 = f32[] constant(3)\n  %broadcast.3 = f32[10]{0} broadcast(%constant.3), dimensions={}\n  %mul.0 = f32[10]{0} multiply(%select_n.8, %broadcast.3), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/mul\" stack_frame_id=6}\n  ROOT %add_any.0 = f32[10]{0} add(%add_any.1, %mul.0), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/add_any\" stack_frame_id=6}\n}\n\nENTRY %main.2 (args_0_.1: f32[10]) -> f32[10] {\n  %args_0_.1 = f32[10]{0} parameter(0), metadata={op_name=\"args[0]\"}\n  ROOT %multiply_add_fusion = f32[10]{0} fusion(%args_0_.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/add_any\" stack_frame_id=6}\n}\n\n"}
{"id": 15, "kernel_name": "elementwise_bpan", "python": "def elementwise_bpan(a, b):\n    \"\"\"Elementwise operation on two arrays.\"\"\"\n    return a - b", "type": "generate_simple_elementwise", "hlo": "// ===== FORWARD PASS =====\nmodule @jit_elementwise_bpan attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>, %arg1: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<10xf32>\n    return %0 : tensor<10xf32>\n  }\n}\n\n\n// ===== BACKWARD PASS =====\nmodule @jit_scalar_output_wrapper attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main() -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    return %0 : tensor<10xf32>\n  }\n}\n", "optimized_hlo": "// ===== FORWARD PASS (OPTIMIZED) =====\nHloModule jit_elementwise_bpan, is_scheduled=true, entry_computation_layout={(f32[10]{0}, f32[10]{0})->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmp7zy1b8wk.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"elementwise_bpan\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=41 end_line=41 column=14 end_column=47}\n5 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=11 end_column=16}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n\n\n%wrapped_subtract_computation (param_0: f32[10], param_1: f32[10]) -> f32[10] {\n  %param_0 = f32[10]{0} parameter(0)\n  %param_1 = f32[10]{0} parameter(1)\n  ROOT %sub.0 = f32[10]{0} subtract(%param_0, %param_1), metadata={op_name=\"jit(elementwise_bpan)/sub\" stack_frame_id=5}\n}\n\nENTRY %main.1 (a.1: f32[10], b.1: f32[10]) -> f32[10] {\n  %a.1 = f32[10]{0} parameter(0), metadata={op_name=\"a\"}\n  %b.1 = f32[10]{0} parameter(1), metadata={op_name=\"b\"}\n  ROOT %wrapped_subtract = f32[10]{0} fusion(%a.1, %b.1), kind=kLoop, calls=%wrapped_subtract_computation, metadata={op_name=\"jit(elementwise_bpan)/sub\" stack_frame_id=5}\n}\n\n\n\n// ===== BACKWARD PASS (OPTIMIZED) =====\nHloModule jit_scalar_output_wrapper, is_scheduled=true, entry_computation_layout={()->f32[10]{0}}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"extract_ir.<locals>.scalar_output_wrapper\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=66 end_line=66 column=27 end_column=65}\n5 {file_name_id=2 function_name_id=5 line=61 end_line=61 column=23 end_column=38}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n\n\n%wrapped_broadcast_computation (param_0: f32[]) -> f32[10] {\n  %param_0 = f32[] parameter(0)\n  ROOT %broadcast.0 = f32[10]{0} broadcast(%param_0), dimensions={}\n}\n\nENTRY %main.1 () -> f32[10] {\n  %constant.1 = f32[] constant(1)\n  ROOT %wrapped_broadcast = f32[10]{0} fusion(%constant.1), kind=kLoop, calls=%wrapped_broadcast_computation\n}\n\n"}
{"id": 16, "kernel_name": "compound_pfqy", "python": "def compound_pfqy(a, b, scale):\n    \"\"\"Compound operations with multiple steps.\"\"\"\n    result = (a + b) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "hlo": "// ===== FORWARD PASS =====\nmodule @jit_compound_pfqy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>, %arg1: tensor<10xf32>, %arg2: tensor<f32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<10xf32>\n    %1 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %2 = stablehlo.multiply %0, %1 : tensor<10xf32>\n    %3 = stablehlo.floor %2 : tensor<10xf32>\n    %4 = stablehlo.subtract %2, %3 : tensor<10xf32>\n    %5 = stablehlo.abs %4 : tensor<10xf32>\n    return %5 : tensor<10xf32>\n  }\n}\n\n\n// ===== BACKWARD PASS =====\nmodule @jit_scalar_output_wrapper attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>, %arg1: tensor<10xf32>, %arg2: tensor<f32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<10xf32>\n    %1 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %2 = stablehlo.multiply %0, %1 : tensor<10xf32>\n    %3 = stablehlo.floor %2 : tensor<10xf32>\n    %4 = stablehlo.subtract %2, %3 : tensor<10xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %5 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %6 = stablehlo.compare  GE, %4, %5,  FLOAT : (tensor<10xf32>, tensor<10xf32>) -> tensor<10xi1>\n    %cst_0 = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %8 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %9 = stablehlo.select %6, %8, %7 : tensor<10xi1>, tensor<10xf32>\n    %10 = stablehlo.select %6, %7, %8 : tensor<10xi1>, tensor<10xf32>\n    %11 = stablehlo.negate %9 : tensor<10xf32>\n    %12 = stablehlo.add %10, %11 : tensor<10xf32>\n    %13 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %14 = stablehlo.multiply %12, %13 : tensor<10xf32>\n    return %14 : tensor<10xf32>\n  }\n}\n", "optimized_hlo": "// ===== FORWARD PASS (OPTIMIZED) =====\nHloModule jit_compound_pfqy, is_scheduled=true, entry_computation_layout={(f32[10]{0}, f32[10]{0}, f32[])->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmp2jtx8kjp.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"compound_pfqy\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=41 end_line=41 column=14 end_column=47}\n5 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=14 end_column=19}\n6 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=13 end_column=28}\n7 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=22 end_column=39}\n8 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=13 end_column=39}\n9 {file_name_id=3 function_name_id=5 line=9 end_line=9 column=11 end_column=26}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=5}\n8 {file_location_id=8 parent_frame_id=5}\n9 {file_location_id=9 parent_frame_id=5}\n\n\n%fused_computation (param_0.3: f32[], param_1.3: f32[10], param_2: f32[10]) -> f32[10] {\n  %param_1.3 = f32[10]{0} parameter(1)\n  %param_2 = f32[10]{0} parameter(2)\n  %add.0 = f32[10]{0} add(%param_1.3, %param_2), metadata={op_name=\"jit(compound_pfqy)/add\" stack_frame_id=5}\n  %param_0.3 = f32[] parameter(0)\n  %mul.1 = f32[10]{0} broadcast(%param_0.3), dimensions={}, metadata={op_name=\"jit(compound_pfqy)/mul\" stack_frame_id=6}\n  %mul.0 = f32[10]{0} multiply(%add.0, %mul.1), metadata={op_name=\"jit(compound_pfqy)/mul\" stack_frame_id=6}\n  %floor.0 = f32[10]{0} floor(%mul.0), metadata={op_name=\"jit(compound_pfqy)/floor\" stack_frame_id=7}\n  %sub.0 = f32[10]{0} subtract(%mul.0, %floor.0), metadata={op_name=\"jit(compound_pfqy)/sub\" stack_frame_id=8}\n  ROOT %abs.0 = f32[10]{0} abs(%sub.0), metadata={op_name=\"jit(compound_pfqy)/abs\" stack_frame_id=9}\n}\n\nENTRY %main.1 (a.1: f32[10], b.1: f32[10], scale.1: f32[]) -> f32[10] {\n  %a.1 = f32[10]{0} parameter(0), metadata={op_name=\"a\"}\n  %b.1 = f32[10]{0} parameter(1), metadata={op_name=\"b\"}\n  %scale.1 = f32[] parameter(2), metadata={op_name=\"scale\"}\n  ROOT %subtract_abs_fusion = f32[10]{0} fusion(%scale.1, %a.1, %b.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(compound_pfqy)/abs\" stack_frame_id=9}\n}\n\n\n\n// ===== BACKWARD PASS (OPTIMIZED) =====\nHloModule jit_scalar_output_wrapper, is_scheduled=true, entry_computation_layout={(f32[10]{0}, f32[10]{0}, f32[])->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmp2jtx8kjp.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"extract_ir.<locals>.scalar_output_wrapper\"\n6 \"compound_pfqy\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=66 end_line=66 column=27 end_column=65}\n5 {file_name_id=2 function_name_id=5 line=61 end_line=61 column=23 end_column=38}\n6 {file_name_id=2 function_name_id=5 line=58 end_line=58 column=21 end_column=32}\n7 {file_name_id=3 function_name_id=6 line=7 end_line=7 column=14 end_column=19}\n8 {file_name_id=3 function_name_id=6 line=7 end_line=7 column=13 end_column=28}\n9 {file_name_id=3 function_name_id=6 line=8 end_line=8 column=22 end_column=39}\n10 {file_name_id=3 function_name_id=6 line=8 end_line=8 column=13 end_column=39}\n11 {file_name_id=3 function_name_id=6 line=9 end_line=9 column=11 end_column=26}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=7}\n8 {file_location_id=8 parent_frame_id=7}\n9 {file_location_id=9 parent_frame_id=7}\n10 {file_location_id=10 parent_frame_id=7}\n11 {file_location_id=11 parent_frame_id=7}\n\n\n%fused_computation (param_0.3: f32[], param_1.8: f32[10], param_2.7: f32[10]) -> f32[10] {\n  %param_1.8 = f32[10]{0} parameter(1)\n  %param_2.7 = f32[10]{0} parameter(2)\n  %add.0 = f32[10]{0} add(%param_1.8, %param_2.7), metadata={op_name=\"jit(scalar_output_wrapper)/jvp()/add\" stack_frame_id=7}\n  %param_0.3 = f32[] parameter(0)\n  %mul.2 = f32[10]{0} broadcast(%param_0.3), dimensions={}, metadata={op_name=\"jit(scalar_output_wrapper)/jvp()/mul\" stack_frame_id=8}\n  %mul.1 = f32[10]{0} multiply(%add.0, %mul.2), metadata={op_name=\"jit(scalar_output_wrapper)/jvp()/mul\" stack_frame_id=8}\n  %floor.0 = f32[10]{0} floor(%mul.1), metadata={op_name=\"jit(scalar_output_wrapper)/jvp()/floor\" stack_frame_id=9}\n  %sub.0 = f32[10]{0} subtract(%mul.1, %floor.0), metadata={op_name=\"jit(scalar_output_wrapper)/jvp()/sub\" stack_frame_id=10}\n  %constant.1 = f32[] constant(0)\n  %broadcast.1 = f32[10]{0} broadcast(%constant.1), dimensions={}\n  %ge.0 = pred[10]{0} compare(%sub.0, %broadcast.1), direction=GE, metadata={op_name=\"jit(scalar_output_wrapper)/jvp()/ge\" stack_frame_id=11}\n  %constant.0 = f32[] constant(1)\n  %broadcast.0 = f32[10]{0} broadcast(%constant.0), dimensions={}\n  %select_n.1 = f32[10]{0} select(%ge.0, %broadcast.0, %broadcast.1), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/select_n\" stack_frame_id=11}\n  %select_n.0 = f32[10]{0} select(%ge.0, %broadcast.1, %broadcast.0), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/select_n\" stack_frame_id=11}\n  %neg.0 = f32[10]{0} negate(%select_n.0), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/neg\" stack_frame_id=11}\n  %add_any.0 = f32[10]{0} add(%select_n.1, %neg.0), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/add_any\" stack_frame_id=11}\n  ROOT %mul.0 = f32[10]{0} multiply(%add_any.0, %mul.2), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/mul\" stack_frame_id=8}\n}\n\nENTRY %main.1 (args_0_.1: f32[10], args_1_.1: f32[10], args_2_.1: f32[]) -> f32[10] {\n  %args_0_.1 = f32[10]{0} parameter(0), metadata={op_name=\"args[0]\"}\n  %args_1_.1 = f32[10]{0} parameter(1), metadata={op_name=\"args[1]\"}\n  %args_2_.1 = f32[] parameter(2), metadata={op_name=\"args[2]\"}\n  ROOT %add_multiply_fusion = f32[10]{0} fusion(%args_2_.1, %args_0_.1, %args_1_.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/mul\" stack_frame_id=8}\n}\n\n"}
{"id": 17, "kernel_name": "branch_fyya", "python": "def branch_fyya(a):\n    \"\"\"Conditional operation using jnp.where.\"\"\"\n    return jnp.where(a > 0.69, a * 2.2, a + 0.5)", "type": "generate_branch_kernel", "hlo": "// ===== FORWARD PASS =====\nmodule @jit_branch_fyya attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.689999997> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<10xf32>, tensor<10xf32>) -> tensor<10xi1>\n    %cst_0 = stablehlo.constant dense<2.200000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<10xf32>\n    %cst_1 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<10xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<10xi1>, tensor<10xf32>, tensor<10xf32>) -> tensor<10xf32>\n    return %6 : tensor<10xf32>\n  }\n  func.func private @_where(%arg0: tensor<10xi1>, %arg1: tensor<10xf32>, %arg2: tensor<10xf32>) -> tensor<10xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<10xi1>, tensor<10xf32>\n    return %0 : tensor<10xf32>\n  }\n}\n\n\n// ===== BACKWARD PASS =====\nmodule @jit_scalar_output_wrapper attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.689999997> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<10xf32>, tensor<10xf32>) -> tensor<10xi1>\n    %cst_0 = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %3:2 = call @_where(%1, %2) : (tensor<10xi1>, tensor<10xf32>) -> (tensor<10xf32>, tensor<10xf32>)\n    %cst_1 = stablehlo.constant dense<2.200000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %5 = stablehlo.multiply %3#0, %4 : tensor<10xf32>\n    %6 = stablehlo.add %3#1, %5 : tensor<10xf32>\n    return %6 : tensor<10xf32>\n  }\n  func.func private @_where(%arg0: tensor<10xi1>, %arg1: tensor<10xf32>) -> (tensor<10xf32>, tensor<10xf32>) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %1 = stablehlo.select %arg0, %0, %arg1 : tensor<10xi1>, tensor<10xf32>\n    %2 = stablehlo.select %arg0, %arg1, %0 : tensor<10xi1>, tensor<10xf32>\n    return %2, %1 : tensor<10xf32>, tensor<10xf32>\n  }\n}\n", "optimized_hlo": "// ===== FORWARD PASS (OPTIMIZED) =====\nHloModule jit_branch_fyya, is_scheduled=true, entry_computation_layout={(f32[10]{0})->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmpn9wc20hc.py\"\n4 \"/tmp/tmphr9su7i4.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"branch_fyya\"\n6 \"nested_odsh\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=41 end_line=41 column=14 end_column=47}\n5 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=40 end_column=47}\n6 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=31 end_column=38}\n7 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=21 end_column=29}\n8 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=11 end_column=48}\n9 {file_name_id=4 function_name_id=6 line=8 end_line=8 column=21 end_column=58}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=5}\n8 {file_location_id=8 parent_frame_id=5}\n9 {file_location_id=9 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[10]) -> f32[10] {\n  %param_0.1 = f32[10]{0} parameter(0)\n  %constant.2 = f32[] constant(0.69)\n  %broadcast.2 = f32[10]{0} broadcast(%constant.2), dimensions={}\n  %gt.0 = pred[10]{0} compare(%param_0.1, %broadcast.2), direction=GT, metadata={op_name=\"jit(branch_fyya)/gt\" stack_frame_id=7}\n  %constant.1 = f32[] constant(2.2)\n  %broadcast.1 = f32[10]{0} broadcast(%constant.1), dimensions={}\n  %mul.0 = f32[10]{0} multiply(%param_0.1, %broadcast.1), metadata={op_name=\"jit(branch_fyya)/mul\" stack_frame_id=6}\n  %constant.0 = f32[] constant(0.5)\n  %broadcast.0 = f32[10]{0} broadcast(%constant.0), dimensions={}\n  %add.0 = f32[10]{0} add(%param_0.1, %broadcast.0), metadata={op_name=\"jit(branch_fyya)/add\" stack_frame_id=5}\n  ROOT %select_n.2 = f32[10]{0} select(%gt.0, %mul.0, %add.0), metadata={op_name=\"jit(branch_fyya)/jit(_where)/select_n\" stack_frame_id=9}\n}\n\nENTRY %main.2 (a.1: f32[10]) -> f32[10] {\n  %a.1 = f32[10]{0} parameter(0), metadata={op_name=\"a\"}\n  ROOT %add_select_fusion = f32[10]{0} fusion(%a.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(branch_fyya)/jit(_where)/select_n\" stack_frame_id=9}\n}\n\n\n\n// ===== BACKWARD PASS (OPTIMIZED) =====\nHloModule jit_scalar_output_wrapper, is_scheduled=true, entry_computation_layout={(f32[10]{0})->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmpn9wc20hc.py\"\n4 \"/tmp/tmphr9su7i4.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"extract_ir.<locals>.scalar_output_wrapper\"\n6 \"branch_fyya\"\n7 \"nested_odsh\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=66 end_line=66 column=27 end_column=65}\n5 {file_name_id=2 function_name_id=5 line=58 end_line=58 column=21 end_column=32}\n6 {file_name_id=3 function_name_id=6 line=7 end_line=7 column=31 end_column=38}\n7 {file_name_id=2 function_name_id=5 line=61 end_line=61 column=23 end_column=38}\n8 {file_name_id=3 function_name_id=6 line=7 end_line=7 column=21 end_column=29}\n9 {file_name_id=3 function_name_id=6 line=7 end_line=7 column=11 end_column=48}\n10 {file_name_id=2 function_name_id=4 line=41 end_line=41 column=14 end_column=47}\n11 {file_name_id=4 function_name_id=7 line=8 end_line=8 column=21 end_column=58}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=6}\n7 {file_location_id=7 parent_frame_id=5}\n8 {file_location_id=8 parent_frame_id=6}\n9 {file_location_id=9 parent_frame_id=6}\n10 {file_location_id=10 parent_frame_id=4}\n11 {file_location_id=11 parent_frame_id=11}\n\n\n%fused_computation (param_0.2: f32[10]) -> f32[10] {\n  %param_0.2 = f32[10]{0} parameter(0)\n  %constant.8 = f32[] constant(0.69)\n  %broadcast.8 = f32[10]{0} broadcast(%constant.8), dimensions={}\n  %gt.0 = pred[10]{0} compare(%param_0.2, %broadcast.8), direction=GT, metadata={op_name=\"jit(scalar_output_wrapper)/jvp()/gt\" stack_frame_id=8}\n  %constant.3 = f32[] constant(0), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp(jit(_where)))\"}\n  %broadcast.3 = f32[10]{0} broadcast(%constant.3), dimensions={}, metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp(jit(_where)))\"}\n  %constant.2 = f32[] constant(1)\n  %broadcast.2 = f32[10]{0} broadcast(%constant.2), dimensions={}\n  %select_n.5 = f32[10]{0} select(%gt.0, %broadcast.3, %broadcast.2), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp(jit(_where)))/select_n\" stack_frame_id=11}\n  %select_n.4 = f32[10]{0} select(%gt.0, %broadcast.2, %broadcast.3), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp(jit(_where)))/select_n\" stack_frame_id=11}\n  %constant.1 = f32[] constant(2.2)\n  %broadcast.1 = f32[10]{0} broadcast(%constant.1), dimensions={}\n  %mul.0 = f32[10]{0} multiply(%select_n.4, %broadcast.1), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/mul\" stack_frame_id=6}\n  ROOT %add_any.0 = f32[10]{0} add(%select_n.5, %mul.0), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/add_any\" stack_frame_id=6}\n}\n\nENTRY %main.2 (args_0_.1: f32[10]) -> f32[10] {\n  %args_0_.1 = f32[10]{0} parameter(0), metadata={op_name=\"args[0]\"}\n  ROOT %multiply_add_fusion = f32[10]{0} fusion(%args_0_.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/add_any\" stack_frame_id=6}\n}\n\n"}
{"id": 18, "kernel_name": "loop_ipgv", "python": "def loop_ipgv(a):\n    \"\"\"Reduction operation simulating loop accumulation.\"\"\"\n    # Accumulate n=5 times\n    result = a\n    for _ in range(5):\n        result = result - a\n    return result", "type": "generate_loop_kernel", "hlo": "// ===== FORWARD PASS =====\nmodule @jit_loop_ipgv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg0 : tensor<10xf32>\n    %1 = stablehlo.subtract %0, %arg0 : tensor<10xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<10xf32>\n    %3 = stablehlo.subtract %2, %arg0 : tensor<10xf32>\n    %4 = stablehlo.subtract %3, %arg0 : tensor<10xf32>\n    return %4 : tensor<10xf32>\n  }\n}\n\n\n// ===== BACKWARD PASS =====\nmodule @jit_scalar_output_wrapper attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main() -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %1 = stablehlo.negate %0 : tensor<10xf32>\n    %2 = stablehlo.negate %0 : tensor<10xf32>\n    %3 = stablehlo.add %1, %2 : tensor<10xf32>\n    %4 = stablehlo.negate %0 : tensor<10xf32>\n    %5 = stablehlo.add %3, %4 : tensor<10xf32>\n    %6 = stablehlo.negate %0 : tensor<10xf32>\n    %7 = stablehlo.add %5, %6 : tensor<10xf32>\n    %8 = stablehlo.negate %0 : tensor<10xf32>\n    %9 = stablehlo.add %7, %0 : tensor<10xf32>\n    %10 = stablehlo.add %9, %8 : tensor<10xf32>\n    return %10 : tensor<10xf32>\n  }\n}\n", "optimized_hlo": "// ===== FORWARD PASS (OPTIMIZED) =====\nHloModule jit_loop_ipgv, is_scheduled=true, entry_computation_layout={(f32[10]{0})->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmp8frxga7y.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"loop_ipgv\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=41 end_line=41 column=14 end_column=47}\n5 {file_name_id=3 function_name_id=5 line=10 end_line=10 column=17 end_column=27}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[10]) -> f32[10] {\n  %param_0.1 = f32[10]{0} parameter(0)\n  %sub.4 = f32[10]{0} subtract(%param_0.1, %param_0.1), metadata={op_name=\"jit(loop_ipgv)/sub\" stack_frame_id=5}\n  %sub.3 = f32[10]{0} subtract(%sub.4, %param_0.1), metadata={op_name=\"jit(loop_ipgv)/sub\" stack_frame_id=5}\n  %sub.2 = f32[10]{0} subtract(%sub.3, %param_0.1), metadata={op_name=\"jit(loop_ipgv)/sub\" stack_frame_id=5}\n  %sub.1 = f32[10]{0} subtract(%sub.2, %param_0.1), metadata={op_name=\"jit(loop_ipgv)/sub\" stack_frame_id=5}\n  ROOT %sub.0 = f32[10]{0} subtract(%sub.1, %param_0.1), metadata={op_name=\"jit(loop_ipgv)/sub\" stack_frame_id=5}\n}\n\nENTRY %main.1 (a.1: f32[10]) -> f32[10] {\n  %a.1 = f32[10]{0} parameter(0), metadata={op_name=\"a\"}\n  ROOT %subtract_subtract_fusion = f32[10]{0} fusion(%a.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(loop_ipgv)/sub\" stack_frame_id=5}\n}\n\n\n\n// ===== BACKWARD PASS (OPTIMIZED) =====\nHloModule jit_scalar_output_wrapper, is_scheduled=true, entry_computation_layout={()->f32[10]{0}}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmp8frxga7y.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"extract_ir.<locals>.scalar_output_wrapper\"\n6 \"loop_ipgv\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=66 end_line=66 column=27 end_column=65}\n5 {file_name_id=2 function_name_id=5 line=58 end_line=58 column=21 end_column=32}\n6 {file_name_id=3 function_name_id=6 line=10 end_line=10 column=17 end_column=27}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=6}\n\n\n%wrapped_broadcast_computation (param_0: f32[]) -> f32[10] {\n  %param_0 = f32[] parameter(0)\n  ROOT %broadcast.0 = f32[10]{0} broadcast(%param_0), dimensions={}\n}\n\nENTRY %main.1 () -> f32[10] {\n  %constant.1 = f32[] constant(-4)\n  ROOT %wrapped_broadcast = f32[10]{0} fusion(%constant.1), kind=kLoop, calls=%wrapped_broadcast_computation\n}\n\n"}
{"id": 19, "kernel_name": "multi_movi", "python": "def multi_movi(a, b):\n    \"\"\"Multi-statement computation.\"\"\"\n    temp1 = a - b\n    temp2 = jnp.sqrt(temp1)\n    result = temp2 * a\n    return result", "type": "generate_multi_statement_kernel", "hlo": "// ===== FORWARD PASS =====\nmodule @jit_multi_movi attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>, %arg1: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<10xf32>\n    %1 = stablehlo.sqrt %0 : tensor<10xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<10xf32>\n    return %2 : tensor<10xf32>\n  }\n}\n\n\n// ===== BACKWARD PASS =====\nmodule @jit_scalar_output_wrapper attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>, %arg1: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<10xf32>\n    %1 = stablehlo.sqrt %0 : tensor<10xf32>\n    %cst = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %3 = stablehlo.divide %2, %1 : tensor<10xf32>\n    %cst_0 = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %5 = stablehlo.multiply %1, %4 : tensor<10xf32>\n    %6 = stablehlo.multiply %4, %arg0 : tensor<10xf32>\n    %7 = stablehlo.multiply %6, %3 : tensor<10xf32>\n    %8 = stablehlo.add %5, %7 : tensor<10xf32>\n    return %8 : tensor<10xf32>\n  }\n}\n", "optimized_hlo": "// ===== FORWARD PASS (OPTIMIZED) =====\nHloModule jit_multi_movi, is_scheduled=true, entry_computation_layout={(f32[10]{0}, f32[10]{0})->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmp0lxpumz7.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"multi_movi\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=41 end_line=41 column=14 end_column=47}\n5 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=12 end_column=17}\n6 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=12 end_column=27}\n7 {file_name_id=3 function_name_id=5 line=9 end_line=9 column=13 end_column=22}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[10], param_1.2: f32[10]) -> f32[10] {\n  %param_0.1 = f32[10]{0} parameter(0)\n  %param_1.2 = f32[10]{0} parameter(1)\n  %sub.0 = f32[10]{0} subtract(%param_0.1, %param_1.2), metadata={op_name=\"jit(multi_movi)/sub\" stack_frame_id=5}\n  %sqrt.0 = f32[10]{0} sqrt(%sub.0), metadata={op_name=\"jit(multi_movi)/sqrt\" stack_frame_id=6}\n  ROOT %mul.0 = f32[10]{0} multiply(%sqrt.0, %param_0.1), metadata={op_name=\"jit(multi_movi)/mul\" stack_frame_id=7}\n}\n\nENTRY %main.1 (a.1: f32[10], b.1: f32[10]) -> f32[10] {\n  %a.1 = f32[10]{0} parameter(0), metadata={op_name=\"a\"}\n  %b.1 = f32[10]{0} parameter(1), metadata={op_name=\"b\"}\n  ROOT %sqrt_multiply_fusion = f32[10]{0} fusion(%a.1, %b.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(multi_movi)/mul\" stack_frame_id=7}\n}\n\n\n\n// ===== BACKWARD PASS (OPTIMIZED) =====\nHloModule jit_scalar_output_wrapper, is_scheduled=true, entry_computation_layout={(f32[10]{0}, f32[10]{0})->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmp0lxpumz7.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"extract_ir.<locals>.scalar_output_wrapper\"\n6 \"multi_movi\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=66 end_line=66 column=27 end_column=65}\n5 {file_name_id=2 function_name_id=5 line=58 end_line=58 column=21 end_column=32}\n6 {file_name_id=3 function_name_id=6 line=8 end_line=8 column=12 end_column=27}\n7 {file_name_id=3 function_name_id=6 line=7 end_line=7 column=12 end_column=17}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=6}\n7 {file_location_id=7 parent_frame_id=6}\n\n\n%fused_computation (param_0.1: f32[10], param_1.3: f32[10]) -> f32[10] {\n  %param_0.1 = f32[10]{0} parameter(0)\n  %param_1.3 = f32[10]{0} parameter(1)\n  %sub.0 = f32[10]{0} subtract(%param_0.1, %param_1.3), metadata={op_name=\"jit(scalar_output_wrapper)/jvp()/sub\" stack_frame_id=7}\n  %sqrt.0 = f32[10]{0} sqrt(%sub.0), metadata={op_name=\"jit(scalar_output_wrapper)/jvp()/sqrt\" stack_frame_id=6}\n  %constant.0 = f32[] constant(0.5)\n  %broadcast.0 = f32[10]{0} broadcast(%constant.0), dimensions={}\n  %div.0 = f32[10]{0} divide(%broadcast.0, %sqrt.0), metadata={op_name=\"jit(scalar_output_wrapper)/jvp()/div\" stack_frame_id=6}\n  %mul.0 = f32[10]{0} multiply(%param_0.1, %div.0), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/mul\" stack_frame_id=6}\n  ROOT %add_any.0 = f32[10]{0} add(%sqrt.0, %mul.0), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/add_any\" stack_frame_id=6}\n}\n\nENTRY %main.1 (args_0_.1: f32[10], args_1_.1: f32[10]) -> f32[10] {\n  %args_0_.1 = f32[10]{0} parameter(0), metadata={op_name=\"args[0]\"}\n  %args_1_.1 = f32[10]{0} parameter(1), metadata={op_name=\"args[1]\"}\n  ROOT %multiply_add_fusion = f32[10]{0} fusion(%args_0_.1, %args_1_.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/add_any\" stack_frame_id=6}\n}\n\n"}
{"id": 20, "kernel_name": "compound_yezg", "python": "def compound_yezg(a, b, scale):\n    \"\"\"Compound operations with multiple steps.\"\"\"\n    result = (a + b) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "hlo": "// ===== FORWARD PASS =====\nmodule @jit_compound_yezg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>, %arg1: tensor<10xf32>, %arg2: tensor<f32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<10xf32>\n    %1 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %2 = stablehlo.multiply %0, %1 : tensor<10xf32>\n    %3 = stablehlo.floor %2 : tensor<10xf32>\n    %4 = stablehlo.subtract %2, %3 : tensor<10xf32>\n    %5 = stablehlo.abs %4 : tensor<10xf32>\n    return %5 : tensor<10xf32>\n  }\n}\n\n\n// ===== BACKWARD PASS =====\nmodule @jit_scalar_output_wrapper attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>, %arg1: tensor<10xf32>, %arg2: tensor<f32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<10xf32>\n    %1 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %2 = stablehlo.multiply %0, %1 : tensor<10xf32>\n    %3 = stablehlo.floor %2 : tensor<10xf32>\n    %4 = stablehlo.subtract %2, %3 : tensor<10xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %5 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %6 = stablehlo.compare  GE, %4, %5,  FLOAT : (tensor<10xf32>, tensor<10xf32>) -> tensor<10xi1>\n    %cst_0 = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %8 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %9 = stablehlo.select %6, %8, %7 : tensor<10xi1>, tensor<10xf32>\n    %10 = stablehlo.select %6, %7, %8 : tensor<10xi1>, tensor<10xf32>\n    %11 = stablehlo.negate %9 : tensor<10xf32>\n    %12 = stablehlo.add %10, %11 : tensor<10xf32>\n    %13 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %14 = stablehlo.multiply %12, %13 : tensor<10xf32>\n    return %14 : tensor<10xf32>\n  }\n}\n", "optimized_hlo": "// ===== FORWARD PASS (OPTIMIZED) =====\nHloModule jit_compound_yezg, is_scheduled=true, entry_computation_layout={(f32[10]{0}, f32[10]{0}, f32[])->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmp4k_c8ygt.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"compound_yezg\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=41 end_line=41 column=14 end_column=47}\n5 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=14 end_column=19}\n6 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=13 end_column=28}\n7 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=22 end_column=39}\n8 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=13 end_column=39}\n9 {file_name_id=3 function_name_id=5 line=9 end_line=9 column=11 end_column=26}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=5}\n8 {file_location_id=8 parent_frame_id=5}\n9 {file_location_id=9 parent_frame_id=5}\n\n\n%fused_computation (param_0.3: f32[], param_1.3: f32[10], param_2: f32[10]) -> f32[10] {\n  %param_1.3 = f32[10]{0} parameter(1)\n  %param_2 = f32[10]{0} parameter(2)\n  %add.0 = f32[10]{0} add(%param_1.3, %param_2), metadata={op_name=\"jit(compound_yezg)/add\" stack_frame_id=5}\n  %param_0.3 = f32[] parameter(0)\n  %mul.1 = f32[10]{0} broadcast(%param_0.3), dimensions={}, metadata={op_name=\"jit(compound_yezg)/mul\" stack_frame_id=6}\n  %mul.0 = f32[10]{0} multiply(%add.0, %mul.1), metadata={op_name=\"jit(compound_yezg)/mul\" stack_frame_id=6}\n  %floor.0 = f32[10]{0} floor(%mul.0), metadata={op_name=\"jit(compound_yezg)/floor\" stack_frame_id=7}\n  %sub.0 = f32[10]{0} subtract(%mul.0, %floor.0), metadata={op_name=\"jit(compound_yezg)/sub\" stack_frame_id=8}\n  ROOT %abs.0 = f32[10]{0} abs(%sub.0), metadata={op_name=\"jit(compound_yezg)/abs\" stack_frame_id=9}\n}\n\nENTRY %main.1 (a.1: f32[10], b.1: f32[10], scale.1: f32[]) -> f32[10] {\n  %a.1 = f32[10]{0} parameter(0), metadata={op_name=\"a\"}\n  %b.1 = f32[10]{0} parameter(1), metadata={op_name=\"b\"}\n  %scale.1 = f32[] parameter(2), metadata={op_name=\"scale\"}\n  ROOT %subtract_abs_fusion = f32[10]{0} fusion(%scale.1, %a.1, %b.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(compound_yezg)/abs\" stack_frame_id=9}\n}\n\n\n\n// ===== BACKWARD PASS (OPTIMIZED) =====\nHloModule jit_scalar_output_wrapper, is_scheduled=true, entry_computation_layout={(f32[10]{0}, f32[10]{0}, f32[])->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmp4k_c8ygt.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"extract_ir.<locals>.scalar_output_wrapper\"\n6 \"compound_yezg\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=66 end_line=66 column=27 end_column=65}\n5 {file_name_id=2 function_name_id=5 line=61 end_line=61 column=23 end_column=38}\n6 {file_name_id=2 function_name_id=5 line=58 end_line=58 column=21 end_column=32}\n7 {file_name_id=3 function_name_id=6 line=7 end_line=7 column=14 end_column=19}\n8 {file_name_id=3 function_name_id=6 line=7 end_line=7 column=13 end_column=28}\n9 {file_name_id=3 function_name_id=6 line=8 end_line=8 column=22 end_column=39}\n10 {file_name_id=3 function_name_id=6 line=8 end_line=8 column=13 end_column=39}\n11 {file_name_id=3 function_name_id=6 line=9 end_line=9 column=11 end_column=26}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=7}\n8 {file_location_id=8 parent_frame_id=7}\n9 {file_location_id=9 parent_frame_id=7}\n10 {file_location_id=10 parent_frame_id=7}\n11 {file_location_id=11 parent_frame_id=7}\n\n\n%fused_computation (param_0.3: f32[], param_1.8: f32[10], param_2.7: f32[10]) -> f32[10] {\n  %param_1.8 = f32[10]{0} parameter(1)\n  %param_2.7 = f32[10]{0} parameter(2)\n  %add.0 = f32[10]{0} add(%param_1.8, %param_2.7), metadata={op_name=\"jit(scalar_output_wrapper)/jvp()/add\" stack_frame_id=7}\n  %param_0.3 = f32[] parameter(0)\n  %mul.2 = f32[10]{0} broadcast(%param_0.3), dimensions={}, metadata={op_name=\"jit(scalar_output_wrapper)/jvp()/mul\" stack_frame_id=8}\n  %mul.1 = f32[10]{0} multiply(%add.0, %mul.2), metadata={op_name=\"jit(scalar_output_wrapper)/jvp()/mul\" stack_frame_id=8}\n  %floor.0 = f32[10]{0} floor(%mul.1), metadata={op_name=\"jit(scalar_output_wrapper)/jvp()/floor\" stack_frame_id=9}\n  %sub.0 = f32[10]{0} subtract(%mul.1, %floor.0), metadata={op_name=\"jit(scalar_output_wrapper)/jvp()/sub\" stack_frame_id=10}\n  %constant.1 = f32[] constant(0)\n  %broadcast.1 = f32[10]{0} broadcast(%constant.1), dimensions={}\n  %ge.0 = pred[10]{0} compare(%sub.0, %broadcast.1), direction=GE, metadata={op_name=\"jit(scalar_output_wrapper)/jvp()/ge\" stack_frame_id=11}\n  %constant.0 = f32[] constant(1)\n  %broadcast.0 = f32[10]{0} broadcast(%constant.0), dimensions={}\n  %select_n.1 = f32[10]{0} select(%ge.0, %broadcast.0, %broadcast.1), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/select_n\" stack_frame_id=11}\n  %select_n.0 = f32[10]{0} select(%ge.0, %broadcast.1, %broadcast.0), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/select_n\" stack_frame_id=11}\n  %neg.0 = f32[10]{0} negate(%select_n.0), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/neg\" stack_frame_id=11}\n  %add_any.0 = f32[10]{0} add(%select_n.1, %neg.0), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/add_any\" stack_frame_id=11}\n  ROOT %mul.0 = f32[10]{0} multiply(%add_any.0, %mul.2), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/mul\" stack_frame_id=8}\n}\n\nENTRY %main.1 (args_0_.1: f32[10], args_1_.1: f32[10], args_2_.1: f32[]) -> f32[10] {\n  %args_0_.1 = f32[10]{0} parameter(0), metadata={op_name=\"args[0]\"}\n  %args_1_.1 = f32[10]{0} parameter(1), metadata={op_name=\"args[1]\"}\n  %args_2_.1 = f32[] parameter(2), metadata={op_name=\"args[2]\"}\n  ROOT %add_multiply_fusion = f32[10]{0} fusion(%args_2_.1, %args_0_.1, %args_1_.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/mul\" stack_frame_id=8}\n}\n\n"}
{"id": 21, "kernel_name": "multi_lhxm", "python": "def multi_lhxm(a, b):\n    \"\"\"Multi-statement computation.\"\"\"\n    temp1 = a + b\n    temp2 = jnp.sqrt(temp1)\n    result = temp2 - a\n    return result", "type": "generate_multi_statement_kernel", "hlo": "// ===== FORWARD PASS =====\nmodule @jit_multi_lhxm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>, %arg1: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<10xf32>\n    %1 = stablehlo.sqrt %0 : tensor<10xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<10xf32>\n    return %2 : tensor<10xf32>\n  }\n}\n\n\n// ===== BACKWARD PASS =====\nmodule @jit_scalar_output_wrapper attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>, %arg1: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<10xf32>\n    %1 = stablehlo.sqrt %0 : tensor<10xf32>\n    %cst = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %3 = stablehlo.divide %2, %1 : tensor<10xf32>\n    %cst_0 = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %5 = stablehlo.negate %4 : tensor<10xf32>\n    %6 = stablehlo.multiply %4, %3 : tensor<10xf32>\n    %7 = stablehlo.add %5, %6 : tensor<10xf32>\n    return %7 : tensor<10xf32>\n  }\n}\n", "optimized_hlo": "// ===== FORWARD PASS (OPTIMIZED) =====\nHloModule jit_multi_lhxm, is_scheduled=true, entry_computation_layout={(f32[10]{0}, f32[10]{0})->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmp4gmkctb3.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"multi_lhxm\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=41 end_line=41 column=14 end_column=47}\n5 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=12 end_column=17}\n6 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=12 end_column=27}\n7 {file_name_id=3 function_name_id=5 line=9 end_line=9 column=13 end_column=22}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[10], param_1.2: f32[10]) -> f32[10] {\n  %param_0.1 = f32[10]{0} parameter(0)\n  %param_1.2 = f32[10]{0} parameter(1)\n  %add.0 = f32[10]{0} add(%param_0.1, %param_1.2), metadata={op_name=\"jit(multi_lhxm)/add\" stack_frame_id=5}\n  %sqrt.0 = f32[10]{0} sqrt(%add.0), metadata={op_name=\"jit(multi_lhxm)/sqrt\" stack_frame_id=6}\n  ROOT %sub.0 = f32[10]{0} subtract(%sqrt.0, %param_0.1), metadata={op_name=\"jit(multi_lhxm)/sub\" stack_frame_id=7}\n}\n\nENTRY %main.1 (a.1: f32[10], b.1: f32[10]) -> f32[10] {\n  %a.1 = f32[10]{0} parameter(0), metadata={op_name=\"a\"}\n  %b.1 = f32[10]{0} parameter(1), metadata={op_name=\"b\"}\n  ROOT %sqrt_subtract_fusion = f32[10]{0} fusion(%a.1, %b.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(multi_lhxm)/sub\" stack_frame_id=7}\n}\n\n\n\n// ===== BACKWARD PASS (OPTIMIZED) =====\nHloModule jit_scalar_output_wrapper, is_scheduled=true, entry_computation_layout={(f32[10]{0}, f32[10]{0})->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmp4gmkctb3.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"extract_ir.<locals>.scalar_output_wrapper\"\n6 \"multi_lhxm\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=66 end_line=66 column=27 end_column=65}\n5 {file_name_id=2 function_name_id=5 line=58 end_line=58 column=21 end_column=32}\n6 {file_name_id=3 function_name_id=6 line=9 end_line=9 column=13 end_column=22}\n7 {file_name_id=3 function_name_id=6 line=8 end_line=8 column=12 end_column=27}\n8 {file_name_id=3 function_name_id=6 line=7 end_line=7 column=12 end_column=17}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=6}\n7 {file_location_id=7 parent_frame_id=6}\n8 {file_location_id=8 parent_frame_id=6}\n\n\n%fused_computation (param_0.2: f32[10], param_1.5: f32[10]) -> f32[10] {\n  %constant.1 = f32[] constant(0.5)\n  %broadcast.1 = f32[10]{0} broadcast(%constant.1), dimensions={}\n  %param_0.2 = f32[10]{0} parameter(0)\n  %param_1.5 = f32[10]{0} parameter(1)\n  %add.0 = f32[10]{0} add(%param_0.2, %param_1.5), metadata={op_name=\"jit(scalar_output_wrapper)/jvp()/add\" stack_frame_id=8}\n  %rsqrt.1 = f32[10]{0} rsqrt(%add.0), metadata={op_name=\"jit(scalar_output_wrapper)/jvp()/sqrt\" stack_frame_id=7}\n  %multiply.1 = f32[10]{0} multiply(%broadcast.1, %rsqrt.1), metadata={op_name=\"jit(scalar_output_wrapper)/jvp()/div\" stack_frame_id=7}\n  %constant.0 = f32[] constant(-1)\n  %broadcast.0 = f32[10]{0} broadcast(%constant.0), dimensions={}\n  ROOT %neg.0 = f32[10]{0} add(%multiply.1, %broadcast.0), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/add_any;jit(scalar_output_wrapper)/transpose(jvp())/neg\"}\n}\n\nENTRY %main.1 (args_0_.1: f32[10], args_1_.1: f32[10]) -> f32[10] {\n  %args_0_.1 = f32[10]{0} parameter(0), metadata={op_name=\"args[0]\"}\n  %args_1_.1 = f32[10]{0} parameter(1), metadata={op_name=\"args[1]\"}\n  ROOT %broadcast_add_fusion = f32[10]{0} fusion(%args_0_.1, %args_1_.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/add_any;jit(scalar_output_wrapper)/transpose(jvp())/neg\"}\n}\n\n"}
{"id": 22, "kernel_name": "multi_mqkx", "python": "def multi_mqkx(a, b):\n    \"\"\"Multi-statement computation.\"\"\"\n    temp1 = a + b\n    temp2 = jnp.sin(temp1)\n    result = temp2 + a\n    return result", "type": "generate_multi_statement_kernel", "hlo": "// ===== FORWARD PASS =====\nmodule @jit_multi_mqkx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>, %arg1: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<10xf32>\n    %1 = stablehlo.sine %0 : tensor<10xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<10xf32>\n    return %2 : tensor<10xf32>\n  }\n}\n\n\n// ===== BACKWARD PASS =====\nmodule @jit_scalar_output_wrapper attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>, %arg1: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<10xf32>\n    %1 = stablehlo.cosine %0 : tensor<10xf32>\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %3 = stablehlo.multiply %2, %1 : tensor<10xf32>\n    %4 = stablehlo.add %2, %3 : tensor<10xf32>\n    return %4 : tensor<10xf32>\n  }\n}\n", "optimized_hlo": "// ===== FORWARD PASS (OPTIMIZED) =====\nHloModule jit_multi_mqkx, is_scheduled=true, entry_computation_layout={(f32[10]{0}, f32[10]{0})->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmpda0q0qfi.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"multi_mqkx\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=41 end_line=41 column=14 end_column=47}\n5 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=12 end_column=17}\n6 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=12 end_column=26}\n7 {file_name_id=3 function_name_id=5 line=9 end_line=9 column=13 end_column=22}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[10], param_1.2: f32[10]) -> f32[10] {\n  %param_0.1 = f32[10]{0} parameter(0)\n  %param_1.2 = f32[10]{0} parameter(1)\n  %add.1 = f32[10]{0} add(%param_0.1, %param_1.2), metadata={op_name=\"jit(multi_mqkx)/add\" stack_frame_id=5}\n  %sin.0 = f32[10]{0} sine(%add.1), metadata={op_name=\"jit(multi_mqkx)/sin\" stack_frame_id=6}\n  ROOT %add.0 = f32[10]{0} add(%sin.0, %param_0.1), metadata={op_name=\"jit(multi_mqkx)/add\" stack_frame_id=7}\n}\n\nENTRY %main.1 (a.1: f32[10], b.1: f32[10]) -> f32[10] {\n  %a.1 = f32[10]{0} parameter(0), metadata={op_name=\"a\"}\n  %b.1 = f32[10]{0} parameter(1), metadata={op_name=\"b\"}\n  ROOT %sine_add_fusion = f32[10]{0} fusion(%a.1, %b.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(multi_mqkx)/add\" stack_frame_id=7}\n}\n\n\n\n// ===== BACKWARD PASS (OPTIMIZED) =====\nHloModule jit_scalar_output_wrapper, is_scheduled=true, entry_computation_layout={(f32[10]{0}, f32[10]{0})->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmpda0q0qfi.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"extract_ir.<locals>.scalar_output_wrapper\"\n6 \"multi_mqkx\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=66 end_line=66 column=27 end_column=65}\n5 {file_name_id=2 function_name_id=5 line=61 end_line=61 column=23 end_column=38}\n6 {file_name_id=2 function_name_id=5 line=58 end_line=58 column=21 end_column=32}\n7 {file_name_id=3 function_name_id=6 line=7 end_line=7 column=12 end_column=17}\n8 {file_name_id=3 function_name_id=6 line=8 end_line=8 column=12 end_column=26}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=7}\n8 {file_location_id=8 parent_frame_id=7}\n\n\n%fused_computation (param_0.2: f32[10], param_1.2: f32[10]) -> f32[10] {\n  %param_0.2 = f32[10]{0} parameter(0)\n  %param_1.2 = f32[10]{0} parameter(1)\n  %add.0 = f32[10]{0} add(%param_0.2, %param_1.2), metadata={op_name=\"jit(scalar_output_wrapper)/jvp()/add\" stack_frame_id=7}\n  %cos.0 = f32[10]{0} cosine(%add.0), metadata={op_name=\"jit(scalar_output_wrapper)/jvp()/cos\" stack_frame_id=8}\n  %constant.0 = f32[] constant(1)\n  %broadcast.0 = f32[10]{0} broadcast(%constant.0), dimensions={}\n  ROOT %broadcast_in_dim.0 = f32[10]{0} add(%cos.0, %broadcast.0), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/add_any;jit(scalar_output_wrapper)/transpose(jvp())/broadcast_in_dim\"}\n}\n\nENTRY %main.1 (args_0_.1: f32[10], args_1_.1: f32[10]) -> f32[10] {\n  %args_0_.1 = f32[10]{0} parameter(0), metadata={op_name=\"args[0]\"}\n  %args_1_.1 = f32[10]{0} parameter(1), metadata={op_name=\"args[1]\"}\n  ROOT %broadcast_add_fusion = f32[10]{0} fusion(%args_0_.1, %args_1_.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/add_any;jit(scalar_output_wrapper)/transpose(jvp())/broadcast_in_dim\"}\n}\n\n"}
{"id": 23, "kernel_name": "vec_khvl", "python": "def vec_khvl(a):\n    \"\"\"Vector norm computation.\"\"\"\n    return jnp.linalg.norm(a, axis=-1)", "type": "generate_vector_kernel", "hlo": "// ===== FORWARD PASS =====\nmodule @jit_vec_khvl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x3xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<10x3xf32>) -> tensor<10xf32>\n    return %0 : tensor<10xf32>\n  }\n  func.func private @norm(%arg0: tensor<10x3xf32>) -> tensor<10xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<10x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<10x3xf32>, tensor<f32>) -> tensor<10xf32>\n    %2 = stablehlo.sqrt %1 : tensor<10xf32>\n    return %2 : tensor<10xf32>\n  }\n}\n\n\n// ===== BACKWARD PASS =====\nmodule @jit_scalar_output_wrapper attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x3xf32>) -> (tensor<10x3xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<10x3xf32>) -> tensor<10xf32>\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %2 = call @norm_0(%arg0, %0, %1) : (tensor<10x3xf32>, tensor<10xf32>, tensor<10xf32>) -> tensor<10x3xf32>\n    return %2 : tensor<10x3xf32>\n  }\n  func.func private @norm(%arg0: tensor<10x3xf32>) -> tensor<10xf32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<10x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<10x3xf32>, tensor<f32>) -> tensor<10xf32>\n    %2 = stablehlo.sqrt %1 : tensor<10xf32>\n    %cst_0 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %3 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %4 = stablehlo.divide %3, %2 : tensor<10xf32>\n    return %4 : tensor<10xf32>\n  }\n  func.func private @norm_0(%arg0: tensor<10x3xf32>, %arg1: tensor<10xf32>, %arg2: tensor<10xf32>) -> tensor<10x3xf32> {\n    %0 = stablehlo.multiply %arg2, %arg1 : tensor<10xf32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [0] : (tensor<10xf32>) -> tensor<10x3xf32>\n    %2 = stablehlo.multiply %arg0, %1 : tensor<10x3xf32>\n    %3 = stablehlo.multiply %1, %arg0 : tensor<10x3xf32>\n    %4 = stablehlo.add %2, %3 : tensor<10x3xf32>\n    return %4 : tensor<10x3xf32>\n  }\n}\n", "optimized_hlo": "// ===== FORWARD PASS (OPTIMIZED) =====\nHloModule jit_vec_khvl, is_scheduled=true, entry_computation_layout={(f32[10,3]{1,0})->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmp0wosud95.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"vec_khvl\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=41 end_line=41 column=14 end_column=47}\n5 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=11 end_column=38}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n\n\n%region_0.1 (reduce_sum.3: f32[], reduce_sum.4: f32[]) -> f32[] {\n  %reduce_sum.3 = f32[] parameter(0), metadata={op_name=\"reduce_sum\"}\n  %reduce_sum.4 = f32[] parameter(1), metadata={op_name=\"reduce_sum\"}\n  ROOT %reduce_sum.5 = f32[] add(%reduce_sum.3, %reduce_sum.4), metadata={op_name=\"reduce_sum\" stack_frame_id=5}\n}\n\n%fused_computation (param_0.2: f32[10,3]) -> f32[10] {\n  %param_0.2 = f32[10,3]{1,0} parameter(0)\n  %mul.2 = f32[10,3]{1,0} multiply(%param_0.2, %param_0.2), metadata={op_name=\"jit(vec_khvl)/jit(norm)/mul\" stack_frame_id=5}\n  %constant.2 = f32[] constant(0), metadata={op_name=\"jit(vec_khvl)/jit(norm)\"}\n  %reduce_sum.1 = f32[10]{0} reduce(%mul.2, %constant.2), dimensions={1}, to_apply=%region_0.1, metadata={op_name=\"jit(vec_khvl)/jit(norm)/reduce_sum\" stack_frame_id=5}\n  ROOT %sqrt.2 = f32[10]{0} sqrt(%reduce_sum.1), metadata={op_name=\"jit(vec_khvl)/jit(norm)/sqrt\" stack_frame_id=5}\n}\n\nENTRY %main.3 (a.1: f32[10,3]) -> f32[10] {\n  %a.1 = f32[10,3]{1,0} parameter(0), metadata={op_name=\"a\"}\n  ROOT %reduce_sqrt_fusion = f32[10]{0} fusion(%a.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(vec_khvl)/jit(norm)/sqrt\" stack_frame_id=5}\n}\n\n\n\n// ===== BACKWARD PASS (OPTIMIZED) =====\nHloModule jit_scalar_output_wrapper, is_scheduled=true, entry_computation_layout={(f32[10,3]{1,0})->f32[10,3]{1,0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmp0wosud95.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"extract_ir.<locals>.scalar_output_wrapper\"\n6 \"vec_khvl\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=66 end_line=66 column=27 end_column=65}\n5 {file_name_id=2 function_name_id=5 line=61 end_line=61 column=23 end_column=38}\n6 {file_name_id=2 function_name_id=5 line=58 end_line=58 column=21 end_column=32}\n7 {file_name_id=3 function_name_id=6 line=7 end_line=7 column=11 end_column=38}\n8 {file_name_id=2 function_name_id=4 line=41 end_line=41 column=14 end_column=47}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=7}\n8 {file_location_id=8 parent_frame_id=4}\n9 {file_location_id=7 parent_frame_id=9}\n\n\n%fused_computation (param_0.1: f32[10,3], param_1.3: f32[10]) -> f32[10,3] {\n  %param_0.1 = f32[10,3]{1,0} parameter(0)\n  %constant.2 = f32[] constant(0.5), metadata={op_name=\"jit(scalar_output_wrapper)/jvp(jit(norm))\"}\n  %broadcast.1 = f32[10]{0} broadcast(%constant.2), dimensions={}, metadata={op_name=\"jit(scalar_output_wrapper)/jvp(jit(norm))\"}\n  %param_1.3 = f32[10]{0} parameter(1)\n  %mul.9 = f32[10]{0} multiply(%broadcast.1, %param_1.3), metadata={op_name=\"jit(scalar_output_wrapper)/jvp(jit(norm))/div\" stack_frame_id=9}\n  %broadcast_in_dim.2 = f32[10,3]{1,0} broadcast(%mul.9), dimensions={0}, metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp(jit(norm)))/broadcast_in_dim\" stack_frame_id=9}\n  %mul.8 = f32[10,3]{1,0} multiply(%param_0.1, %broadcast_in_dim.2), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp(jit(norm)))/mul\" stack_frame_id=9}\n  ROOT %add_any.2 = f32[10,3]{1,0} add(%mul.8, %mul.8), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp(jit(norm)))/add_any\" stack_frame_id=9}\n}\n\n%region_0.1 (reduce_sum.3: f32[], reduce_sum.4: f32[]) -> f32[] {\n  %reduce_sum.3 = f32[] parameter(0), metadata={op_name=\"reduce_sum\"}\n  %reduce_sum.4 = f32[] parameter(1), metadata={op_name=\"reduce_sum\"}\n  ROOT %reduce_sum.5 = f32[] add(%reduce_sum.3, %reduce_sum.4), metadata={op_name=\"reduce_sum\" stack_frame_id=9}\n}\n\n%fused_computation.1 (param_0.4: f32[10,3]) -> f32[10] {\n  %param_0.4 = f32[10,3]{1,0} parameter(0)\n  %mul.10 = f32[10,3]{1,0} multiply(%param_0.4, %param_0.4), metadata={op_name=\"jit(scalar_output_wrapper)/jvp(jit(norm))/mul\" stack_frame_id=9}\n  %constant.6 = f32[] constant(0), metadata={op_name=\"jit(scalar_output_wrapper)/jvp(jit(norm))\"}\n  %reduce_sum.1 = f32[10]{0} reduce(%mul.10, %constant.6), dimensions={1}, to_apply=%region_0.1, metadata={op_name=\"jit(scalar_output_wrapper)/jvp(jit(norm))/reduce_sum\" stack_frame_id=9}\n  ROOT %rsqrt.1 = f32[10]{0} rsqrt(%reduce_sum.1), metadata={op_name=\"jit(scalar_output_wrapper)/jvp(jit(norm))/sqrt\" stack_frame_id=9}\n}\n\nENTRY %main.4 (args_0_.1: f32[10,3]) -> f32[10,3] {\n  %args_0_.1 = f32[10,3]{1,0} parameter(0), metadata={op_name=\"args[0]\"}\n  %reduce_rsqrt_fusion = f32[10]{0} fusion(%args_0_.1), kind=kLoop, calls=%fused_computation.1, metadata={op_name=\"jit(scalar_output_wrapper)/jvp(jit(norm))/sqrt\" stack_frame_id=9}\n  ROOT %multiply_add_fusion = f32[10,3]{1,0} fusion(%args_0_.1, %reduce_rsqrt_fusion), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp(jit(norm)))/add_any\" stack_frame_id=9}\n}\n\n"}
{"id": 24, "kernel_name": "scalar_arr_blgl", "python": "def scalar_arr_blgl(alpha, x, y):\n    \"\"\"Scalar and array operations.\"\"\"\n    return alpha - x * y", "type": "generate_scalar_array_op", "hlo": "// ===== FORWARD PASS =====\nmodule @jit_scalar_arr_blgl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<10xf32>, %arg2: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<10xf32>\n    %1 = stablehlo.broadcast_in_dim %arg0, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %2 = stablehlo.subtract %1, %0 : tensor<10xf32>\n    return %2 : tensor<10xf32>\n  }\n}\n\n\n// ===== BACKWARD PASS =====\nmodule @jit_scalar_output_wrapper attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main() -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst_0) applies stablehlo.add across dimensions = [0] : (tensor<10xf32>, tensor<f32>) -> tensor<f32>\n    return %1 : tensor<f32>\n  }\n}\n", "optimized_hlo": "// ===== FORWARD PASS (OPTIMIZED) =====\nHloModule jit_scalar_arr_blgl, is_scheduled=true, entry_computation_layout={(f32[], f32[10]{0}, f32[10]{0})->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmp_kmxh6sy.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"scalar_arr_blgl\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=41 end_line=41 column=14 end_column=47}\n5 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=19 end_column=24}\n6 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=11 end_column=24}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[10], param_1.2: f32[10], param_2.1: f32[]) -> f32[10] {\n  %param_2.1 = f32[] parameter(2)\n  %sub.1 = f32[10]{0} broadcast(%param_2.1), dimensions={}, metadata={op_name=\"jit(scalar_arr_blgl)/sub\" stack_frame_id=6}\n  %param_0.1 = f32[10]{0} parameter(0)\n  %param_1.2 = f32[10]{0} parameter(1)\n  %mul.0 = f32[10]{0} multiply(%param_0.1, %param_1.2), metadata={op_name=\"jit(scalar_arr_blgl)/mul\" stack_frame_id=5}\n  ROOT %sub.0 = f32[10]{0} subtract(%sub.1, %mul.0), metadata={op_name=\"jit(scalar_arr_blgl)/sub\" stack_frame_id=6}\n}\n\nENTRY %main.1 (alpha.1: f32[], x.1: f32[10], y.1: f32[10]) -> f32[10] {\n  %alpha.1 = f32[] parameter(0), metadata={op_name=\"alpha\"}\n  %x.1 = f32[10]{0} parameter(1), metadata={op_name=\"x\"}\n  %y.1 = f32[10]{0} parameter(2), metadata={op_name=\"y\"}\n  ROOT %multiply_subtract_fusion = f32[10]{0} fusion(%x.1, %y.1, %alpha.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(scalar_arr_blgl)/sub\" stack_frame_id=6}\n}\n\n\n\n// ===== BACKWARD PASS (OPTIMIZED) =====\nHloModule jit_scalar_output_wrapper, is_scheduled=true, entry_computation_layout={()->f32[]}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmp_kmxh6sy.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"extract_ir.<locals>.scalar_output_wrapper\"\n6 \"scalar_arr_blgl\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=66 end_line=66 column=27 end_column=65}\n5 {file_name_id=2 function_name_id=5 line=58 end_line=58 column=21 end_column=32}\n6 {file_name_id=3 function_name_id=6 line=7 end_line=7 column=11 end_column=24}\n7 {file_name_id=2 function_name_id=5 line=61 end_line=61 column=23 end_column=38}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=6}\n7 {file_location_id=7 parent_frame_id=5}\n\n\nENTRY %main.2 () -> f32[] {\n  %constant = f32[] constant(10), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/reduce_sum\" stack_frame_id=6}\n  ROOT %copy = f32[] copy(%constant)\n}\n\n"}
{"id": 25, "kernel_name": "scalar_arr_btzk", "python": "def scalar_arr_btzk(alpha, x, y):\n    \"\"\"Scalar and array operations.\"\"\"\n    return alpha * x - y", "type": "generate_scalar_array_op", "hlo": "// ===== FORWARD PASS =====\nmodule @jit_scalar_arr_btzk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<10xf32>, %arg2: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %1 = stablehlo.multiply %0, %arg1 : tensor<10xf32>\n    %2 = stablehlo.subtract %1, %arg2 : tensor<10xf32>\n    return %2 : tensor<10xf32>\n  }\n}\n\n\n// ===== BACKWARD PASS =====\nmodule @jit_scalar_output_wrapper attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %1 = stablehlo.multiply %0, %arg0 : tensor<10xf32>\n    %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %2 = stablehlo.reduce(%1 init: %cst_0) applies stablehlo.add across dimensions = [0] : (tensor<10xf32>, tensor<f32>) -> tensor<f32>\n    return %2 : tensor<f32>\n  }\n}\n", "optimized_hlo": "// ===== FORWARD PASS (OPTIMIZED) =====\nHloModule jit_scalar_arr_btzk, is_scheduled=true, entry_computation_layout={(f32[], f32[10]{0}, f32[10]{0})->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmp75vpy4x4.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"scalar_arr_btzk\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=41 end_line=41 column=14 end_column=47}\n5 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=11 end_column=20}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[10], param_1.2: f32[10], param_2.1: f32[]) -> f32[10] {\n  %param_2.1 = f32[] parameter(2)\n  %mul.1 = f32[10]{0} broadcast(%param_2.1), dimensions={}, metadata={op_name=\"jit(scalar_arr_btzk)/mul\" stack_frame_id=5}\n  %param_1.2 = f32[10]{0} parameter(1)\n  %mul.0 = f32[10]{0} multiply(%mul.1, %param_1.2), metadata={op_name=\"jit(scalar_arr_btzk)/mul\" stack_frame_id=5}\n  %param_0.1 = f32[10]{0} parameter(0)\n  ROOT %sub.0 = f32[10]{0} subtract(%mul.0, %param_0.1), metadata={op_name=\"jit(scalar_arr_btzk)/sub\" stack_frame_id=5}\n}\n\nENTRY %main.1 (alpha.1: f32[], x.1: f32[10], y.1: f32[10]) -> f32[10] {\n  %alpha.1 = f32[] parameter(0), metadata={op_name=\"alpha\"}\n  %x.1 = f32[10]{0} parameter(1), metadata={op_name=\"x\"}\n  %y.1 = f32[10]{0} parameter(2), metadata={op_name=\"y\"}\n  ROOT %multiply_subtract_fusion = f32[10]{0} fusion(%y.1, %x.1, %alpha.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(scalar_arr_btzk)/sub\" stack_frame_id=5}\n}\n\n\n\n// ===== BACKWARD PASS (OPTIMIZED) =====\nHloModule jit_scalar_output_wrapper, is_scheduled=true, entry_computation_layout={(f32[10]{0})->f32[]}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmp75vpy4x4.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"extract_ir.<locals>.scalar_output_wrapper\"\n6 \"scalar_arr_btzk\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=66 end_line=66 column=27 end_column=65}\n5 {file_name_id=2 function_name_id=5 line=58 end_line=58 column=21 end_column=32}\n6 {file_name_id=3 function_name_id=6 line=7 end_line=7 column=11 end_column=20}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=6}\n\n\n%region_0.1 (reduce_sum.3: f32[], reduce_sum.4: f32[]) -> f32[] {\n  %reduce_sum.3 = f32[] parameter(0), metadata={op_name=\"reduce_sum\"}\n  %reduce_sum.4 = f32[] parameter(1), metadata={op_name=\"reduce_sum\"}\n  ROOT %reduce_sum.5 = f32[] add(%reduce_sum.3, %reduce_sum.4), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/reduce_sum\" stack_frame_id=6}\n}\n\n%wrapped_reduce_computation (param_0: f32[10], param_1: f32[]) -> f32[] {\n  %param_0 = f32[10]{0} parameter(0)\n  %param_1 = f32[] parameter(1)\n  ROOT %reduce_sum.0 = f32[] reduce(%param_0, %param_1), dimensions={0}, to_apply=%region_0.1, metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/reduce_sum\" stack_frame_id=6}\n}\n\nENTRY %main.2 (args_1_.1: f32[10]) -> f32[] {\n  %args_1_.1 = f32[10]{0} parameter(0), metadata={op_name=\"args[1]\"}\n  %constant.1 = f32[] constant(0)\n  ROOT %wrapped_reduce = f32[] fusion(%args_1_.1, %constant.1), kind=kLoop, calls=%wrapped_reduce_computation, metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/reduce_sum\" stack_frame_id=6}\n}\n\n"}
{"id": 26, "kernel_name": "multi_tytx", "python": "def multi_tytx(a, b):\n    \"\"\"Multi-statement computation.\"\"\"\n    temp1 = a + b\n    temp2 = jnp.abs(temp1)\n    result = temp2 * a\n    return result", "type": "generate_multi_statement_kernel", "hlo": "// ===== FORWARD PASS =====\nmodule @jit_multi_tytx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>, %arg1: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<10xf32>\n    %1 = stablehlo.abs %0 : tensor<10xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<10xf32>\n    return %2 : tensor<10xf32>\n  }\n}\n\n\n// ===== BACKWARD PASS =====\nmodule @jit_scalar_output_wrapper attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>, %arg1: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<10xf32>\n    %1 = stablehlo.abs %0 : tensor<10xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %3 = stablehlo.compare  GE, %0, %2,  FLOAT : (tensor<10xf32>, tensor<10xf32>) -> tensor<10xi1>\n    %cst_0 = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %5 = stablehlo.multiply %1, %4 : tensor<10xf32>\n    %6 = stablehlo.multiply %4, %arg0 : tensor<10xf32>\n    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %8 = stablehlo.select %3, %7, %6 : tensor<10xi1>, tensor<10xf32>\n    %9 = stablehlo.select %3, %6, %7 : tensor<10xi1>, tensor<10xf32>\n    %10 = stablehlo.add %5, %9 : tensor<10xf32>\n    %11 = stablehlo.negate %8 : tensor<10xf32>\n    %12 = stablehlo.add %10, %11 : tensor<10xf32>\n    return %12 : tensor<10xf32>\n  }\n}\n", "optimized_hlo": "// ===== FORWARD PASS (OPTIMIZED) =====\nHloModule jit_multi_tytx, is_scheduled=true, entry_computation_layout={(f32[10]{0}, f32[10]{0})->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmpkqw6l192.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"multi_tytx\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=41 end_line=41 column=14 end_column=47}\n5 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=12 end_column=17}\n6 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=12 end_column=26}\n7 {file_name_id=3 function_name_id=5 line=9 end_line=9 column=13 end_column=22}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[10], param_1.2: f32[10]) -> f32[10] {\n  %param_0.1 = f32[10]{0} parameter(0)\n  %param_1.2 = f32[10]{0} parameter(1)\n  %add.0 = f32[10]{0} add(%param_0.1, %param_1.2), metadata={op_name=\"jit(multi_tytx)/add\" stack_frame_id=5}\n  %abs.0 = f32[10]{0} abs(%add.0), metadata={op_name=\"jit(multi_tytx)/abs\" stack_frame_id=6}\n  ROOT %mul.0 = f32[10]{0} multiply(%abs.0, %param_0.1), metadata={op_name=\"jit(multi_tytx)/mul\" stack_frame_id=7}\n}\n\nENTRY %main.1 (a.1: f32[10], b.1: f32[10]) -> f32[10] {\n  %a.1 = f32[10]{0} parameter(0), metadata={op_name=\"a\"}\n  %b.1 = f32[10]{0} parameter(1), metadata={op_name=\"b\"}\n  ROOT %abs_multiply_fusion = f32[10]{0} fusion(%a.1, %b.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(multi_tytx)/mul\" stack_frame_id=7}\n}\n\n\n\n// ===== BACKWARD PASS (OPTIMIZED) =====\nHloModule jit_scalar_output_wrapper, is_scheduled=true, entry_computation_layout={(f32[10]{0}, f32[10]{0})->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmpkqw6l192.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"extract_ir.<locals>.scalar_output_wrapper\"\n6 \"multi_tytx\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=66 end_line=66 column=27 end_column=65}\n5 {file_name_id=2 function_name_id=5 line=58 end_line=58 column=21 end_column=32}\n6 {file_name_id=3 function_name_id=6 line=7 end_line=7 column=12 end_column=17}\n7 {file_name_id=3 function_name_id=6 line=8 end_line=8 column=12 end_column=26}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=6}\n7 {file_location_id=7 parent_frame_id=6}\n\n\n%fused_computation (param_0.3: f32[10], param_1.7: f32[10]) -> f32[10] {\n  %param_0.3 = f32[10]{0} parameter(0)\n  %param_1.7 = f32[10]{0} parameter(1)\n  %add.0 = f32[10]{0} add(%param_0.3, %param_1.7), metadata={op_name=\"jit(scalar_output_wrapper)/jvp()/add\" stack_frame_id=6}\n  %abs.0 = f32[10]{0} abs(%add.0), metadata={op_name=\"jit(scalar_output_wrapper)/jvp()/abs\" stack_frame_id=7}\n  %constant.0 = f32[] constant(0)\n  %broadcast.0 = f32[10]{0} broadcast(%constant.0), dimensions={}\n  %ge.0 = pred[10]{0} compare(%add.0, %broadcast.0), direction=GE, metadata={op_name=\"jit(scalar_output_wrapper)/jvp()/ge\" stack_frame_id=7}\n  %select_n.1 = f32[10]{0} select(%ge.0, %param_0.3, %broadcast.0), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/select_n\" stack_frame_id=7}\n  %add_any.1 = f32[10]{0} add(%abs.0, %select_n.1), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/add_any\" stack_frame_id=7}\n  %select_n.0 = f32[10]{0} select(%ge.0, %broadcast.0, %param_0.3), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/select_n\" stack_frame_id=7}\n  %neg.0 = f32[10]{0} negate(%select_n.0), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/neg\" stack_frame_id=7}\n  ROOT %add_any.0 = f32[10]{0} add(%add_any.1, %neg.0), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/add_any\" stack_frame_id=7}\n}\n\nENTRY %main.1 (args_0_.1: f32[10], args_1_.1: f32[10]) -> f32[10] {\n  %args_0_.1 = f32[10]{0} parameter(0), metadata={op_name=\"args[0]\"}\n  %args_1_.1 = f32[10]{0} parameter(1), metadata={op_name=\"args[1]\"}\n  ROOT %negate_add_fusion = f32[10]{0} fusion(%args_0_.1, %args_1_.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/add_any\" stack_frame_id=7}\n}\n\n"}
{"id": 27, "kernel_name": "elementwise_rcep", "python": "def elementwise_rcep(a, b):\n    \"\"\"Elementwise operation on two arrays.\"\"\"\n    return a - b", "type": "generate_simple_elementwise", "hlo": "// ===== FORWARD PASS =====\nmodule @jit_elementwise_rcep attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>, %arg1: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<10xf32>\n    return %0 : tensor<10xf32>\n  }\n}\n\n\n// ===== BACKWARD PASS =====\nmodule @jit_scalar_output_wrapper attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main() -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    return %0 : tensor<10xf32>\n  }\n}\n", "optimized_hlo": "// ===== FORWARD PASS (OPTIMIZED) =====\nHloModule jit_elementwise_rcep, is_scheduled=true, entry_computation_layout={(f32[10]{0}, f32[10]{0})->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmpdtf3xb5k.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"elementwise_rcep\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=41 end_line=41 column=14 end_column=47}\n5 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=11 end_column=16}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n\n\n%wrapped_subtract_computation (param_0: f32[10], param_1: f32[10]) -> f32[10] {\n  %param_0 = f32[10]{0} parameter(0)\n  %param_1 = f32[10]{0} parameter(1)\n  ROOT %sub.0 = f32[10]{0} subtract(%param_0, %param_1), metadata={op_name=\"jit(elementwise_rcep)/sub\" stack_frame_id=5}\n}\n\nENTRY %main.1 (a.1: f32[10], b.1: f32[10]) -> f32[10] {\n  %a.1 = f32[10]{0} parameter(0), metadata={op_name=\"a\"}\n  %b.1 = f32[10]{0} parameter(1), metadata={op_name=\"b\"}\n  ROOT %wrapped_subtract = f32[10]{0} fusion(%a.1, %b.1), kind=kLoop, calls=%wrapped_subtract_computation, metadata={op_name=\"jit(elementwise_rcep)/sub\" stack_frame_id=5}\n}\n\n\n\n// ===== BACKWARD PASS (OPTIMIZED) =====\nHloModule jit_scalar_output_wrapper, is_scheduled=true, entry_computation_layout={()->f32[10]{0}}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"extract_ir.<locals>.scalar_output_wrapper\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=66 end_line=66 column=27 end_column=65}\n5 {file_name_id=2 function_name_id=5 line=61 end_line=61 column=23 end_column=38}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n\n\n%wrapped_broadcast_computation (param_0: f32[]) -> f32[10] {\n  %param_0 = f32[] parameter(0)\n  ROOT %broadcast.0 = f32[10]{0} broadcast(%param_0), dimensions={}\n}\n\nENTRY %main.1 () -> f32[10] {\n  %constant.1 = f32[] constant(1)\n  ROOT %wrapped_broadcast = f32[10]{0} fusion(%constant.1), kind=kLoop, calls=%wrapped_broadcast_computation\n}\n\n"}
{"id": 28, "kernel_name": "scalar_arr_xhld", "python": "def scalar_arr_xhld(alpha, x, y):\n    \"\"\"Scalar and array operations.\"\"\"\n    return alpha + x + y", "type": "generate_scalar_array_op", "hlo": "// ===== FORWARD PASS =====\nmodule @jit_scalar_arr_xhld attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<10xf32>, %arg2: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %1 = stablehlo.add %0, %arg1 : tensor<10xf32>\n    %2 = stablehlo.add %1, %arg2 : tensor<10xf32>\n    return %2 : tensor<10xf32>\n  }\n}\n\n\n// ===== BACKWARD PASS =====\nmodule @jit_scalar_output_wrapper attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main() -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst_0) applies stablehlo.add across dimensions = [0] : (tensor<10xf32>, tensor<f32>) -> tensor<f32>\n    return %1 : tensor<f32>\n  }\n}\n", "optimized_hlo": "// ===== FORWARD PASS (OPTIMIZED) =====\nHloModule jit_scalar_arr_xhld, is_scheduled=true, entry_computation_layout={(f32[], f32[10]{0}, f32[10]{0})->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmpcj7jaboh.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"scalar_arr_xhld\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=41 end_line=41 column=14 end_column=47}\n5 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=11 end_column=20}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[10], param_1.2: f32[10], param_2.1: f32[]) -> f32[10] {\n  %param_2.1 = f32[] parameter(2)\n  %add.2 = f32[10]{0} broadcast(%param_2.1), dimensions={}, metadata={op_name=\"jit(scalar_arr_xhld)/add\" stack_frame_id=5}\n  %param_1.2 = f32[10]{0} parameter(1)\n  %add.1 = f32[10]{0} add(%add.2, %param_1.2), metadata={op_name=\"jit(scalar_arr_xhld)/add\" stack_frame_id=5}\n  %param_0.1 = f32[10]{0} parameter(0)\n  ROOT %add.0 = f32[10]{0} add(%add.1, %param_0.1), metadata={op_name=\"jit(scalar_arr_xhld)/add\" stack_frame_id=5}\n}\n\nENTRY %main.1 (alpha.1: f32[], x.1: f32[10], y.1: f32[10]) -> f32[10] {\n  %alpha.1 = f32[] parameter(0), metadata={op_name=\"alpha\"}\n  %x.1 = f32[10]{0} parameter(1), metadata={op_name=\"x\"}\n  %y.1 = f32[10]{0} parameter(2), metadata={op_name=\"y\"}\n  ROOT %add_add_fusion = f32[10]{0} fusion(%y.1, %x.1, %alpha.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(scalar_arr_xhld)/add\" stack_frame_id=5}\n}\n\n\n\n// ===== BACKWARD PASS (OPTIMIZED) =====\nHloModule jit_scalar_output_wrapper, is_scheduled=true, entry_computation_layout={()->f32[]}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmpcj7jaboh.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"extract_ir.<locals>.scalar_output_wrapper\"\n6 \"scalar_arr_xhld\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=66 end_line=66 column=27 end_column=65}\n5 {file_name_id=2 function_name_id=5 line=58 end_line=58 column=21 end_column=32}\n6 {file_name_id=3 function_name_id=6 line=7 end_line=7 column=11 end_column=20}\n7 {file_name_id=2 function_name_id=5 line=61 end_line=61 column=23 end_column=38}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=6}\n7 {file_location_id=7 parent_frame_id=5}\n\n\nENTRY %main.2 () -> f32[] {\n  %constant = f32[] constant(10), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/reduce_sum\" stack_frame_id=6}\n  ROOT %copy = f32[] copy(%constant)\n}\n\n"}
{"id": 29, "kernel_name": "reduce_iqaz", "python": "def reduce_iqaz(a):\n    \"\"\"Sum reduction over array.\"\"\"\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "hlo": "// ===== FORWARD PASS =====\nmodule @jit_reduce_iqaz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<10xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n\n\n// ===== BACKWARD PASS =====\nmodule @jit_scalar_output_wrapper attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main() -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    return %0 : tensor<10xf32>\n  }\n}\n", "optimized_hlo": "// ===== FORWARD PASS (OPTIMIZED) =====\nHloModule jit_reduce_iqaz, is_scheduled=true, entry_computation_layout={(f32[10]{0})->f32[]}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmpk_g0ui1o.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"reduce_iqaz\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=41 end_line=41 column=14 end_column=47}\n5 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=11 end_column=21}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n\n\n%region_0.1 (reduce_sum.3: f32[], reduce_sum.4: f32[]) -> f32[] {\n  %reduce_sum.3 = f32[] parameter(0), metadata={op_name=\"reduce_sum\"}\n  %reduce_sum.4 = f32[] parameter(1), metadata={op_name=\"reduce_sum\"}\n  ROOT %reduce_sum.5 = f32[] add(%reduce_sum.3, %reduce_sum.4), metadata={op_name=\"jit(reduce_iqaz)/reduce_sum\" stack_frame_id=5}\n}\n\n%wrapped_reduce_computation (param_0: f32[10], param_1: f32[]) -> f32[] {\n  %param_0 = f32[10]{0} parameter(0)\n  %param_1 = f32[] parameter(1)\n  ROOT %reduce_sum.0 = f32[] reduce(%param_0, %param_1), dimensions={0}, to_apply=%region_0.1, metadata={op_name=\"jit(reduce_iqaz)/reduce_sum\" stack_frame_id=5}\n}\n\nENTRY %main.2 (a.1: f32[10]) -> f32[] {\n  %a.1 = f32[10]{0} parameter(0), metadata={op_name=\"a\"}\n  %constant.1 = f32[] constant(0)\n  ROOT %wrapped_reduce = f32[] fusion(%a.1, %constant.1), kind=kLoop, calls=%wrapped_reduce_computation, metadata={op_name=\"jit(reduce_iqaz)/reduce_sum\" stack_frame_id=5}\n}\n\n\n\n// ===== BACKWARD PASS (OPTIMIZED) =====\nHloModule jit_scalar_output_wrapper, is_scheduled=true, entry_computation_layout={()->f32[10]{0}}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmpk_g0ui1o.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"extract_ir.<locals>.scalar_output_wrapper\"\n6 \"reduce_iqaz\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=66 end_line=66 column=27 end_column=65}\n5 {file_name_id=2 function_name_id=5 line=58 end_line=58 column=21 end_column=32}\n6 {file_name_id=3 function_name_id=6 line=7 end_line=7 column=11 end_column=21}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=6}\n\n\n%wrapped_broadcast_computation (param_0: f32[]) -> f32[10] {\n  %param_0 = f32[] parameter(0)\n  ROOT %broadcast.0 = f32[10]{0} broadcast(%param_0), dimensions={}\n}\n\nENTRY %main.1 () -> f32[10] {\n  %constant.1 = f32[] constant(1)\n  ROOT %wrapped_broadcast = f32[10]{0} fusion(%constant.1), kind=kLoop, calls=%wrapped_broadcast_computation\n}\n\n"}
{"id": 30, "kernel_name": "scalar_arr_bpzi", "python": "def scalar_arr_bpzi(alpha, x, y):\n    \"\"\"Scalar and array operations.\"\"\"\n    return alpha * x * y", "type": "generate_scalar_array_op", "hlo": "// ===== FORWARD PASS =====\nmodule @jit_scalar_arr_bpzi attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<10xf32>, %arg2: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %1 = stablehlo.multiply %0, %arg1 : tensor<10xf32>\n    %2 = stablehlo.multiply %1, %arg2 : tensor<10xf32>\n    return %2 : tensor<10xf32>\n  }\n}\n\n\n// ===== BACKWARD PASS =====\nmodule @jit_scalar_output_wrapper attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>, %arg1: tensor<10xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %1 = stablehlo.multiply %0, %arg1 : tensor<10xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<10xf32>\n    %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %3 = stablehlo.reduce(%2 init: %cst_0) applies stablehlo.add across dimensions = [0] : (tensor<10xf32>, tensor<f32>) -> tensor<f32>\n    return %3 : tensor<f32>\n  }\n}\n", "optimized_hlo": "// ===== FORWARD PASS (OPTIMIZED) =====\nHloModule jit_scalar_arr_bpzi, is_scheduled=true, entry_computation_layout={(f32[], f32[10]{0}, f32[10]{0})->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmpvbh53c7r.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"scalar_arr_bpzi\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=41 end_line=41 column=14 end_column=47}\n5 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=11 end_column=20}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[10], param_1.2: f32[10], param_2.1: f32[]) -> f32[10] {\n  %param_2.1 = f32[] parameter(2)\n  %mul.2 = f32[10]{0} broadcast(%param_2.1), dimensions={}, metadata={op_name=\"jit(scalar_arr_bpzi)/mul\" stack_frame_id=5}\n  %param_1.2 = f32[10]{0} parameter(1)\n  %mul.1 = f32[10]{0} multiply(%mul.2, %param_1.2), metadata={op_name=\"jit(scalar_arr_bpzi)/mul\" stack_frame_id=5}\n  %param_0.1 = f32[10]{0} parameter(0)\n  ROOT %mul.0 = f32[10]{0} multiply(%mul.1, %param_0.1), metadata={op_name=\"jit(scalar_arr_bpzi)/mul\" stack_frame_id=5}\n}\n\nENTRY %main.1 (alpha.1: f32[], x.1: f32[10], y.1: f32[10]) -> f32[10] {\n  %alpha.1 = f32[] parameter(0), metadata={op_name=\"alpha\"}\n  %x.1 = f32[10]{0} parameter(1), metadata={op_name=\"x\"}\n  %y.1 = f32[10]{0} parameter(2), metadata={op_name=\"y\"}\n  ROOT %multiply_multiply_fusion = f32[10]{0} fusion(%y.1, %x.1, %alpha.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(scalar_arr_bpzi)/mul\" stack_frame_id=5}\n}\n\n\n\n// ===== BACKWARD PASS (OPTIMIZED) =====\nHloModule jit_scalar_output_wrapper, is_scheduled=true, entry_computation_layout={(f32[10]{0}, f32[10]{0})->f32[]}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmpvbh53c7r.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"extract_ir.<locals>.scalar_output_wrapper\"\n6 \"scalar_arr_bpzi\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=66 end_line=66 column=27 end_column=65}\n5 {file_name_id=2 function_name_id=5 line=58 end_line=58 column=21 end_column=32}\n6 {file_name_id=3 function_name_id=6 line=7 end_line=7 column=11 end_column=20}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=6}\n\n\n%region_0.1 (reduce_sum.3: f32[], reduce_sum.4: f32[]) -> f32[] {\n  %reduce_sum.3 = f32[] parameter(0), metadata={op_name=\"reduce_sum\"}\n  %reduce_sum.4 = f32[] parameter(1), metadata={op_name=\"reduce_sum\"}\n  ROOT %reduce_sum.5 = f32[] add(%reduce_sum.3, %reduce_sum.4), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/reduce_sum\" stack_frame_id=6}\n}\n\n%fused_computation (param_0.2: f32[10], param_1.2: f32[10]) -> f32[] {\n  %param_0.2 = f32[10]{0} parameter(0)\n  %param_1.2 = f32[10]{0} parameter(1)\n  %mul.0 = f32[10]{0} multiply(%param_0.2, %param_1.2), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/mul\" stack_frame_id=6}\n  %constant.0 = f32[] constant(0)\n  ROOT %reduce_sum.0 = f32[] reduce(%mul.0, %constant.0), dimensions={0}, to_apply=%region_0.1, metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/reduce_sum\" stack_frame_id=6}\n}\n\nENTRY %main.2 (args_1_.1: f32[10], args_2_.1: f32[10]) -> f32[] {\n  %args_1_.1 = f32[10]{0} parameter(0), metadata={op_name=\"args[1]\"}\n  %args_2_.1 = f32[10]{0} parameter(1), metadata={op_name=\"args[2]\"}\n  ROOT %multiply_reduce_fusion = f32[] fusion(%args_2_.1, %args_1_.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/reduce_sum\" stack_frame_id=6}\n}\n\n"}
{"id": 31, "kernel_name": "loop_hnol", "python": "def loop_hnol(a):\n    \"\"\"Reduction operation simulating loop accumulation.\"\"\"\n    # Accumulate n=3 times\n    result = a\n    for _ in range(3):\n        result = result * a\n    return result", "type": "generate_loop_kernel", "hlo": "// ===== FORWARD PASS =====\nmodule @jit_loop_hnol attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<10xf32>\n    %1 = stablehlo.multiply %0, %arg0 : tensor<10xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<10xf32>\n    return %2 : tensor<10xf32>\n  }\n}\n\n\n// ===== BACKWARD PASS =====\nmodule @jit_scalar_output_wrapper attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<10xf32>\n    %1 = stablehlo.multiply %0, %arg0 : tensor<10xf32>\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %3 = stablehlo.multiply %1, %2 : tensor<10xf32>\n    %4 = stablehlo.multiply %2, %arg0 : tensor<10xf32>\n    %5 = stablehlo.multiply %0, %4 : tensor<10xf32>\n    %6 = stablehlo.add %3, %5 : tensor<10xf32>\n    %7 = stablehlo.multiply %4, %arg0 : tensor<10xf32>\n    %8 = stablehlo.multiply %arg0, %7 : tensor<10xf32>\n    %9 = stablehlo.add %6, %8 : tensor<10xf32>\n    %10 = stablehlo.multiply %7, %arg0 : tensor<10xf32>\n    %11 = stablehlo.add %9, %10 : tensor<10xf32>\n    return %11 : tensor<10xf32>\n  }\n}\n", "optimized_hlo": "// ===== FORWARD PASS (OPTIMIZED) =====\nHloModule jit_loop_hnol, is_scheduled=true, entry_computation_layout={(f32[10]{0})->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmpoklz1opg.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"loop_hnol\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=41 end_line=41 column=14 end_column=47}\n5 {file_name_id=3 function_name_id=5 line=10 end_line=10 column=17 end_column=27}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[10]) -> f32[10] {\n  %param_0.1 = f32[10]{0} parameter(0)\n  %mul.2 = f32[10]{0} multiply(%param_0.1, %param_0.1), metadata={op_name=\"jit(loop_hnol)/mul\" stack_frame_id=5}\n  %mul.1 = f32[10]{0} multiply(%mul.2, %param_0.1), metadata={op_name=\"jit(loop_hnol)/mul\" stack_frame_id=5}\n  ROOT %mul.0 = f32[10]{0} multiply(%mul.1, %param_0.1), metadata={op_name=\"jit(loop_hnol)/mul\" stack_frame_id=5}\n}\n\nENTRY %main.1 (a.1: f32[10]) -> f32[10] {\n  %a.1 = f32[10]{0} parameter(0), metadata={op_name=\"a\"}\n  ROOT %multiply_multiply_fusion = f32[10]{0} fusion(%a.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(loop_hnol)/mul\" stack_frame_id=5}\n}\n\n\n\n// ===== BACKWARD PASS (OPTIMIZED) =====\nHloModule jit_scalar_output_wrapper, is_scheduled=true, entry_computation_layout={(f32[10]{0})->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmpoklz1opg.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"extract_ir.<locals>.scalar_output_wrapper\"\n6 \"loop_hnol\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=66 end_line=66 column=27 end_column=65}\n5 {file_name_id=2 function_name_id=5 line=58 end_line=58 column=21 end_column=32}\n6 {file_name_id=3 function_name_id=6 line=10 end_line=10 column=17 end_column=27}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=6}\n\n\n%fused_computation (param_0.3: f32[10]) -> f32[10] {\n  %param_0.3 = f32[10]{0} parameter(0)\n  %mul.1 = f32[10]{0} multiply(%param_0.3, %param_0.3), metadata={op_name=\"jit(scalar_output_wrapper)/jvp()/mul\" stack_frame_id=6}\n  %mul.0 = f32[10]{0} multiply(%mul.1, %param_0.3), metadata={op_name=\"jit(scalar_output_wrapper)/jvp()/mul\" stack_frame_id=6}\n  %add_any.2 = f32[10]{0} add(%mul.0, %mul.0), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/add_any\" stack_frame_id=6}\n  %add_any.1 = f32[10]{0} add(%add_any.2, %mul.0), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/add_any\" stack_frame_id=6}\n  ROOT %add_any.0 = f32[10]{0} add(%add_any.1, %mul.0), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/add_any\" stack_frame_id=6}\n}\n\nENTRY %main.1 (args_0_.1: f32[10]) -> f32[10] {\n  %args_0_.1 = f32[10]{0} parameter(0), metadata={op_name=\"args[0]\"}\n  ROOT %add_add_fusion = f32[10]{0} fusion(%args_0_.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/add_any\" stack_frame_id=6}\n}\n\n"}
{"id": 32, "kernel_name": "compound_wncd", "python": "def compound_wncd(a, b, scale):\n    \"\"\"Compound operations with multiple steps.\"\"\"\n    result = (a + b) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "hlo": "// ===== FORWARD PASS =====\nmodule @jit_compound_wncd attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>, %arg1: tensor<10xf32>, %arg2: tensor<f32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<10xf32>\n    %1 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %2 = stablehlo.multiply %0, %1 : tensor<10xf32>\n    %3 = stablehlo.floor %2 : tensor<10xf32>\n    %4 = stablehlo.subtract %2, %3 : tensor<10xf32>\n    %5 = stablehlo.abs %4 : tensor<10xf32>\n    return %5 : tensor<10xf32>\n  }\n}\n\n\n// ===== BACKWARD PASS =====\nmodule @jit_scalar_output_wrapper attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>, %arg1: tensor<10xf32>, %arg2: tensor<f32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<10xf32>\n    %1 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %2 = stablehlo.multiply %0, %1 : tensor<10xf32>\n    %3 = stablehlo.floor %2 : tensor<10xf32>\n    %4 = stablehlo.subtract %2, %3 : tensor<10xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %5 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %6 = stablehlo.compare  GE, %4, %5,  FLOAT : (tensor<10xf32>, tensor<10xf32>) -> tensor<10xi1>\n    %cst_0 = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %8 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %9 = stablehlo.select %6, %8, %7 : tensor<10xi1>, tensor<10xf32>\n    %10 = stablehlo.select %6, %7, %8 : tensor<10xi1>, tensor<10xf32>\n    %11 = stablehlo.negate %9 : tensor<10xf32>\n    %12 = stablehlo.add %10, %11 : tensor<10xf32>\n    %13 = stablehlo.broadcast_in_dim %arg2, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %14 = stablehlo.multiply %12, %13 : tensor<10xf32>\n    return %14 : tensor<10xf32>\n  }\n}\n", "optimized_hlo": "// ===== FORWARD PASS (OPTIMIZED) =====\nHloModule jit_compound_wncd, is_scheduled=true, entry_computation_layout={(f32[10]{0}, f32[10]{0}, f32[])->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmpajoq5kdk.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"compound_wncd\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=41 end_line=41 column=14 end_column=47}\n5 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=14 end_column=19}\n6 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=13 end_column=28}\n7 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=22 end_column=39}\n8 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=13 end_column=39}\n9 {file_name_id=3 function_name_id=5 line=9 end_line=9 column=11 end_column=26}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=5}\n8 {file_location_id=8 parent_frame_id=5}\n9 {file_location_id=9 parent_frame_id=5}\n\n\n%fused_computation (param_0.3: f32[], param_1.3: f32[10], param_2: f32[10]) -> f32[10] {\n  %param_1.3 = f32[10]{0} parameter(1)\n  %param_2 = f32[10]{0} parameter(2)\n  %add.0 = f32[10]{0} add(%param_1.3, %param_2), metadata={op_name=\"jit(compound_wncd)/add\" stack_frame_id=5}\n  %param_0.3 = f32[] parameter(0)\n  %mul.1 = f32[10]{0} broadcast(%param_0.3), dimensions={}, metadata={op_name=\"jit(compound_wncd)/mul\" stack_frame_id=6}\n  %mul.0 = f32[10]{0} multiply(%add.0, %mul.1), metadata={op_name=\"jit(compound_wncd)/mul\" stack_frame_id=6}\n  %floor.0 = f32[10]{0} floor(%mul.0), metadata={op_name=\"jit(compound_wncd)/floor\" stack_frame_id=7}\n  %sub.0 = f32[10]{0} subtract(%mul.0, %floor.0), metadata={op_name=\"jit(compound_wncd)/sub\" stack_frame_id=8}\n  ROOT %abs.0 = f32[10]{0} abs(%sub.0), metadata={op_name=\"jit(compound_wncd)/abs\" stack_frame_id=9}\n}\n\nENTRY %main.1 (a.1: f32[10], b.1: f32[10], scale.1: f32[]) -> f32[10] {\n  %a.1 = f32[10]{0} parameter(0), metadata={op_name=\"a\"}\n  %b.1 = f32[10]{0} parameter(1), metadata={op_name=\"b\"}\n  %scale.1 = f32[] parameter(2), metadata={op_name=\"scale\"}\n  ROOT %subtract_abs_fusion = f32[10]{0} fusion(%scale.1, %a.1, %b.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(compound_wncd)/abs\" stack_frame_id=9}\n}\n\n\n\n// ===== BACKWARD PASS (OPTIMIZED) =====\nHloModule jit_scalar_output_wrapper, is_scheduled=true, entry_computation_layout={(f32[10]{0}, f32[10]{0}, f32[])->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmpajoq5kdk.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"extract_ir.<locals>.scalar_output_wrapper\"\n6 \"compound_wncd\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=66 end_line=66 column=27 end_column=65}\n5 {file_name_id=2 function_name_id=5 line=61 end_line=61 column=23 end_column=38}\n6 {file_name_id=2 function_name_id=5 line=58 end_line=58 column=21 end_column=32}\n7 {file_name_id=3 function_name_id=6 line=7 end_line=7 column=14 end_column=19}\n8 {file_name_id=3 function_name_id=6 line=7 end_line=7 column=13 end_column=28}\n9 {file_name_id=3 function_name_id=6 line=8 end_line=8 column=22 end_column=39}\n10 {file_name_id=3 function_name_id=6 line=8 end_line=8 column=13 end_column=39}\n11 {file_name_id=3 function_name_id=6 line=9 end_line=9 column=11 end_column=26}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=7}\n8 {file_location_id=8 parent_frame_id=7}\n9 {file_location_id=9 parent_frame_id=7}\n10 {file_location_id=10 parent_frame_id=7}\n11 {file_location_id=11 parent_frame_id=7}\n\n\n%fused_computation (param_0.3: f32[], param_1.8: f32[10], param_2.7: f32[10]) -> f32[10] {\n  %param_1.8 = f32[10]{0} parameter(1)\n  %param_2.7 = f32[10]{0} parameter(2)\n  %add.0 = f32[10]{0} add(%param_1.8, %param_2.7), metadata={op_name=\"jit(scalar_output_wrapper)/jvp()/add\" stack_frame_id=7}\n  %param_0.3 = f32[] parameter(0)\n  %mul.2 = f32[10]{0} broadcast(%param_0.3), dimensions={}, metadata={op_name=\"jit(scalar_output_wrapper)/jvp()/mul\" stack_frame_id=8}\n  %mul.1 = f32[10]{0} multiply(%add.0, %mul.2), metadata={op_name=\"jit(scalar_output_wrapper)/jvp()/mul\" stack_frame_id=8}\n  %floor.0 = f32[10]{0} floor(%mul.1), metadata={op_name=\"jit(scalar_output_wrapper)/jvp()/floor\" stack_frame_id=9}\n  %sub.0 = f32[10]{0} subtract(%mul.1, %floor.0), metadata={op_name=\"jit(scalar_output_wrapper)/jvp()/sub\" stack_frame_id=10}\n  %constant.1 = f32[] constant(0)\n  %broadcast.1 = f32[10]{0} broadcast(%constant.1), dimensions={}\n  %ge.0 = pred[10]{0} compare(%sub.0, %broadcast.1), direction=GE, metadata={op_name=\"jit(scalar_output_wrapper)/jvp()/ge\" stack_frame_id=11}\n  %constant.0 = f32[] constant(1)\n  %broadcast.0 = f32[10]{0} broadcast(%constant.0), dimensions={}\n  %select_n.1 = f32[10]{0} select(%ge.0, %broadcast.0, %broadcast.1), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/select_n\" stack_frame_id=11}\n  %select_n.0 = f32[10]{0} select(%ge.0, %broadcast.1, %broadcast.0), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/select_n\" stack_frame_id=11}\n  %neg.0 = f32[10]{0} negate(%select_n.0), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/neg\" stack_frame_id=11}\n  %add_any.0 = f32[10]{0} add(%select_n.1, %neg.0), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/add_any\" stack_frame_id=11}\n  ROOT %mul.0 = f32[10]{0} multiply(%add_any.0, %mul.2), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/mul\" stack_frame_id=8}\n}\n\nENTRY %main.1 (args_0_.1: f32[10], args_1_.1: f32[10], args_2_.1: f32[]) -> f32[10] {\n  %args_0_.1 = f32[10]{0} parameter(0), metadata={op_name=\"args[0]\"}\n  %args_1_.1 = f32[10]{0} parameter(1), metadata={op_name=\"args[1]\"}\n  %args_2_.1 = f32[] parameter(2), metadata={op_name=\"args[2]\"}\n  ROOT %add_multiply_fusion = f32[10]{0} fusion(%args_2_.1, %args_0_.1, %args_1_.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/mul\" stack_frame_id=8}\n}\n\n"}
{"id": 33, "kernel_name": "multi_llbt", "python": "def multi_llbt(a, b):\n    \"\"\"Multi-statement computation.\"\"\"\n    temp1 = a * b\n    temp2 = jnp.cos(temp1)\n    result = temp2 * a\n    return result", "type": "generate_multi_statement_kernel", "hlo": "// ===== FORWARD PASS =====\nmodule @jit_multi_llbt attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>, %arg1: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<10xf32>\n    %1 = stablehlo.cosine %0 : tensor<10xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<10xf32>\n    return %2 : tensor<10xf32>\n  }\n}\n\n\n// ===== BACKWARD PASS =====\nmodule @jit_scalar_output_wrapper attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>, %arg1: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<10xf32>\n    %1 = stablehlo.cosine %0 : tensor<10xf32>\n    %2 = stablehlo.sine %0 : tensor<10xf32>\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %3 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %4 = stablehlo.multiply %1, %3 : tensor<10xf32>\n    %5 = stablehlo.multiply %3, %arg0 : tensor<10xf32>\n    %6 = stablehlo.negate %5 : tensor<10xf32>\n    %7 = stablehlo.multiply %6, %2 : tensor<10xf32>\n    %8 = stablehlo.multiply %7, %arg1 : tensor<10xf32>\n    %9 = stablehlo.add %4, %8 : tensor<10xf32>\n    return %9 : tensor<10xf32>\n  }\n}\n", "optimized_hlo": "// ===== FORWARD PASS (OPTIMIZED) =====\nHloModule jit_multi_llbt, is_scheduled=true, entry_computation_layout={(f32[10]{0}, f32[10]{0})->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmpvqu703lc.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"multi_llbt\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=41 end_line=41 column=14 end_column=47}\n5 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=12 end_column=17}\n6 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=12 end_column=26}\n7 {file_name_id=3 function_name_id=5 line=9 end_line=9 column=13 end_column=22}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[10], param_1.2: f32[10]) -> f32[10] {\n  %param_0.1 = f32[10]{0} parameter(0)\n  %param_1.2 = f32[10]{0} parameter(1)\n  %mul.1 = f32[10]{0} multiply(%param_0.1, %param_1.2), metadata={op_name=\"jit(multi_llbt)/mul\" stack_frame_id=5}\n  %cos.0 = f32[10]{0} cosine(%mul.1), metadata={op_name=\"jit(multi_llbt)/cos\" stack_frame_id=6}\n  ROOT %mul.0 = f32[10]{0} multiply(%cos.0, %param_0.1), metadata={op_name=\"jit(multi_llbt)/mul\" stack_frame_id=7}\n}\n\nENTRY %main.1 (a.1: f32[10], b.1: f32[10]) -> f32[10] {\n  %a.1 = f32[10]{0} parameter(0), metadata={op_name=\"a\"}\n  %b.1 = f32[10]{0} parameter(1), metadata={op_name=\"b\"}\n  ROOT %cosine_multiply_fusion = f32[10]{0} fusion(%a.1, %b.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(multi_llbt)/mul\" stack_frame_id=7}\n}\n\n\n\n// ===== BACKWARD PASS (OPTIMIZED) =====\nHloModule jit_scalar_output_wrapper, is_scheduled=true, entry_computation_layout={(f32[10]{0}, f32[10]{0})->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmpvqu703lc.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"extract_ir.<locals>.scalar_output_wrapper\"\n6 \"multi_llbt\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=66 end_line=66 column=27 end_column=65}\n5 {file_name_id=2 function_name_id=5 line=58 end_line=58 column=21 end_column=32}\n6 {file_name_id=3 function_name_id=6 line=7 end_line=7 column=12 end_column=17}\n7 {file_name_id=3 function_name_id=6 line=8 end_line=8 column=12 end_column=26}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=6}\n7 {file_location_id=7 parent_frame_id=6}\n\n\n%fused_computation (param_0.1: f32[10], param_1.4: f32[10]) -> f32[10] {\n  %param_1.4 = f32[10]{0} parameter(1)\n  %param_0.1 = f32[10]{0} parameter(0)\n  %mul.2 = f32[10]{0} multiply(%param_1.4, %param_0.1), metadata={op_name=\"jit(scalar_output_wrapper)/jvp()/mul\" stack_frame_id=6}\n  %cos.0 = f32[10]{0} cosine(%mul.2), metadata={op_name=\"jit(scalar_output_wrapper)/jvp()/cos\" stack_frame_id=7}\n  %neg.0 = f32[10]{0} negate(%param_1.4), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/neg\" stack_frame_id=7}\n  %sin.0 = f32[10]{0} sine(%mul.2), metadata={op_name=\"jit(scalar_output_wrapper)/jvp()/sin\" stack_frame_id=7}\n  %mul.1 = f32[10]{0} multiply(%neg.0, %sin.0), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/mul\" stack_frame_id=7}\n  %mul.0 = f32[10]{0} multiply(%mul.1, %param_0.1), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/mul\" stack_frame_id=6}\n  ROOT %add_any.0 = f32[10]{0} add(%cos.0, %mul.0), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/add_any\" stack_frame_id=6}\n}\n\nENTRY %main.1 (args_0_.1: f32[10], args_1_.1: f32[10]) -> f32[10] {\n  %args_0_.1 = f32[10]{0} parameter(0), metadata={op_name=\"args[0]\"}\n  %args_1_.1 = f32[10]{0} parameter(1), metadata={op_name=\"args[1]\"}\n  ROOT %multiply_add_fusion = f32[10]{0} fusion(%args_1_.1, %args_0_.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/add_any\" stack_frame_id=6}\n}\n\n"}
{"id": 34, "kernel_name": "reduce_jyfh", "python": "def reduce_jyfh(a):\n    \"\"\"Sum reduction over array.\"\"\"\n    return jnp.sum(a)", "type": "generate_reduction_kernel", "hlo": "// ===== FORWARD PASS =====\nmodule @jit_reduce_jyfh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<10xf32>, tensor<f32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n}\n\n\n// ===== BACKWARD PASS =====\nmodule @jit_scalar_output_wrapper attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main() -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    return %0 : tensor<10xf32>\n  }\n}\n", "optimized_hlo": "// ===== FORWARD PASS (OPTIMIZED) =====\nHloModule jit_reduce_jyfh, is_scheduled=true, entry_computation_layout={(f32[10]{0})->f32[]}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmpiff0rbvl.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"reduce_jyfh\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=41 end_line=41 column=14 end_column=47}\n5 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=11 end_column=21}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n\n\n%region_0.1 (reduce_sum.3: f32[], reduce_sum.4: f32[]) -> f32[] {\n  %reduce_sum.3 = f32[] parameter(0), metadata={op_name=\"reduce_sum\"}\n  %reduce_sum.4 = f32[] parameter(1), metadata={op_name=\"reduce_sum\"}\n  ROOT %reduce_sum.5 = f32[] add(%reduce_sum.3, %reduce_sum.4), metadata={op_name=\"jit(reduce_jyfh)/reduce_sum\" stack_frame_id=5}\n}\n\n%wrapped_reduce_computation (param_0: f32[10], param_1: f32[]) -> f32[] {\n  %param_0 = f32[10]{0} parameter(0)\n  %param_1 = f32[] parameter(1)\n  ROOT %reduce_sum.0 = f32[] reduce(%param_0, %param_1), dimensions={0}, to_apply=%region_0.1, metadata={op_name=\"jit(reduce_jyfh)/reduce_sum\" stack_frame_id=5}\n}\n\nENTRY %main.2 (a.1: f32[10]) -> f32[] {\n  %a.1 = f32[10]{0} parameter(0), metadata={op_name=\"a\"}\n  %constant.1 = f32[] constant(0)\n  ROOT %wrapped_reduce = f32[] fusion(%a.1, %constant.1), kind=kLoop, calls=%wrapped_reduce_computation, metadata={op_name=\"jit(reduce_jyfh)/reduce_sum\" stack_frame_id=5}\n}\n\n\n\n// ===== BACKWARD PASS (OPTIMIZED) =====\nHloModule jit_scalar_output_wrapper, is_scheduled=true, entry_computation_layout={()->f32[10]{0}}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmpiff0rbvl.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"extract_ir.<locals>.scalar_output_wrapper\"\n6 \"reduce_jyfh\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=66 end_line=66 column=27 end_column=65}\n5 {file_name_id=2 function_name_id=5 line=58 end_line=58 column=21 end_column=32}\n6 {file_name_id=3 function_name_id=6 line=7 end_line=7 column=11 end_column=21}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=6}\n\n\n%wrapped_broadcast_computation (param_0: f32[]) -> f32[10] {\n  %param_0 = f32[] parameter(0)\n  ROOT %broadcast.0 = f32[10]{0} broadcast(%param_0), dimensions={}\n}\n\nENTRY %main.1 () -> f32[10] {\n  %constant.1 = f32[] constant(1)\n  ROOT %wrapped_broadcast = f32[10]{0} fusion(%constant.1), kind=kLoop, calls=%wrapped_broadcast_computation\n}\n\n"}
{"id": 35, "kernel_name": "loop_uigv", "python": "def loop_uigv(a):\n    \"\"\"Reduction operation simulating loop accumulation.\"\"\"\n    # Accumulate n=5 times\n    result = a\n    for _ in range(5):\n        result = result + a\n    return result", "type": "generate_loop_kernel", "hlo": "// ===== FORWARD PASS =====\nmodule @jit_loop_uigv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg0 : tensor<10xf32>\n    %1 = stablehlo.add %0, %arg0 : tensor<10xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<10xf32>\n    %3 = stablehlo.add %2, %arg0 : tensor<10xf32>\n    %4 = stablehlo.add %3, %arg0 : tensor<10xf32>\n    return %4 : tensor<10xf32>\n  }\n}\n\n\n// ===== BACKWARD PASS =====\nmodule @jit_scalar_output_wrapper attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main() -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %1 = stablehlo.add %0, %0 : tensor<10xf32>\n    %2 = stablehlo.add %1, %0 : tensor<10xf32>\n    %3 = stablehlo.add %2, %0 : tensor<10xf32>\n    %4 = stablehlo.add %3, %0 : tensor<10xf32>\n    %5 = stablehlo.add %4, %0 : tensor<10xf32>\n    return %5 : tensor<10xf32>\n  }\n}\n", "optimized_hlo": "// ===== FORWARD PASS (OPTIMIZED) =====\nHloModule jit_loop_uigv, is_scheduled=true, entry_computation_layout={(f32[10]{0})->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmpetzbpda1.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"loop_uigv\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=41 end_line=41 column=14 end_column=47}\n5 {file_name_id=3 function_name_id=5 line=10 end_line=10 column=17 end_column=27}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[10]) -> f32[10] {\n  %param_0.1 = f32[10]{0} parameter(0)\n  %add.4 = f32[10]{0} add(%param_0.1, %param_0.1), metadata={op_name=\"jit(loop_uigv)/add\" stack_frame_id=5}\n  %add.3 = f32[10]{0} add(%add.4, %param_0.1), metadata={op_name=\"jit(loop_uigv)/add\" stack_frame_id=5}\n  %add.2 = f32[10]{0} add(%add.3, %param_0.1), metadata={op_name=\"jit(loop_uigv)/add\" stack_frame_id=5}\n  %add.1 = f32[10]{0} add(%add.2, %param_0.1), metadata={op_name=\"jit(loop_uigv)/add\" stack_frame_id=5}\n  ROOT %add.0 = f32[10]{0} add(%add.1, %param_0.1), metadata={op_name=\"jit(loop_uigv)/add\" stack_frame_id=5}\n}\n\nENTRY %main.1 (a.1: f32[10]) -> f32[10] {\n  %a.1 = f32[10]{0} parameter(0), metadata={op_name=\"a\"}\n  ROOT %add_add_fusion = f32[10]{0} fusion(%a.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(loop_uigv)/add\" stack_frame_id=5}\n}\n\n\n\n// ===== BACKWARD PASS (OPTIMIZED) =====\nHloModule jit_scalar_output_wrapper, is_scheduled=true, entry_computation_layout={()->f32[10]{0}}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmpetzbpda1.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"extract_ir.<locals>.scalar_output_wrapper\"\n6 \"loop_uigv\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=66 end_line=66 column=27 end_column=65}\n5 {file_name_id=2 function_name_id=5 line=58 end_line=58 column=21 end_column=32}\n6 {file_name_id=3 function_name_id=6 line=10 end_line=10 column=17 end_column=27}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=6}\n\n\n%wrapped_broadcast_computation (param_0: f32[]) -> f32[10] {\n  %param_0 = f32[] parameter(0)\n  ROOT %broadcast.0 = f32[10]{0} broadcast(%param_0), dimensions={}\n}\n\nENTRY %main.1 () -> f32[10] {\n  %constant.1 = f32[] constant(6)\n  ROOT %wrapped_broadcast = f32[10]{0} fusion(%constant.1), kind=kLoop, calls=%wrapped_broadcast_computation\n}\n\n"}
{"id": 36, "kernel_name": "branch_vcys", "python": "def branch_vcys(a):\n    \"\"\"Conditional operation using jnp.where.\"\"\"\n    return jnp.where(a > -0.49, a - 2.1, a + 2.7)", "type": "generate_branch_kernel", "hlo": "// ===== FORWARD PASS =====\nmodule @jit_branch_vcys attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-4.900000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<10xf32>, tensor<10xf32>) -> tensor<10xi1>\n    %cst_0 = stablehlo.constant dense<2.100000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<10xf32>\n    %cst_1 = stablehlo.constant dense<2.700000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %5 = stablehlo.add %arg0, %4 : tensor<10xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<10xi1>, tensor<10xf32>, tensor<10xf32>) -> tensor<10xf32>\n    return %6 : tensor<10xf32>\n  }\n  func.func private @_where(%arg0: tensor<10xi1>, %arg1: tensor<10xf32>, %arg2: tensor<10xf32>) -> tensor<10xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<10xi1>, tensor<10xf32>\n    return %0 : tensor<10xf32>\n  }\n}\n\n\n// ===== BACKWARD PASS =====\nmodule @jit_scalar_output_wrapper attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-4.900000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<10xf32>, tensor<10xf32>) -> tensor<10xi1>\n    %cst_0 = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %3:2 = call @_where(%1, %2) : (tensor<10xi1>, tensor<10xf32>) -> (tensor<10xf32>, tensor<10xf32>)\n    %4 = stablehlo.add %3#0, %3#1 : tensor<10xf32>\n    return %4 : tensor<10xf32>\n  }\n  func.func private @_where(%arg0: tensor<10xi1>, %arg1: tensor<10xf32>) -> (tensor<10xf32>, tensor<10xf32>) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %1 = stablehlo.select %arg0, %0, %arg1 : tensor<10xi1>, tensor<10xf32>\n    %2 = stablehlo.select %arg0, %arg1, %0 : tensor<10xi1>, tensor<10xf32>\n    return %2, %1 : tensor<10xf32>, tensor<10xf32>\n  }\n}\n", "optimized_hlo": "// ===== FORWARD PASS (OPTIMIZED) =====\nHloModule jit_branch_vcys, is_scheduled=true, entry_computation_layout={(f32[10]{0})->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmp53ibs6nj.py\"\n4 \"/tmp/tmphr9su7i4.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"branch_vcys\"\n6 \"nested_odsh\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=41 end_line=41 column=14 end_column=47}\n5 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=41 end_column=48}\n6 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=32 end_column=39}\n7 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=21 end_column=30}\n8 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=11 end_column=49}\n9 {file_name_id=4 function_name_id=6 line=8 end_line=8 column=21 end_column=58}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=5}\n8 {file_location_id=8 parent_frame_id=5}\n9 {file_location_id=9 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[10]) -> f32[10] {\n  %param_0.1 = f32[10]{0} parameter(0)\n  %constant.6 = f32[] constant(-0.49)\n  %broadcast.6 = f32[10]{0} broadcast(%constant.6), dimensions={}\n  %gt.0 = pred[10]{0} compare(%param_0.1, %broadcast.6), direction=GT, metadata={op_name=\"jit(branch_vcys)/gt\" stack_frame_id=7}\n  %constant.2 = f32[] constant(-2.1)\n  %broadcast.2 = f32[10]{0} broadcast(%constant.2), dimensions={}\n  %add.3 = f32[10]{0} add(%param_0.1, %broadcast.2), metadata={op_name=\"jit(branch_vcys)/sub\" stack_frame_id=6}\n  %constant.1 = f32[] constant(2.7)\n  %broadcast.1 = f32[10]{0} broadcast(%constant.1), dimensions={}\n  %add.2 = f32[10]{0} add(%param_0.1, %broadcast.1), metadata={op_name=\"jit(branch_vcys)/add\" stack_frame_id=5}\n  ROOT %select_n.2 = f32[10]{0} select(%gt.0, %add.3, %add.2), metadata={op_name=\"jit(branch_vcys)/jit(_where)/select_n\" stack_frame_id=9}\n}\n\nENTRY %main.2 (a.1: f32[10]) -> f32[10] {\n  %a.1 = f32[10]{0} parameter(0), metadata={op_name=\"a\"}\n  ROOT %add_select_fusion = f32[10]{0} fusion(%a.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(branch_vcys)/jit(_where)/select_n\" stack_frame_id=9}\n}\n\n\n\n// ===== BACKWARD PASS (OPTIMIZED) =====\nHloModule jit_scalar_output_wrapper, is_scheduled=true, entry_computation_layout={(f32[10]{0})->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmp53ibs6nj.py\"\n4 \"/tmp/tmphr9su7i4.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"extract_ir.<locals>.scalar_output_wrapper\"\n6 \"branch_vcys\"\n7 \"nested_odsh\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=66 end_line=66 column=27 end_column=65}\n5 {file_name_id=2 function_name_id=5 line=61 end_line=61 column=23 end_column=38}\n6 {file_name_id=2 function_name_id=5 line=58 end_line=58 column=21 end_column=32}\n7 {file_name_id=3 function_name_id=6 line=7 end_line=7 column=21 end_column=30}\n8 {file_name_id=3 function_name_id=6 line=7 end_line=7 column=11 end_column=49}\n9 {file_name_id=2 function_name_id=4 line=41 end_line=41 column=14 end_column=47}\n10 {file_name_id=4 function_name_id=7 line=8 end_line=8 column=21 end_column=58}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=7}\n8 {file_location_id=8 parent_frame_id=7}\n9 {file_location_id=9 parent_frame_id=4}\n10 {file_location_id=10 parent_frame_id=10}\n\n\n%fused_computation (param_0.2: f32[10]) -> f32[10] {\n  %param_0.2 = f32[10]{0} parameter(0)\n  %constant.6 = f32[] constant(-0.49)\n  %broadcast.6 = f32[10]{0} broadcast(%constant.6), dimensions={}\n  %gt.0 = pred[10]{0} compare(%param_0.2, %broadcast.6), direction=GT, metadata={op_name=\"jit(scalar_output_wrapper)/jvp()/gt\" stack_frame_id=7}\n  %constant.2 = f32[] constant(1)\n  %broadcast.2 = f32[10]{0} broadcast(%constant.2), dimensions={}\n  %constant.1 = f32[] constant(0), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp(jit(_where)))\"}\n  %broadcast.1 = f32[10]{0} broadcast(%constant.1), dimensions={}, metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp(jit(_where)))\"}\n  %select_n.5 = f32[10]{0} select(%gt.0, %broadcast.2, %broadcast.1), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp(jit(_where)))/select_n\" stack_frame_id=10}\n  %select_n.4 = f32[10]{0} select(%gt.0, %broadcast.1, %broadcast.2), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp(jit(_where)))/select_n\" stack_frame_id=10}\n  ROOT %add_any.0 = f32[10]{0} add(%select_n.5, %select_n.4), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/add_any\" stack_frame_id=8}\n}\n\nENTRY %main.2 (args_0_.1: f32[10]) -> f32[10] {\n  %args_0_.1 = f32[10]{0} parameter(0), metadata={op_name=\"args[0]\"}\n  ROOT %select_add_fusion = f32[10]{0} fusion(%args_0_.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/add_any\" stack_frame_id=8}\n}\n\n"}
{"id": 37, "kernel_name": "unary_djeg", "python": "def unary_djeg(a):\n    \"\"\"Unary operation on array.\"\"\"\n    return jnp.cos(a)", "type": "generate_unary_kernel", "hlo": "// ===== FORWARD PASS =====\nmodule @jit_unary_djeg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.cosine %arg0 : tensor<10xf32>\n    return %0 : tensor<10xf32>\n  }\n}\n\n\n// ===== BACKWARD PASS =====\nmodule @jit_scalar_output_wrapper attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<10xf32>\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %2 = stablehlo.negate %1 : tensor<10xf32>\n    %3 = stablehlo.multiply %2, %0 : tensor<10xf32>\n    return %3 : tensor<10xf32>\n  }\n}\n", "optimized_hlo": "// ===== FORWARD PASS (OPTIMIZED) =====\nHloModule jit_unary_djeg, is_scheduled=true, entry_computation_layout={(f32[10]{0})->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmpvg1uu94k.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"unary_djeg\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=41 end_line=41 column=14 end_column=47}\n5 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=11 end_column=21}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n\n\n%wrapped_cosine_computation (param_0: f32[10]) -> f32[10] {\n  %param_0 = f32[10]{0} parameter(0)\n  ROOT %cos.0 = f32[10]{0} cosine(%param_0), metadata={op_name=\"jit(unary_djeg)/cos\" stack_frame_id=5}\n}\n\nENTRY %main.1 (a.1: f32[10]) -> f32[10] {\n  %a.1 = f32[10]{0} parameter(0), metadata={op_name=\"a\"}\n  ROOT %wrapped_cosine = f32[10]{0} fusion(%a.1), kind=kLoop, calls=%wrapped_cosine_computation, metadata={op_name=\"jit(unary_djeg)/cos\" stack_frame_id=5}\n}\n\n\n\n// ===== BACKWARD PASS (OPTIMIZED) =====\nHloModule jit_scalar_output_wrapper, is_scheduled=true, entry_computation_layout={(f32[10]{0})->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmpvg1uu94k.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"extract_ir.<locals>.scalar_output_wrapper\"\n6 \"unary_djeg\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=66 end_line=66 column=27 end_column=65}\n5 {file_name_id=2 function_name_id=5 line=58 end_line=58 column=21 end_column=32}\n6 {file_name_id=3 function_name_id=6 line=7 end_line=7 column=11 end_column=21}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=6}\n\n\n%fused_computation (param_0.1: f32[10]) -> f32[10] {\n  %param_0.1 = f32[10]{0} parameter(0)\n  %sin.0 = f32[10]{0} sine(%param_0.1), metadata={op_name=\"jit(scalar_output_wrapper)/jvp()/sin\" stack_frame_id=6}\n  %constant.0 = f32[] constant(-1)\n  %broadcast.0 = f32[10]{0} broadcast(%constant.0), dimensions={}\n  ROOT %neg.0 = f32[10]{0} multiply(%sin.0, %broadcast.0), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/mul;jit(scalar_output_wrapper)/transpose(jvp())/neg\"}\n}\n\nENTRY %main.1 (args_0_.1: f32[10]) -> f32[10] {\n  %args_0_.1 = f32[10]{0} parameter(0), metadata={op_name=\"args[0]\"}\n  ROOT %broadcast_multiply_fusion = f32[10]{0} fusion(%args_0_.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/mul;jit(scalar_output_wrapper)/transpose(jvp())/neg\"}\n}\n\n"}
{"id": 38, "kernel_name": "loop_hosy", "python": "def loop_hosy(a):\n    \"\"\"Reduction operation simulating loop accumulation.\"\"\"\n    # Accumulate n=5 times\n    result = a\n    for _ in range(5):\n        result = result - a\n    return result", "type": "generate_loop_kernel", "hlo": "// ===== FORWARD PASS =====\nmodule @jit_loop_hosy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg0 : tensor<10xf32>\n    %1 = stablehlo.subtract %0, %arg0 : tensor<10xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<10xf32>\n    %3 = stablehlo.subtract %2, %arg0 : tensor<10xf32>\n    %4 = stablehlo.subtract %3, %arg0 : tensor<10xf32>\n    return %4 : tensor<10xf32>\n  }\n}\n\n\n// ===== BACKWARD PASS =====\nmodule @jit_scalar_output_wrapper attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main() -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %1 = stablehlo.negate %0 : tensor<10xf32>\n    %2 = stablehlo.negate %0 : tensor<10xf32>\n    %3 = stablehlo.add %1, %2 : tensor<10xf32>\n    %4 = stablehlo.negate %0 : tensor<10xf32>\n    %5 = stablehlo.add %3, %4 : tensor<10xf32>\n    %6 = stablehlo.negate %0 : tensor<10xf32>\n    %7 = stablehlo.add %5, %6 : tensor<10xf32>\n    %8 = stablehlo.negate %0 : tensor<10xf32>\n    %9 = stablehlo.add %7, %0 : tensor<10xf32>\n    %10 = stablehlo.add %9, %8 : tensor<10xf32>\n    return %10 : tensor<10xf32>\n  }\n}\n", "optimized_hlo": "// ===== FORWARD PASS (OPTIMIZED) =====\nHloModule jit_loop_hosy, is_scheduled=true, entry_computation_layout={(f32[10]{0})->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmpkjpw7rjk.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"loop_hosy\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=41 end_line=41 column=14 end_column=47}\n5 {file_name_id=3 function_name_id=5 line=10 end_line=10 column=17 end_column=27}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[10]) -> f32[10] {\n  %param_0.1 = f32[10]{0} parameter(0)\n  %sub.4 = f32[10]{0} subtract(%param_0.1, %param_0.1), metadata={op_name=\"jit(loop_hosy)/sub\" stack_frame_id=5}\n  %sub.3 = f32[10]{0} subtract(%sub.4, %param_0.1), metadata={op_name=\"jit(loop_hosy)/sub\" stack_frame_id=5}\n  %sub.2 = f32[10]{0} subtract(%sub.3, %param_0.1), metadata={op_name=\"jit(loop_hosy)/sub\" stack_frame_id=5}\n  %sub.1 = f32[10]{0} subtract(%sub.2, %param_0.1), metadata={op_name=\"jit(loop_hosy)/sub\" stack_frame_id=5}\n  ROOT %sub.0 = f32[10]{0} subtract(%sub.1, %param_0.1), metadata={op_name=\"jit(loop_hosy)/sub\" stack_frame_id=5}\n}\n\nENTRY %main.1 (a.1: f32[10]) -> f32[10] {\n  %a.1 = f32[10]{0} parameter(0), metadata={op_name=\"a\"}\n  ROOT %subtract_subtract_fusion = f32[10]{0} fusion(%a.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(loop_hosy)/sub\" stack_frame_id=5}\n}\n\n\n\n// ===== BACKWARD PASS (OPTIMIZED) =====\nHloModule jit_scalar_output_wrapper, is_scheduled=true, entry_computation_layout={()->f32[10]{0}}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmpkjpw7rjk.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"extract_ir.<locals>.scalar_output_wrapper\"\n6 \"loop_hosy\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=66 end_line=66 column=27 end_column=65}\n5 {file_name_id=2 function_name_id=5 line=58 end_line=58 column=21 end_column=32}\n6 {file_name_id=3 function_name_id=6 line=10 end_line=10 column=17 end_column=27}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=6}\n\n\n%wrapped_broadcast_computation (param_0: f32[]) -> f32[10] {\n  %param_0 = f32[] parameter(0)\n  ROOT %broadcast.0 = f32[10]{0} broadcast(%param_0), dimensions={}\n}\n\nENTRY %main.1 () -> f32[10] {\n  %constant.1 = f32[] constant(-4)\n  ROOT %wrapped_broadcast = f32[10]{0} fusion(%constant.1), kind=kLoop, calls=%wrapped_broadcast_computation\n}\n\n"}
{"id": 39, "kernel_name": "nested_nloj", "python": "def nested_nloj(a):\n    \"\"\"Nested conditional using jnp.where.\"\"\"\n    return jnp.where(a > 0.33, \n                     jnp.where(a > 0.66, a * 3.0, a * 2.0),\n                     a * 0.5)", "type": "generate_nested_branch_kernel", "hlo": "// ===== FORWARD PASS =====\nmodule @jit_nested_nloj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<3.300000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<10xf32>, tensor<10xf32>) -> tensor<10xi1>\n    %cst_0 = stablehlo.constant dense<6.600000e-01> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %3 = stablehlo.compare  GT, %arg0, %2,  FLOAT : (tensor<10xf32>, tensor<10xf32>) -> tensor<10xi1>\n    %cst_1 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<10xf32>\n    %cst_2 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %6 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %7 = stablehlo.multiply %arg0, %6 : tensor<10xf32>\n    %8 = call @_where(%3, %5, %7) : (tensor<10xi1>, tensor<10xf32>, tensor<10xf32>) -> tensor<10xf32>\n    %cst_3 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %10 = stablehlo.multiply %arg0, %9 : tensor<10xf32>\n    %11 = call @_where(%1, %8, %10) : (tensor<10xi1>, tensor<10xf32>, tensor<10xf32>) -> tensor<10xf32>\n    return %11 : tensor<10xf32>\n  }\n  func.func private @_where(%arg0: tensor<10xi1>, %arg1: tensor<10xf32>, %arg2: tensor<10xf32>) -> tensor<10xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<10xi1>, tensor<10xf32>\n    return %0 : tensor<10xf32>\n  }\n}\n\n\n// ===== BACKWARD PASS =====\nmodule @jit_scalar_output_wrapper attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<3.300000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<10xf32>, tensor<10xf32>) -> tensor<10xi1>\n    %cst_0 = stablehlo.constant dense<6.600000e-01> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %3 = stablehlo.compare  GT, %arg0, %2,  FLOAT : (tensor<10xf32>, tensor<10xf32>) -> tensor<10xi1>\n    %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %5:2 = call @_where(%1, %4) : (tensor<10xi1>, tensor<10xf32>) -> (tensor<10xf32>, tensor<10xf32>)\n    %cst_2 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %6 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %7 = stablehlo.multiply %5#1, %6 : tensor<10xf32>\n    %8:2 = call @_where(%3, %5#0) : (tensor<10xi1>, tensor<10xf32>) -> (tensor<10xf32>, tensor<10xf32>)\n    %cst_3 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_3, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %10 = stablehlo.multiply %8#1, %9 : tensor<10xf32>\n    %11 = stablehlo.add %7, %10 : tensor<10xf32>\n    %cst_4 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %12 = stablehlo.broadcast_in_dim %cst_4, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %13 = stablehlo.multiply %8#0, %12 : tensor<10xf32>\n    %14 = stablehlo.add %11, %13 : tensor<10xf32>\n    return %14 : tensor<10xf32>\n  }\n  func.func private @_where(%arg0: tensor<10xi1>, %arg1: tensor<10xf32>) -> (tensor<10xf32>, tensor<10xf32>) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %1 = stablehlo.select %arg0, %0, %arg1 : tensor<10xi1>, tensor<10xf32>\n    %2 = stablehlo.select %arg0, %arg1, %0 : tensor<10xi1>, tensor<10xf32>\n    return %2, %1 : tensor<10xf32>, tensor<10xf32>\n  }\n}\n", "optimized_hlo": "// ===== FORWARD PASS (OPTIMIZED) =====\nHloModule jit_nested_nloj, is_scheduled=true, entry_computation_layout={(f32[10]{0})->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmplx1ssnrr.py\"\n4 \"/tmp/tmphr9su7i4.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"nested_nloj\"\n6 \"nested_odsh\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=41 end_line=41 column=14 end_column=47}\n5 {file_name_id=3 function_name_id=5 line=9 end_line=9 column=21 end_column=28}\n6 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=50 end_column=57}\n7 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=41 end_column=48}\n8 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=31 end_column=39}\n9 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=21 end_column=29}\n10 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=21 end_column=58}\n11 {file_name_id=4 function_name_id=6 line=8 end_line=8 column=21 end_column=58}\n12 {file_name_id=3 function_name_id=5 line=7 end_line=9 column=11 end_column=29}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=5}\n8 {file_location_id=8 parent_frame_id=5}\n9 {file_location_id=9 parent_frame_id=5}\n10 {file_location_id=10 parent_frame_id=5}\n11 {file_location_id=11 parent_frame_id=5}\n12 {file_location_id=12 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[10]) -> f32[10] {\n  %param_0.1 = f32[10]{0} parameter(0)\n  %constant.4 = f32[] constant(0.33)\n  %broadcast.4 = f32[10]{0} broadcast(%constant.4), dimensions={}\n  %gt.1 = pred[10]{0} compare(%param_0.1, %broadcast.4), direction=GT, metadata={op_name=\"jit(nested_nloj)/gt\" stack_frame_id=9}\n  %constant.3 = f32[] constant(0.66)\n  %broadcast.3 = f32[10]{0} broadcast(%constant.3), dimensions={}\n  %gt.0 = pred[10]{0} compare(%param_0.1, %broadcast.3), direction=GT, metadata={op_name=\"jit(nested_nloj)/gt\" stack_frame_id=8}\n  %constant.2 = f32[] constant(3)\n  %broadcast.2 = f32[10]{0} broadcast(%constant.2), dimensions={}\n  %mul.2 = f32[10]{0} multiply(%param_0.1, %broadcast.2), metadata={op_name=\"jit(nested_nloj)/mul\" stack_frame_id=7}\n  %constant.1 = f32[] constant(2)\n  %broadcast.1 = f32[10]{0} broadcast(%constant.1), dimensions={}\n  %mul.1 = f32[10]{0} multiply(%param_0.1, %broadcast.1), metadata={op_name=\"jit(nested_nloj)/mul\" stack_frame_id=6}\n  %select_n.5 = f32[10]{0} select(%gt.0, %mul.2, %mul.1), metadata={op_name=\"jit(nested_nloj)/jit(_where)/select_n\" stack_frame_id=11}\n  %constant.0 = f32[] constant(0.5)\n  %broadcast.0 = f32[10]{0} broadcast(%constant.0), dimensions={}\n  %mul.0 = f32[10]{0} multiply(%param_0.1, %broadcast.0), metadata={op_name=\"jit(nested_nloj)/mul\" stack_frame_id=5}\n  ROOT %select_n.4 = f32[10]{0} select(%gt.1, %select_n.5, %mul.0), metadata={op_name=\"jit(nested_nloj)/jit(_where)/select_n\" stack_frame_id=11}\n}\n\nENTRY %main.2 (a.1: f32[10]) -> f32[10] {\n  %a.1 = f32[10]{0} parameter(0), metadata={op_name=\"a\"}\n  ROOT %multiply_select_fusion = f32[10]{0} fusion(%a.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(nested_nloj)/jit(_where)/select_n\" stack_frame_id=11}\n}\n\n\n\n// ===== BACKWARD PASS (OPTIMIZED) =====\nHloModule jit_scalar_output_wrapper, is_scheduled=true, entry_computation_layout={(f32[10]{0})->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmplx1ssnrr.py\"\n4 \"/tmp/tmphr9su7i4.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"extract_ir.<locals>.scalar_output_wrapper\"\n6 \"nested_nloj\"\n7 \"nested_odsh\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=66 end_line=66 column=27 end_column=65}\n5 {file_name_id=2 function_name_id=5 line=58 end_line=58 column=21 end_column=32}\n6 {file_name_id=3 function_name_id=6 line=8 end_line=8 column=41 end_column=48}\n7 {file_name_id=3 function_name_id=6 line=8 end_line=8 column=50 end_column=57}\n8 {file_name_id=3 function_name_id=6 line=9 end_line=9 column=21 end_column=28}\n9 {file_name_id=2 function_name_id=5 line=61 end_line=61 column=23 end_column=38}\n10 {file_name_id=3 function_name_id=6 line=8 end_line=8 column=31 end_column=39}\n11 {file_name_id=3 function_name_id=6 line=7 end_line=7 column=21 end_column=29}\n12 {file_name_id=3 function_name_id=6 line=7 end_line=9 column=11 end_column=29}\n13 {file_name_id=2 function_name_id=4 line=41 end_line=41 column=14 end_column=47}\n14 {file_name_id=4 function_name_id=7 line=8 end_line=8 column=21 end_column=58}\n15 {file_name_id=3 function_name_id=6 line=8 end_line=8 column=21 end_column=58}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=6}\n7 {file_location_id=7 parent_frame_id=6}\n8 {file_location_id=8 parent_frame_id=6}\n9 {file_location_id=9 parent_frame_id=5}\n10 {file_location_id=10 parent_frame_id=6}\n11 {file_location_id=11 parent_frame_id=6}\n12 {file_location_id=12 parent_frame_id=6}\n13 {file_location_id=13 parent_frame_id=4}\n14 {file_location_id=14 parent_frame_id=14}\n15 {file_location_id=15 parent_frame_id=6}\n\n\n%fused_computation (param_0.4: f32[10]) -> f32[10] {\n  %param_0.4 = f32[10]{0} parameter(0)\n  %constant.16 = f32[] constant(0.33)\n  %broadcast.16 = f32[10]{0} broadcast(%constant.16), dimensions={}\n  %gt.1 = pred[10]{0} compare(%param_0.4, %broadcast.16), direction=GT, metadata={op_name=\"jit(scalar_output_wrapper)/jvp()/gt\" stack_frame_id=11}\n  %constant.15 = f32[] constant(0), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp(jit(_where)))\"}\n  %broadcast.15 = f32[10]{0} broadcast(%constant.15), dimensions={}, metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp(jit(_where)))\"}\n  %constant.14 = f32[] constant(1)\n  %broadcast.14 = f32[10]{0} broadcast(%constant.14), dimensions={}\n  %select_n.11 = f32[10]{0} select(%gt.1, %broadcast.15, %broadcast.14), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp(jit(_where)))/select_n\" stack_frame_id=14}\n  %constant.6 = f32[] constant(0.5)\n  %broadcast.6 = f32[10]{0} broadcast(%constant.6), dimensions={}\n  %mul.2 = f32[10]{0} multiply(%select_n.11, %broadcast.6), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/mul\" stack_frame_id=8}\n  %constant.5 = f32[] constant(0.66)\n  %broadcast.5 = f32[10]{0} broadcast(%constant.5), dimensions={}\n  %gt.0 = pred[10]{0} compare(%param_0.4, %broadcast.5), direction=GT, metadata={op_name=\"jit(scalar_output_wrapper)/jvp()/gt\" stack_frame_id=10}\n  %select_n.10 = f32[10]{0} select(%gt.1, %broadcast.14, %broadcast.15), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp(jit(_where)))/select_n\" stack_frame_id=14}\n  %select_n.9 = f32[10]{0} select(%gt.0, %broadcast.15, %select_n.10), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp(jit(_where)))/select_n\" stack_frame_id=14}\n  %constant.4 = f32[] constant(2)\n  %broadcast.4 = f32[10]{0} broadcast(%constant.4), dimensions={}\n  %mul.1 = f32[10]{0} multiply(%select_n.9, %broadcast.4), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/mul\" stack_frame_id=7}\n  %add_any.1 = f32[10]{0} add(%mul.2, %mul.1), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/add_any\" stack_frame_id=7}\n  %select_n.8 = f32[10]{0} select(%gt.0, %select_n.10, %broadcast.15), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp(jit(_where)))/select_n\" stack_frame_id=14}\n  %constant.3 = f32[] constant(3)\n  %broadcast.3 = f32[10]{0} broadcast(%constant.3), dimensions={}\n  %mul.0 = f32[10]{0} multiply(%select_n.8, %broadcast.3), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/mul\" stack_frame_id=6}\n  ROOT %add_any.0 = f32[10]{0} add(%add_any.1, %mul.0), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/add_any\" stack_frame_id=6}\n}\n\nENTRY %main.2 (args_0_.1: f32[10]) -> f32[10] {\n  %args_0_.1 = f32[10]{0} parameter(0), metadata={op_name=\"args[0]\"}\n  ROOT %multiply_add_fusion = f32[10]{0} fusion(%args_0_.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/add_any\" stack_frame_id=6}\n}\n\n"}
{"id": 40, "kernel_name": "unary_dmnz", "python": "def unary_dmnz(a):\n    \"\"\"Unary operation on array.\"\"\"\n    return jnp.abs(a)", "type": "generate_unary_kernel", "hlo": "// ===== FORWARD PASS =====\nmodule @jit_unary_dmnz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.abs %arg0 : tensor<10xf32>\n    return %0 : tensor<10xf32>\n  }\n}\n\n\n// ===== BACKWARD PASS =====\nmodule @jit_scalar_output_wrapper attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %1 = stablehlo.compare  GE, %arg0, %0,  FLOAT : (tensor<10xf32>, tensor<10xf32>) -> tensor<10xi1>\n    %cst_0 = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %3 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %4 = stablehlo.select %1, %3, %2 : tensor<10xi1>, tensor<10xf32>\n    %5 = stablehlo.select %1, %2, %3 : tensor<10xi1>, tensor<10xf32>\n    %6 = stablehlo.negate %4 : tensor<10xf32>\n    %7 = stablehlo.add %5, %6 : tensor<10xf32>\n    return %7 : tensor<10xf32>\n  }\n}\n", "optimized_hlo": "// ===== FORWARD PASS (OPTIMIZED) =====\nHloModule jit_unary_dmnz, is_scheduled=true, entry_computation_layout={(f32[10]{0})->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmpo1hs2hi0.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"unary_dmnz\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=41 end_line=41 column=14 end_column=47}\n5 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=11 end_column=21}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n\n\n%wrapped_abs_computation (param_0: f32[10]) -> f32[10] {\n  %param_0 = f32[10]{0} parameter(0)\n  ROOT %abs.0 = f32[10]{0} abs(%param_0), metadata={op_name=\"jit(unary_dmnz)/abs\" stack_frame_id=5}\n}\n\nENTRY %main.1 (a.1: f32[10]) -> f32[10] {\n  %a.1 = f32[10]{0} parameter(0), metadata={op_name=\"a\"}\n  ROOT %wrapped_abs = f32[10]{0} fusion(%a.1), kind=kLoop, calls=%wrapped_abs_computation, metadata={op_name=\"jit(unary_dmnz)/abs\" stack_frame_id=5}\n}\n\n\n\n// ===== BACKWARD PASS (OPTIMIZED) =====\nHloModule jit_scalar_output_wrapper, is_scheduled=true, entry_computation_layout={(f32[10]{0})->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmpo1hs2hi0.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"extract_ir.<locals>.scalar_output_wrapper\"\n6 \"unary_dmnz\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=66 end_line=66 column=27 end_column=65}\n5 {file_name_id=2 function_name_id=5 line=61 end_line=61 column=23 end_column=38}\n6 {file_name_id=2 function_name_id=5 line=58 end_line=58 column=21 end_column=32}\n7 {file_name_id=3 function_name_id=6 line=7 end_line=7 column=11 end_column=21}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=7}\n\n\n%fused_computation (param_0.3: f32[10]) -> f32[10] {\n  %param_0.3 = f32[10]{0} parameter(0)\n  %constant.1 = f32[] constant(0)\n  %broadcast.1 = f32[10]{0} broadcast(%constant.1), dimensions={}\n  %ge.0 = pred[10]{0} compare(%param_0.3, %broadcast.1), direction=GE, metadata={op_name=\"jit(scalar_output_wrapper)/jvp()/ge\" stack_frame_id=7}\n  %constant.0 = f32[] constant(1)\n  %broadcast.0 = f32[10]{0} broadcast(%constant.0), dimensions={}\n  %select_n.1 = f32[10]{0} select(%ge.0, %broadcast.0, %broadcast.1), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/select_n\" stack_frame_id=7}\n  %select_n.0 = f32[10]{0} select(%ge.0, %broadcast.1, %broadcast.0), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/select_n\" stack_frame_id=7}\n  %neg.0 = f32[10]{0} negate(%select_n.0), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/neg\" stack_frame_id=7}\n  ROOT %add_any.0 = f32[10]{0} add(%select_n.1, %neg.0), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/add_any\" stack_frame_id=7}\n}\n\nENTRY %main.1 (args_0_.1: f32[10]) -> f32[10] {\n  %args_0_.1 = f32[10]{0} parameter(0), metadata={op_name=\"args[0]\"}\n  ROOT %negate_add_fusion = f32[10]{0} fusion(%args_0_.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/add_any\" stack_frame_id=7}\n}\n\n"}
{"id": 41, "kernel_name": "multi_mwcj", "python": "def multi_mwcj(a, b):\n    \"\"\"Multi-statement computation.\"\"\"\n    temp1 = a + b\n    temp2 = jnp.abs(temp1)\n    result = temp2 * a\n    return result", "type": "generate_multi_statement_kernel", "hlo": "// ===== FORWARD PASS =====\nmodule @jit_multi_mwcj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>, %arg1: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<10xf32>\n    %1 = stablehlo.abs %0 : tensor<10xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<10xf32>\n    return %2 : tensor<10xf32>\n  }\n}\n\n\n// ===== BACKWARD PASS =====\nmodule @jit_scalar_output_wrapper attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>, %arg1: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<10xf32>\n    %1 = stablehlo.abs %0 : tensor<10xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %3 = stablehlo.compare  GE, %0, %2,  FLOAT : (tensor<10xf32>, tensor<10xf32>) -> tensor<10xi1>\n    %cst_0 = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %5 = stablehlo.multiply %1, %4 : tensor<10xf32>\n    %6 = stablehlo.multiply %4, %arg0 : tensor<10xf32>\n    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %8 = stablehlo.select %3, %7, %6 : tensor<10xi1>, tensor<10xf32>\n    %9 = stablehlo.select %3, %6, %7 : tensor<10xi1>, tensor<10xf32>\n    %10 = stablehlo.add %5, %9 : tensor<10xf32>\n    %11 = stablehlo.negate %8 : tensor<10xf32>\n    %12 = stablehlo.add %10, %11 : tensor<10xf32>\n    return %12 : tensor<10xf32>\n  }\n}\n", "optimized_hlo": "// ===== FORWARD PASS (OPTIMIZED) =====\nHloModule jit_multi_mwcj, is_scheduled=true, entry_computation_layout={(f32[10]{0}, f32[10]{0})->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmp22olxwbu.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"multi_mwcj\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=41 end_line=41 column=14 end_column=47}\n5 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=12 end_column=17}\n6 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=12 end_column=26}\n7 {file_name_id=3 function_name_id=5 line=9 end_line=9 column=13 end_column=22}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[10], param_1.2: f32[10]) -> f32[10] {\n  %param_0.1 = f32[10]{0} parameter(0)\n  %param_1.2 = f32[10]{0} parameter(1)\n  %add.0 = f32[10]{0} add(%param_0.1, %param_1.2), metadata={op_name=\"jit(multi_mwcj)/add\" stack_frame_id=5}\n  %abs.0 = f32[10]{0} abs(%add.0), metadata={op_name=\"jit(multi_mwcj)/abs\" stack_frame_id=6}\n  ROOT %mul.0 = f32[10]{0} multiply(%abs.0, %param_0.1), metadata={op_name=\"jit(multi_mwcj)/mul\" stack_frame_id=7}\n}\n\nENTRY %main.1 (a.1: f32[10], b.1: f32[10]) -> f32[10] {\n  %a.1 = f32[10]{0} parameter(0), metadata={op_name=\"a\"}\n  %b.1 = f32[10]{0} parameter(1), metadata={op_name=\"b\"}\n  ROOT %abs_multiply_fusion = f32[10]{0} fusion(%a.1, %b.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(multi_mwcj)/mul\" stack_frame_id=7}\n}\n\n\n\n// ===== BACKWARD PASS (OPTIMIZED) =====\nHloModule jit_scalar_output_wrapper, is_scheduled=true, entry_computation_layout={(f32[10]{0}, f32[10]{0})->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmp22olxwbu.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"extract_ir.<locals>.scalar_output_wrapper\"\n6 \"multi_mwcj\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=66 end_line=66 column=27 end_column=65}\n5 {file_name_id=2 function_name_id=5 line=58 end_line=58 column=21 end_column=32}\n6 {file_name_id=3 function_name_id=6 line=7 end_line=7 column=12 end_column=17}\n7 {file_name_id=3 function_name_id=6 line=8 end_line=8 column=12 end_column=26}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=6}\n7 {file_location_id=7 parent_frame_id=6}\n\n\n%fused_computation (param_0.3: f32[10], param_1.7: f32[10]) -> f32[10] {\n  %param_0.3 = f32[10]{0} parameter(0)\n  %param_1.7 = f32[10]{0} parameter(1)\n  %add.0 = f32[10]{0} add(%param_0.3, %param_1.7), metadata={op_name=\"jit(scalar_output_wrapper)/jvp()/add\" stack_frame_id=6}\n  %abs.0 = f32[10]{0} abs(%add.0), metadata={op_name=\"jit(scalar_output_wrapper)/jvp()/abs\" stack_frame_id=7}\n  %constant.0 = f32[] constant(0)\n  %broadcast.0 = f32[10]{0} broadcast(%constant.0), dimensions={}\n  %ge.0 = pred[10]{0} compare(%add.0, %broadcast.0), direction=GE, metadata={op_name=\"jit(scalar_output_wrapper)/jvp()/ge\" stack_frame_id=7}\n  %select_n.1 = f32[10]{0} select(%ge.0, %param_0.3, %broadcast.0), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/select_n\" stack_frame_id=7}\n  %add_any.1 = f32[10]{0} add(%abs.0, %select_n.1), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/add_any\" stack_frame_id=7}\n  %select_n.0 = f32[10]{0} select(%ge.0, %broadcast.0, %param_0.3), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/select_n\" stack_frame_id=7}\n  %neg.0 = f32[10]{0} negate(%select_n.0), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/neg\" stack_frame_id=7}\n  ROOT %add_any.0 = f32[10]{0} add(%add_any.1, %neg.0), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/add_any\" stack_frame_id=7}\n}\n\nENTRY %main.1 (args_0_.1: f32[10], args_1_.1: f32[10]) -> f32[10] {\n  %args_0_.1 = f32[10]{0} parameter(0), metadata={op_name=\"args[0]\"}\n  %args_1_.1 = f32[10]{0} parameter(1), metadata={op_name=\"args[1]\"}\n  ROOT %negate_add_fusion = f32[10]{0} fusion(%args_0_.1, %args_1_.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/add_any\" stack_frame_id=7}\n}\n\n"}
{"id": 42, "kernel_name": "loop_txaa", "python": "def loop_txaa(a):\n    \"\"\"Reduction operation simulating loop accumulation.\"\"\"\n    # Accumulate n=5 times\n    result = a\n    for _ in range(5):\n        result = result * a\n    return result", "type": "generate_loop_kernel", "hlo": "// ===== FORWARD PASS =====\nmodule @jit_loop_txaa attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<10xf32>\n    %1 = stablehlo.multiply %0, %arg0 : tensor<10xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<10xf32>\n    %3 = stablehlo.multiply %2, %arg0 : tensor<10xf32>\n    %4 = stablehlo.multiply %3, %arg0 : tensor<10xf32>\n    return %4 : tensor<10xf32>\n  }\n}\n\n\n// ===== BACKWARD PASS =====\nmodule @jit_scalar_output_wrapper attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<10xf32>\n    %1 = stablehlo.multiply %0, %arg0 : tensor<10xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<10xf32>\n    %3 = stablehlo.multiply %2, %arg0 : tensor<10xf32>\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %5 = stablehlo.multiply %3, %4 : tensor<10xf32>\n    %6 = stablehlo.multiply %4, %arg0 : tensor<10xf32>\n    %7 = stablehlo.multiply %2, %6 : tensor<10xf32>\n    %8 = stablehlo.add %5, %7 : tensor<10xf32>\n    %9 = stablehlo.multiply %6, %arg0 : tensor<10xf32>\n    %10 = stablehlo.multiply %1, %9 : tensor<10xf32>\n    %11 = stablehlo.add %8, %10 : tensor<10xf32>\n    %12 = stablehlo.multiply %9, %arg0 : tensor<10xf32>\n    %13 = stablehlo.multiply %0, %12 : tensor<10xf32>\n    %14 = stablehlo.add %11, %13 : tensor<10xf32>\n    %15 = stablehlo.multiply %12, %arg0 : tensor<10xf32>\n    %16 = stablehlo.multiply %arg0, %15 : tensor<10xf32>\n    %17 = stablehlo.add %14, %16 : tensor<10xf32>\n    %18 = stablehlo.multiply %15, %arg0 : tensor<10xf32>\n    %19 = stablehlo.add %17, %18 : tensor<10xf32>\n    return %19 : tensor<10xf32>\n  }\n}\n", "optimized_hlo": "// ===== FORWARD PASS (OPTIMIZED) =====\nHloModule jit_loop_txaa, is_scheduled=true, entry_computation_layout={(f32[10]{0})->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmp9oa4a4ow.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"loop_txaa\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=41 end_line=41 column=14 end_column=47}\n5 {file_name_id=3 function_name_id=5 line=10 end_line=10 column=17 end_column=27}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[10]) -> f32[10] {\n  %param_0.1 = f32[10]{0} parameter(0)\n  %mul.4 = f32[10]{0} multiply(%param_0.1, %param_0.1), metadata={op_name=\"jit(loop_txaa)/mul\" stack_frame_id=5}\n  %mul.3 = f32[10]{0} multiply(%mul.4, %param_0.1), metadata={op_name=\"jit(loop_txaa)/mul\" stack_frame_id=5}\n  %mul.2 = f32[10]{0} multiply(%mul.3, %param_0.1), metadata={op_name=\"jit(loop_txaa)/mul\" stack_frame_id=5}\n  %mul.1 = f32[10]{0} multiply(%mul.2, %param_0.1), metadata={op_name=\"jit(loop_txaa)/mul\" stack_frame_id=5}\n  ROOT %mul.0 = f32[10]{0} multiply(%mul.1, %param_0.1), metadata={op_name=\"jit(loop_txaa)/mul\" stack_frame_id=5}\n}\n\nENTRY %main.1 (a.1: f32[10]) -> f32[10] {\n  %a.1 = f32[10]{0} parameter(0), metadata={op_name=\"a\"}\n  ROOT %multiply_multiply_fusion = f32[10]{0} fusion(%a.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(loop_txaa)/mul\" stack_frame_id=5}\n}\n\n\n\n// ===== BACKWARD PASS (OPTIMIZED) =====\nHloModule jit_scalar_output_wrapper, is_scheduled=true, entry_computation_layout={(f32[10]{0})->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmp9oa4a4ow.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"extract_ir.<locals>.scalar_output_wrapper\"\n6 \"loop_txaa\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=66 end_line=66 column=27 end_column=65}\n5 {file_name_id=2 function_name_id=5 line=58 end_line=58 column=21 end_column=32}\n6 {file_name_id=3 function_name_id=6 line=10 end_line=10 column=17 end_column=27}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=6}\n\n\n%fused_computation (param_0.4: f32[10]) -> f32[10] {\n  %param_0.4 = f32[10]{0} parameter(0)\n  %mul.4 = f32[10]{0} multiply(%param_0.4, %param_0.4), metadata={op_name=\"jit(scalar_output_wrapper)/jvp()/mul\" stack_frame_id=6}\n  %mul.3 = f32[10]{0} multiply(%mul.4, %param_0.4), metadata={op_name=\"jit(scalar_output_wrapper)/jvp()/mul\" stack_frame_id=6}\n  %mul.2 = f32[10]{0} multiply(%mul.3, %param_0.4), metadata={op_name=\"jit(scalar_output_wrapper)/jvp()/mul\" stack_frame_id=6}\n  %mul.1 = f32[10]{0} multiply(%mul.2, %param_0.4), metadata={op_name=\"jit(scalar_output_wrapper)/jvp()/mul\" stack_frame_id=6}\n  %add_any.4 = f32[10]{0} add(%mul.1, %mul.1), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/add_any\" stack_frame_id=6}\n  %mul.0 = f32[10]{0} multiply(%mul.3, %mul.4), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/mul\" stack_frame_id=6}\n  %add_any.3 = f32[10]{0} add(%add_any.4, %mul.0), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/add_any\" stack_frame_id=6}\n  %add_any.2 = f32[10]{0} add(%add_any.3, %mul.0), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/add_any\" stack_frame_id=6}\n  %add_any.1 = f32[10]{0} add(%add_any.2, %mul.1), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/add_any\" stack_frame_id=6}\n  ROOT %add_any.0 = f32[10]{0} add(%add_any.1, %mul.1), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/add_any\" stack_frame_id=6}\n}\n\nENTRY %main.1 (args_0_.1: f32[10]) -> f32[10] {\n  %args_0_.1 = f32[10]{0} parameter(0), metadata={op_name=\"args[0]\"}\n  ROOT %add_add_fusion = f32[10]{0} fusion(%args_0_.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/add_any\" stack_frame_id=6}\n}\n\n"}
{"id": 43, "kernel_name": "branch_fpcx", "python": "def branch_fpcx(a):\n    \"\"\"Conditional operation using jnp.where.\"\"\"\n    return jnp.where(a > -0.62, a - 2.0, a - 2.2)", "type": "generate_branch_kernel", "hlo": "// ===== FORWARD PASS =====\nmodule @jit_branch_fpcx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-6.200000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<10xf32>, tensor<10xf32>) -> tensor<10xi1>\n    %cst_0 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<10xf32>\n    %cst_1 = stablehlo.constant dense<2.200000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<10xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<10xi1>, tensor<10xf32>, tensor<10xf32>) -> tensor<10xf32>\n    return %6 : tensor<10xf32>\n  }\n  func.func private @_where(%arg0: tensor<10xi1>, %arg1: tensor<10xf32>, %arg2: tensor<10xf32>) -> tensor<10xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<10xi1>, tensor<10xf32>\n    return %0 : tensor<10xf32>\n  }\n}\n\n\n// ===== BACKWARD PASS =====\nmodule @jit_scalar_output_wrapper attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-6.200000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<10xf32>, tensor<10xf32>) -> tensor<10xi1>\n    %cst_0 = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %3:2 = call @_where(%1, %2) : (tensor<10xi1>, tensor<10xf32>) -> (tensor<10xf32>, tensor<10xf32>)\n    %4 = stablehlo.add %3#0, %3#1 : tensor<10xf32>\n    return %4 : tensor<10xf32>\n  }\n  func.func private @_where(%arg0: tensor<10xi1>, %arg1: tensor<10xf32>) -> (tensor<10xf32>, tensor<10xf32>) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %1 = stablehlo.select %arg0, %0, %arg1 : tensor<10xi1>, tensor<10xf32>\n    %2 = stablehlo.select %arg0, %arg1, %0 : tensor<10xi1>, tensor<10xf32>\n    return %2, %1 : tensor<10xf32>, tensor<10xf32>\n  }\n}\n", "optimized_hlo": "// ===== FORWARD PASS (OPTIMIZED) =====\nHloModule jit_branch_fpcx, is_scheduled=true, entry_computation_layout={(f32[10]{0})->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmpq1aem3c9.py\"\n4 \"/tmp/tmphr9su7i4.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"branch_fpcx\"\n6 \"nested_odsh\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=41 end_line=41 column=14 end_column=47}\n5 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=41 end_column=48}\n6 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=32 end_column=39}\n7 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=21 end_column=30}\n8 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=11 end_column=49}\n9 {file_name_id=4 function_name_id=6 line=8 end_line=8 column=21 end_column=58}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=5}\n8 {file_location_id=8 parent_frame_id=5}\n9 {file_location_id=9 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[10]) -> f32[10] {\n  %param_0.1 = f32[10]{0} parameter(0)\n  %constant.7 = f32[] constant(-0.62)\n  %broadcast.7 = f32[10]{0} broadcast(%constant.7), dimensions={}\n  %gt.0 = pred[10]{0} compare(%param_0.1, %broadcast.7), direction=GT, metadata={op_name=\"jit(branch_fpcx)/gt\" stack_frame_id=7}\n  %constant.6 = f32[] constant(-2)\n  %broadcast.6 = f32[10]{0} broadcast(%constant.6), dimensions={}\n  %add.3 = f32[10]{0} add(%param_0.1, %broadcast.6), metadata={op_name=\"jit(branch_fpcx)/sub\" stack_frame_id=6}\n  %constant.2 = f32[] constant(-2.2)\n  %broadcast.2 = f32[10]{0} broadcast(%constant.2), dimensions={}\n  %add.2 = f32[10]{0} add(%param_0.1, %broadcast.2), metadata={op_name=\"jit(branch_fpcx)/sub\" stack_frame_id=5}\n  ROOT %select_n.2 = f32[10]{0} select(%gt.0, %add.3, %add.2), metadata={op_name=\"jit(branch_fpcx)/jit(_where)/select_n\" stack_frame_id=9}\n}\n\nENTRY %main.2 (a.1: f32[10]) -> f32[10] {\n  %a.1 = f32[10]{0} parameter(0), metadata={op_name=\"a\"}\n  ROOT %add_select_fusion = f32[10]{0} fusion(%a.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(branch_fpcx)/jit(_where)/select_n\" stack_frame_id=9}\n}\n\n\n\n// ===== BACKWARD PASS (OPTIMIZED) =====\nHloModule jit_scalar_output_wrapper, is_scheduled=true, entry_computation_layout={(f32[10]{0})->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmpq1aem3c9.py\"\n4 \"/tmp/tmphr9su7i4.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"extract_ir.<locals>.scalar_output_wrapper\"\n6 \"branch_fpcx\"\n7 \"nested_odsh\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=66 end_line=66 column=27 end_column=65}\n5 {file_name_id=2 function_name_id=5 line=61 end_line=61 column=23 end_column=38}\n6 {file_name_id=2 function_name_id=5 line=58 end_line=58 column=21 end_column=32}\n7 {file_name_id=3 function_name_id=6 line=7 end_line=7 column=21 end_column=30}\n8 {file_name_id=3 function_name_id=6 line=7 end_line=7 column=11 end_column=49}\n9 {file_name_id=2 function_name_id=4 line=41 end_line=41 column=14 end_column=47}\n10 {file_name_id=4 function_name_id=7 line=8 end_line=8 column=21 end_column=58}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=7}\n8 {file_location_id=8 parent_frame_id=7}\n9 {file_location_id=9 parent_frame_id=4}\n10 {file_location_id=10 parent_frame_id=10}\n\n\n%fused_computation (param_0.2: f32[10]) -> f32[10] {\n  %param_0.2 = f32[10]{0} parameter(0)\n  %constant.6 = f32[] constant(-0.62)\n  %broadcast.6 = f32[10]{0} broadcast(%constant.6), dimensions={}\n  %gt.0 = pred[10]{0} compare(%param_0.2, %broadcast.6), direction=GT, metadata={op_name=\"jit(scalar_output_wrapper)/jvp()/gt\" stack_frame_id=7}\n  %constant.2 = f32[] constant(1)\n  %broadcast.2 = f32[10]{0} broadcast(%constant.2), dimensions={}\n  %constant.1 = f32[] constant(0), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp(jit(_where)))\"}\n  %broadcast.1 = f32[10]{0} broadcast(%constant.1), dimensions={}, metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp(jit(_where)))\"}\n  %select_n.5 = f32[10]{0} select(%gt.0, %broadcast.2, %broadcast.1), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp(jit(_where)))/select_n\" stack_frame_id=10}\n  %select_n.4 = f32[10]{0} select(%gt.0, %broadcast.1, %broadcast.2), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp(jit(_where)))/select_n\" stack_frame_id=10}\n  ROOT %add_any.0 = f32[10]{0} add(%select_n.5, %select_n.4), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/add_any\" stack_frame_id=8}\n}\n\nENTRY %main.2 (args_0_.1: f32[10]) -> f32[10] {\n  %args_0_.1 = f32[10]{0} parameter(0), metadata={op_name=\"args[0]\"}\n  ROOT %select_add_fusion = f32[10]{0} fusion(%args_0_.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/add_any\" stack_frame_id=8}\n}\n\n"}
{"id": 44, "kernel_name": "elementwise_uavi", "python": "def elementwise_uavi(a, b):\n    \"\"\"Elementwise operation on two arrays.\"\"\"\n    return a + b", "type": "generate_simple_elementwise", "hlo": "// ===== FORWARD PASS =====\nmodule @jit_elementwise_uavi attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>, %arg1: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<10xf32>\n    return %0 : tensor<10xf32>\n  }\n}\n\n\n// ===== BACKWARD PASS =====\nmodule @jit_scalar_output_wrapper attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main() -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    return %0 : tensor<10xf32>\n  }\n}\n", "optimized_hlo": "// ===== FORWARD PASS (OPTIMIZED) =====\nHloModule jit_elementwise_uavi, is_scheduled=true, entry_computation_layout={(f32[10]{0}, f32[10]{0})->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmp786yw9dt.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"elementwise_uavi\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=41 end_line=41 column=14 end_column=47}\n5 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=11 end_column=16}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n\n\n%wrapped_add_computation (param_0: f32[10], param_1: f32[10]) -> f32[10] {\n  %param_0 = f32[10]{0} parameter(0)\n  %param_1 = f32[10]{0} parameter(1)\n  ROOT %add.0 = f32[10]{0} add(%param_0, %param_1), metadata={op_name=\"jit(elementwise_uavi)/add\" stack_frame_id=5}\n}\n\nENTRY %main.1 (a.1: f32[10], b.1: f32[10]) -> f32[10] {\n  %a.1 = f32[10]{0} parameter(0), metadata={op_name=\"a\"}\n  %b.1 = f32[10]{0} parameter(1), metadata={op_name=\"b\"}\n  ROOT %wrapped_add = f32[10]{0} fusion(%a.1, %b.1), kind=kLoop, calls=%wrapped_add_computation, metadata={op_name=\"jit(elementwise_uavi)/add\" stack_frame_id=5}\n}\n\n\n\n// ===== BACKWARD PASS (OPTIMIZED) =====\nHloModule jit_scalar_output_wrapper, is_scheduled=true, entry_computation_layout={()->f32[10]{0}}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"extract_ir.<locals>.scalar_output_wrapper\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=66 end_line=66 column=27 end_column=65}\n5 {file_name_id=2 function_name_id=5 line=61 end_line=61 column=23 end_column=38}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n\n\n%wrapped_broadcast_computation (param_0: f32[]) -> f32[10] {\n  %param_0 = f32[] parameter(0)\n  ROOT %broadcast.0 = f32[10]{0} broadcast(%param_0), dimensions={}\n}\n\nENTRY %main.1 () -> f32[10] {\n  %constant.1 = f32[] constant(1)\n  ROOT %wrapped_broadcast = f32[10]{0} fusion(%constant.1), kind=kLoop, calls=%wrapped_broadcast_computation\n}\n\n"}
{"id": 45, "kernel_name": "unary_devo", "python": "def unary_devo(a):\n    \"\"\"Unary operation on array.\"\"\"\n    return jnp.sin(a)", "type": "generate_unary_kernel", "hlo": "// ===== FORWARD PASS =====\nmodule @jit_unary_devo attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<10xf32>\n    return %0 : tensor<10xf32>\n  }\n}\n\n\n// ===== BACKWARD PASS =====\nmodule @jit_scalar_output_wrapper attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.cosine %arg0 : tensor<10xf32>\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %2 = stablehlo.multiply %1, %0 : tensor<10xf32>\n    return %2 : tensor<10xf32>\n  }\n}\n", "optimized_hlo": "// ===== FORWARD PASS (OPTIMIZED) =====\nHloModule jit_unary_devo, is_scheduled=true, entry_computation_layout={(f32[10]{0})->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmpc0aui30o.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"unary_devo\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=41 end_line=41 column=14 end_column=47}\n5 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=11 end_column=21}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n\n\n%wrapped_sine_computation (param_0: f32[10]) -> f32[10] {\n  %param_0 = f32[10]{0} parameter(0)\n  ROOT %sin.0 = f32[10]{0} sine(%param_0), metadata={op_name=\"jit(unary_devo)/sin\" stack_frame_id=5}\n}\n\nENTRY %main.1 (a.1: f32[10]) -> f32[10] {\n  %a.1 = f32[10]{0} parameter(0), metadata={op_name=\"a\"}\n  ROOT %wrapped_sine = f32[10]{0} fusion(%a.1), kind=kLoop, calls=%wrapped_sine_computation, metadata={op_name=\"jit(unary_devo)/sin\" stack_frame_id=5}\n}\n\n\n\n// ===== BACKWARD PASS (OPTIMIZED) =====\nHloModule jit_scalar_output_wrapper, is_scheduled=true, entry_computation_layout={(f32[10]{0})->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmpc0aui30o.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"extract_ir.<locals>.scalar_output_wrapper\"\n6 \"unary_devo\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=66 end_line=66 column=27 end_column=65}\n5 {file_name_id=2 function_name_id=5 line=58 end_line=58 column=21 end_column=32}\n6 {file_name_id=3 function_name_id=6 line=7 end_line=7 column=11 end_column=21}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=6}\n\n\n%wrapped_cosine_computation (param_0: f32[10]) -> f32[10] {\n  %param_0 = f32[10]{0} parameter(0)\n  ROOT %cos.0 = f32[10]{0} cosine(%param_0), metadata={op_name=\"jit(scalar_output_wrapper)/jvp()/cos\" stack_frame_id=6}\n}\n\nENTRY %main.1 (args_0_.1: f32[10]) -> f32[10] {\n  %args_0_.1 = f32[10]{0} parameter(0), metadata={op_name=\"args[0]\"}\n  ROOT %wrapped_cosine = f32[10]{0} fusion(%args_0_.1), kind=kLoop, calls=%wrapped_cosine_computation, metadata={op_name=\"jit(scalar_output_wrapper)/jvp()/cos\" stack_frame_id=6}\n}\n\n"}
{"id": 46, "kernel_name": "vec_kexq", "python": "def vec_kexq(a, b):\n    \"\"\"Dot product along last axis.\"\"\"\n    # Assuming last dimension is vector dimension\n    return jnp.sum(a * b, axis=-1)", "type": "generate_vector_kernel", "hlo": "// ===== FORWARD PASS =====\nmodule @jit_vec_kexq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x3xf32>, %arg1: tensor<10x3xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<10x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<10x3xf32>, tensor<f32>) -> tensor<10xf32>\n    return %1 : tensor<10xf32>\n  }\n}\n\n\n// ===== BACKWARD PASS =====\nmodule @jit_scalar_output_wrapper attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x3xf32>) -> (tensor<10x3xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [0] : (tensor<10xf32>) -> tensor<10x3xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<10x3xf32>\n    return %2 : tensor<10x3xf32>\n  }\n}\n", "optimized_hlo": "// ===== FORWARD PASS (OPTIMIZED) =====\nHloModule jit_vec_kexq, is_scheduled=true, entry_computation_layout={(f32[10,3]{1,0}, f32[10,3]{1,0})->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmp0dueott1.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"vec_kexq\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=41 end_line=41 column=14 end_column=47}\n5 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=11 end_column=34}\n6 {file_name_id=3 function_name_id=5 line=8 end_line=8 column=19 end_column=24}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n\n\n%region_0.1 (reduce_sum.3: f32[], reduce_sum.4: f32[]) -> f32[] {\n  %reduce_sum.3 = f32[] parameter(0), metadata={op_name=\"reduce_sum\"}\n  %reduce_sum.4 = f32[] parameter(1), metadata={op_name=\"reduce_sum\"}\n  ROOT %reduce_sum.5 = f32[] add(%reduce_sum.3, %reduce_sum.4), metadata={op_name=\"jit(vec_kexq)/reduce_sum\" stack_frame_id=5}\n}\n\n%fused_computation (param_0.2: f32[10,3], param_1.2: f32[10,3]) -> f32[10] {\n  %param_0.2 = f32[10,3]{1,0} parameter(0)\n  %param_1.2 = f32[10,3]{1,0} parameter(1)\n  %mul.0 = f32[10,3]{1,0} multiply(%param_0.2, %param_1.2), metadata={op_name=\"jit(vec_kexq)/mul\" stack_frame_id=6}\n  %constant.0 = f32[] constant(0)\n  ROOT %reduce_sum.0 = f32[10]{0} reduce(%mul.0, %constant.0), dimensions={1}, to_apply=%region_0.1, metadata={op_name=\"jit(vec_kexq)/reduce_sum\" stack_frame_id=5}\n}\n\nENTRY %main.2 (a.1: f32[10,3], b.1: f32[10,3]) -> f32[10] {\n  %a.1 = f32[10,3]{1,0} parameter(0), metadata={op_name=\"a\"}\n  %b.1 = f32[10,3]{1,0} parameter(1), metadata={op_name=\"b\"}\n  ROOT %multiply_reduce_fusion = f32[10]{0} fusion(%a.1, %b.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(vec_kexq)/reduce_sum\" stack_frame_id=5}\n}\n\n\n\n// ===== BACKWARD PASS (OPTIMIZED) =====\nHloModule jit_scalar_output_wrapper, is_scheduled=true, entry_computation_layout={(f32[10,3]{1,0})->f32[10,3]{1,0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nENTRY %main.1 (args_1_.1: f32[10,3]) -> f32[10,3] {\n  %args_1_.1 = f32[10,3]{1,0} parameter(0), metadata={op_name=\"args[1]\"}\n  ROOT %copy = f32[10,3]{1,0} copy(%args_1_.1)\n}\n\n"}
{"id": 47, "kernel_name": "scalar_arr_cpzd", "python": "def scalar_arr_cpzd(alpha, x, y):\n    \"\"\"Scalar and array operations.\"\"\"\n    return alpha + x + y", "type": "generate_scalar_array_op", "hlo": "// ===== FORWARD PASS =====\nmodule @jit_scalar_arr_cpzd attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<10xf32>, %arg2: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %1 = stablehlo.add %0, %arg1 : tensor<10xf32>\n    %2 = stablehlo.add %1, %arg2 : tensor<10xf32>\n    return %2 : tensor<10xf32>\n  }\n}\n\n\n// ===== BACKWARD PASS =====\nmodule @jit_scalar_output_wrapper attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main() -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst_0) applies stablehlo.add across dimensions = [0] : (tensor<10xf32>, tensor<f32>) -> tensor<f32>\n    return %1 : tensor<f32>\n  }\n}\n", "optimized_hlo": "// ===== FORWARD PASS (OPTIMIZED) =====\nHloModule jit_scalar_arr_cpzd, is_scheduled=true, entry_computation_layout={(f32[], f32[10]{0}, f32[10]{0})->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmpilt7e96p.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"scalar_arr_cpzd\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=41 end_line=41 column=14 end_column=47}\n5 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=11 end_column=20}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[10], param_1.2: f32[10], param_2.1: f32[]) -> f32[10] {\n  %param_2.1 = f32[] parameter(2)\n  %add.2 = f32[10]{0} broadcast(%param_2.1), dimensions={}, metadata={op_name=\"jit(scalar_arr_cpzd)/add\" stack_frame_id=5}\n  %param_1.2 = f32[10]{0} parameter(1)\n  %add.1 = f32[10]{0} add(%add.2, %param_1.2), metadata={op_name=\"jit(scalar_arr_cpzd)/add\" stack_frame_id=5}\n  %param_0.1 = f32[10]{0} parameter(0)\n  ROOT %add.0 = f32[10]{0} add(%add.1, %param_0.1), metadata={op_name=\"jit(scalar_arr_cpzd)/add\" stack_frame_id=5}\n}\n\nENTRY %main.1 (alpha.1: f32[], x.1: f32[10], y.1: f32[10]) -> f32[10] {\n  %alpha.1 = f32[] parameter(0), metadata={op_name=\"alpha\"}\n  %x.1 = f32[10]{0} parameter(1), metadata={op_name=\"x\"}\n  %y.1 = f32[10]{0} parameter(2), metadata={op_name=\"y\"}\n  ROOT %add_add_fusion = f32[10]{0} fusion(%y.1, %x.1, %alpha.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(scalar_arr_cpzd)/add\" stack_frame_id=5}\n}\n\n\n\n// ===== BACKWARD PASS (OPTIMIZED) =====\nHloModule jit_scalar_output_wrapper, is_scheduled=true, entry_computation_layout={()->f32[]}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmpilt7e96p.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"extract_ir.<locals>.scalar_output_wrapper\"\n6 \"scalar_arr_cpzd\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=66 end_line=66 column=27 end_column=65}\n5 {file_name_id=2 function_name_id=5 line=58 end_line=58 column=21 end_column=32}\n6 {file_name_id=3 function_name_id=6 line=7 end_line=7 column=11 end_column=20}\n7 {file_name_id=2 function_name_id=5 line=61 end_line=61 column=23 end_column=38}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=6}\n7 {file_location_id=7 parent_frame_id=5}\n\n\nENTRY %main.2 () -> f32[] {\n  %constant = f32[] constant(10), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/reduce_sum\" stack_frame_id=6}\n  ROOT %copy = f32[] copy(%constant)\n}\n\n"}
{"id": 48, "kernel_name": "branch_fcrp", "python": "def branch_fcrp(a):\n    \"\"\"Conditional operation using jnp.where.\"\"\"\n    return jnp.where(a > 0.27, a - 1.9, a - 2.9)", "type": "generate_branch_kernel", "hlo": "// ===== FORWARD PASS =====\nmodule @jit_branch_fcrp attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<2.700000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<10xf32>, tensor<10xf32>) -> tensor<10xi1>\n    %cst_0 = stablehlo.constant dense<1.900000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<10xf32>\n    %cst_1 = stablehlo.constant dense<2.900000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<10xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<10xi1>, tensor<10xf32>, tensor<10xf32>) -> tensor<10xf32>\n    return %6 : tensor<10xf32>\n  }\n  func.func private @_where(%arg0: tensor<10xi1>, %arg1: tensor<10xf32>, %arg2: tensor<10xf32>) -> tensor<10xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<10xi1>, tensor<10xf32>\n    return %0 : tensor<10xf32>\n  }\n}\n\n\n// ===== BACKWARD PASS =====\nmodule @jit_scalar_output_wrapper attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<2.700000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<10xf32>, tensor<10xf32>) -> tensor<10xi1>\n    %cst_0 = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %3:2 = call @_where(%1, %2) : (tensor<10xi1>, tensor<10xf32>) -> (tensor<10xf32>, tensor<10xf32>)\n    %4 = stablehlo.add %3#0, %3#1 : tensor<10xf32>\n    return %4 : tensor<10xf32>\n  }\n  func.func private @_where(%arg0: tensor<10xi1>, %arg1: tensor<10xf32>) -> (tensor<10xf32>, tensor<10xf32>) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %1 = stablehlo.select %arg0, %0, %arg1 : tensor<10xi1>, tensor<10xf32>\n    %2 = stablehlo.select %arg0, %arg1, %0 : tensor<10xi1>, tensor<10xf32>\n    return %2, %1 : tensor<10xf32>, tensor<10xf32>\n  }\n}\n", "optimized_hlo": "// ===== FORWARD PASS (OPTIMIZED) =====\nHloModule jit_branch_fcrp, is_scheduled=true, entry_computation_layout={(f32[10]{0})->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmpxq8jljin.py\"\n4 \"/tmp/tmphr9su7i4.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"branch_fcrp\"\n6 \"nested_odsh\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=41 end_line=41 column=14 end_column=47}\n5 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=40 end_column=47}\n6 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=31 end_column=38}\n7 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=21 end_column=29}\n8 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=11 end_column=48}\n9 {file_name_id=4 function_name_id=6 line=8 end_line=8 column=21 end_column=58}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=5}\n8 {file_location_id=8 parent_frame_id=5}\n9 {file_location_id=9 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[10]) -> f32[10] {\n  %param_0.1 = f32[10]{0} parameter(0)\n  %constant.7 = f32[] constant(0.27)\n  %broadcast.7 = f32[10]{0} broadcast(%constant.7), dimensions={}\n  %gt.0 = pred[10]{0} compare(%param_0.1, %broadcast.7), direction=GT, metadata={op_name=\"jit(branch_fcrp)/gt\" stack_frame_id=7}\n  %constant.6 = f32[] constant(-1.9)\n  %broadcast.6 = f32[10]{0} broadcast(%constant.6), dimensions={}\n  %add.3 = f32[10]{0} add(%param_0.1, %broadcast.6), metadata={op_name=\"jit(branch_fcrp)/sub\" stack_frame_id=6}\n  %constant.2 = f32[] constant(-2.9)\n  %broadcast.2 = f32[10]{0} broadcast(%constant.2), dimensions={}\n  %add.2 = f32[10]{0} add(%param_0.1, %broadcast.2), metadata={op_name=\"jit(branch_fcrp)/sub\" stack_frame_id=5}\n  ROOT %select_n.2 = f32[10]{0} select(%gt.0, %add.3, %add.2), metadata={op_name=\"jit(branch_fcrp)/jit(_where)/select_n\" stack_frame_id=9}\n}\n\nENTRY %main.2 (a.1: f32[10]) -> f32[10] {\n  %a.1 = f32[10]{0} parameter(0), metadata={op_name=\"a\"}\n  ROOT %add_select_fusion = f32[10]{0} fusion(%a.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(branch_fcrp)/jit(_where)/select_n\" stack_frame_id=9}\n}\n\n\n\n// ===== BACKWARD PASS (OPTIMIZED) =====\nHloModule jit_scalar_output_wrapper, is_scheduled=true, entry_computation_layout={(f32[10]{0})->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmpxq8jljin.py\"\n4 \"/tmp/tmphr9su7i4.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"extract_ir.<locals>.scalar_output_wrapper\"\n6 \"branch_fcrp\"\n7 \"nested_odsh\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=66 end_line=66 column=27 end_column=65}\n5 {file_name_id=2 function_name_id=5 line=61 end_line=61 column=23 end_column=38}\n6 {file_name_id=2 function_name_id=5 line=58 end_line=58 column=21 end_column=32}\n7 {file_name_id=3 function_name_id=6 line=7 end_line=7 column=21 end_column=29}\n8 {file_name_id=3 function_name_id=6 line=7 end_line=7 column=11 end_column=48}\n9 {file_name_id=2 function_name_id=4 line=41 end_line=41 column=14 end_column=47}\n10 {file_name_id=4 function_name_id=7 line=8 end_line=8 column=21 end_column=58}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n7 {file_location_id=7 parent_frame_id=7}\n8 {file_location_id=8 parent_frame_id=7}\n9 {file_location_id=9 parent_frame_id=4}\n10 {file_location_id=10 parent_frame_id=10}\n\n\n%fused_computation (param_0.2: f32[10]) -> f32[10] {\n  %param_0.2 = f32[10]{0} parameter(0)\n  %constant.6 = f32[] constant(0.27)\n  %broadcast.6 = f32[10]{0} broadcast(%constant.6), dimensions={}\n  %gt.0 = pred[10]{0} compare(%param_0.2, %broadcast.6), direction=GT, metadata={op_name=\"jit(scalar_output_wrapper)/jvp()/gt\" stack_frame_id=7}\n  %constant.2 = f32[] constant(1)\n  %broadcast.2 = f32[10]{0} broadcast(%constant.2), dimensions={}\n  %constant.1 = f32[] constant(0), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp(jit(_where)))\"}\n  %broadcast.1 = f32[10]{0} broadcast(%constant.1), dimensions={}, metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp(jit(_where)))\"}\n  %select_n.5 = f32[10]{0} select(%gt.0, %broadcast.2, %broadcast.1), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp(jit(_where)))/select_n\" stack_frame_id=10}\n  %select_n.4 = f32[10]{0} select(%gt.0, %broadcast.1, %broadcast.2), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp(jit(_where)))/select_n\" stack_frame_id=10}\n  ROOT %add_any.0 = f32[10]{0} add(%select_n.5, %select_n.4), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/add_any\" stack_frame_id=8}\n}\n\nENTRY %main.2 (args_0_.1: f32[10]) -> f32[10] {\n  %args_0_.1 = f32[10]{0} parameter(0), metadata={op_name=\"args[0]\"}\n  ROOT %select_add_fusion = f32[10]{0} fusion(%args_0_.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/add_any\" stack_frame_id=8}\n}\n\n"}
{"id": 49, "kernel_name": "scalar_arr_cere", "python": "def scalar_arr_cere(alpha, x, y):\n    \"\"\"Scalar and array operations.\"\"\"\n    return alpha - x * y", "type": "generate_scalar_array_op", "hlo": "// ===== FORWARD PASS =====\nmodule @jit_scalar_arr_cere attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<10xf32>, %arg2: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<10xf32>\n    %1 = stablehlo.broadcast_in_dim %arg0, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %2 = stablehlo.subtract %1, %0 : tensor<10xf32>\n    return %2 : tensor<10xf32>\n  }\n}\n\n\n// ===== BACKWARD PASS =====\nmodule @jit_scalar_output_wrapper attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main() -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst_0) applies stablehlo.add across dimensions = [0] : (tensor<10xf32>, tensor<f32>) -> tensor<f32>\n    return %1 : tensor<f32>\n  }\n}\n", "optimized_hlo": "// ===== FORWARD PASS (OPTIMIZED) =====\nHloModule jit_scalar_arr_cere, is_scheduled=true, entry_computation_layout={(f32[], f32[10]{0}, f32[10]{0})->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmpua36hyy3.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"scalar_arr_cere\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=41 end_line=41 column=14 end_column=47}\n5 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=19 end_column=24}\n6 {file_name_id=3 function_name_id=5 line=7 end_line=7 column=11 end_column=24}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=5}\n\n\n%fused_computation (param_0.1: f32[10], param_1.2: f32[10], param_2.1: f32[]) -> f32[10] {\n  %param_2.1 = f32[] parameter(2)\n  %sub.1 = f32[10]{0} broadcast(%param_2.1), dimensions={}, metadata={op_name=\"jit(scalar_arr_cere)/sub\" stack_frame_id=6}\n  %param_0.1 = f32[10]{0} parameter(0)\n  %param_1.2 = f32[10]{0} parameter(1)\n  %mul.0 = f32[10]{0} multiply(%param_0.1, %param_1.2), metadata={op_name=\"jit(scalar_arr_cere)/mul\" stack_frame_id=5}\n  ROOT %sub.0 = f32[10]{0} subtract(%sub.1, %mul.0), metadata={op_name=\"jit(scalar_arr_cere)/sub\" stack_frame_id=6}\n}\n\nENTRY %main.1 (alpha.1: f32[], x.1: f32[10], y.1: f32[10]) -> f32[10] {\n  %alpha.1 = f32[] parameter(0), metadata={op_name=\"alpha\"}\n  %x.1 = f32[10]{0} parameter(1), metadata={op_name=\"x\"}\n  %y.1 = f32[10]{0} parameter(2), metadata={op_name=\"y\"}\n  ROOT %multiply_subtract_fusion = f32[10]{0} fusion(%x.1, %y.1, %alpha.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(scalar_arr_cere)/sub\" stack_frame_id=6}\n}\n\n\n\n// ===== BACKWARD PASS (OPTIMIZED) =====\nHloModule jit_scalar_output_wrapper, is_scheduled=true, entry_computation_layout={()->f32[]}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"/workspace/jit/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmpua36hyy3.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"extract_ir.<locals>.scalar_output_wrapper\"\n6 \"scalar_arr_cere\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=277 end_line=277 column=8 end_column=80}\n2 {file_name_id=1 function_name_id=2 line=230 end_line=230 column=42 end_column=95}\n3 {file_name_id=1 function_name_id=3 line=119 end_line=119 column=13 end_column=51}\n4 {file_name_id=2 function_name_id=4 line=66 end_line=66 column=27 end_column=65}\n5 {file_name_id=2 function_name_id=5 line=58 end_line=58 column=21 end_column=32}\n6 {file_name_id=3 function_name_id=6 line=7 end_line=7 column=11 end_column=24}\n7 {file_name_id=2 function_name_id=5 line=61 end_line=61 column=23 end_column=38}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=6}\n7 {file_location_id=7 parent_frame_id=5}\n\n\nENTRY %main.2 () -> f32[] {\n  %constant = f32[] constant(10), metadata={op_name=\"jit(scalar_output_wrapper)/transpose(jvp())/reduce_sum\" stack_frame_id=6}\n  ROOT %copy = f32[] copy(%constant)\n}\n\n"}
