{"id": 0, "kernel_name": "scalar_arr_qahf", "python": "def scalar_arr_qahf(alpha, x, y):\n    return (alpha * x) + y", "type": "generate_scalar_array_op", "cpp": "### FORWARD (CPU HLO)\nHloModule jit_forward, entry_computation_layout={(f32[], f32[8]{0}, f32[8]{0})->f32[8]{0}}, frontend_attributes={xla.sdy.meshes={empty_mesh = #sdy.mesh<[]>}}\n\nENTRY main.1 {\n  args_0_.1 = f32[] parameter(0), sharding={replicated}, frontend_attributes={xla.sdy.sharding=\"#sdy.sharding<@empty_mesh, []>\"}\n  mul.2 = f32[8]{0} broadcast(args_0_.1), dimensions={}\n  args_1_.1 = f32[8]{0} parameter(1), sharding={replicated}, frontend_attributes={xla.sdy.sharding=\"#sdy.sharding<@empty_mesh, [{}]>\"}\n  mul.3 = f32[8]{0} multiply(mul.2, args_1_.1)\n  args_2_.1 = f32[8]{0} parameter(2), sharding={replicated}, frontend_attributes={xla.sdy.sharding=\"#sdy.sharding<@empty_mesh, [{}]>\"}\n  add.2 = f32[8]{0} add(mul.3, args_2_.1)\n  add.3 = f32[8]{0} custom-call(add.2), custom_call_target=\"xla.sdy.FuncResultSharding\", custom_call_has_side_effect=true, frontend_attributes={xla.sdy.sharding=\"#sdy.sharding_per_value<[<@empty_mesh, [{}]>]>\"}\n  tuple.1 = (f32[8]{0}) tuple(add.3)\n  ROOT get-tuple-element.1 = f32[8]{0} get-tuple-element(tuple.1), index=0, sharding={replicated}\n}\n\n\n\n### BACKWARD (CPU HLO)\nHloModule jit_backward, entry_computation_layout={(f32[], f32[8]{0})->(f32[], f32[8]{0}, f32[8]{0})}, frontend_attributes={xla.sdy.meshes={empty_mesh = #sdy.mesh<[]>}}\n\nregion_0.1 {\n  reduce_sum.3 = f32[] parameter(0)\n  reduce_sum.4 = f32[] parameter(1)\n  ROOT reduce_sum.5 = f32[] add(reduce_sum.3, reduce_sum.4)\n}\n\nENTRY main.2 {\n  args_1_.1 = f32[8]{0} parameter(1), sharding={replicated}, frontend_attributes={xla.sdy.sharding=\"#sdy.sharding<@empty_mesh, [{}]>\"}\n  constant.2 = f32[] constant(0)\n  reduce_sum.7 = f32[] reduce(args_1_.1, constant.2), dimensions={0}, to_apply=region_0.1\n  convert_element_type.1 = f32[] custom-call(reduce_sum.7), custom_call_target=\"xla.sdy.FuncResultSharding\", custom_call_has_side_effect=true, frontend_attributes={xla.sdy.sharding=\"#sdy.sharding_per_value<[<@empty_mesh, []>]>\"}\n  reshape.3 = f32[] reshape(convert_element_type.1), sharding={replicated}\n  args_0_.1 = f32[] parameter(0), sharding={replicated}, frontend_attributes={xla.sdy.sharding=\"#sdy.sharding<@empty_mesh, []>\"}\n  mul.2 = f32[8]{0} broadcast(args_0_.1), dimensions={}\n  mul.3 = f32[8]{0} custom-call(mul.2), custom_call_target=\"xla.sdy.FuncResultSharding\", custom_call_has_side_effect=true, frontend_attributes={xla.sdy.sharding=\"#sdy.sharding_per_value<[<@empty_mesh, [{}]>]>\"}\n  reshape.4 = f32[8]{0} reshape(mul.3), sharding={replicated}\n  constant.3 = f32[] constant(1)\n  broadcast.1 = f32[8]{0} broadcast(constant.3), dimensions={}\n  broadcast_in_dim.1 = f32[8]{0} custom-call(broadcast.1), custom_call_target=\"xla.sdy.FuncResultSharding\", custom_call_has_side_effect=true, frontend_attributes={xla.sdy.sharding=\"#sdy.sharding_per_value<[<@empty_mesh, [{}]>]>\"}\n  reshape.5 = f32[8]{0} reshape(broadcast_in_dim.1), sharding={replicated}\n  ROOT tuple.1 = (f32[], f32[8]{0}, f32[8]{0}) tuple(convert_element_type.1, mul.3, broadcast_in_dim.1), sharding={{replicated}, {replicated}, {replicated}}\n}\n\n"}
{"id": 1, "kernel_name": "elementwise_bsdm", "python": "def elementwise_bsdm(a, b):\n    return a * b", "type": "generate_simple_elementwise", "cpp": "### FORWARD (CPU HLO)\nHloModule jit_forward, entry_computation_layout={(f32[8]{0}, f32[8]{0})->f32[8]{0}}, frontend_attributes={xla.sdy.meshes={empty_mesh = #sdy.mesh<[]>}}\n\nENTRY main.1 {\n  args_0_.1 = f32[8]{0} parameter(0), sharding={replicated}, frontend_attributes={xla.sdy.sharding=\"#sdy.sharding<@empty_mesh, [{}]>\"}\n  args_1_.1 = f32[8]{0} parameter(1), sharding={replicated}, frontend_attributes={xla.sdy.sharding=\"#sdy.sharding<@empty_mesh, [{}]>\"}\n  mul.2 = f32[8]{0} multiply(args_0_.1, args_1_.1)\n  mul.3 = f32[8]{0} custom-call(mul.2), custom_call_target=\"xla.sdy.FuncResultSharding\", custom_call_has_side_effect=true, frontend_attributes={xla.sdy.sharding=\"#sdy.sharding_per_value<[<@empty_mesh, [{}]>]>\"}\n  tuple.1 = (f32[8]{0}) tuple(mul.3)\n  ROOT get-tuple-element.1 = f32[8]{0} get-tuple-element(tuple.1), index=0, sharding={replicated}\n}\n\n\n\n### BACKWARD (CPU HLO)\nHloModule jit_backward, entry_computation_layout={(f32[8]{0}, f32[8]{0})->(f32[8]{0}, f32[8]{0})}, frontend_attributes={xla.sdy.meshes={empty_mesh = #sdy.mesh<[]>}}\n\nENTRY main.1 {\n  args_1_.1 = f32[8]{0} parameter(1), sharding={replicated}, frontend_attributes={xla.sdy.sharding=\"#sdy.sharding<@empty_mesh, [{}]>\"}\n  mul.2 = f32[8]{0} custom-call(args_1_.1), custom_call_target=\"xla.sdy.FuncResultSharding\", custom_call_has_side_effect=true, frontend_attributes={xla.sdy.sharding=\"#sdy.sharding_per_value<[<@empty_mesh, [{}]>]>\"}\n  reshape.2 = f32[8]{0} reshape(mul.2), sharding={replicated}\n  args_0_.1 = f32[8]{0} parameter(0), sharding={replicated}, frontend_attributes={xla.sdy.sharding=\"#sdy.sharding<@empty_mesh, [{}]>\"}\n  mul.3 = f32[8]{0} custom-call(args_0_.1), custom_call_target=\"xla.sdy.FuncResultSharding\", custom_call_has_side_effect=true, frontend_attributes={xla.sdy.sharding=\"#sdy.sharding_per_value<[<@empty_mesh, [{}]>]>\"}\n  reshape.3 = f32[8]{0} reshape(mul.3), sharding={replicated}\n  ROOT tuple.1 = (f32[8]{0}, f32[8]{0}) tuple(mul.2, mul.3), sharding={{replicated}, {replicated}}\n}\n\n"}
{"id": 2, "kernel_name": "vec_kowe", "python": "def vec_kowe(a, b):\n    return jnp.sum(a * b, axis=-1)", "type": "generate_vector_kernel", "cpp": "### FORWARD (CPU HLO)\nHloModule jit_forward, entry_computation_layout={(f32[8,3]{1,0}, f32[8,3]{1,0})->f32[8]{0}}, frontend_attributes={xla.sdy.meshes={empty_mesh = #sdy.mesh<[]>}}\n\nregion_0.1 {\n  reduce_sum.3 = f32[] parameter(0)\n  reduce_sum.4 = f32[] parameter(1)\n  ROOT reduce_sum.5 = f32[] add(reduce_sum.3, reduce_sum.4)\n}\n\nENTRY main.2 {\n  args_0_.1 = f32[8,3]{1,0} parameter(0), sharding={replicated}, frontend_attributes={xla.sdy.sharding=\"#sdy.sharding<@empty_mesh, [{}, {}]>\"}\n  args_1_.1 = f32[8,3]{1,0} parameter(1), sharding={replicated}, frontend_attributes={xla.sdy.sharding=\"#sdy.sharding<@empty_mesh, [{}, {}]>\"}\n  mul.1 = f32[8,3]{1,0} multiply(args_0_.1, args_1_.1)\n  constant.1 = f32[] constant(0)\n  reduce_sum.8 = f32[8]{0} reduce(mul.1, constant.1), dimensions={1}, to_apply=region_0.1\n  reduce_sum.9 = f32[8]{0} custom-call(reduce_sum.8), custom_call_target=\"xla.sdy.FuncResultSharding\", custom_call_has_side_effect=true, frontend_attributes={xla.sdy.sharding=\"#sdy.sharding_per_value<[<@empty_mesh, [{}]>]>\"}\n  tuple.1 = (f32[8]{0}) tuple(reduce_sum.9)\n  ROOT get-tuple-element.1 = f32[8]{0} get-tuple-element(tuple.1), index=0, sharding={replicated}\n}\n\n\n\n### BACKWARD (CPU HLO)\nHloModule jit_backward, entry_computation_layout={(f32[8,3]{1,0}, f32[8,3]{1,0})->(f32[8,3]{1,0}, f32[8,3]{1,0})}, frontend_attributes={xla.sdy.meshes={empty_mesh = #sdy.mesh<[]>}}\n\nENTRY main.1 {\n  args_1_.1 = f32[8,3]{1,0} parameter(1), sharding={replicated}, frontend_attributes={xla.sdy.sharding=\"#sdy.sharding<@empty_mesh, [{}, {}]>\"}\n  mul.2 = f32[8,3]{1,0} custom-call(args_1_.1), custom_call_target=\"xla.sdy.FuncResultSharding\", custom_call_has_side_effect=true, frontend_attributes={xla.sdy.sharding=\"#sdy.sharding_per_value<[<@empty_mesh, [{}, {}]>]>\"}\n  reshape.2 = f32[8,3]{1,0} reshape(mul.2), sharding={replicated}\n  args_0_.1 = f32[8,3]{1,0} parameter(0), sharding={replicated}, frontend_attributes={xla.sdy.sharding=\"#sdy.sharding<@empty_mesh, [{}, {}]>\"}\n  mul.3 = f32[8,3]{1,0} custom-call(args_0_.1), custom_call_target=\"xla.sdy.FuncResultSharding\", custom_call_has_side_effect=true, frontend_attributes={xla.sdy.sharding=\"#sdy.sharding_per_value<[<@empty_mesh, [{}, {}]>]>\"}\n  reshape.3 = f32[8,3]{1,0} reshape(mul.3), sharding={replicated}\n  ROOT tuple.1 = (f32[8,3]{1,0}, f32[8,3]{1,0}) tuple(mul.2, mul.3), sharding={{replicated}, {replicated}}\n}\n\n"}
{"id": 3, "kernel_name": "loop_hmci", "python": "def loop_hmci(a, n):\n    def body(i, acc):\n        return acc + a\n    init = jnp.zeros_like(a)\n    return lax.fori_loop(0, n, body, init)", "type": "generate_loop_kernel", "cpp": "### FORWARD (CPU HLO)\nHloModule jit_forward, entry_computation_layout={(f32[8]{0})->f32[8]{0}}, frontend_attributes={xla.sdy.meshes={empty_mesh = #sdy.mesh<[]>}}\n\nclosed_call.1 {\n  Arg_1.1 = f32[8]{0} parameter(1)\n  Arg_0.1 = f32[8]{0} parameter(0)\n  ROOT add.1 = f32[8]{0} add(Arg_1.1, Arg_0.1)\n}\n\nregion_0.2 {\n  arg_tuple.1 = (s32[], f32[8]{0}, f32[8]{0}) parameter(0)\n  get-tuple-element.3 = s32[] get-tuple-element(arg_tuple.1), index=0\n  constant.3 = s32[] constant(1)\n  add.3 = s32[] add(get-tuple-element.3, constant.3)\n  get-tuple-element.5 = f32[8]{0} get-tuple-element(arg_tuple.1), index=2\n  get-tuple-element.4 = f32[8]{0} get-tuple-element(arg_tuple.1), index=1\n  closed_call.1 = f32[8]{0} call(get-tuple-element.5, get-tuple-element.4), to_apply=closed_call.1\n  ROOT tuple.1 = (s32[], f32[8]{0}, f32[8]{0}) tuple(add.3, closed_call.1, get-tuple-element.5)\n}\n\nregion_1.3 {\n  arg_tuple.3 = (s32[], f32[8]{0}, f32[8]{0}) parameter(0)\n  get-tuple-element.10 = f32[8]{0} get-tuple-element(arg_tuple.3), index=1\n  get-tuple-element.11 = f32[8]{0} get-tuple-element(arg_tuple.3), index=2\n  get-tuple-element.9 = s32[] get-tuple-element(arg_tuple.3), index=0\n  constant.5 = s32[] constant(3)\n  ROOT lt.1 = pred[] compare(get-tuple-element.9, constant.5), direction=LT\n}\n\nENTRY main.4 {\n  constant.6 = s32[] constant(0)\n  constant.7 = f32[] constant(0)\n  broadcast.1 = f32[8]{0} broadcast(constant.7), dimensions={}\n  args_0_.1 = f32[8]{0} parameter(0), sharding={replicated}, frontend_attributes={xla.sdy.sharding=\"#sdy.sharding<@empty_mesh, [{}]>\"}\n  while.5 = (s32[], f32[8]{0}, f32[8]{0}) tuple(constant.6, broadcast.1, args_0_.1)\n  while.6 = (s32[], f32[8]{0}, f32[8]{0}) while(while.5), condition=region_1.3, body=region_0.2\n  while.7 = s32[] get-tuple-element(while.6), index=0\n  while.8 = f32[8]{0} get-tuple-element(while.6), index=1\n  while.9 = f32[8]{0} custom-call(while.8), custom_call_target=\"xla.sdy.FuncResultSharding\", custom_call_has_side_effect=true, frontend_attributes={xla.sdy.sharding=\"#sdy.sharding_per_value<[<@empty_mesh, [{}]>]>\"}\n  tuple.3 = (f32[8]{0}) tuple(while.9)\n  ROOT get-tuple-element.13 = f32[8]{0} get-tuple-element(tuple.3), index=0, sharding={replicated}\n}\n\n\n\n### BACKWARD (CPU HLO)\nHloModule jit_backward, entry_computation_layout={()->f32[8]{0}}, frontend_attributes={xla.sdy.meshes={empty_mesh = #sdy.mesh<[]>}}\n\nclosed_call.1 {\n  Arg_0.1 = f32[8]{0} parameter(0)\n  Arg_1.1 = f32[8]{0} parameter(1)\n  add_any.1 = f32[8]{0} add(Arg_0.1, Arg_1.1)\n  ROOT tuple.1 = (f32[8]{0}, f32[8]{0}) tuple(add_any.1, Arg_1.1)\n}\n\nregion_0.2 {\n  arg_tuple.1 = (s32[], f32[8]{0}, f32[8]{0}) parameter(0)\n  get-tuple-element.3 = s32[] get-tuple-element(arg_tuple.1), index=0\n  constant.4 = s32[] constant(1)\n  add.1 = s32[] add(get-tuple-element.3, constant.4)\n  get-tuple-element.4 = f32[8]{0} get-tuple-element(arg_tuple.1), index=1\n  get-tuple-element.5 = f32[8]{0} get-tuple-element(arg_tuple.1), index=2\n  closed_call.3 = (f32[8]{0}, f32[8]{0}) call(get-tuple-element.4, get-tuple-element.5), to_apply=closed_call.1\n  closed_call.4 = f32[8]{0} get-tuple-element(closed_call.3), index=0\n  closed_call.5 = f32[8]{0} get-tuple-element(closed_call.3), index=1\n  ROOT tuple.3 = (s32[], f32[8]{0}, f32[8]{0}) tuple(add.1, closed_call.4, closed_call.5)\n}\n\nregion_1.3 {\n  arg_tuple.3 = (s32[], f32[8]{0}, f32[8]{0}) parameter(0)\n  get-tuple-element.10 = f32[8]{0} get-tuple-element(arg_tuple.3), index=1\n  get-tuple-element.11 = f32[8]{0} get-tuple-element(arg_tuple.3), index=2\n  get-tuple-element.9 = s32[] get-tuple-element(arg_tuple.3), index=0\n  constant.6 = s32[] constant(3)\n  ROOT lt.1 = pred[] compare(get-tuple-element.9, constant.6), direction=LT\n}\n\nENTRY main.4 {\n  constant.7 = s32[] constant(0)\n  constant.9 = f32[] constant(0)\n  broadcast.3 = f32[8]{0} broadcast(constant.9), dimensions={}\n  constant.8 = f32[] constant(1)\n  broadcast.2 = f32[8]{0} broadcast(constant.8), dimensions={}\n  while.6 = (s32[], f32[8]{0}, f32[8]{0}) tuple(constant.7, broadcast.3, broadcast.2)\n  while.7 = (s32[], f32[8]{0}, f32[8]{0}) while(while.6), condition=region_1.3, body=region_0.2\n  while.8 = s32[] get-tuple-element(while.7), index=0\n  while.10 = f32[8]{0} get-tuple-element(while.7), index=2\n  while.9 = f32[8]{0} get-tuple-element(while.7), index=1\n  while.11 = f32[8]{0} custom-call(while.9), custom_call_target=\"xla.sdy.FuncResultSharding\", custom_call_has_side_effect=true, frontend_attributes={xla.sdy.sharding=\"#sdy.sharding_per_value<[<@empty_mesh, [{}]>]>\"}\n  tuple.5 = (f32[8]{0}) tuple(while.11)\n  ROOT get-tuple-element.13 = f32[8]{0} get-tuple-element(tuple.5), index=0, sharding={replicated}\n}\n\n"}
{"id": 4, "kernel_name": "scalar_arr_xkpw", "python": "def scalar_arr_xkpw(alpha, x, y):\n    return (alpha + x) * y", "type": "generate_scalar_array_op", "cpp": "### FORWARD (CPU HLO)\nHloModule jit_forward, entry_computation_layout={(f32[], f32[8]{0}, f32[8]{0})->f32[8]{0}}, frontend_attributes={xla.sdy.meshes={empty_mesh = #sdy.mesh<[]>}}\n\nENTRY main.1 {\n  args_0_.1 = f32[] parameter(0), sharding={replicated}, frontend_attributes={xla.sdy.sharding=\"#sdy.sharding<@empty_mesh, []>\"}\n  add.2 = f32[8]{0} broadcast(args_0_.1), dimensions={}\n  args_1_.1 = f32[8]{0} parameter(1), sharding={replicated}, frontend_attributes={xla.sdy.sharding=\"#sdy.sharding<@empty_mesh, [{}]>\"}\n  add.3 = f32[8]{0} add(add.2, args_1_.1)\n  args_2_.1 = f32[8]{0} parameter(2), sharding={replicated}, frontend_attributes={xla.sdy.sharding=\"#sdy.sharding<@empty_mesh, [{}]>\"}\n  mul.2 = f32[8]{0} multiply(add.3, args_2_.1)\n  mul.3 = f32[8]{0} custom-call(mul.2), custom_call_target=\"xla.sdy.FuncResultSharding\", custom_call_has_side_effect=true, frontend_attributes={xla.sdy.sharding=\"#sdy.sharding_per_value<[<@empty_mesh, [{}]>]>\"}\n  tuple.1 = (f32[8]{0}) tuple(mul.3)\n  ROOT get-tuple-element.1 = f32[8]{0} get-tuple-element(tuple.1), index=0, sharding={replicated}\n}\n\n\n\n### BACKWARD (CPU HLO)\nHloModule jit_backward, entry_computation_layout={(f32[], f32[8]{0}, f32[8]{0})->(f32[], f32[8]{0}, f32[8]{0})}, frontend_attributes={xla.sdy.meshes={empty_mesh = #sdy.mesh<[]>}}\n\nregion_0.1 {\n  reduce_sum.3 = f32[] parameter(0)\n  reduce_sum.4 = f32[] parameter(1)\n  ROOT reduce_sum.5 = f32[] add(reduce_sum.3, reduce_sum.4)\n}\n\nENTRY main.2 {\n  args_2_.1 = f32[8]{0} parameter(2), sharding={replicated}, frontend_attributes={xla.sdy.sharding=\"#sdy.sharding<@empty_mesh, [{}]>\"}\n  constant.1 = f32[] constant(0)\n  reduce_sum.7 = f32[] reduce(args_2_.1, constant.1), dimensions={0}, to_apply=region_0.1\n  convert_element_type.1 = f32[] custom-call(reduce_sum.7), custom_call_target=\"xla.sdy.FuncResultSharding\", custom_call_has_side_effect=true, frontend_attributes={xla.sdy.sharding=\"#sdy.sharding_per_value<[<@empty_mesh, []>]>\"}\n  reshape.3 = f32[] reshape(convert_element_type.1), sharding={replicated}\n  mul.2 = f32[8]{0} custom-call(args_2_.1), custom_call_target=\"xla.sdy.FuncResultSharding\", custom_call_has_side_effect=true, frontend_attributes={xla.sdy.sharding=\"#sdy.sharding_per_value<[<@empty_mesh, [{}]>]>\"}\n  reshape.4 = f32[8]{0} reshape(mul.2), sharding={replicated}\n  args_0_.1 = f32[] parameter(0), sharding={replicated}, frontend_attributes={xla.sdy.sharding=\"#sdy.sharding<@empty_mesh, []>\"}\n  add.2 = f32[8]{0} broadcast(args_0_.1), dimensions={}\n  args_1_.1 = f32[8]{0} parameter(1), sharding={replicated}, frontend_attributes={xla.sdy.sharding=\"#sdy.sharding<@empty_mesh, [{}]>\"}\n  add.3 = f32[8]{0} add(add.2, args_1_.1)\n  mul.3 = f32[8]{0} custom-call(add.3), custom_call_target=\"xla.sdy.FuncResultSharding\", custom_call_has_side_effect=true, frontend_attributes={xla.sdy.sharding=\"#sdy.sharding_per_value<[<@empty_mesh, [{}]>]>\"}\n  reshape.5 = f32[8]{0} reshape(mul.3), sharding={replicated}\n  ROOT tuple.1 = (f32[], f32[8]{0}, f32[8]{0}) tuple(convert_element_type.1, mul.2, mul.3), sharding={{replicated}, {replicated}, {replicated}}\n}\n\n"}
