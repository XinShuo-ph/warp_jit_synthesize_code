[
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>) -> (tensor<7xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (54,))",
    "input_info": [
      {
        "shape": [
          6,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,9]. let\n    b:f32[54] = reshape[dimensions=None new_sizes=(54,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x9xf32>) -> (tensor<54xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<6x9xf32>) -> tensor<54xf32>\n    return %0 : tensor<54xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9]. let\n    b:f32[8] = slice[limit_indices=(9,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>) -> (tensor<8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:9] : (tensor<9xf32>) -> tensor<8xf32>\n    return %0 : tensor<8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7] b:f32[7]. let\n    c:f32[14] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>, %arg1: tensor<7xf32>) -> (tensor<14xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<7xf32>, tensor<7xf32>) -> tensor<14xf32>\n    return %0 : tensor<14xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          10,
          9,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,9,6]. let\n    b:f32[6,9,10] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x9x6xf32>) -> (tensor<6x9x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<10x9x6xf32>) -> tensor<6x9x10xf32>\n    return %0 : tensor<6x9x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (7,))",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>) -> (tensor<7xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          9,
          10,
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9,
          10,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,10,5] b:f32[9,10,5]. let\n    c:f32[18,10,5] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x10x5xf32>, %arg1: tensor<9x10x5xf32>) -> (tensor<18x10x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<9x10x5xf32>, tensor<9x10x5xf32>) -> tensor<18x10x5xf32>\n    return %0 : tensor<18x10x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>) -> (tensor<9xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2,))",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>) -> (tensor<2xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          8,
          10,
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          10,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,10,3] b:f32[8,10,3]. let\n    c:f32[1,8,10,3] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 8, 10, 3)\n      sharding=None\n    ] a\n    d:f32[1,8,10,3] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 8, 10, 3)\n      sharding=None\n    ] b\n    e:f32[2,8,10,3] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x10x3xf32>, %arg1: tensor<8x10x3xf32>) -> (tensor<2x8x10x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<8x10x3xf32>) -> tensor<1x8x10x3xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<8x10x3xf32>) -> tensor<1x8x10x3xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x8x10x3xf32>, tensor<1x8x10x3xf32>) -> tensor<2x8x10x3xf32>\n    return %2 : tensor<2x8x10x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          8,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,2]. let\n    b:f32[2,8] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x2xf32>) -> (tensor<2x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<8x2xf32>) -> tensor<2x8xf32>\n    return %0 : tensor<2x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (350,))",
    "input_info": [
      {
        "shape": [
          7,
          10,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,10,5]. let\n    b:f32[350] = reshape[dimensions=None new_sizes=(350,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x10x5xf32>) -> (tensor<350xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<7x10x5xf32>) -> tensor<350xf32>\n    return %0 : tensor<350xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>) -> (tensor<9xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          7,
          10,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,10,6]. let\n    b:f32[6,10,7] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x10x6xf32>) -> (tensor<6x10x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<7x10x6xf32>) -> tensor<6x10x7xf32>\n    return %0 : tensor<6x10x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>) -> (tensor<5xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5] b:f32[5]. let\n    c:f32[10] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>, %arg1: tensor<5xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<5xf32>, tensor<5xf32>) -> tensor<10xf32>\n    return %0 : tensor<10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>) -> (tensor<9xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>) -> (tensor<6xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 162))",
    "input_info": [
      {
        "shape": [
          9,
          6,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,6,6]. let\n    b:f32[2,162] = reshape[dimensions=None new_sizes=(2, 162) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x6x6xf32>) -> (tensor<2x162xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<9x6x6xf32>) -> tensor<2x162xf32>\n    return %0 : tensor<2x162xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          8,
          10,
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          10,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,10,8] b:f32[8,10,8]. let\n    c:f32[1,8,10,8] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 8, 10, 8)\n      sharding=None\n    ] a\n    d:f32[1,8,10,8] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 8, 10, 8)\n      sharding=None\n    ] b\n    e:f32[2,8,10,8] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x10x8xf32>, %arg1: tensor<8x10x8xf32>) -> (tensor<2x8x10x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<8x10x8xf32>) -> tensor<1x8x10x8xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<8x10x8xf32>) -> tensor<1x8x10x8xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x8x10x8xf32>, tensor<1x8x10x8xf32>) -> tensor<2x8x10x8xf32>\n    return %2 : tensor<2x8x10x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          5,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,5]. let\n    b:f32[4,5] = slice[limit_indices=(5, 5) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x5xf32>) -> (tensor<4x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:5, 0:5] : (tensor<5x5xf32>) -> tensor<4x5xf32>\n    return %0 : tensor<4x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          7,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,8]. let\n    b:f32[8,7] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x8xf32>) -> (tensor<8x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<7x8xf32>) -> tensor<8x7xf32>\n    return %0 : tensor<8x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          5,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,2]. let\n    b:f32[2,5] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x2xf32>) -> (tensor<2x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<5x2xf32>) -> tensor<2x5xf32>\n    return %0 : tensor<2x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          7,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,3]. let\n    b:f32[3,7] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x3xf32>) -> (tensor<3x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<7x3xf32>) -> tensor<3x7xf32>\n    return %0 : tensor<3x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          8,
          7,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,7,9]. let\n    b:f32[9,7,8] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x7x9xf32>) -> (tensor<9x7x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<8x7x9xf32>) -> tensor<9x7x8xf32>\n    return %0 : tensor<9x7x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          2,
          4,
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2,
          4,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,4,2] b:f32[2,4,2]. let\n    c:f32[1,2,4,2] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 2, 4, 2)\n      sharding=None\n    ] a\n    d:f32[1,2,4,2] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 2, 4, 2)\n      sharding=None\n    ] b\n    e:f32[2,2,4,2] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x4x2xf32>, %arg1: tensor<2x4x2xf32>) -> (tensor<2x2x4x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<2x4x2xf32>) -> tensor<1x2x4x2xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<2x4x2xf32>) -> tensor<1x2x4x2xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x2x4x2xf32>, tensor<1x2x4x2xf32>) -> tensor<2x2x4x2xf32>\n    return %2 : tensor<2x2x4x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          8,
          2,
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          2,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,2,5] b:f32[8,2,5]. let\n    c:f32[16,2,5] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x2x5xf32>, %arg1: tensor<8x2x5xf32>) -> (tensor<16x2x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<8x2x5xf32>, tensor<8x2x5xf32>) -> tensor<16x2x5xf32>\n    return %0 : tensor<16x2x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (10,))",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          8,
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,5] b:f32[8,5]. let\n    c:f32[16,5] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x5xf32>, %arg1: tensor<8x5xf32>) -> (tensor<16x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<8x5xf32>, tensor<8x5xf32>) -> tensor<16x5xf32>\n    return %0 : tensor<16x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          6,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,6]. let\n    b:f32[5,6] = slice[limit_indices=(6, 6) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x6xf32>) -> (tensor<5x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:6, 0:6] : (tensor<6x6xf32>) -> tensor<5x6xf32>\n    return %0 : tensor<5x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (16,))",
    "input_info": [
      {
        "shape": [
          4,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,4]. let\n    b:f32[16] = reshape[dimensions=None new_sizes=(16,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x4xf32>) -> (tensor<16xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<4x4xf32>) -> tensor<16xf32>\n    return %0 : tensor<16xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          5,
          4,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,4,3]. let\n    b:f32[4,4,3] = slice[\n      limit_indices=(5, 4, 3)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x4x3xf32>) -> (tensor<4x4x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:5, 0:4, 0:3] : (tensor<5x4x3xf32>) -> tensor<4x4x3xf32>\n    return %0 : tensor<4x4x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          6,
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,6] b:f32[6,6]. let\n    c:f32[1,6,6] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 6, 6)\n      sharding=None\n    ] a\n    d:f32[1,6,6] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 6, 6)\n      sharding=None\n    ] b\n    e:f32[2,6,6] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x6xf32>, %arg1: tensor<6x6xf32>) -> (tensor<2x6x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<6x6xf32>) -> tensor<1x6x6xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<6x6xf32>) -> tensor<1x6x6xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x6x6xf32>, tensor<1x6x6xf32>) -> tensor<2x6x6xf32>\n    return %2 : tensor<2x6x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          5,
          6,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,6,7]. let\n    b:f32[4,6,7] = slice[\n      limit_indices=(5, 6, 7)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x6x7xf32>) -> (tensor<4x6x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:5, 0:6, 0:7] : (tensor<5x6x7xf32>) -> tensor<4x6x7xf32>\n    return %0 : tensor<4x6x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          9,
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,6] b:f32[9,6]. let\n    c:f32[1,9,6] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 9, 6)\n      sharding=None\n    ] a\n    d:f32[1,9,6] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 9, 6)\n      sharding=None\n    ] b\n    e:f32[2,9,6] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x6xf32>, %arg1: tensor<9x6xf32>) -> (tensor<2x9x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<9x6xf32>) -> tensor<1x9x6xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<9x6xf32>) -> tensor<1x9x6xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x9x6xf32>, tensor<1x9x6xf32>) -> tensor<2x9x6xf32>\n    return %2 : tensor<2x9x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          5,
          9,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,9,3]. let\n    b:f32[4,9,3] = slice[\n      limit_indices=(5, 9, 3)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x9x3xf32>) -> (tensor<4x9x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:5, 0:9, 0:3] : (tensor<5x9x3xf32>) -> tensor<4x9x3xf32>\n    return %0 : tensor<4x9x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          8,
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,5] b:f32[8,5]. let\n    c:f32[1,8,5] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 8, 5)\n      sharding=None\n    ] a\n    d:f32[1,8,5] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 8, 5)\n      sharding=None\n    ] b\n    e:f32[2,8,5] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x5xf32>, %arg1: tensor<8x5xf32>) -> (tensor<2x8x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<8x5xf32>) -> tensor<1x8x5xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<8x5xf32>) -> tensor<1x8x5xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x8x5xf32>, tensor<1x8x5xf32>) -> tensor<2x8x5xf32>\n    return %2 : tensor<2x8x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 20))",
    "input_info": [
      {
        "shape": [
          5,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,8]. let\n    b:f32[2,20] = reshape[dimensions=None new_sizes=(2, 20) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x8xf32>) -> (tensor<2x20xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<5x8xf32>) -> tensor<2x20xf32>\n    return %0 : tensor<2x20xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          10,
          9,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,9,7]. let\n    b:f32[7,9,10] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x9x7xf32>) -> (tensor<7x9x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<10x9x7xf32>) -> tensor<7x9x10xf32>\n    return %0 : tensor<7x9x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (21,))",
    "input_info": [
      {
        "shape": [
          3,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,7]. let\n    b:f32[21] = reshape[dimensions=None new_sizes=(21,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x7xf32>) -> (tensor<21xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<3x7xf32>) -> tensor<21xf32>\n    return %0 : tensor<21xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          6,
          8,
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6,
          8,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,8,4] b:f32[6,8,4]. let\n    c:f32[12,8,4] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x8x4xf32>, %arg1: tensor<6x8x4xf32>) -> (tensor<12x8x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<6x8x4xf32>, tensor<6x8x4xf32>) -> tensor<12x8x4xf32>\n    return %0 : tensor<12x8x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          7,
          8,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,8,2]. let\n    b:f32[2,8,7] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x8x2xf32>) -> (tensor<2x8x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<7x8x2xf32>) -> tensor<2x8x7xf32>\n    return %0 : tensor<2x8x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (28,))",
    "input_info": [
      {
        "shape": [
          4,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,7]. let\n    b:f32[28] = reshape[dimensions=None new_sizes=(28,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x7xf32>) -> (tensor<28xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<4x7xf32>) -> tensor<28xf32>\n    return %0 : tensor<28xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 18))",
    "input_info": [
      {
        "shape": [
          4,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,9]. let\n    b:f32[2,18] = reshape[dimensions=None new_sizes=(2, 18) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x9xf32>) -> (tensor<2x18xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<4x9xf32>) -> tensor<2x18xf32>\n    return %0 : tensor<2x18xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>) -> (tensor<3xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (5,))",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>) -> (tensor<5xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4] b:f32[4]. let\n    c:f32[8] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> (tensor<8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<4xf32>, tensor<4xf32>) -> tensor<8xf32>\n    return %0 : tensor<8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          3,
          2,
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3,
          2,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,2,2] b:f32[3,2,2]. let\n    c:f32[6,2,2] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x2x2xf32>, %arg1: tensor<3x2x2xf32>) -> (tensor<6x2x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<3x2x2xf32>, tensor<3x2x2xf32>) -> tensor<6x2x2xf32>\n    return %0 : tensor<6x2x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          5,
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,9] b:f32[5,9]. let\n    c:f32[1,5,9] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 5, 9)\n      sharding=None\n    ] a\n    d:f32[1,5,9] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 5, 9)\n      sharding=None\n    ] b\n    e:f32[2,5,9] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x9xf32>, %arg1: tensor<5x9xf32>) -> (tensor<2x5x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<5x9xf32>) -> tensor<1x5x9xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<5x9xf32>) -> tensor<1x5x9xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x5x9xf32>, tensor<1x5x9xf32>) -> tensor<2x5x9xf32>\n    return %2 : tensor<2x5x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          5,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,2]. let\n    b:f32[2,5] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x2xf32>) -> (tensor<2x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<5x2xf32>) -> tensor<2x5xf32>\n    return %0 : tensor<2x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10] b:f32[10]. let\n    c:f32[20] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>, %arg1: tensor<10xf32>) -> (tensor<20xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<10xf32>, tensor<10xf32>) -> tensor<20xf32>\n    return %0 : tensor<20xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          7,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,8]. let\n    b:f32[8,7] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x8xf32>) -> (tensor<8x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<7x8xf32>) -> tensor<8x7xf32>\n    return %0 : tensor<8x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          4,
          8,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,8,9]. let\n    b:f32[3,8,9] = slice[\n      limit_indices=(4, 8, 9)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x8x9xf32>) -> (tensor<3x8x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:4, 0:8, 0:9] : (tensor<4x8x9xf32>) -> tensor<3x8x9xf32>\n    return %0 : tensor<3x8x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          8,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,10]. let\n    b:f32[10,8] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x10xf32>) -> (tensor<10x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<8x10xf32>) -> tensor<10x8xf32>\n    return %0 : tensor<10x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 2))",
    "input_info": [
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4]. let\n    b:f32[2,2] = reshape[dimensions=None new_sizes=(2, 2) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<2x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<4xf32>) -> tensor<2x2xf32>\n    return %0 : tensor<2x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (5, 7))",
    "input_info": [
      {
        "shape": [
          5,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,7]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x7xf32>) -> (tensor<5x7xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<5x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          4,
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,10] b:f32[4,10]. let\n    c:f32[1,4,10] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 4, 10)\n      sharding=None\n    ] a\n    d:f32[1,4,10] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 4, 10)\n      sharding=None\n    ] b\n    e:f32[2,4,10] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x10xf32>, %arg1: tensor<4x10xf32>) -> (tensor<2x4x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<4x10xf32>) -> tensor<1x4x10xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<4x10xf32>) -> tensor<1x4x10xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x4x10xf32>, tensor<1x4x10xf32>) -> tensor<2x4x10xf32>\n    return %2 : tensor<2x4x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9] b:f32[9]. let\n    c:f32[18] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>, %arg1: tensor<9xf32>) -> (tensor<18xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<9xf32>, tensor<9xf32>) -> tensor<18xf32>\n    return %0 : tensor<18xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3] b:f32[3]. let\n    c:f32[6] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>, %arg1: tensor<3xf32>) -> (tensor<6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<3xf32>, tensor<3xf32>) -> tensor<6xf32>\n    return %0 : tensor<6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (28,))",
    "input_info": [
      {
        "shape": [
          7,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,4]. let\n    b:f32[28] = reshape[dimensions=None new_sizes=(28,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x4xf32>) -> (tensor<28xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<7x4xf32>) -> tensor<28xf32>\n    return %0 : tensor<28xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          4,
          7,
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4,
          7,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,7,6] b:f32[4,7,6]. let\n    c:f32[1,4,7,6] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 4, 7, 6)\n      sharding=None\n    ] a\n    d:f32[1,4,7,6] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 4, 7, 6)\n      sharding=None\n    ] b\n    e:f32[2,4,7,6] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x7x6xf32>, %arg1: tensor<4x7x6xf32>) -> (tensor<2x4x7x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<4x7x6xf32>) -> tensor<1x4x7x6xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<4x7x6xf32>) -> tensor<1x4x7x6xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x4x7x6xf32>, tensor<1x4x7x6xf32>) -> tensor<2x4x7x6xf32>\n    return %2 : tensor<2x4x7x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          7,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,10]. let\n    b:f32[6,10] = slice[limit_indices=(7, 10) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x10xf32>) -> (tensor<6x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:7, 0:10] : (tensor<7x10xf32>) -> tensor<6x10xf32>\n    return %0 : tensor<6x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (5,))",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>) -> (tensor<5xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          7,
          10,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,10,5]. let\n    b:f32[6,10,5] = slice[\n      limit_indices=(7, 10, 5)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x10x5xf32>) -> (tensor<6x10x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:7, 0:10, 0:5] : (tensor<7x10x5xf32>) -> tensor<6x10x5xf32>\n    return %0 : tensor<6x10x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          7,
          8,
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7,
          8,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,8,7] b:f32[7,8,7]. let\n    c:f32[1,7,8,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 7, 8, 7)\n      sharding=None\n    ] a\n    d:f32[1,7,8,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 7, 8, 7)\n      sharding=None\n    ] b\n    e:f32[2,7,8,7] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x8x7xf32>, %arg1: tensor<7x8x7xf32>) -> (tensor<2x7x8x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<7x8x7xf32>) -> tensor<1x7x8x7xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<7x8x7xf32>) -> tensor<1x7x8x7xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x7x8x7xf32>, tensor<1x7x8x7xf32>) -> tensor<2x7x8x7xf32>\n    return %2 : tensor<2x7x8x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10] b:f32[10]. let\n    c:f32[20] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>, %arg1: tensor<10xf32>) -> (tensor<20xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<10xf32>, tensor<10xf32>) -> tensor<20xf32>\n    return %0 : tensor<20xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 8))",
    "input_info": [
      {
        "shape": [
          2,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,8]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x8xf32>) -> (tensor<2x8xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<2x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          6,
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,2] b:f32[6,2]. let\n    c:f32[1,6,2] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 6, 2)\n      sharding=None\n    ] a\n    d:f32[1,6,2] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 6, 2)\n      sharding=None\n    ] b\n    e:f32[2,6,2] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x2xf32>, %arg1: tensor<6x2xf32>) -> (tensor<2x6x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<6x2xf32>) -> tensor<1x6x2xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<6x2xf32>) -> tensor<1x6x2xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x6x2xf32>, tensor<1x6x2xf32>) -> tensor<2x6x2xf32>\n    return %2 : tensor<2x6x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          3,
          3,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,3,5]. let\n    b:f32[2,3,5] = slice[\n      limit_indices=(3, 3, 5)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x3x5xf32>) -> (tensor<2x3x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:3, 0:3, 0:5] : (tensor<3x3x5xf32>) -> tensor<2x3x5xf32>\n    return %0 : tensor<2x3x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>) -> (tensor<3xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          3,
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,8] b:f32[3,8]. let\n    c:f32[6,8] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x8xf32>, %arg1: tensor<3x8xf32>) -> (tensor<6x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<3x8xf32>, tensor<3x8xf32>) -> tensor<6x8xf32>\n    return %0 : tensor<6x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          6,
          5,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,5,6]. let\n    b:f32[5,5,6] = slice[\n      limit_indices=(6, 5, 6)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x5x6xf32>) -> (tensor<5x5x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:6, 0:5, 0:6] : (tensor<6x5x6xf32>) -> tensor<5x5x6xf32>\n    return %0 : tensor<5x5x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          9,
          3,
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9,
          3,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,3,10] b:f32[9,3,10]. let\n    c:f32[18,3,10] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x3x10xf32>, %arg1: tensor<9x3x10xf32>) -> (tensor<18x3x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<9x3x10xf32>, tensor<9x3x10xf32>) -> tensor<18x3x10xf32>\n    return %0 : tensor<18x3x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          8,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,4]. let\n    b:f32[7,4] = slice[limit_indices=(8, 4) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x4xf32>) -> (tensor<7x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:8, 0:4] : (tensor<8x4xf32>) -> tensor<7x4xf32>\n    return %0 : tensor<7x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          6,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,7]. let\n    b:f32[7,6] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x7xf32>) -> (tensor<7x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<6x7xf32>) -> tensor<7x6xf32>\n    return %0 : tensor<7x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          9,
          4,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,4,9]. let\n    b:f32[9,4,9] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x4x9xf32>) -> (tensor<9x4x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<9x4x9xf32>) -> tensor<9x4x9xf32>\n    return %0 : tensor<9x4x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          6,
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,4] b:f32[6,4]. let\n    c:f32[1,6,4] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 6, 4)\n      sharding=None\n    ] a\n    d:f32[1,6,4] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 6, 4)\n      sharding=None\n    ] b\n    e:f32[2,6,4] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x4xf32>, %arg1: tensor<6x4xf32>) -> (tensor<2x6x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<6x4xf32>) -> tensor<1x6x4xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<6x4xf32>) -> tensor<1x6x4xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x6x4xf32>, tensor<1x6x4xf32>) -> tensor<2x6x4xf32>\n    return %2 : tensor<2x6x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (10,))",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          8,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,8]. let\n    b:f32[7,8] = slice[limit_indices=(8, 8) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x8xf32>) -> (tensor<7x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:8, 0:8] : (tensor<8x8xf32>) -> tensor<7x8xf32>\n    return %0 : tensor<7x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10]. let\n    b:f32[9] = slice[limit_indices=(10,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:10] : (tensor<10xf32>) -> tensor<9xf32>\n    return %0 : tensor<9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          10,
          8,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,8,4]. let\n    b:f32[9,8,4] = slice[\n      limit_indices=(10, 8, 4)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x8x4xf32>) -> (tensor<9x8x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:10, 0:8, 0:4] : (tensor<10x8x4xf32>) -> tensor<9x8x4xf32>\n    return %0 : tensor<9x8x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 50))",
    "input_info": [
      {
        "shape": [
          10,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,10]. let\n    b:f32[2,50] = reshape[dimensions=None new_sizes=(2, 50) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x10xf32>) -> (tensor<2x50xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<10x10xf32>) -> tensor<2x50xf32>\n    return %0 : tensor<2x50xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          8,
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,6] b:f32[8,6]. let\n    c:f32[16,6] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x6xf32>, %arg1: tensor<8x6xf32>) -> (tensor<16x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<8x6xf32>, tensor<8x6xf32>) -> tensor<16x6xf32>\n    return %0 : tensor<16x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3]. let\n    b:f32[2] = slice[limit_indices=(3,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>) -> (tensor<2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:3] : (tensor<3xf32>) -> tensor<2xf32>\n    return %0 : tensor<2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (48,))",
    "input_info": [
      {
        "shape": [
          6,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,8]. let\n    b:f32[48] = reshape[dimensions=None new_sizes=(48,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x8xf32>) -> (tensor<48xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<6x8xf32>) -> tensor<48xf32>\n    return %0 : tensor<48xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          6,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,4]. let\n    b:f32[4,6] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x4xf32>) -> (tensor<4x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<6x4xf32>) -> tensor<4x6xf32>\n    return %0 : tensor<4x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7] b:f32[7]. let\n    c:f32[1,7] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 7)\n      sharding=None\n    ] a\n    d:f32[1,7] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 7)\n      sharding=None\n    ] b\n    e:f32[2,7] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>, %arg1: tensor<7xf32>) -> (tensor<2x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<7xf32>) -> tensor<1x7xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<7xf32>) -> tensor<1x7xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x7xf32>, tensor<1x7xf32>) -> tensor<2x7xf32>\n    return %2 : tensor<2x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          3,
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,7] b:f32[3,7]. let\n    c:f32[6,7] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x7xf32>, %arg1: tensor<3x7xf32>) -> (tensor<6x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<3x7xf32>, tensor<3x7xf32>) -> tensor<6x7xf32>\n    return %0 : tensor<6x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3] b:f32[3]. let\n    c:f32[1,3] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 3)\n      sharding=None\n    ] a\n    d:f32[1,3] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 3)\n      sharding=None\n    ] b\n    e:f32[2,3] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>, %arg1: tensor<3xf32>) -> (tensor<2x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<3xf32>) -> tensor<1x3xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<3xf32>) -> tensor<1x3xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x3xf32>, tensor<1x3xf32>) -> tensor<2x3xf32>\n    return %2 : tensor<2x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          7,
          6,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,6,9]. let\n    b:f32[9,6,7] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x6x9xf32>) -> (tensor<9x6x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<7x6x9xf32>) -> tensor<9x6x7xf32>\n    return %0 : tensor<9x6x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          9,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,6]. let\n    b:f32[6,9] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x6xf32>) -> (tensor<6x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<9x6xf32>) -> tensor<6x9xf32>\n    return %0 : tensor<6x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          3,
          2,
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3,
          2,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,2,5] b:f32[3,2,5]. let\n    c:f32[1,3,2,5] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 3, 2, 5)\n      sharding=None\n    ] a\n    d:f32[1,3,2,5] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 3, 2, 5)\n      sharding=None\n    ] b\n    e:f32[2,3,2,5] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x2x5xf32>, %arg1: tensor<3x2x5xf32>) -> (tensor<2x3x2x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<3x2x5xf32>) -> tensor<1x3x2x5xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<3x2x5xf32>) -> tensor<1x3x2x5xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x3x2x5xf32>, tensor<1x3x2x5xf32>) -> tensor<2x3x2x5xf32>\n    return %2 : tensor<2x3x2x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          9,
          8,
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9,
          8,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,8,4] b:f32[9,8,4]. let\n    c:f32[18,8,4] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x8x4xf32>, %arg1: tensor<9x8x4xf32>) -> (tensor<18x8x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<9x8x4xf32>, tensor<9x8x4xf32>) -> tensor<18x8x4xf32>\n    return %0 : tensor<18x8x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          9,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,10]. let\n    b:f32[8,10] = slice[limit_indices=(9, 10) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x10xf32>) -> (tensor<8x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:9, 0:10] : (tensor<9x10xf32>) -> tensor<8x10xf32>\n    return %0 : tensor<8x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          10,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,3]. let\n    b:f32[3,10] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x3xf32>) -> (tensor<3x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<10x3xf32>) -> tensor<3x10xf32>\n    return %0 : tensor<3x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (49,))",
    "input_info": [
      {
        "shape": [
          7,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,7]. let\n    b:f32[49] = reshape[dimensions=None new_sizes=(49,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x7xf32>) -> (tensor<49xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<7x7xf32>) -> tensor<49xf32>\n    return %0 : tensor<49xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          7,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,7]. let\n    b:f32[6,7] = slice[limit_indices=(7, 7) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x7xf32>) -> (tensor<6x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:7, 0:7] : (tensor<7x7xf32>) -> tensor<6x7xf32>\n    return %0 : tensor<6x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5] b:f32[5]. let\n    c:f32[10] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>, %arg1: tensor<5xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<5xf32>, tensor<5xf32>) -> tensor<10xf32>\n    return %0 : tensor<10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          10,
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,6] b:f32[10,6]. let\n    c:f32[1,10,6] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 10, 6)\n      sharding=None\n    ] a\n    d:f32[1,10,6] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 10, 6)\n      sharding=None\n    ] b\n    e:f32[2,10,6] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x6xf32>, %arg1: tensor<10x6xf32>) -> (tensor<2x10x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<10x6xf32>) -> tensor<1x10x6xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<10x6xf32>) -> tensor<1x10x6xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x10x6xf32>, tensor<1x10x6xf32>) -> tensor<2x10x6xf32>\n    return %2 : tensor<2x10x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          7,
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,4] b:f32[7,4]. let\n    c:f32[1,7,4] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 7, 4)\n      sharding=None\n    ] a\n    d:f32[1,7,4] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 7, 4)\n      sharding=None\n    ] b\n    e:f32[2,7,4] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x4xf32>, %arg1: tensor<7x4xf32>) -> (tensor<2x7x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<7x4xf32>) -> tensor<1x7x4xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<7x4xf32>) -> tensor<1x7x4xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x7x4xf32>, tensor<1x7x4xf32>) -> tensor<2x7x4xf32>\n    return %2 : tensor<2x7x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          3,
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,3] b:f32[3,3]. let\n    c:f32[6,3] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x3xf32>, %arg1: tensor<3x3xf32>) -> (tensor<6x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<3x3xf32>, tensor<3x3xf32>) -> tensor<6x3xf32>\n    return %0 : tensor<6x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          2,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,9]. let\n    b:f32[1,9] = slice[limit_indices=(2, 9) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x9xf32>) -> (tensor<1x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:2, 0:9] : (tensor<2x9xf32>) -> tensor<1x9xf32>\n    return %0 : tensor<1x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          6,
          3,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,3,3]. let\n    b:f32[3,3,6] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x3x3xf32>) -> (tensor<3x3x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<6x3x3xf32>) -> tensor<3x3x6xf32>\n    return %0 : tensor<3x3x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          2,
          3,
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2,
          3,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,3,7] b:f32[2,3,7]. let\n    c:f32[4,3,7] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x3x7xf32>, %arg1: tensor<2x3x7xf32>) -> (tensor<4x3x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<2x3x7xf32>, tensor<2x3x7xf32>) -> tensor<4x3x7xf32>\n    return %0 : tensor<4x3x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9] b:f32[9]. let\n    c:f32[18] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>, %arg1: tensor<9xf32>) -> (tensor<18xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<9xf32>, tensor<9xf32>) -> tensor<18xf32>\n    return %0 : tensor<18xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>) -> (tensor<3xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (75,))",
    "input_info": [
      {
        "shape": [
          5,
          5,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,5,3]. let\n    b:f32[75] = reshape[dimensions=None new_sizes=(75,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x5x3xf32>) -> (tensor<75xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<5x5x3xf32>) -> tensor<75xf32>\n    return %0 : tensor<75xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          3,
          3,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,3,10]. let\n    b:f32[2,3,10] = slice[\n      limit_indices=(3, 3, 10)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x3x10xf32>) -> (tensor<2x3x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:3, 0:3, 0:10] : (tensor<3x3x10xf32>) -> tensor<2x3x10xf32>\n    return %0 : tensor<2x3x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3] b:f32[3]. let\n    c:f32[6] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>, %arg1: tensor<3xf32>) -> (tensor<6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<3xf32>, tensor<3xf32>) -> tensor<6xf32>\n    return %0 : tensor<6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (3, 105))",
    "input_info": [
      {
        "shape": [
          5,
          9,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,9,7]. let\n    b:f32[3,105] = reshape[dimensions=None new_sizes=(3, 105) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x9x7xf32>) -> (tensor<3x105xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<5x9x7xf32>) -> tensor<3x105xf32>\n    return %0 : tensor<3x105xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 20))",
    "input_info": [
      {
        "shape": [
          10,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,4]. let\n    b:f32[2,20] = reshape[dimensions=None new_sizes=(2, 20) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x4xf32>) -> (tensor<2x20xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<10x4xf32>) -> tensor<2x20xf32>\n    return %0 : tensor<2x20xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          8,
          2,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,2,6]. let\n    b:f32[6,2,8] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x2x6xf32>) -> (tensor<6x2x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<8x2x6xf32>) -> tensor<6x2x8xf32>\n    return %0 : tensor<6x2x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          3,
          8,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,8,9]. let\n    b:f32[2,8,9] = slice[\n      limit_indices=(3, 8, 9)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x8x9xf32>) -> (tensor<2x8x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:3, 0:8, 0:9] : (tensor<3x8x9xf32>) -> tensor<2x8x9xf32>\n    return %0 : tensor<2x8x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          9,
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,5] b:f32[9,5]. let\n    c:f32[1,9,5] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 9, 5)\n      sharding=None\n    ] a\n    d:f32[1,9,5] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 9, 5)\n      sharding=None\n    ] b\n    e:f32[2,9,5] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x5xf32>, %arg1: tensor<9x5xf32>) -> (tensor<2x9x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<9x5xf32>) -> tensor<1x9x5xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<9x5xf32>) -> tensor<1x9x5xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x9x5xf32>, tensor<1x9x5xf32>) -> tensor<2x9x5xf32>\n    return %2 : tensor<2x9x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5] b:f32[5]. let\n    c:f32[1,5] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 5)\n      sharding=None\n    ] a\n    d:f32[1,5] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 5)\n      sharding=None\n    ] b\n    e:f32[2,5] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>, %arg1: tensor<5xf32>) -> (tensor<2x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<5xf32>) -> tensor<1x5xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<5xf32>) -> tensor<1x5xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x5xf32>, tensor<1x5xf32>) -> tensor<2x5xf32>\n    return %2 : tensor<2x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 196))",
    "input_info": [
      {
        "shape": [
          7,
          8,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,8,7]. let\n    b:f32[2,196] = reshape[dimensions=None new_sizes=(2, 196) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x8x7xf32>) -> (tensor<2x196xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<7x8x7xf32>) -> tensor<2x196xf32>\n    return %0 : tensor<2x196xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          2,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,7]. let\n    b:f32[1,7] = slice[limit_indices=(2, 7) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x7xf32>) -> (tensor<1x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:2, 0:7] : (tensor<2x7xf32>) -> tensor<1x7xf32>\n    return %0 : tensor<1x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          5,
          3,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,3,6]. let\n    b:f32[6,3,5] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x3x6xf32>) -> (tensor<6x3x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<5x3x6xf32>) -> tensor<6x3x5xf32>\n    return %0 : tensor<6x3x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (10,))",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          4,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,3]. let\n    b:f32[3,3] = slice[limit_indices=(4, 3) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x3xf32>) -> (tensor<3x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:4, 0:3] : (tensor<4x3xf32>) -> tensor<3x3xf32>\n    return %0 : tensor<3x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          9,
          8,
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9,
          8,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,8,3] b:f32[9,8,3]. let\n    c:f32[18,8,3] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x8x3xf32>, %arg1: tensor<9x8x3xf32>) -> (tensor<18x8x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<9x8x3xf32>, tensor<9x8x3xf32>) -> tensor<18x8x3xf32>\n    return %0 : tensor<18x8x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8xf32>) -> (tensor<8xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (30,))",
    "input_info": [
      {
        "shape": [
          10,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,3]. let\n    b:f32[30] = reshape[dimensions=None new_sizes=(30,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x3xf32>) -> (tensor<30xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<10x3xf32>) -> tensor<30xf32>\n    return %0 : tensor<30xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          9,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,3]. let\n    b:f32[3,9] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x3xf32>) -> (tensor<3x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<9x3xf32>) -> tensor<3x9xf32>\n    return %0 : tensor<3x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5] b:f32[5]. let\n    c:f32[1,5] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 5)\n      sharding=None\n    ] a\n    d:f32[1,5] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 5)\n      sharding=None\n    ] b\n    e:f32[2,5] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>, %arg1: tensor<5xf32>) -> (tensor<2x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<5xf32>) -> tensor<1x5xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<5xf32>) -> tensor<1x5xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x5xf32>, tensor<1x5xf32>) -> tensor<2x5xf32>\n    return %2 : tensor<2x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 6))",
    "input_info": [
      {
        "shape": [
          2,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,6]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x6xf32>) -> (tensor<2x6xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<2x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (270,))",
    "input_info": [
      {
        "shape": [
          6,
          5,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,5,9]. let\n    b:f32[270] = reshape[dimensions=None new_sizes=(270,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x5x9xf32>) -> (tensor<270xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<6x5x9xf32>) -> tensor<270xf32>\n    return %0 : tensor<270xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (162,))",
    "input_info": [
      {
        "shape": [
          9,
          6,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,6,3]. let\n    b:f32[162] = reshape[dimensions=None new_sizes=(162,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x6x3xf32>) -> (tensor<162xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<9x6x3xf32>) -> tensor<162xf32>\n    return %0 : tensor<162xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7] b:f32[7]. let\n    c:f32[14] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>, %arg1: tensor<7xf32>) -> (tensor<14xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<7xf32>, tensor<7xf32>) -> tensor<14xf32>\n    return %0 : tensor<14xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 60))",
    "input_info": [
      {
        "shape": [
          5,
          3,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,3,8]. let\n    b:f32[2,60] = reshape[dimensions=None new_sizes=(2, 60) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x3x8xf32>) -> (tensor<2x60xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<5x3x8xf32>) -> tensor<2x60xf32>\n    return %0 : tensor<2x60xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          10,
          3,
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10,
          3,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,3,2] b:f32[10,3,2]. let\n    c:f32[1,10,3,2] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 10, 3, 2)\n      sharding=None\n    ] a\n    d:f32[1,10,3,2] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 10, 3, 2)\n      sharding=None\n    ] b\n    e:f32[2,10,3,2] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x3x2xf32>, %arg1: tensor<10x3x2xf32>) -> (tensor<2x10x3x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<10x3x2xf32>) -> tensor<1x10x3x2xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<10x3x2xf32>) -> tensor<1x10x3x2xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x10x3x2xf32>, tensor<1x10x3x2xf32>) -> tensor<2x10x3x2xf32>\n    return %2 : tensor<2x10x3x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          4,
          5,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,5,6]. let\n    b:f32[6,5,4] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x5x6xf32>) -> (tensor<6x5x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<4x5x6xf32>) -> tensor<6x5x4xf32>\n    return %0 : tensor<6x5x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          10,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,8]. let\n    b:f32[9,8] = slice[limit_indices=(10, 8) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x8xf32>) -> (tensor<9x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:10, 0:8] : (tensor<10x8xf32>) -> tensor<9x8xf32>\n    return %0 : tensor<9x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 5))",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10]. let\n    b:f32[2,5] = reshape[dimensions=None new_sizes=(2, 5) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<2x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<10xf32>) -> tensor<2x5xf32>\n    return %0 : tensor<2x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          4,
          5,
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4,
          5,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,5,9] b:f32[4,5,9]. let\n    c:f32[8,5,9] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x5x9xf32>, %arg1: tensor<4x5x9xf32>) -> (tensor<8x5x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<4x5x9xf32>, tensor<4x5x9xf32>) -> tensor<8x5x9xf32>\n    return %0 : tensor<8x5x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7]. let\n    b:f32[6] = slice[limit_indices=(7,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>) -> (tensor<6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:7] : (tensor<7xf32>) -> tensor<6xf32>\n    return %0 : tensor<6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          10,
          9,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,9,3]. let\n    b:f32[9,9,3] = slice[\n      limit_indices=(10, 9, 3)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x9x3xf32>) -> (tensor<9x9x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:10, 0:9, 0:3] : (tensor<10x9x3xf32>) -> tensor<9x9x3xf32>\n    return %0 : tensor<9x9x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          4,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,10]. let\n    b:f32[3,10] = slice[limit_indices=(4, 10) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x10xf32>) -> (tensor<3x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:4, 0:10] : (tensor<4x10xf32>) -> tensor<3x10xf32>\n    return %0 : tensor<3x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          9,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,10]. let\n    b:f32[8,10] = slice[limit_indices=(9, 10) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x10xf32>) -> (tensor<8x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:9, 0:10] : (tensor<9x10xf32>) -> tensor<8x10xf32>\n    return %0 : tensor<8x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          5,
          3,
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5,
          3,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,3,4] b:f32[5,3,4]. let\n    c:f32[1,5,3,4] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 5, 3, 4)\n      sharding=None\n    ] a\n    d:f32[1,5,3,4] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 5, 3, 4)\n      sharding=None\n    ] b\n    e:f32[2,5,3,4] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x3x4xf32>, %arg1: tensor<5x3x4xf32>) -> (tensor<2x5x3x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<5x3x4xf32>) -> tensor<1x5x3x4xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<5x3x4xf32>) -> tensor<1x5x3x4xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x5x3x4xf32>, tensor<1x5x3x4xf32>) -> tensor<2x5x3x4xf32>\n    return %2 : tensor<2x5x3x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          7,
          7,
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7,
          7,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,7,10] b:f32[7,7,10]. let\n    c:f32[14,7,10] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x7x10xf32>, %arg1: tensor<7x7x10xf32>) -> (tensor<14x7x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<7x7x10xf32>, tensor<7x7x10xf32>) -> tensor<14x7x10xf32>\n    return %0 : tensor<14x7x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 112))",
    "input_info": [
      {
        "shape": [
          7,
          4,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,4,8]. let\n    b:f32[2,112] = reshape[dimensions=None new_sizes=(2, 112) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x4x8xf32>) -> (tensor<2x112xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<7x4x8xf32>) -> tensor<2x112xf32>\n    return %0 : tensor<2x112xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (8,))",
    "input_info": [
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8xf32>) -> (tensor<8xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2] b:f32[2]. let\n    c:f32[1,2] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 2)\n      sharding=None\n    ] a\n    d:f32[1,2] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 2)\n      sharding=None\n    ] b\n    e:f32[2,2] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>, %arg1: tensor<2xf32>) -> (tensor<2x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<2xf32>) -> tensor<1x2xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<2xf32>) -> tensor<1x2xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x2xf32>, tensor<1x2xf32>) -> tensor<2x2xf32>\n    return %2 : tensor<2x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          8,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,5]. let\n    b:f32[7,5] = slice[limit_indices=(8, 5) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x5xf32>) -> (tensor<7x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:8, 0:5] : (tensor<8x5xf32>) -> tensor<7x5xf32>\n    return %0 : tensor<7x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2] b:f32[2]. let\n    c:f32[4] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>, %arg1: tensor<2xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<2xf32>, tensor<2xf32>) -> tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (24,))",
    "input_info": [
      {
        "shape": [
          8,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,3]. let\n    b:f32[24] = reshape[dimensions=None new_sizes=(24,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x3xf32>) -> (tensor<24xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<8x3xf32>) -> tensor<24xf32>\n    return %0 : tensor<24xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          7,
          10,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,10,4]. let\n    b:f32[4,10,7] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x10x4xf32>) -> (tensor<4x10x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<7x10x4xf32>) -> tensor<4x10x7xf32>\n    return %0 : tensor<4x10x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10]. let\n    b:f32[9] = slice[limit_indices=(10,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:10] : (tensor<10xf32>) -> tensor<9xf32>\n    return %0 : tensor<9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9] b:f32[9]. let\n    c:f32[1,9] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 9)\n      sharding=None\n    ] a\n    d:f32[1,9] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 9)\n      sharding=None\n    ] b\n    e:f32[2,9] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>, %arg1: tensor<9xf32>) -> (tensor<2x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<9xf32>) -> tensor<1x9xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<9xf32>) -> tensor<1x9xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x9xf32>, tensor<1x9xf32>) -> tensor<2x9xf32>\n    return %2 : tensor<2x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          9,
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,5] b:f32[9,5]. let\n    c:f32[1,9,5] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 9, 5)\n      sharding=None\n    ] a\n    d:f32[1,9,5] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 9, 5)\n      sharding=None\n    ] b\n    e:f32[2,9,5] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x5xf32>, %arg1: tensor<9x5xf32>) -> (tensor<2x9x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<9x5xf32>) -> tensor<1x9x5xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<9x5xf32>) -> tensor<1x9x5xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x9x5xf32>, tensor<1x9x5xf32>) -> tensor<2x9x5xf32>\n    return %2 : tensor<2x9x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          7,
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,5] b:f32[7,5]. let\n    c:f32[14,5] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x5xf32>, %arg1: tensor<7x5xf32>) -> (tensor<14x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<7x5xf32>, tensor<7x5xf32>) -> tensor<14x5xf32>\n    return %0 : tensor<14x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          5,
          2,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,2,4]. let\n    b:f32[4,2,4] = slice[\n      limit_indices=(5, 2, 4)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x2x4xf32>) -> (tensor<4x2x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:5, 0:2, 0:4] : (tensor<5x2x4xf32>) -> tensor<4x2x4xf32>\n    return %0 : tensor<4x2x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9] b:f32[9]. let\n    c:f32[1,9] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 9)\n      sharding=None\n    ] a\n    d:f32[1,9] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 9)\n      sharding=None\n    ] b\n    e:f32[2,9] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>, %arg1: tensor<9xf32>) -> (tensor<2x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<9xf32>) -> tensor<1x9xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<9xf32>) -> tensor<1x9xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x9xf32>, tensor<1x9xf32>) -> tensor<2x9xf32>\n    return %2 : tensor<2x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          9,
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,10] b:f32[9,10]. let\n    c:f32[1,9,10] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 9, 10)\n      sharding=None\n    ] a\n    d:f32[1,9,10] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 9, 10)\n      sharding=None\n    ] b\n    e:f32[2,9,10] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x10xf32>, %arg1: tensor<9x10xf32>) -> (tensor<2x9x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<9x10xf32>) -> tensor<1x9x10xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<9x10xf32>) -> tensor<1x9x10xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x9x10xf32>, tensor<1x9x10xf32>) -> tensor<2x9x10xf32>\n    return %2 : tensor<2x9x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 9))",
    "input_info": [
      {
        "shape": [
          6,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,3]. let\n    b:f32[2,9] = reshape[dimensions=None new_sizes=(2, 9) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x3xf32>) -> (tensor<2x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<6x3xf32>) -> tensor<2x9xf32>\n    return %0 : tensor<2x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (10,))",
    "input_info": [
      {
        "shape": [
          2,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,5]. let\n    b:f32[10] = reshape[dimensions=None new_sizes=(10,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x5xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<2x5xf32>) -> tensor<10xf32>\n    return %0 : tensor<10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (54,))",
    "input_info": [
      {
        "shape": [
          2,
          3,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,3,9]. let\n    b:f32[54] = reshape[dimensions=None new_sizes=(54,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x3x9xf32>) -> (tensor<54xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<2x3x9xf32>) -> tensor<54xf32>\n    return %0 : tensor<54xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (4,))",
    "input_info": [
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          9,
          3,
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9,
          3,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,3,3] b:f32[9,3,3]. let\n    c:f32[1,9,3,3] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 9, 3, 3)\n      sharding=None\n    ] a\n    d:f32[1,9,3,3] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 9, 3, 3)\n      sharding=None\n    ] b\n    e:f32[2,9,3,3] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x3x3xf32>, %arg1: tensor<9x3x3xf32>) -> (tensor<2x9x3x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<9x3x3xf32>) -> tensor<1x9x3x3xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<9x3x3xf32>) -> tensor<1x9x3x3xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x9x3x3xf32>, tensor<1x9x3x3xf32>) -> tensor<2x9x3x3xf32>\n    return %2 : tensor<2x9x3x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          4,
          10,
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4,
          10,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,10,5] b:f32[4,10,5]. let\n    c:f32[1,4,10,5] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 4, 10, 5)\n      sharding=None\n    ] a\n    d:f32[1,4,10,5] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 4, 10, 5)\n      sharding=None\n    ] b\n    e:f32[2,4,10,5] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x10x5xf32>, %arg1: tensor<4x10x5xf32>) -> (tensor<2x4x10x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<4x10x5xf32>) -> tensor<1x4x10x5xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<4x10x5xf32>) -> tensor<1x4x10x5xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x4x10x5xf32>, tensor<1x4x10x5xf32>) -> tensor<2x4x10x5xf32>\n    return %2 : tensor<2x4x10x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10]. let\n    b:f32[9] = slice[limit_indices=(10,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:10] : (tensor<10xf32>) -> tensor<9xf32>\n    return %0 : tensor<9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          3,
          5,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,5,3]. let\n    b:f32[2,5,3] = slice[\n      limit_indices=(3, 5, 3)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x5x3xf32>) -> (tensor<2x5x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:3, 0:5, 0:3] : (tensor<3x5x3xf32>) -> tensor<2x5x3xf32>\n    return %0 : tensor<2x5x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10] b:f32[10]. let\n    c:f32[1,10] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 10)\n      sharding=None\n    ] a\n    d:f32[1,10] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 10)\n      sharding=None\n    ] b\n    e:f32[2,10] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>, %arg1: tensor<10xf32>) -> (tensor<2x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<10xf32>) -> tensor<1x10xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<10xf32>) -> tensor<1x10xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<2x10xf32>\n    return %2 : tensor<2x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (7,))",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>) -> (tensor<7xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>) -> (tensor<6xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (5,))",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>) -> (tensor<5xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 45))",
    "input_info": [
      {
        "shape": [
          5,
          3,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,3,6]. let\n    b:f32[2,45] = reshape[dimensions=None new_sizes=(2, 45) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x3x6xf32>) -> (tensor<2x45xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<5x3x6xf32>) -> tensor<2x45xf32>\n    return %0 : tensor<2x45xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9] b:f32[9]. let\n    c:f32[18] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>, %arg1: tensor<9xf32>) -> (tensor<18xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<9xf32>, tensor<9xf32>) -> tensor<18xf32>\n    return %0 : tensor<18xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8]. let\n    b:f32[7] = slice[limit_indices=(8,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8xf32>) -> (tensor<7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:8] : (tensor<8xf32>) -> tensor<7xf32>\n    return %0 : tensor<7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          3,
          8,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,8,3]. let\n    b:f32[2,8,3] = slice[\n      limit_indices=(3, 8, 3)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x8x3xf32>) -> (tensor<2x8x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:3, 0:8, 0:3] : (tensor<3x8x3xf32>) -> tensor<2x8x3xf32>\n    return %0 : tensor<2x8x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (72,))",
    "input_info": [
      {
        "shape": [
          4,
          6,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,6,3]. let\n    b:f32[72] = reshape[dimensions=None new_sizes=(72,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x6x3xf32>) -> (tensor<72xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<4x6x3xf32>) -> tensor<72xf32>\n    return %0 : tensor<72xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2] b:f32[2]. let\n    c:f32[1,2] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 2)\n      sharding=None\n    ] a\n    d:f32[1,2] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 2)\n      sharding=None\n    ] b\n    e:f32[2,2] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>, %arg1: tensor<2xf32>) -> (tensor<2x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<2xf32>) -> tensor<1x2xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<2xf32>) -> tensor<1x2xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x2xf32>, tensor<1x2xf32>) -> tensor<2x2xf32>\n    return %2 : tensor<2x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3] b:f32[3]. let\n    c:f32[1,3] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 3)\n      sharding=None\n    ] a\n    d:f32[1,3] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 3)\n      sharding=None\n    ] b\n    e:f32[2,3] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>, %arg1: tensor<3xf32>) -> (tensor<2x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<3xf32>) -> tensor<1x3xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<3xf32>) -> tensor<1x3xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x3xf32>, tensor<1x3xf32>) -> tensor<2x3xf32>\n    return %2 : tensor<2x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          3,
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,8] b:f32[3,8]. let\n    c:f32[1,3,8] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 3, 8)\n      sharding=None\n    ] a\n    d:f32[1,3,8] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 3, 8)\n      sharding=None\n    ] b\n    e:f32[2,3,8] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x8xf32>, %arg1: tensor<3x8xf32>) -> (tensor<2x3x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<3x8xf32>) -> tensor<1x3x8xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<3x8xf32>) -> tensor<1x3x8xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x3x8xf32>, tensor<1x3x8xf32>) -> tensor<2x3x8xf32>\n    return %2 : tensor<2x3x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 12))",
    "input_info": [
      {
        "shape": [
          8,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,3]. let\n    b:f32[2,12] = reshape[dimensions=None new_sizes=(2, 12) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x3xf32>) -> (tensor<2x12xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<8x3xf32>) -> tensor<2x12xf32>\n    return %0 : tensor<2x12xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5] b:f32[5]. let\n    c:f32[1,5] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 5)\n      sharding=None\n    ] a\n    d:f32[1,5] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 5)\n      sharding=None\n    ] b\n    e:f32[2,5] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>, %arg1: tensor<5xf32>) -> (tensor<2x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<5xf32>) -> tensor<1x5xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<5xf32>) -> tensor<1x5xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x5xf32>, tensor<1x5xf32>) -> tensor<2x5xf32>\n    return %2 : tensor<2x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          9,
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,4] b:f32[9,4]. let\n    c:f32[1,9,4] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 9, 4)\n      sharding=None\n    ] a\n    d:f32[1,9,4] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 9, 4)\n      sharding=None\n    ] b\n    e:f32[2,9,4] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x4xf32>, %arg1: tensor<9x4xf32>) -> (tensor<2x9x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<9x4xf32>) -> tensor<1x9x4xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<9x4xf32>) -> tensor<1x9x4xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x9x4xf32>, tensor<1x9x4xf32>) -> tensor<2x9x4xf32>\n    return %2 : tensor<2x9x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          3,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,4]. let\n    b:f32[2,4] = slice[limit_indices=(3, 4) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x4xf32>) -> (tensor<2x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:3, 0:4] : (tensor<3x4xf32>) -> tensor<2x4xf32>\n    return %0 : tensor<2x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          6,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,10]. let\n    b:f32[10,6] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x10xf32>) -> (tensor<10x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<6x10xf32>) -> tensor<10x6xf32>\n    return %0 : tensor<10x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2]. let\n    b:f32[1] = slice[limit_indices=(2,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>) -> (tensor<1xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:2] : (tensor<2xf32>) -> tensor<1xf32>\n    return %0 : tensor<1xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          7,
          10,
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7,
          10,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,10,5] b:f32[7,10,5]. let\n    c:f32[14,10,5] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x10x5xf32>, %arg1: tensor<7x10x5xf32>) -> (tensor<14x10x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<7x10x5xf32>, tensor<7x10x5xf32>) -> tensor<14x10x5xf32>\n    return %0 : tensor<14x10x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8] b:f32[8]. let\n    c:f32[16] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8xf32>, %arg1: tensor<8xf32>) -> (tensor<16xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<8xf32>, tensor<8xf32>) -> tensor<16xf32>\n    return %0 : tensor<16xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          5,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,3]. let\n    b:f32[4,3] = slice[limit_indices=(5, 3) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x3xf32>) -> (tensor<4x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:5, 0:3] : (tensor<5x3xf32>) -> tensor<4x3xf32>\n    return %0 : tensor<4x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9] b:f32[9]. let\n    c:f32[18] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>, %arg1: tensor<9xf32>) -> (tensor<18xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<9xf32>, tensor<9xf32>) -> tensor<18xf32>\n    return %0 : tensor<18xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          7,
          4,
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7,
          4,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,4,5] b:f32[7,4,5]. let\n    c:f32[1,7,4,5] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 7, 4, 5)\n      sharding=None\n    ] a\n    d:f32[1,7,4,5] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 7, 4, 5)\n      sharding=None\n    ] b\n    e:f32[2,7,4,5] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x4x5xf32>, %arg1: tensor<7x4x5xf32>) -> (tensor<2x7x4x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<7x4x5xf32>) -> tensor<1x7x4x5xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<7x4x5xf32>) -> tensor<1x7x4x5xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x7x4x5xf32>, tensor<1x7x4x5xf32>) -> tensor<2x7x4x5xf32>\n    return %2 : tensor<2x7x4x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>) -> (tensor<6xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          10,
          3,
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10,
          3,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,3,7] b:f32[10,3,7]. let\n    c:f32[20,3,7] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x3x7xf32>, %arg1: tensor<10x3x7xf32>) -> (tensor<20x3x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<10x3x7xf32>, tensor<10x3x7xf32>) -> tensor<20x3x7xf32>\n    return %0 : tensor<20x3x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>) -> (tensor<7xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          5,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,9]. let\n    b:f32[4,9] = slice[limit_indices=(5, 9) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x9xf32>) -> (tensor<4x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:5, 0:9] : (tensor<5x9xf32>) -> tensor<4x9xf32>\n    return %0 : tensor<4x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2,))",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>) -> (tensor<2xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3] b:f32[3]. let\n    c:f32[6] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>, %arg1: tensor<3xf32>) -> (tensor<6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<3xf32>, tensor<3xf32>) -> tensor<6xf32>\n    return %0 : tensor<6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 42))",
    "input_info": [
      {
        "shape": [
          2,
          7,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,7,6]. let\n    b:f32[2,42] = reshape[dimensions=None new_sizes=(2, 42) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x7x6xf32>) -> (tensor<2x42xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<2x7x6xf32>) -> tensor<2x42xf32>\n    return %0 : tensor<2x42xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (81,))",
    "input_info": [
      {
        "shape": [
          9,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,9]. let\n    b:f32[81] = reshape[dimensions=None new_sizes=(81,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x9xf32>) -> (tensor<81xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<9x9xf32>) -> tensor<81xf32>\n    return %0 : tensor<81xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          8,
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,2] b:f32[8,2]. let\n    c:f32[1,8,2] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 8, 2)\n      sharding=None\n    ] a\n    d:f32[1,8,2] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 8, 2)\n      sharding=None\n    ] b\n    e:f32[2,8,2] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x2xf32>, %arg1: tensor<8x2xf32>) -> (tensor<2x8x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<8x2xf32>) -> tensor<1x8x2xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<8x2xf32>) -> tensor<1x8x2xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x8x2xf32>, tensor<1x8x2xf32>) -> tensor<2x8x2xf32>\n    return %2 : tensor<2x8x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2] b:f32[2]. let\n    c:f32[1,2] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 2)\n      sharding=None\n    ] a\n    d:f32[1,2] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 2)\n      sharding=None\n    ] b\n    e:f32[2,2] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>, %arg1: tensor<2xf32>) -> (tensor<2x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<2xf32>) -> tensor<1x2xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<2xf32>) -> tensor<1x2xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x2xf32>, tensor<1x2xf32>) -> tensor<2x2xf32>\n    return %2 : tensor<2x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          5,
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,10] b:f32[5,10]. let\n    c:f32[10,10] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x10xf32>, %arg1: tensor<5x10xf32>) -> (tensor<10x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<5x10xf32>, tensor<5x10xf32>) -> tensor<10x10xf32>\n    return %0 : tensor<10x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          7,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,2]. let\n    b:f32[6,2] = slice[limit_indices=(7, 2) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x2xf32>) -> (tensor<6x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:7, 0:2] : (tensor<7x2xf32>) -> tensor<6x2xf32>\n    return %0 : tensor<6x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          2,
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,7] b:f32[2,7]. let\n    c:f32[4,7] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x7xf32>, %arg1: tensor<2x7xf32>) -> (tensor<4x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<2x7xf32>, tensor<2x7xf32>) -> tensor<4x7xf32>\n    return %0 : tensor<4x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (7,))",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>) -> (tensor<7xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (35,))",
    "input_info": [
      {
        "shape": [
          5,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,7]. let\n    b:f32[35] = reshape[dimensions=None new_sizes=(35,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x7xf32>) -> (tensor<35xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<5x7xf32>) -> tensor<35xf32>\n    return %0 : tensor<35xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (27,))",
    "input_info": [
      {
        "shape": [
          9,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,3]. let\n    b:f32[27] = reshape[dimensions=None new_sizes=(27,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x3xf32>) -> (tensor<27xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<9x3xf32>) -> tensor<27xf32>\n    return %0 : tensor<27xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (27,))",
    "input_info": [
      {
        "shape": [
          3,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,9]. let\n    b:f32[27] = reshape[dimensions=None new_sizes=(27,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x9xf32>) -> (tensor<27xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<3x9xf32>) -> tensor<27xf32>\n    return %0 : tensor<27xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 12))",
    "input_info": [
      {
        "shape": [
          8,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,3]. let\n    b:f32[2,12] = reshape[dimensions=None new_sizes=(2, 12) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x3xf32>) -> (tensor<2x12xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<8x3xf32>) -> tensor<2x12xf32>\n    return %0 : tensor<2x12xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7] b:f32[7]. let\n    c:f32[14] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>, %arg1: tensor<7xf32>) -> (tensor<14xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<7xf32>, tensor<7xf32>) -> tensor<14xf32>\n    return %0 : tensor<14xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3]. let\n    b:f32[2] = slice[limit_indices=(3,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>) -> (tensor<2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:3] : (tensor<3xf32>) -> tensor<2xf32>\n    return %0 : tensor<2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10]. let\n    b:f32[9] = slice[limit_indices=(10,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:10] : (tensor<10xf32>) -> tensor<9xf32>\n    return %0 : tensor<9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          3,
          3,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,3,4]. let\n    b:f32[2,3,4] = slice[\n      limit_indices=(3, 3, 4)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x3x4xf32>) -> (tensor<2x3x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:3, 0:3, 0:4] : (tensor<3x3x4xf32>) -> tensor<2x3x4xf32>\n    return %0 : tensor<2x3x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          10,
          2,
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10,
          2,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,2,9] b:f32[10,2,9]. let\n    c:f32[20,2,9] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x2x9xf32>, %arg1: tensor<10x2x9xf32>) -> (tensor<20x2x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<10x2x9xf32>, tensor<10x2x9xf32>) -> tensor<20x2x9xf32>\n    return %0 : tensor<20x2x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (448,))",
    "input_info": [
      {
        "shape": [
          7,
          8,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,8,8]. let\n    b:f32[448] = reshape[dimensions=None new_sizes=(448,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x8x8xf32>) -> (tensor<448xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<7x8x8xf32>) -> tensor<448xf32>\n    return %0 : tensor<448xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          9,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,6]. let\n    b:f32[8,6] = slice[limit_indices=(9, 6) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x6xf32>) -> (tensor<8x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:9, 0:6] : (tensor<9x6xf32>) -> tensor<8x6xf32>\n    return %0 : tensor<8x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          4,
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,3] b:f32[4,3]. let\n    c:f32[8,3] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x3xf32>, %arg1: tensor<4x3xf32>) -> (tensor<8x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<4x3xf32>, tensor<4x3xf32>) -> tensor<8x3xf32>\n    return %0 : tensor<8x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          5,
          9,
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5,
          9,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,9,3] b:f32[5,9,3]. let\n    c:f32[1,5,9,3] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 5, 9, 3)\n      sharding=None\n    ] a\n    d:f32[1,5,9,3] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 5, 9, 3)\n      sharding=None\n    ] b\n    e:f32[2,5,9,3] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x9x3xf32>, %arg1: tensor<5x9x3xf32>) -> (tensor<2x5x9x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<5x9x3xf32>) -> tensor<1x5x9x3xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<5x9x3xf32>) -> tensor<1x5x9x3xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x5x9x3xf32>, tensor<1x5x9x3xf32>) -> tensor<2x5x9x3xf32>\n    return %2 : tensor<2x5x9x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 30))",
    "input_info": [
      {
        "shape": [
          6,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,10]. let\n    b:f32[2,30] = reshape[dimensions=None new_sizes=(2, 30) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x10xf32>) -> (tensor<2x30xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<6x10xf32>) -> tensor<2x30xf32>\n    return %0 : tensor<2x30xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>) -> (tensor<3xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>) -> (tensor<9xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          3,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,10]. let\n    b:f32[10,3] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x10xf32>) -> (tensor<10x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<3x10xf32>) -> tensor<10x3xf32>\n    return %0 : tensor<10x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          5,
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,3] b:f32[5,3]. let\n    c:f32[10,3] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x3xf32>, %arg1: tensor<5x3xf32>) -> (tensor<10x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<5x3xf32>, tensor<5x3xf32>) -> tensor<10x3xf32>\n    return %0 : tensor<10x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (45,))",
    "input_info": [
      {
        "shape": [
          9,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,5]. let\n    b:f32[45] = reshape[dimensions=None new_sizes=(45,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x5xf32>) -> (tensor<45xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<9x5xf32>) -> tensor<45xf32>\n    return %0 : tensor<45xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (28,))",
    "input_info": [
      {
        "shape": [
          4,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,7]. let\n    b:f32[28] = reshape[dimensions=None new_sizes=(28,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x7xf32>) -> (tensor<28xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<4x7xf32>) -> tensor<28xf32>\n    return %0 : tensor<28xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          10,
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,8] b:f32[10,8]. let\n    c:f32[20,8] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x8xf32>, %arg1: tensor<10x8xf32>) -> (tensor<20x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<10x8xf32>, tensor<10x8xf32>) -> tensor<20x8xf32>\n    return %0 : tensor<20x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>) -> (tensor<3xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2] b:f32[2]. let\n    c:f32[4] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>, %arg1: tensor<2xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<2xf32>, tensor<2xf32>) -> tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          8,
          7,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,7,7]. let\n    b:f32[7,7,7] = slice[\n      limit_indices=(8, 7, 7)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x7x7xf32>) -> (tensor<7x7x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:8, 0:7, 0:7] : (tensor<8x7x7xf32>) -> tensor<7x7x7xf32>\n    return %0 : tensor<7x7x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          5,
          2,
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5,
          2,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,2,6] b:f32[5,2,6]. let\n    c:f32[1,5,2,6] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 5, 2, 6)\n      sharding=None\n    ] a\n    d:f32[1,5,2,6] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 5, 2, 6)\n      sharding=None\n    ] b\n    e:f32[2,5,2,6] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x2x6xf32>, %arg1: tensor<5x2x6xf32>) -> (tensor<2x5x2x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<5x2x6xf32>) -> tensor<1x5x2x6xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<5x2x6xf32>) -> tensor<1x5x2x6xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x5x2x6xf32>, tensor<1x5x2x6xf32>) -> tensor<2x5x2x6xf32>\n    return %2 : tensor<2x5x2x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8] b:f32[8]. let\n    c:f32[1,8] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 8)\n      sharding=None\n    ] a\n    d:f32[1,8] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 8)\n      sharding=None\n    ] b\n    e:f32[2,8] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8xf32>, %arg1: tensor<8xf32>) -> (tensor<2x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<8xf32>) -> tensor<1x8xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<8xf32>) -> tensor<1x8xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x8xf32>, tensor<1x8xf32>) -> tensor<2x8xf32>\n    return %2 : tensor<2x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          5,
          8,
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5,
          8,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,8,5] b:f32[5,8,5]. let\n    c:f32[10,8,5] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x8x5xf32>, %arg1: tensor<5x8x5xf32>) -> (tensor<10x8x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<5x8x5xf32>, tensor<5x8x5xf32>) -> tensor<10x8x5xf32>\n    return %0 : tensor<10x8x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5] b:f32[5]. let\n    c:f32[1,5] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 5)\n      sharding=None\n    ] a\n    d:f32[1,5] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 5)\n      sharding=None\n    ] b\n    e:f32[2,5] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>, %arg1: tensor<5xf32>) -> (tensor<2x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<5xf32>) -> tensor<1x5xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<5xf32>) -> tensor<1x5xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x5xf32>, tensor<1x5xf32>) -> tensor<2x5xf32>\n    return %2 : tensor<2x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          7,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,9]. let\n    b:f32[6,9] = slice[limit_indices=(7, 9) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x9xf32>) -> (tensor<6x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:7, 0:9] : (tensor<7x9xf32>) -> tensor<6x9xf32>\n    return %0 : tensor<6x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          2,
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,6] b:f32[2,6]. let\n    c:f32[1,2,6] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 2, 6)\n      sharding=None\n    ] a\n    d:f32[1,2,6] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 2, 6)\n      sharding=None\n    ] b\n    e:f32[2,2,6] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x6xf32>, %arg1: tensor<2x6xf32>) -> (tensor<2x2x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<2x6xf32>) -> tensor<1x2x6xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<2x6xf32>) -> tensor<1x2x6xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x2x6xf32>, tensor<1x2x6xf32>) -> tensor<2x2x6xf32>\n    return %2 : tensor<2x2x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 3))",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6]. let\n    b:f32[2,3] = reshape[dimensions=None new_sizes=(2, 3) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>) -> (tensor<2x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<6xf32>) -> tensor<2x3xf32>\n    return %0 : tensor<2x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (90,))",
    "input_info": [
      {
        "shape": [
          9,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,10]. let\n    b:f32[90] = reshape[dimensions=None new_sizes=(90,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x10xf32>) -> (tensor<90xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<9x10xf32>) -> tensor<90xf32>\n    return %0 : tensor<90xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5] b:f32[5]. let\n    c:f32[10] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>, %arg1: tensor<5xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<5xf32>, tensor<5xf32>) -> tensor<10xf32>\n    return %0 : tensor<10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7] b:f32[7]. let\n    c:f32[1,7] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 7)\n      sharding=None\n    ] a\n    d:f32[1,7] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 7)\n      sharding=None\n    ] b\n    e:f32[2,7] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>, %arg1: tensor<7xf32>) -> (tensor<2x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<7xf32>) -> tensor<1x7xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<7xf32>) -> tensor<1x7xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x7xf32>, tensor<1x7xf32>) -> tensor<2x7xf32>\n    return %2 : tensor<2x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9] b:f32[9]. let\n    c:f32[18] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>, %arg1: tensor<9xf32>) -> (tensor<18xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<9xf32>, tensor<9xf32>) -> tensor<18xf32>\n    return %0 : tensor<18xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          8,
          7,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,7,5]. let\n    b:f32[5,7,8] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x7x5xf32>) -> (tensor<5x7x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<8x7x5xf32>) -> tensor<5x7x8xf32>\n    return %0 : tensor<5x7x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          2,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,10]. let\n    b:f32[1,10] = slice[limit_indices=(2, 10) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x10xf32>) -> (tensor<1x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:2, 0:10] : (tensor<2x10xf32>) -> tensor<1x10xf32>\n    return %0 : tensor<1x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8]. let\n    b:f32[7] = slice[limit_indices=(8,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8xf32>) -> (tensor<7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:8] : (tensor<8xf32>) -> tensor<7xf32>\n    return %0 : tensor<7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5] b:f32[5]. let\n    c:f32[1,5] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 5)\n      sharding=None\n    ] a\n    d:f32[1,5] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 5)\n      sharding=None\n    ] b\n    e:f32[2,5] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>, %arg1: tensor<5xf32>) -> (tensor<2x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<5xf32>) -> tensor<1x5xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<5xf32>) -> tensor<1x5xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x5xf32>, tensor<1x5xf32>) -> tensor<2x5xf32>\n    return %2 : tensor<2x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          6,
          3,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,3,5]. let\n    b:f32[5,3,5] = slice[\n      limit_indices=(6, 3, 5)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x3x5xf32>) -> (tensor<5x3x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:6, 0:3, 0:5] : (tensor<6x3x5xf32>) -> tensor<5x3x5xf32>\n    return %0 : tensor<5x3x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 45))",
    "input_info": [
      {
        "shape": [
          3,
          6,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,6,5]. let\n    b:f32[2,45] = reshape[dimensions=None new_sizes=(2, 45) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x6x5xf32>) -> (tensor<2x45xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<3x6x5xf32>) -> tensor<2x45xf32>\n    return %0 : tensor<2x45xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9]. let\n    b:f32[8] = slice[limit_indices=(9,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>) -> (tensor<8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:9] : (tensor<9xf32>) -> tensor<8xf32>\n    return %0 : tensor<8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (96,))",
    "input_info": [
      {
        "shape": [
          8,
          6,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,6,2]. let\n    b:f32[96] = reshape[dimensions=None new_sizes=(96,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x6x2xf32>) -> (tensor<96xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<8x6x2xf32>) -> tensor<96xf32>\n    return %0 : tensor<96xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>) -> (tensor<2xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          3,
          7,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,7,4]. let\n    b:f32[2,7,4] = slice[\n      limit_indices=(3, 7, 4)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x7x4xf32>) -> (tensor<2x7x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:3, 0:7, 0:4] : (tensor<3x7x4xf32>) -> tensor<2x7x4xf32>\n    return %0 : tensor<2x7x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          4,
          10,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,10,6]. let\n    b:f32[3,10,6] = slice[\n      limit_indices=(4, 10, 6)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x10x6xf32>) -> (tensor<3x10x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:4, 0:10, 0:6] : (tensor<4x10x6xf32>) -> tensor<3x10x6xf32>\n    return %0 : tensor<3x10x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          10,
          2,
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10,
          2,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,2,6] b:f32[10,2,6]. let\n    c:f32[20,2,6] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x2x6xf32>, %arg1: tensor<10x2x6xf32>) -> (tensor<20x2x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<10x2x6xf32>, tensor<10x2x6xf32>) -> tensor<20x2x6xf32>\n    return %0 : tensor<20x2x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 3))",
    "input_info": [
      {
        "shape": [
          2,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,3]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x3xf32>) -> (tensor<2x3xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<2x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          10,
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,7] b:f32[10,7]. let\n    c:f32[1,10,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 10, 7)\n      sharding=None\n    ] a\n    d:f32[1,10,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 10, 7)\n      sharding=None\n    ] b\n    e:f32[2,10,7] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x7xf32>, %arg1: tensor<10x7xf32>) -> (tensor<2x10x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<10x7xf32>) -> tensor<1x10x7xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<10x7xf32>) -> tensor<1x10x7xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x10x7xf32>, tensor<1x10x7xf32>) -> tensor<2x10x7xf32>\n    return %2 : tensor<2x10x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (30,))",
    "input_info": [
      {
        "shape": [
          10,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,3]. let\n    b:f32[30] = reshape[dimensions=None new_sizes=(30,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x3xf32>) -> (tensor<30xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<10x3xf32>) -> tensor<30xf32>\n    return %0 : tensor<30xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          3,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,2]. let\n    b:f32[2,2] = slice[limit_indices=(3, 2) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x2xf32>) -> (tensor<2x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:3, 0:2] : (tensor<3x2xf32>) -> tensor<2x2xf32>\n    return %0 : tensor<2x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          6,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,10]. let\n    b:f32[5,10] = slice[limit_indices=(6, 10) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x10xf32>) -> (tensor<5x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:6, 0:10] : (tensor<6x10xf32>) -> tensor<5x10xf32>\n    return %0 : tensor<5x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (5,))",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>) -> (tensor<5xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (24,))",
    "input_info": [
      {
        "shape": [
          8,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,3]. let\n    b:f32[24] = reshape[dimensions=None new_sizes=(24,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x3xf32>) -> (tensor<24xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<8x3xf32>) -> tensor<24xf32>\n    return %0 : tensor<24xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8]. let\n    b:f32[7] = slice[limit_indices=(8,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8xf32>) -> (tensor<7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:8] : (tensor<8xf32>) -> tensor<7xf32>\n    return %0 : tensor<7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          3,
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,5] b:f32[3,5]. let\n    c:f32[6,5] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x5xf32>, %arg1: tensor<3x5xf32>) -> (tensor<6x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<3x5xf32>, tensor<3x5xf32>) -> tensor<6x5xf32>\n    return %0 : tensor<6x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>) -> (tensor<9xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          5,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,8]. let\n    b:f32[8,5] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x8xf32>) -> (tensor<8x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<5x8xf32>) -> tensor<8x5xf32>\n    return %0 : tensor<8x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6] b:f32[6]. let\n    c:f32[12] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>, %arg1: tensor<6xf32>) -> (tensor<12xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<6xf32>, tensor<6xf32>) -> tensor<12xf32>\n    return %0 : tensor<12xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          6,
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,7] b:f32[6,7]. let\n    c:f32[1,6,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 6, 7)\n      sharding=None\n    ] a\n    d:f32[1,6,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 6, 7)\n      sharding=None\n    ] b\n    e:f32[2,6,7] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x7xf32>, %arg1: tensor<6x7xf32>) -> (tensor<2x6x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<6x7xf32>) -> tensor<1x6x7xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<6x7xf32>) -> tensor<1x6x7xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x6x7xf32>, tensor<1x6x7xf32>) -> tensor<2x6x7xf32>\n    return %2 : tensor<2x6x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          6,
          3,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,3,8]. let\n    b:f32[8,3,6] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x3x8xf32>) -> (tensor<8x3x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<6x3x8xf32>) -> tensor<8x3x6xf32>\n    return %0 : tensor<8x3x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5]. let\n    b:f32[4] = slice[limit_indices=(5,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:5] : (tensor<5xf32>) -> tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          4,
          2,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,2,4]. let\n    b:f32[4,2,4] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x2x4xf32>) -> (tensor<4x2x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<4x2x4xf32>) -> tensor<4x2x4xf32>\n    return %0 : tensor<4x2x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (30,))",
    "input_info": [
      {
        "shape": [
          6,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,5]. let\n    b:f32[30] = reshape[dimensions=None new_sizes=(30,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x5xf32>) -> (tensor<30xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<6x5xf32>) -> tensor<30xf32>\n    return %0 : tensor<30xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          6,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,3]. let\n    b:f32[3,6] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x3xf32>) -> (tensor<3x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<6x3xf32>) -> tensor<3x6xf32>\n    return %0 : tensor<3x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          7,
          4,
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7,
          4,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,4,9] b:f32[7,4,9]. let\n    c:f32[1,7,4,9] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 7, 4, 9)\n      sharding=None\n    ] a\n    d:f32[1,7,4,9] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 7, 4, 9)\n      sharding=None\n    ] b\n    e:f32[2,7,4,9] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x4x9xf32>, %arg1: tensor<7x4x9xf32>) -> (tensor<2x7x4x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<7x4x9xf32>) -> tensor<1x7x4x9xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<7x4x9xf32>) -> tensor<1x7x4x9xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x7x4x9xf32>, tensor<1x7x4x9xf32>) -> tensor<2x7x4x9xf32>\n    return %2 : tensor<2x7x4x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8] b:f32[8]. let\n    c:f32[1,8] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 8)\n      sharding=None\n    ] a\n    d:f32[1,8] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 8)\n      sharding=None\n    ] b\n    e:f32[2,8] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8xf32>, %arg1: tensor<8xf32>) -> (tensor<2x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<8xf32>) -> tensor<1x8xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<8xf32>) -> tensor<1x8xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x8xf32>, tensor<1x8xf32>) -> tensor<2x8xf32>\n    return %2 : tensor<2x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          9,
          9,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,9,7]. let\n    b:f32[7,9,9] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x9x7xf32>) -> (tensor<7x9x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<9x9x7xf32>) -> tensor<7x9x9xf32>\n    return %0 : tensor<7x9x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          3,
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,3] b:f32[3,3]. let\n    c:f32[6,3] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x3xf32>, %arg1: tensor<3x3xf32>) -> (tensor<6x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<3x3xf32>, tensor<3x3xf32>) -> tensor<6x3xf32>\n    return %0 : tensor<6x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 2))",
    "input_info": [
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4]. let\n    b:f32[2,2] = reshape[dimensions=None new_sizes=(2, 2) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<2x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<4xf32>) -> tensor<2x2xf32>\n    return %0 : tensor<2x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10] b:f32[10]. let\n    c:f32[1,10] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 10)\n      sharding=None\n    ] a\n    d:f32[1,10] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 10)\n      sharding=None\n    ] b\n    e:f32[2,10] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>, %arg1: tensor<10xf32>) -> (tensor<2x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<10xf32>) -> tensor<1x10xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<10xf32>) -> tensor<1x10xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<2x10xf32>\n    return %2 : tensor<2x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          4,
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,6] b:f32[4,6]. let\n    c:f32[8,6] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x6xf32>, %arg1: tensor<4x6xf32>) -> (tensor<8x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<4x6xf32>, tensor<4x6xf32>) -> tensor<8x6xf32>\n    return %0 : tensor<8x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          6,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,8]. let\n    b:f32[5,8] = slice[limit_indices=(6, 8) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x8xf32>) -> (tensor<5x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:6, 0:8] : (tensor<6x8xf32>) -> tensor<5x8xf32>\n    return %0 : tensor<5x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          7,
          8,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,8,9]. let\n    b:f32[6,8,9] = slice[\n      limit_indices=(7, 8, 9)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x8x9xf32>) -> (tensor<6x8x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:7, 0:8, 0:9] : (tensor<7x8x9xf32>) -> tensor<6x8x9xf32>\n    return %0 : tensor<6x8x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          6,
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,10] b:f32[6,10]. let\n    c:f32[1,6,10] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 6, 10)\n      sharding=None\n    ] a\n    d:f32[1,6,10] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 6, 10)\n      sharding=None\n    ] b\n    e:f32[2,6,10] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x10xf32>, %arg1: tensor<6x10xf32>) -> (tensor<2x6x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<6x10xf32>) -> tensor<1x6x10xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<6x10xf32>) -> tensor<1x6x10xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x6x10xf32>, tensor<1x6x10xf32>) -> tensor<2x6x10xf32>\n    return %2 : tensor<2x6x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          7,
          2,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,2,10]. let\n    b:f32[6,2,10] = slice[\n      limit_indices=(7, 2, 10)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x2x10xf32>) -> (tensor<6x2x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:7, 0:2, 0:10] : (tensor<7x2x10xf32>) -> tensor<6x2x10xf32>\n    return %0 : tensor<6x2x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          5,
          4,
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5,
          4,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,4,4] b:f32[5,4,4]. let\n    c:f32[10,4,4] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x4x4xf32>, %arg1: tensor<5x4x4xf32>) -> (tensor<10x4x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<5x4x4xf32>, tensor<5x4x4xf32>) -> tensor<10x4x4xf32>\n    return %0 : tensor<10x4x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          8,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,9]. let\n    b:f32[7,9] = slice[limit_indices=(8, 9) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x9xf32>) -> (tensor<7x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:8, 0:9] : (tensor<8x9xf32>) -> tensor<7x9xf32>\n    return %0 : tensor<7x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          8,
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,6] b:f32[8,6]. let\n    c:f32[1,8,6] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 8, 6)\n      sharding=None\n    ] a\n    d:f32[1,8,6] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 8, 6)\n      sharding=None\n    ] b\n    e:f32[2,8,6] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x6xf32>, %arg1: tensor<8x6xf32>) -> (tensor<2x8x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<8x6xf32>) -> tensor<1x8x6xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<8x6xf32>) -> tensor<1x8x6xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x8x6xf32>, tensor<1x8x6xf32>) -> tensor<2x8x6xf32>\n    return %2 : tensor<2x8x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          10,
          8,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,8,6]. let\n    b:f32[6,8,10] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x8x6xf32>) -> (tensor<6x8x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<10x8x6xf32>) -> tensor<6x8x10xf32>\n    return %0 : tensor<6x8x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          3,
          7,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,7,6]. let\n    b:f32[6,7,3] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x7x6xf32>) -> (tensor<6x7x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<3x7x6xf32>) -> tensor<6x7x3xf32>\n    return %0 : tensor<6x7x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8] b:f32[8]. let\n    c:f32[16] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8xf32>, %arg1: tensor<8xf32>) -> (tensor<16xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<8xf32>, tensor<8xf32>) -> tensor<16xf32>\n    return %0 : tensor<16xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          2,
          8,
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2,
          8,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,8,6] b:f32[2,8,6]. let\n    c:f32[1,2,8,6] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 2, 8, 6)\n      sharding=None\n    ] a\n    d:f32[1,2,8,6] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 2, 8, 6)\n      sharding=None\n    ] b\n    e:f32[2,2,8,6] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x8x6xf32>, %arg1: tensor<2x8x6xf32>) -> (tensor<2x2x8x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<2x8x6xf32>) -> tensor<1x2x8x6xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<2x8x6xf32>) -> tensor<1x2x8x6xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x2x8x6xf32>, tensor<1x2x8x6xf32>) -> tensor<2x2x8x6xf32>\n    return %2 : tensor<2x2x8x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3] b:f32[3]. let\n    c:f32[6] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>, %arg1: tensor<3xf32>) -> (tensor<6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<3xf32>, tensor<3xf32>) -> tensor<6xf32>\n    return %0 : tensor<6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10] b:f32[10]. let\n    c:f32[20] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>, %arg1: tensor<10xf32>) -> (tensor<20xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<10xf32>, tensor<10xf32>) -> tensor<20xf32>\n    return %0 : tensor<20xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          2,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,8]. let\n    b:f32[1,8] = slice[limit_indices=(2, 8) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x8xf32>) -> (tensor<1x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:2, 0:8] : (tensor<2x8xf32>) -> tensor<1x8xf32>\n    return %0 : tensor<1x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          8,
          6,
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          6,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,6,3] b:f32[8,6,3]. let\n    c:f32[16,6,3] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x6x3xf32>, %arg1: tensor<8x6x3xf32>) -> (tensor<16x6x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<8x6x3xf32>, tensor<8x6x3xf32>) -> tensor<16x6x3xf32>\n    return %0 : tensor<16x6x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4] b:f32[4]. let\n    c:f32[1,4] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 4)\n      sharding=None\n    ] a\n    d:f32[1,4] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 4)\n      sharding=None\n    ] b\n    e:f32[2,4] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> (tensor<2x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<4xf32>) -> tensor<1x4xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<4xf32>) -> tensor<1x4xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x4xf32>, tensor<1x4xf32>) -> tensor<2x4xf32>\n    return %2 : tensor<2x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          8,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,7]. let\n    b:f32[7,8] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x7xf32>) -> (tensor<7x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<8x7xf32>) -> tensor<7x8xf32>\n    return %0 : tensor<7x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (5,))",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>) -> (tensor<5xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (8,))",
    "input_info": [
      {
        "shape": [
          4,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,2]. let\n    b:f32[8] = reshape[dimensions=None new_sizes=(8,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x2xf32>) -> (tensor<8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<4x2xf32>) -> tensor<8xf32>\n    return %0 : tensor<8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>) -> (tensor<7xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5] b:f32[5]. let\n    c:f32[1,5] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 5)\n      sharding=None\n    ] a\n    d:f32[1,5] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 5)\n      sharding=None\n    ] b\n    e:f32[2,5] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>, %arg1: tensor<5xf32>) -> (tensor<2x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<5xf32>) -> tensor<1x5xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<5xf32>) -> tensor<1x5xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x5xf32>, tensor<1x5xf32>) -> tensor<2x5xf32>\n    return %2 : tensor<2x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          2,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,5]. let\n    b:f32[1,5] = slice[limit_indices=(2, 5) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x5xf32>) -> (tensor<1x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:2, 0:5] : (tensor<2x5xf32>) -> tensor<1x5xf32>\n    return %0 : tensor<1x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          3,
          4,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,4,4]. let\n    b:f32[2,4,4] = slice[\n      limit_indices=(3, 4, 4)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x4x4xf32>) -> (tensor<2x4x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:3, 0:4, 0:4] : (tensor<3x4x4xf32>) -> tensor<2x4x4xf32>\n    return %0 : tensor<2x4x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          8,
          10,
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          10,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,10,10] b:f32[8,10,10]. let\n    c:f32[1,8,10,10] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 8, 10, 10)\n      sharding=None\n    ] a\n    d:f32[1,8,10,10] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 8, 10, 10)\n      sharding=None\n    ] b\n    e:f32[2,8,10,10] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x10x10xf32>, %arg1: tensor<8x10x10xf32>) -> (tensor<2x8x10x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<8x10x10xf32>) -> tensor<1x8x10x10xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<8x10x10xf32>) -> tensor<1x8x10x10xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x8x10x10xf32>, tensor<1x8x10x10xf32>) -> tensor<2x8x10x10xf32>\n    return %2 : tensor<2x8x10x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          9,
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,2] b:f32[9,2]. let\n    c:f32[18,2] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x2xf32>, %arg1: tensor<9x2xf32>) -> (tensor<18x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<9x2xf32>, tensor<9x2xf32>) -> tensor<18x2xf32>\n    return %0 : tensor<18x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          9,
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,10] b:f32[9,10]. let\n    c:f32[18,10] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x10xf32>, %arg1: tensor<9x10xf32>) -> (tensor<18x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<9x10xf32>, tensor<9x10xf32>) -> tensor<18x10xf32>\n    return %0 : tensor<18x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>) -> (tensor<5xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (3, 63))",
    "input_info": [
      {
        "shape": [
          7,
          9,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,9,3]. let\n    b:f32[3,63] = reshape[dimensions=None new_sizes=(3, 63) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x9x3xf32>) -> (tensor<3x63xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<7x9x3xf32>) -> tensor<3x63xf32>\n    return %0 : tensor<3x63xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2]. let\n    b:f32[1] = slice[limit_indices=(2,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>) -> (tensor<1xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:2] : (tensor<2xf32>) -> tensor<1xf32>\n    return %0 : tensor<1xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          8,
          7,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,7,4]. let\n    b:f32[7,7,4] = slice[\n      limit_indices=(8, 7, 4)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x7x4xf32>) -> (tensor<7x7x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:8, 0:7, 0:4] : (tensor<8x7x4xf32>) -> tensor<7x7x4xf32>\n    return %0 : tensor<7x7x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>) -> (tensor<2xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          7,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,9]. let\n    b:f32[6,9] = slice[limit_indices=(7, 9) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x9xf32>) -> (tensor<6x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:7, 0:9] : (tensor<7x9xf32>) -> tensor<6x9xf32>\n    return %0 : tensor<6x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          5,
          6,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,6,3]. let\n    b:f32[4,6,3] = slice[\n      limit_indices=(5, 6, 3)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x6x3xf32>) -> (tensor<4x6x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:5, 0:6, 0:3] : (tensor<5x6x3xf32>) -> tensor<4x6x3xf32>\n    return %0 : tensor<4x6x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7] b:f32[7]. let\n    c:f32[14] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>, %arg1: tensor<7xf32>) -> (tensor<14xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<7xf32>, tensor<7xf32>) -> tensor<14xf32>\n    return %0 : tensor<14xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2]. let\n    b:f32[1] = slice[limit_indices=(2,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>) -> (tensor<1xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:2] : (tensor<2xf32>) -> tensor<1xf32>\n    return %0 : tensor<1xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7]. let\n    b:f32[6] = slice[limit_indices=(7,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>) -> (tensor<6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:7] : (tensor<7xf32>) -> tensor<6xf32>\n    return %0 : tensor<6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          2,
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,5] b:f32[2,5]. let\n    c:f32[4,5] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x5xf32>, %arg1: tensor<2x5xf32>) -> (tensor<4x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<2x5xf32>, tensor<2x5xf32>) -> tensor<4x5xf32>\n    return %0 : tensor<4x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          3,
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,6] b:f32[3,6]. let\n    c:f32[1,3,6] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 3, 6)\n      sharding=None\n    ] a\n    d:f32[1,3,6] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 3, 6)\n      sharding=None\n    ] b\n    e:f32[2,3,6] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x6xf32>, %arg1: tensor<3x6xf32>) -> (tensor<2x3x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<3x6xf32>) -> tensor<1x3x6xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<3x6xf32>) -> tensor<1x3x6xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x3x6xf32>, tensor<1x3x6xf32>) -> tensor<2x3x6xf32>\n    return %2 : tensor<2x3x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          3,
          5,
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3,
          5,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,5,7] b:f32[3,5,7]. let\n    c:f32[1,3,5,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 3, 5, 7)\n      sharding=None\n    ] a\n    d:f32[1,3,5,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 3, 5, 7)\n      sharding=None\n    ] b\n    e:f32[2,3,5,7] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x5x7xf32>, %arg1: tensor<3x5x7xf32>) -> (tensor<2x3x5x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<3x5x7xf32>) -> tensor<1x3x5x7xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<3x5x7xf32>) -> tensor<1x3x5x7xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x3x5x7xf32>, tensor<1x3x5x7xf32>) -> tensor<2x3x5x7xf32>\n    return %2 : tensor<2x3x5x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          8,
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,10] b:f32[8,10]. let\n    c:f32[1,8,10] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 8, 10)\n      sharding=None\n    ] a\n    d:f32[1,8,10] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 8, 10)\n      sharding=None\n    ] b\n    e:f32[2,8,10] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x10xf32>, %arg1: tensor<8x10xf32>) -> (tensor<2x8x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<8x10xf32>) -> tensor<1x8x10xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<8x10xf32>) -> tensor<1x8x10xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x8x10xf32>, tensor<1x8x10xf32>) -> tensor<2x8x10xf32>\n    return %2 : tensor<2x8x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          9,
          3,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,3,2]. let\n    b:f32[2,3,9] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x3x2xf32>) -> (tensor<2x3x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<9x3x2xf32>) -> tensor<2x3x9xf32>\n    return %0 : tensor<2x3x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          7,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,5]. let\n    b:f32[5,7] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x5xf32>) -> (tensor<5x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<7x5xf32>) -> tensor<5x7xf32>\n    return %0 : tensor<5x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>) -> (tensor<6xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          5,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,3]. let\n    b:f32[4,3] = slice[limit_indices=(5, 3) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x3xf32>) -> (tensor<4x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:5, 0:3] : (tensor<5x3xf32>) -> tensor<4x3xf32>\n    return %0 : tensor<4x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          8,
          2,
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          2,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,2,5] b:f32[8,2,5]. let\n    c:f32[16,2,5] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x2x5xf32>, %arg1: tensor<8x2x5xf32>) -> (tensor<16x2x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<8x2x5xf32>, tensor<8x2x5xf32>) -> tensor<16x2x5xf32>\n    return %0 : tensor<16x2x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          4,
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,5] b:f32[4,5]. let\n    c:f32[1,4,5] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 4, 5)\n      sharding=None\n    ] a\n    d:f32[1,4,5] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 4, 5)\n      sharding=None\n    ] b\n    e:f32[2,4,5] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x5xf32>, %arg1: tensor<4x5xf32>) -> (tensor<2x4x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<4x5xf32>) -> tensor<1x4x5xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<4x5xf32>) -> tensor<1x4x5xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x4x5xf32>, tensor<1x4x5xf32>) -> tensor<2x4x5xf32>\n    return %2 : tensor<2x4x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          5,
          10,
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5,
          10,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,10,4] b:f32[5,10,4]. let\n    c:f32[10,10,4] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x10x4xf32>, %arg1: tensor<5x10x4xf32>) -> (tensor<10x10x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<5x10x4xf32>, tensor<5x10x4xf32>) -> tensor<10x10x4xf32>\n    return %0 : tensor<10x10x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          3,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,10]. let\n    b:f32[2,10] = slice[limit_indices=(3, 10) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x10xf32>) -> (tensor<2x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:3, 0:10] : (tensor<3x10xf32>) -> tensor<2x10xf32>\n    return %0 : tensor<2x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2]. let\n    b:f32[1] = slice[limit_indices=(2,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>) -> (tensor<1xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:2] : (tensor<2xf32>) -> tensor<1xf32>\n    return %0 : tensor<1xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (360,))",
    "input_info": [
      {
        "shape": [
          8,
          9,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,9,5]. let\n    b:f32[360] = reshape[dimensions=None new_sizes=(360,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x9x5xf32>) -> (tensor<360xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<8x9x5xf32>) -> tensor<360xf32>\n    return %0 : tensor<360xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>) -> (tensor<9xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7] b:f32[7]. let\n    c:f32[14] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>, %arg1: tensor<7xf32>) -> (tensor<14xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<7xf32>, tensor<7xf32>) -> tensor<14xf32>\n    return %0 : tensor<14xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (3, 3))",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9]. let\n    b:f32[3,3] = reshape[dimensions=None new_sizes=(3, 3) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>) -> (tensor<3x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<9xf32>) -> tensor<3x3xf32>\n    return %0 : tensor<3x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          5,
          9,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,9,8]. let\n    b:f32[8,9,5] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x9x8xf32>) -> (tensor<8x9x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<5x9x8xf32>) -> tensor<8x9x5xf32>\n    return %0 : tensor<8x9x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (15,))",
    "input_info": [
      {
        "shape": [
          3,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,5]. let\n    b:f32[15] = reshape[dimensions=None new_sizes=(15,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x5xf32>) -> (tensor<15xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<3x5xf32>) -> tensor<15xf32>\n    return %0 : tensor<15xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          7,
          10,
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7,
          10,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,10,9] b:f32[7,10,9]. let\n    c:f32[14,10,9] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x10x9xf32>, %arg1: tensor<7x10x9xf32>) -> (tensor<14x10x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<7x10x9xf32>, tensor<7x10x9xf32>) -> tensor<14x10x9xf32>\n    return %0 : tensor<14x10x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3]. let\n    b:f32[2] = slice[limit_indices=(3,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>) -> (tensor<2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:3] : (tensor<3xf32>) -> tensor<2xf32>\n    return %0 : tensor<2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          4,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,10]. let\n    b:f32[10,4] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x10xf32>) -> (tensor<10x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<4x10xf32>) -> tensor<10x4xf32>\n    return %0 : tensor<10x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          2,
          3,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,3,5]. let\n    b:f32[1,3,5] = slice[\n      limit_indices=(2, 3, 5)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x3x5xf32>) -> (tensor<1x3x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:2, 0:3, 0:5] : (tensor<2x3x5xf32>) -> tensor<1x3x5xf32>\n    return %0 : tensor<1x3x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3]. let\n    b:f32[2] = slice[limit_indices=(3,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>) -> (tensor<2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:3] : (tensor<3xf32>) -> tensor<2xf32>\n    return %0 : tensor<2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7]. let\n    b:f32[6] = slice[limit_indices=(7,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>) -> (tensor<6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:7] : (tensor<7xf32>) -> tensor<6xf32>\n    return %0 : tensor<6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          5,
          7,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,7,10]. let\n    b:f32[10,7,5] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x7x10xf32>) -> (tensor<10x7x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<5x7x10xf32>) -> tensor<10x7x5xf32>\n    return %0 : tensor<10x7x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (7,))",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>) -> (tensor<7xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          5,
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,4] b:f32[5,4]. let\n    c:f32[10,4] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x4xf32>, %arg1: tensor<5x4xf32>) -> (tensor<10x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<5x4xf32>, tensor<5x4xf32>) -> tensor<10x4xf32>\n    return %0 : tensor<10x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          8,
          5,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,5,7]. let\n    b:f32[7,5,8] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x5x7xf32>) -> (tensor<7x5x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<8x5x7xf32>) -> tensor<7x5x8xf32>\n    return %0 : tensor<7x5x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (60,))",
    "input_info": [
      {
        "shape": [
          6,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,10]. let\n    b:f32[60] = reshape[dimensions=None new_sizes=(60,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x10xf32>) -> (tensor<60xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<6x10xf32>) -> tensor<60xf32>\n    return %0 : tensor<60xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          9,
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,3] b:f32[9,3]. let\n    c:f32[18,3] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x3xf32>, %arg1: tensor<9x3xf32>) -> (tensor<18x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<9x3xf32>, tensor<9x3xf32>) -> tensor<18x3xf32>\n    return %0 : tensor<18x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 21))",
    "input_info": [
      {
        "shape": [
          7,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,6]. let\n    b:f32[2,21] = reshape[dimensions=None new_sizes=(2, 21) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x6xf32>) -> (tensor<2x21xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<7x6xf32>) -> tensor<2x21xf32>\n    return %0 : tensor<2x21xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 15))",
    "input_info": [
      {
        "shape": [
          10,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,3]. let\n    b:f32[2,15] = reshape[dimensions=None new_sizes=(2, 15) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x3xf32>) -> (tensor<2x15xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<10x3xf32>) -> tensor<2x15xf32>\n    return %0 : tensor<2x15xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          4,
          7,
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4,
          7,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,7,5] b:f32[4,7,5]. let\n    c:f32[8,7,5] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x7x5xf32>, %arg1: tensor<4x7x5xf32>) -> (tensor<8x7x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<4x7x5xf32>, tensor<4x7x5xf32>) -> tensor<8x7x5xf32>\n    return %0 : tensor<8x7x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 49))",
    "input_info": [
      {
        "shape": [
          2,
          7,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,7,7]. let\n    b:f32[2,49] = reshape[dimensions=None new_sizes=(2, 49) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x7x7xf32>) -> (tensor<2x49xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<2x7x7xf32>) -> tensor<2x49xf32>\n    return %0 : tensor<2x49xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          9,
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,9] b:f32[9,9]. let\n    c:f32[18,9] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x9xf32>, %arg1: tensor<9x9xf32>) -> (tensor<18x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<9x9xf32>, tensor<9x9xf32>) -> tensor<18x9xf32>\n    return %0 : tensor<18x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          2,
          8,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,8,10]. let\n    b:f32[1,8,10] = slice[\n      limit_indices=(2, 8, 10)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x8x10xf32>) -> (tensor<1x8x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:2, 0:8, 0:10] : (tensor<2x8x10xf32>) -> tensor<1x8x10xf32>\n    return %0 : tensor<1x8x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          6,
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,3] b:f32[6,3]. let\n    c:f32[12,3] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x3xf32>, %arg1: tensor<6x3xf32>) -> (tensor<12x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<6x3xf32>, tensor<6x3xf32>) -> tensor<12x3xf32>\n    return %0 : tensor<12x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          9,
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,3] b:f32[9,3]. let\n    c:f32[1,9,3] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 9, 3)\n      sharding=None\n    ] a\n    d:f32[1,9,3] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 9, 3)\n      sharding=None\n    ] b\n    e:f32[2,9,3] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x3xf32>, %arg1: tensor<9x3xf32>) -> (tensor<2x9x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<9x3xf32>) -> tensor<1x9x3xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<9x3xf32>) -> tensor<1x9x3xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x9x3xf32>, tensor<1x9x3xf32>) -> tensor<2x9x3xf32>\n    return %2 : tensor<2x9x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          5,
          7,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,7,9]. let\n    b:f32[9,7,5] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x7x9xf32>) -> (tensor<9x7x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<5x7x9xf32>) -> tensor<9x7x5xf32>\n    return %0 : tensor<9x7x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          9,
          9,
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9,
          9,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,9,9] b:f32[9,9,9]. let\n    c:f32[1,9,9,9] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 9, 9, 9)\n      sharding=None\n    ] a\n    d:f32[1,9,9,9] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 9, 9, 9)\n      sharding=None\n    ] b\n    e:f32[2,9,9,9] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x9x9xf32>, %arg1: tensor<9x9x9xf32>) -> (tensor<2x9x9x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<9x9x9xf32>) -> tensor<1x9x9x9xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<9x9x9xf32>) -> tensor<1x9x9x9xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x9x9x9xf32>, tensor<1x9x9x9xf32>) -> tensor<2x9x9x9xf32>\n    return %2 : tensor<2x9x9x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (6,))",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>) -> (tensor<6xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4] b:f32[4]. let\n    c:f32[1,4] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 4)\n      sharding=None\n    ] a\n    d:f32[1,4] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 4)\n      sharding=None\n    ] b\n    e:f32[2,4] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> (tensor<2x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<4xf32>) -> tensor<1x4xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<4xf32>) -> tensor<1x4xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x4xf32>, tensor<1x4xf32>) -> tensor<2x4xf32>\n    return %2 : tensor<2x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          10,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,10]. let\n    b:f32[9,10] = slice[\n      limit_indices=(10, 10)\n      start_indices=(1, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x10xf32>) -> (tensor<9x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:10, 0:10] : (tensor<10x10xf32>) -> tensor<9x10xf32>\n    return %0 : tensor<9x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8] b:f32[8]. let\n    c:f32[1,8] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 8)\n      sharding=None\n    ] a\n    d:f32[1,8] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 8)\n      sharding=None\n    ] b\n    e:f32[2,8] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8xf32>, %arg1: tensor<8xf32>) -> (tensor<2x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<8xf32>) -> tensor<1x8xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<8xf32>) -> tensor<1x8xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x8xf32>, tensor<1x8xf32>) -> tensor<2x8xf32>\n    return %2 : tensor<2x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          4,
          9,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,9,2]. let\n    b:f32[3,9,2] = slice[\n      limit_indices=(4, 9, 2)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x9x2xf32>) -> (tensor<3x9x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:4, 0:9, 0:2] : (tensor<4x9x2xf32>) -> tensor<3x9x2xf32>\n    return %0 : tensor<3x9x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          9,
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,9] b:f32[9,9]. let\n    c:f32[18,9] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x9xf32>, %arg1: tensor<9x9xf32>) -> (tensor<18x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<9x9xf32>, tensor<9x9xf32>) -> tensor<18x9xf32>\n    return %0 : tensor<18x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          5,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,6]. let\n    b:f32[4,6] = slice[limit_indices=(5, 6) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x6xf32>) -> (tensor<4x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:5, 0:6] : (tensor<5x6xf32>) -> tensor<4x6xf32>\n    return %0 : tensor<4x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          2,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,4]. let\n    b:f32[4,2] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x4xf32>) -> (tensor<4x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<2x4xf32>) -> tensor<4x2xf32>\n    return %0 : tensor<4x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 18))",
    "input_info": [
      {
        "shape": [
          6,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,6]. let\n    b:f32[2,18] = reshape[dimensions=None new_sizes=(2, 18) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x6xf32>) -> (tensor<2x18xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<6x6xf32>) -> tensor<2x18xf32>\n    return %0 : tensor<2x18xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          10,
          7,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,7,4]. let\n    b:f32[9,7,4] = slice[\n      limit_indices=(10, 7, 4)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x7x4xf32>) -> (tensor<9x7x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:10, 0:7, 0:4] : (tensor<10x7x4xf32>) -> tensor<9x7x4xf32>\n    return %0 : tensor<9x7x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          9,
          2,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,2,4]. let\n    b:f32[4,2,9] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x2x4xf32>) -> (tensor<4x2x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<9x2x4xf32>) -> tensor<4x2x9xf32>\n    return %0 : tensor<4x2x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          6,
          4,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,4,8]. let\n    b:f32[5,4,8] = slice[\n      limit_indices=(6, 4, 8)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x4x8xf32>) -> (tensor<5x4x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:6, 0:4, 0:8] : (tensor<6x4x8xf32>) -> tensor<5x4x8xf32>\n    return %0 : tensor<5x4x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          8,
          7,
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          7,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,7,9] b:f32[8,7,9]. let\n    c:f32[16,7,9] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x7x9xf32>, %arg1: tensor<8x7x9xf32>) -> (tensor<16x7x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<8x7x9xf32>, tensor<8x7x9xf32>) -> tensor<16x7x9xf32>\n    return %0 : tensor<16x7x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (9,))",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>) -> (tensor<9xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          10,
          9,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,9,10]. let\n    b:f32[9,9,10] = slice[\n      limit_indices=(10, 9, 10)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x9x10xf32>) -> (tensor<9x9x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:10, 0:9, 0:10] : (tensor<10x9x10xf32>) -> tensor<9x9x10xf32>\n    return %0 : tensor<9x9x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          9,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,2]. let\n    b:f32[2,9] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x2xf32>) -> (tensor<2x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<9x2xf32>) -> tensor<2x9xf32>\n    return %0 : tensor<2x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (10,))",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          8,
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,8] b:f32[8,8]. let\n    c:f32[16,8] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x8xf32>, %arg1: tensor<8x8xf32>) -> (tensor<16x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<8x8xf32>, tensor<8x8xf32>) -> tensor<16x8xf32>\n    return %0 : tensor<16x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2] b:f32[2]. let\n    c:f32[4] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>, %arg1: tensor<2xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<2xf32>, tensor<2xf32>) -> tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          8,
          5,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,5,10]. let\n    b:f32[10,5,8] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x5x10xf32>) -> (tensor<10x5x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<8x5x10xf32>) -> tensor<10x5x8xf32>\n    return %0 : tensor<10x5x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2,))",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>) -> (tensor<2xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (3,))",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>) -> (tensor<3xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          6,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,5]. let\n    b:f32[5,5] = slice[limit_indices=(6, 5) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x5xf32>) -> (tensor<5x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:6, 0:5] : (tensor<6x5xf32>) -> tensor<5x5xf32>\n    return %0 : tensor<5x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          9,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,10]. let\n    b:f32[8,10] = slice[limit_indices=(9, 10) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x10xf32>) -> (tensor<8x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:9, 0:10] : (tensor<9x10xf32>) -> tensor<8x10xf32>\n    return %0 : tensor<8x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          5,
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,7] b:f32[5,7]. let\n    c:f32[10,7] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x7xf32>, %arg1: tensor<5x7xf32>) -> (tensor<10x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<5x7xf32>, tensor<5x7xf32>) -> tensor<10x7xf32>\n    return %0 : tensor<10x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          3,
          6,
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3,
          6,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,6,9] b:f32[3,6,9]. let\n    c:f32[1,3,6,9] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 3, 6, 9)\n      sharding=None\n    ] a\n    d:f32[1,3,6,9] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 3, 6, 9)\n      sharding=None\n    ] b\n    e:f32[2,3,6,9] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x6x9xf32>, %arg1: tensor<3x6x9xf32>) -> (tensor<2x3x6x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<3x6x9xf32>) -> tensor<1x3x6x9xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<3x6x9xf32>) -> tensor<1x3x6x9xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x3x6x9xf32>, tensor<1x3x6x9xf32>) -> tensor<2x3x6x9xf32>\n    return %2 : tensor<2x3x6x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>) -> (tensor<2xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>) -> (tensor<9xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          3,
          2,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,2,10]. let\n    b:f32[10,2,3] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x2x10xf32>) -> (tensor<10x2x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<3x2x10xf32>) -> tensor<10x2x3xf32>\n    return %0 : tensor<10x2x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          8,
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,8] b:f32[8,8]. let\n    c:f32[1,8,8] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 8, 8)\n      sharding=None\n    ] a\n    d:f32[1,8,8] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 8, 8)\n      sharding=None\n    ] b\n    e:f32[2,8,8] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x8xf32>, %arg1: tensor<8x8xf32>) -> (tensor<2x8x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<8x8xf32>) -> tensor<1x8x8xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<8x8xf32>) -> tensor<1x8x8xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x8x8xf32>, tensor<1x8x8xf32>) -> tensor<2x8x8xf32>\n    return %2 : tensor<2x8x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9] b:f32[9]. let\n    c:f32[18] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>, %arg1: tensor<9xf32>) -> (tensor<18xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<9xf32>, tensor<9xf32>) -> tensor<18xf32>\n    return %0 : tensor<18xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          9,
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,2] b:f32[9,2]. let\n    c:f32[1,9,2] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 9, 2)\n      sharding=None\n    ] a\n    d:f32[1,9,2] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 9, 2)\n      sharding=None\n    ] b\n    e:f32[2,9,2] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x2xf32>, %arg1: tensor<9x2xf32>) -> (tensor<2x9x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<9x2xf32>) -> tensor<1x9x2xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<9x2xf32>) -> tensor<1x9x2xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x9x2xf32>, tensor<1x9x2xf32>) -> tensor<2x9x2xf32>\n    return %2 : tensor<2x9x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (60,))",
    "input_info": [
      {
        "shape": [
          10,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,6]. let\n    b:f32[60] = reshape[dimensions=None new_sizes=(60,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x6xf32>) -> (tensor<60xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<10x6xf32>) -> tensor<60xf32>\n    return %0 : tensor<60xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9] b:f32[9]. let\n    c:f32[1,9] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 9)\n      sharding=None\n    ] a\n    d:f32[1,9] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 9)\n      sharding=None\n    ] b\n    e:f32[2,9] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>, %arg1: tensor<9xf32>) -> (tensor<2x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<9xf32>) -> tensor<1x9xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<9xf32>) -> tensor<1x9xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x9xf32>, tensor<1x9xf32>) -> tensor<2x9xf32>\n    return %2 : tensor<2x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          10,
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,10] b:f32[10,10]. let\n    c:f32[20,10] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x10xf32>, %arg1: tensor<10x10xf32>) -> (tensor<20x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<10x10xf32>, tensor<10x10xf32>) -> tensor<20x10xf32>\n    return %0 : tensor<20x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (810,))",
    "input_info": [
      {
        "shape": [
          9,
          9,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,9,10]. let\n    b:f32[810] = reshape[dimensions=None new_sizes=(810,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x9x10xf32>) -> (tensor<810xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<9x9x10xf32>) -> tensor<810xf32>\n    return %0 : tensor<810xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 3))",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6]. let\n    b:f32[2,3] = reshape[dimensions=None new_sizes=(2, 3) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>) -> (tensor<2x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<6xf32>) -> tensor<2x3xf32>\n    return %0 : tensor<2x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          5,
          2,
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5,
          2,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,2,5] b:f32[5,2,5]. let\n    c:f32[10,2,5] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x2x5xf32>, %arg1: tensor<5x2x5xf32>) -> (tensor<10x2x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<5x2x5xf32>, tensor<5x2x5xf32>) -> tensor<10x2x5xf32>\n    return %0 : tensor<10x2x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4] b:f32[4]. let\n    c:f32[8] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> (tensor<8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<4xf32>, tensor<4xf32>) -> tensor<8xf32>\n    return %0 : tensor<8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10] b:f32[10]. let\n    c:f32[1,10] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 10)\n      sharding=None\n    ] a\n    d:f32[1,10] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 10)\n      sharding=None\n    ] b\n    e:f32[2,10] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>, %arg1: tensor<10xf32>) -> (tensor<2x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<10xf32>) -> tensor<1x10xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<10xf32>) -> tensor<1x10xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<2x10xf32>\n    return %2 : tensor<2x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          2,
          10,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,10,5]. let\n    b:f32[1,10,5] = slice[\n      limit_indices=(2, 10, 5)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x10x5xf32>) -> (tensor<1x10x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:2, 0:10, 0:5] : (tensor<2x10x5xf32>) -> tensor<1x10x5xf32>\n    return %0 : tensor<1x10x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          9,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,5]. let\n    b:f32[8,5] = slice[limit_indices=(9, 5) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x5xf32>) -> (tensor<8x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:9, 0:5] : (tensor<9x5xf32>) -> tensor<8x5xf32>\n    return %0 : tensor<8x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          10,
          2,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,2,6]. let\n    b:f32[9,2,6] = slice[\n      limit_indices=(10, 2, 6)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x2x6xf32>) -> (tensor<9x2x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:10, 0:2, 0:6] : (tensor<10x2x6xf32>) -> tensor<9x2x6xf32>\n    return %0 : tensor<9x2x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2] b:f32[2]. let\n    c:f32[1,2] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 2)\n      sharding=None\n    ] a\n    d:f32[1,2] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 2)\n      sharding=None\n    ] b\n    e:f32[2,2] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>, %arg1: tensor<2xf32>) -> (tensor<2x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<2xf32>) -> tensor<1x2xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<2xf32>) -> tensor<1x2xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x2xf32>, tensor<1x2xf32>) -> tensor<2x2xf32>\n    return %2 : tensor<2x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          7,
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,2] b:f32[7,2]. let\n    c:f32[14,2] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x2xf32>, %arg1: tensor<7x2xf32>) -> (tensor<14x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<7x2xf32>, tensor<7x2xf32>) -> tensor<14x2xf32>\n    return %0 : tensor<14x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          9,
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,3] b:f32[9,3]. let\n    c:f32[18,3] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x3xf32>, %arg1: tensor<9x3xf32>) -> (tensor<18x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<9x3xf32>, tensor<9x3xf32>) -> tensor<18x3xf32>\n    return %0 : tensor<18x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          6,
          9,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,9,3]. let\n    b:f32[3,9,6] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x9x3xf32>) -> (tensor<3x9x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<6x9x3xf32>) -> tensor<3x9x6xf32>\n    return %0 : tensor<3x9x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          7,
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,7] b:f32[7,7]. let\n    c:f32[14,7] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x7xf32>, %arg1: tensor<7x7xf32>) -> (tensor<14x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<7x7xf32>, tensor<7x7xf32>) -> tensor<14x7xf32>\n    return %0 : tensor<14x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          8,
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,6] b:f32[8,6]. let\n    c:f32[16,6] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x6xf32>, %arg1: tensor<8x6xf32>) -> (tensor<16x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<8x6xf32>, tensor<8x6xf32>) -> tensor<16x6xf32>\n    return %0 : tensor<16x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          3,
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,9] b:f32[3,9]. let\n    c:f32[1,3,9] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 3, 9)\n      sharding=None\n    ] a\n    d:f32[1,3,9] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 3, 9)\n      sharding=None\n    ] b\n    e:f32[2,3,9] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x9xf32>, %arg1: tensor<3x9xf32>) -> (tensor<2x3x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<3x9xf32>) -> tensor<1x3x9xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<3x9xf32>) -> tensor<1x3x9xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x3x9xf32>, tensor<1x3x9xf32>) -> tensor<2x3x9xf32>\n    return %2 : tensor<2x3x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          6,
          4,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,4,10]. let\n    b:f32[10,4,6] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x4x10xf32>) -> (tensor<10x4x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<6x4x10xf32>) -> tensor<10x4x6xf32>\n    return %0 : tensor<10x4x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>) -> (tensor<7xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          2,
          3,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,3,8]. let\n    b:f32[1,3,8] = slice[\n      limit_indices=(2, 3, 8)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x3x8xf32>) -> (tensor<1x3x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:2, 0:3, 0:8] : (tensor<2x3x8xf32>) -> tensor<1x3x8xf32>\n    return %0 : tensor<1x3x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>) -> (tensor<5xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (18,))",
    "input_info": [
      {
        "shape": [
          2,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,9]. let\n    b:f32[18] = reshape[dimensions=None new_sizes=(18,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x9xf32>) -> (tensor<18xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<2x9xf32>) -> tensor<18xf32>\n    return %0 : tensor<18xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2,))",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>) -> (tensor<2xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          5,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,6]. let\n    b:f32[4,6] = slice[limit_indices=(5, 6) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x6xf32>) -> (tensor<4x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:5, 0:6] : (tensor<5x6xf32>) -> tensor<4x6xf32>\n    return %0 : tensor<4x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          2,
          8,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,8,8]. let\n    b:f32[8,8,2] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x8x8xf32>) -> (tensor<8x8x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<2x8x8xf32>) -> tensor<8x8x2xf32>\n    return %0 : tensor<8x8x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3]. let\n    b:f32[2] = slice[limit_indices=(3,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>) -> (tensor<2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:3] : (tensor<3xf32>) -> tensor<2xf32>\n    return %0 : tensor<2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          2,
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,2] b:f32[2,2]. let\n    c:f32[4,2] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x2xf32>, %arg1: tensor<2x2xf32>) -> (tensor<4x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<2x2xf32>, tensor<2x2xf32>) -> tensor<4x2xf32>\n    return %0 : tensor<4x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (8,))",
    "input_info": [
      {
        "shape": [
          4,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,2]. let\n    b:f32[8] = reshape[dimensions=None new_sizes=(8,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x2xf32>) -> (tensor<8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<4x2xf32>) -> tensor<8xf32>\n    return %0 : tensor<8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          8,
          4,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,4,10]. let\n    b:f32[7,4,10] = slice[\n      limit_indices=(8, 4, 10)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x4x10xf32>) -> (tensor<7x4x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:8, 0:4, 0:10] : (tensor<8x4x10xf32>) -> tensor<7x4x10xf32>\n    return %0 : tensor<7x4x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          3,
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,6] b:f32[3,6]. let\n    c:f32[1,3,6] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 3, 6)\n      sharding=None\n    ] a\n    d:f32[1,3,6] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 3, 6)\n      sharding=None\n    ] b\n    e:f32[2,3,6] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x6xf32>, %arg1: tensor<3x6xf32>) -> (tensor<2x3x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<3x6xf32>) -> tensor<1x3x6xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<3x6xf32>) -> tensor<1x3x6xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x3x6xf32>, tensor<1x3x6xf32>) -> tensor<2x3x6xf32>\n    return %2 : tensor<2x3x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 28))",
    "input_info": [
      {
        "shape": [
          7,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,8]. let\n    b:f32[2,28] = reshape[dimensions=None new_sizes=(2, 28) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x8xf32>) -> (tensor<2x28xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<7x8xf32>) -> tensor<2x28xf32>\n    return %0 : tensor<2x28xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          10,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,7]. let\n    b:f32[7,10] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x7xf32>) -> (tensor<7x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<10x7xf32>) -> tensor<7x10xf32>\n    return %0 : tensor<7x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          5,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,5]. let\n    b:f32[5,5] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x5xf32>) -> (tensor<5x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<5x5xf32>) -> tensor<5x5xf32>\n    return %0 : tensor<5x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          4,
          6,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,6,6]. let\n    b:f32[3,6,6] = slice[\n      limit_indices=(4, 6, 6)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x6x6xf32>) -> (tensor<3x6x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:4, 0:6, 0:6] : (tensor<4x6x6xf32>) -> tensor<3x6x6xf32>\n    return %0 : tensor<3x6x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 72))",
    "input_info": [
      {
        "shape": [
          4,
          9,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,9,4]. let\n    b:f32[2,72] = reshape[dimensions=None new_sizes=(2, 72) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x9x4xf32>) -> (tensor<2x72xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<4x9x4xf32>) -> tensor<2x72xf32>\n    return %0 : tensor<2x72xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9] b:f32[9]. let\n    c:f32[18] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>, %arg1: tensor<9xf32>) -> (tensor<18xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<9xf32>, tensor<9xf32>) -> tensor<18xf32>\n    return %0 : tensor<18xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          7,
          7,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,7,5]. let\n    b:f32[6,7,5] = slice[\n      limit_indices=(7, 7, 5)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x7x5xf32>) -> (tensor<6x7x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:7, 0:7, 0:5] : (tensor<7x7x5xf32>) -> tensor<6x7x5xf32>\n    return %0 : tensor<6x7x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          3,
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,10] b:f32[3,10]. let\n    c:f32[6,10] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x10xf32>, %arg1: tensor<3x10xf32>) -> (tensor<6x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<3x10xf32>, tensor<3x10xf32>) -> tensor<6x10xf32>\n    return %0 : tensor<6x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          7,
          4,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,4,4]. let\n    b:f32[4,4,7] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x4x4xf32>) -> (tensor<4x4x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<7x4x4xf32>) -> tensor<4x4x7xf32>\n    return %0 : tensor<4x4x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          9,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,7]. let\n    b:f32[8,7] = slice[limit_indices=(9, 7) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x7xf32>) -> (tensor<8x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:9, 0:7] : (tensor<9x7xf32>) -> tensor<8x7xf32>\n    return %0 : tensor<8x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (600,))",
    "input_info": [
      {
        "shape": [
          10,
          10,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,10,6]. let\n    b:f32[600] = reshape[dimensions=None new_sizes=(600,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x10x6xf32>) -> (tensor<600xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<10x10x6xf32>) -> tensor<600xf32>\n    return %0 : tensor<600xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          5,
          3,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,3,7]. let\n    b:f32[4,3,7] = slice[\n      limit_indices=(5, 3, 7)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x3x7xf32>) -> (tensor<4x3x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:5, 0:3, 0:7] : (tensor<5x3x7xf32>) -> tensor<4x3x7xf32>\n    return %0 : tensor<4x3x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          3,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,8]. let\n    b:f32[8,3] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x8xf32>) -> (tensor<8x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<3x8xf32>) -> tensor<8x3xf32>\n    return %0 : tensor<8x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          3,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,9]. let\n    b:f32[2,9] = slice[limit_indices=(3, 9) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x9xf32>) -> (tensor<2x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:3, 0:9] : (tensor<3x9xf32>) -> tensor<2x9xf32>\n    return %0 : tensor<2x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          6,
          7,
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6,
          7,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,7,9] b:f32[6,7,9]. let\n    c:f32[12,7,9] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x7x9xf32>, %arg1: tensor<6x7x9xf32>) -> (tensor<12x7x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<6x7x9xf32>, tensor<6x7x9xf32>) -> tensor<12x7x9xf32>\n    return %0 : tensor<12x7x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          4,
          3,
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4,
          3,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,3,8] b:f32[4,3,8]. let\n    c:f32[8,3,8] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x3x8xf32>, %arg1: tensor<4x3x8xf32>) -> (tensor<8x3x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<4x3x8xf32>, tensor<4x3x8xf32>) -> tensor<8x3x8xf32>\n    return %0 : tensor<8x3x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          2,
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,3] b:f32[2,3]. let\n    c:f32[1,2,3] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 2, 3)\n      sharding=None\n    ] a\n    d:f32[1,2,3] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 2, 3)\n      sharding=None\n    ] b\n    e:f32[2,2,3] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x3xf32>, %arg1: tensor<2x3xf32>) -> (tensor<2x2x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<2x3xf32>) -> tensor<1x2x3xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<2x3xf32>) -> tensor<1x2x3xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x2x3xf32>, tensor<1x2x3xf32>) -> tensor<2x2x3xf32>\n    return %2 : tensor<2x2x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          10,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,6]. let\n    b:f32[9,6] = slice[limit_indices=(10, 6) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x6xf32>) -> (tensor<9x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:10, 0:6] : (tensor<10x6xf32>) -> tensor<9x6xf32>\n    return %0 : tensor<9x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          7,
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,7] b:f32[7,7]. let\n    c:f32[14,7] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x7xf32>, %arg1: tensor<7x7xf32>) -> (tensor<14x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<7x7xf32>, tensor<7x7xf32>) -> tensor<14x7xf32>\n    return %0 : tensor<14x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          2,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,6]. let\n    b:f32[1,6] = slice[limit_indices=(2, 6) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x6xf32>) -> (tensor<1x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:2, 0:6] : (tensor<2x6xf32>) -> tensor<1x6xf32>\n    return %0 : tensor<1x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          6,
          6,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,6,10]. let\n    b:f32[10,6,6] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x6x10xf32>) -> (tensor<10x6x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<6x6x10xf32>) -> tensor<10x6x6xf32>\n    return %0 : tensor<10x6x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          4,
          3,
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4,
          3,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,3,8] b:f32[4,3,8]. let\n    c:f32[1,4,3,8] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 4, 3, 8)\n      sharding=None\n    ] a\n    d:f32[1,4,3,8] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 4, 3, 8)\n      sharding=None\n    ] b\n    e:f32[2,4,3,8] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x3x8xf32>, %arg1: tensor<4x3x8xf32>) -> (tensor<2x4x3x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<4x3x8xf32>) -> tensor<1x4x3x8xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<4x3x8xf32>) -> tensor<1x4x3x8xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x4x3x8xf32>, tensor<1x4x3x8xf32>) -> tensor<2x4x3x8xf32>\n    return %2 : tensor<2x4x3x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          9,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,10]. let\n    b:f32[8,10] = slice[limit_indices=(9, 10) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x10xf32>) -> (tensor<8x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:9, 0:10] : (tensor<9x10xf32>) -> tensor<8x10xf32>\n    return %0 : tensor<8x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 4))",
    "input_info": [
      {
        "shape": [
          4,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,2]. let\n    b:f32[2,4] = reshape[dimensions=None new_sizes=(2, 4) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x2xf32>) -> (tensor<2x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<4x2xf32>) -> tensor<2x4xf32>\n    return %0 : tensor<2x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          6,
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,3] b:f32[6,3]. let\n    c:f32[12,3] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x3xf32>, %arg1: tensor<6x3xf32>) -> (tensor<12x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<6x3xf32>, tensor<6x3xf32>) -> tensor<12x3xf32>\n    return %0 : tensor<12x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          6,
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,10] b:f32[6,10]. let\n    c:f32[12,10] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x10xf32>, %arg1: tensor<6x10xf32>) -> (tensor<12x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<6x10xf32>, tensor<6x10xf32>) -> tensor<12x10xf32>\n    return %0 : tensor<12x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 20))",
    "input_info": [
      {
        "shape": [
          5,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,8]. let\n    b:f32[2,20] = reshape[dimensions=None new_sizes=(2, 20) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x8xf32>) -> (tensor<2x20xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<5x8xf32>) -> tensor<2x20xf32>\n    return %0 : tensor<2x20xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8] b:f32[8]. let\n    c:f32[1,8] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 8)\n      sharding=None\n    ] a\n    d:f32[1,8] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 8)\n      sharding=None\n    ] b\n    e:f32[2,8] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8xf32>, %arg1: tensor<8xf32>) -> (tensor<2x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<8xf32>) -> tensor<1x8xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<8xf32>) -> tensor<1x8xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x8xf32>, tensor<1x8xf32>) -> tensor<2x8xf32>\n    return %2 : tensor<2x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          9,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,3]. let\n    b:f32[8,3] = slice[limit_indices=(9, 3) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x3xf32>) -> (tensor<8x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:9, 0:3] : (tensor<9x3xf32>) -> tensor<8x3xf32>\n    return %0 : tensor<8x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          10,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,9]. let\n    b:f32[9,9] = slice[limit_indices=(10, 9) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x9xf32>) -> (tensor<9x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:10, 0:9] : (tensor<10x9xf32>) -> tensor<9x9xf32>\n    return %0 : tensor<9x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          10,
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,6] b:f32[10,6]. let\n    c:f32[20,6] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x6xf32>, %arg1: tensor<10x6xf32>) -> (tensor<20x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<10x6xf32>, tensor<10x6xf32>) -> tensor<20x6xf32>\n    return %0 : tensor<20x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          2,
          3,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,3,9]. let\n    b:f32[9,3,2] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x3x9xf32>) -> (tensor<9x3x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<2x3x9xf32>) -> tensor<9x3x2xf32>\n    return %0 : tensor<9x3x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          10,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,8]. let\n    b:f32[9,8] = slice[limit_indices=(10, 8) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x8xf32>) -> (tensor<9x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:10, 0:8] : (tensor<10x8xf32>) -> tensor<9x8xf32>\n    return %0 : tensor<9x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7] b:f32[7]. let\n    c:f32[1,7] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 7)\n      sharding=None\n    ] a\n    d:f32[1,7] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 7)\n      sharding=None\n    ] b\n    e:f32[2,7] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>, %arg1: tensor<7xf32>) -> (tensor<2x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<7xf32>) -> tensor<1x7xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<7xf32>) -> tensor<1x7xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x7xf32>, tensor<1x7xf32>) -> tensor<2x7xf32>\n    return %2 : tensor<2x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          10,
          4,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,4,5]. let\n    b:f32[9,4,5] = slice[\n      limit_indices=(10, 4, 5)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x4x5xf32>) -> (tensor<9x4x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:10, 0:4, 0:5] : (tensor<10x4x5xf32>) -> tensor<9x4x5xf32>\n    return %0 : tensor<9x4x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (126,))",
    "input_info": [
      {
        "shape": [
          6,
          3,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,3,7]. let\n    b:f32[126] = reshape[dimensions=None new_sizes=(126,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x3x7xf32>) -> (tensor<126xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<6x3x7xf32>) -> tensor<126xf32>\n    return %0 : tensor<126xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7] b:f32[7]. let\n    c:f32[1,7] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 7)\n      sharding=None\n    ] a\n    d:f32[1,7] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 7)\n      sharding=None\n    ] b\n    e:f32[2,7] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>, %arg1: tensor<7xf32>) -> (tensor<2x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<7xf32>) -> tensor<1x7xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<7xf32>) -> tensor<1x7xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x7xf32>, tensor<1x7xf32>) -> tensor<2x7xf32>\n    return %2 : tensor<2x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          3,
          5,
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3,
          5,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,5,2] b:f32[3,5,2]. let\n    c:f32[1,3,5,2] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 3, 5, 2)\n      sharding=None\n    ] a\n    d:f32[1,3,5,2] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 3, 5, 2)\n      sharding=None\n    ] b\n    e:f32[2,3,5,2] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x5x2xf32>, %arg1: tensor<3x5x2xf32>) -> (tensor<2x3x5x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<3x5x2xf32>) -> tensor<1x3x5x2xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<3x5x2xf32>) -> tensor<1x3x5x2xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x3x5x2xf32>, tensor<1x3x5x2xf32>) -> tensor<2x3x5x2xf32>\n    return %2 : tensor<2x3x5x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7] b:f32[7]. let\n    c:f32[1,7] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 7)\n      sharding=None\n    ] a\n    d:f32[1,7] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 7)\n      sharding=None\n    ] b\n    e:f32[2,7] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>, %arg1: tensor<7xf32>) -> (tensor<2x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<7xf32>) -> tensor<1x7xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<7xf32>) -> tensor<1x7xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x7xf32>, tensor<1x7xf32>) -> tensor<2x7xf32>\n    return %2 : tensor<2x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          2,
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,2] b:f32[2,2]. let\n    c:f32[4,2] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x2xf32>, %arg1: tensor<2x2xf32>) -> (tensor<4x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<2x2xf32>, tensor<2x2xf32>) -> tensor<4x2xf32>\n    return %0 : tensor<4x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          5,
          3,
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5,
          3,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,3,8] b:f32[5,3,8]. let\n    c:f32[1,5,3,8] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 5, 3, 8)\n      sharding=None\n    ] a\n    d:f32[1,5,3,8] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 5, 3, 8)\n      sharding=None\n    ] b\n    e:f32[2,5,3,8] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x3x8xf32>, %arg1: tensor<5x3x8xf32>) -> (tensor<2x5x3x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<5x3x8xf32>) -> tensor<1x5x3x8xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<5x3x8xf32>) -> tensor<1x5x3x8xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x5x3x8xf32>, tensor<1x5x3x8xf32>) -> tensor<2x5x3x8xf32>\n    return %2 : tensor<2x5x3x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 2))",
    "input_info": [
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4]. let\n    b:f32[2,2] = reshape[dimensions=None new_sizes=(2, 2) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<2x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<4xf32>) -> tensor<2x2xf32>\n    return %0 : tensor<2x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          7,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,4]. let\n    b:f32[4,7] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x4xf32>) -> (tensor<4x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<7x4xf32>) -> tensor<4x7xf32>\n    return %0 : tensor<4x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (378,))",
    "input_info": [
      {
        "shape": [
          6,
          9,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,9,7]. let\n    b:f32[378] = reshape[dimensions=None new_sizes=(378,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x9x7xf32>) -> (tensor<378xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<6x9x7xf32>) -> tensor<378xf32>\n    return %0 : tensor<378xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4] b:f32[4]. let\n    c:f32[8] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> (tensor<8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<4xf32>, tensor<4xf32>) -> tensor<8xf32>\n    return %0 : tensor<8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          5,
          4,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,4,4]. let\n    b:f32[4,4,4] = slice[\n      limit_indices=(5, 4, 4)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x4x4xf32>) -> (tensor<4x4x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:5, 0:4, 0:4] : (tensor<5x4x4xf32>) -> tensor<4x4x4xf32>\n    return %0 : tensor<4x4x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          7,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,3]. let\n    b:f32[6,3] = slice[limit_indices=(7, 3) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x3xf32>) -> (tensor<6x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:7, 0:3] : (tensor<7x3xf32>) -> tensor<6x3xf32>\n    return %0 : tensor<6x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          10,
          9,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,9,9]. let\n    b:f32[9,9,10] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x9x9xf32>) -> (tensor<9x9x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<10x9x9xf32>) -> tensor<9x9x10xf32>\n    return %0 : tensor<9x9x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>) -> (tensor<3xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7] b:f32[7]. let\n    c:f32[1,7] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 7)\n      sharding=None\n    ] a\n    d:f32[1,7] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 7)\n      sharding=None\n    ] b\n    e:f32[2,7] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>, %arg1: tensor<7xf32>) -> (tensor<2x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<7xf32>) -> tensor<1x7xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<7xf32>) -> tensor<1x7xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x7xf32>, tensor<1x7xf32>) -> tensor<2x7xf32>\n    return %2 : tensor<2x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          6,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,4]. let\n    b:f32[4,6] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x4xf32>) -> (tensor<4x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<6x4xf32>) -> tensor<4x6xf32>\n    return %0 : tensor<4x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (360,))",
    "input_info": [
      {
        "shape": [
          9,
          8,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,8,5]. let\n    b:f32[360] = reshape[dimensions=None new_sizes=(360,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x8x5xf32>) -> (tensor<360xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<9x8x5xf32>) -> tensor<360xf32>\n    return %0 : tensor<360xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          2,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,5]. let\n    b:f32[1,5] = slice[limit_indices=(2, 5) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x5xf32>) -> (tensor<1x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:2, 0:5] : (tensor<2x5xf32>) -> tensor<1x5xf32>\n    return %0 : tensor<1x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (3, 3))",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9]. let\n    b:f32[3,3] = reshape[dimensions=None new_sizes=(3, 3) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>) -> (tensor<3x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<9xf32>) -> tensor<3x3xf32>\n    return %0 : tensor<3x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>) -> (tensor<2xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5] b:f32[5]. let\n    c:f32[1,5] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 5)\n      sharding=None\n    ] a\n    d:f32[1,5] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 5)\n      sharding=None\n    ] b\n    e:f32[2,5] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>, %arg1: tensor<5xf32>) -> (tensor<2x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<5xf32>) -> tensor<1x5xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<5xf32>) -> tensor<1x5xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x5xf32>, tensor<1x5xf32>) -> tensor<2x5xf32>\n    return %2 : tensor<2x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5] b:f32[5]. let\n    c:f32[10] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>, %arg1: tensor<5xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<5xf32>, tensor<5xf32>) -> tensor<10xf32>\n    return %0 : tensor<10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8xf32>) -> (tensor<8xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2] b:f32[2]. let\n    c:f32[1,2] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 2)\n      sharding=None\n    ] a\n    d:f32[1,2] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 2)\n      sharding=None\n    ] b\n    e:f32[2,2] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>, %arg1: tensor<2xf32>) -> (tensor<2x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<2xf32>) -> tensor<1x2xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<2xf32>) -> tensor<1x2xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x2xf32>, tensor<1x2xf32>) -> tensor<2x2xf32>\n    return %2 : tensor<2x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (5,))",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>) -> (tensor<5xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          5,
          5,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,5,6]. let\n    b:f32[4,5,6] = slice[\n      limit_indices=(5, 5, 6)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x5x6xf32>) -> (tensor<4x5x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:5, 0:5, 0:6] : (tensor<5x5x6xf32>) -> tensor<4x5x6xf32>\n    return %0 : tensor<4x5x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          3,
          7,
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3,
          7,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,7,9] b:f32[3,7,9]. let\n    c:f32[1,3,7,9] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 3, 7, 9)\n      sharding=None\n    ] a\n    d:f32[1,3,7,9] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 3, 7, 9)\n      sharding=None\n    ] b\n    e:f32[2,3,7,9] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x7x9xf32>, %arg1: tensor<3x7x9xf32>) -> (tensor<2x3x7x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<3x7x9xf32>) -> tensor<1x3x7x9xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<3x7x9xf32>) -> tensor<1x3x7x9xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x3x7x9xf32>, tensor<1x3x7x9xf32>) -> tensor<2x3x7x9xf32>\n    return %2 : tensor<2x3x7x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          10,
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,8] b:f32[10,8]. let\n    c:f32[20,8] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x8xf32>, %arg1: tensor<10x8xf32>) -> (tensor<20x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<10x8xf32>, tensor<10x8xf32>) -> tensor<20x8xf32>\n    return %0 : tensor<20x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 63))",
    "input_info": [
      {
        "shape": [
          7,
          3,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,3,6]. let\n    b:f32[2,63] = reshape[dimensions=None new_sizes=(2, 63) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x3x6xf32>) -> (tensor<2x63xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<7x3x6xf32>) -> tensor<2x63xf32>\n    return %0 : tensor<2x63xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9]. let\n    b:f32[8] = slice[limit_indices=(9,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>) -> (tensor<8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:9] : (tensor<9xf32>) -> tensor<8xf32>\n    return %0 : tensor<8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4] b:f32[4]. let\n    c:f32[1,4] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 4)\n      sharding=None\n    ] a\n    d:f32[1,4] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 4)\n      sharding=None\n    ] b\n    e:f32[2,4] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> (tensor<2x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<4xf32>) -> tensor<1x4xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<4xf32>) -> tensor<1x4xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x4xf32>, tensor<1x4xf32>) -> tensor<2x4xf32>\n    return %2 : tensor<2x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 63))",
    "input_info": [
      {
        "shape": [
          6,
          3,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,3,7]. let\n    b:f32[2,63] = reshape[dimensions=None new_sizes=(2, 63) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x3x7xf32>) -> (tensor<2x63xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<6x3x7xf32>) -> tensor<2x63xf32>\n    return %0 : tensor<2x63xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (3, 105))",
    "input_info": [
      {
        "shape": [
          7,
          9,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,9,5]. let\n    b:f32[3,105] = reshape[dimensions=None new_sizes=(3, 105) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x9x5xf32>) -> (tensor<3x105xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<7x9x5xf32>) -> tensor<3x105xf32>\n    return %0 : tensor<3x105xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          3,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,2]. let\n    b:f32[2,3] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x2xf32>) -> (tensor<2x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<3x2xf32>) -> tensor<2x3xf32>\n    return %0 : tensor<2x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5] b:f32[5]. let\n    c:f32[1,5] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 5)\n      sharding=None\n    ] a\n    d:f32[1,5] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 5)\n      sharding=None\n    ] b\n    e:f32[2,5] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>, %arg1: tensor<5xf32>) -> (tensor<2x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<5xf32>) -> tensor<1x5xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<5xf32>) -> tensor<1x5xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x5xf32>, tensor<1x5xf32>) -> tensor<2x5xf32>\n    return %2 : tensor<2x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2]. let\n    b:f32[1] = slice[limit_indices=(2,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>) -> (tensor<1xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:2] : (tensor<2xf32>) -> tensor<1xf32>\n    return %0 : tensor<1xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2] b:f32[2]. let\n    c:f32[4] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>, %arg1: tensor<2xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<2xf32>, tensor<2xf32>) -> tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>) -> (tensor<5xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 280))",
    "input_info": [
      {
        "shape": [
          8,
          7,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,7,10]. let\n    b:f32[2,280] = reshape[dimensions=None new_sizes=(2, 280) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x7x10xf32>) -> (tensor<2x280xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<8x7x10xf32>) -> tensor<2x280xf32>\n    return %0 : tensor<2x280xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 12))",
    "input_info": [
      {
        "shape": [
          3,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,8]. let\n    b:f32[2,12] = reshape[dimensions=None new_sizes=(2, 12) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x8xf32>) -> (tensor<2x12xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<3x8xf32>) -> tensor<2x12xf32>\n    return %0 : tensor<2x12xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          7,
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,7] b:f32[7,7]. let\n    c:f32[14,7] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x7xf32>, %arg1: tensor<7x7xf32>) -> (tensor<14x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<7x7xf32>, tensor<7x7xf32>) -> tensor<14x7xf32>\n    return %0 : tensor<14x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          7,
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,6] b:f32[7,6]. let\n    c:f32[14,6] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x6xf32>, %arg1: tensor<7x6xf32>) -> (tensor<14x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<7x6xf32>, tensor<7x6xf32>) -> tensor<14x6xf32>\n    return %0 : tensor<14x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (16,))",
    "input_info": [
      {
        "shape": [
          8,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,2]. let\n    b:f32[16] = reshape[dimensions=None new_sizes=(16,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x2xf32>) -> (tensor<16xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<8x2xf32>) -> tensor<16xf32>\n    return %0 : tensor<16xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10] b:f32[10]. let\n    c:f32[1,10] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 10)\n      sharding=None\n    ] a\n    d:f32[1,10] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 10)\n      sharding=None\n    ] b\n    e:f32[2,10] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>, %arg1: tensor<10xf32>) -> (tensor<2x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<10xf32>) -> tensor<1x10xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<10xf32>) -> tensor<1x10xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<2x10xf32>\n    return %2 : tensor<2x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (8,))",
    "input_info": [
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8xf32>) -> (tensor<8xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          8,
          10,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,10,7]. let\n    b:f32[7,10,8] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x10x7xf32>) -> (tensor<7x10x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<8x10x7xf32>) -> tensor<7x10x8xf32>\n    return %0 : tensor<7x10x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 2))",
    "input_info": [
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4]. let\n    b:f32[2,2] = reshape[dimensions=None new_sizes=(2, 2) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<2x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<4xf32>) -> tensor<2x2xf32>\n    return %0 : tensor<2x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5] b:f32[5]. let\n    c:f32[10] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>, %arg1: tensor<5xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<5xf32>, tensor<5xf32>) -> tensor<10xf32>\n    return %0 : tensor<10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (8,))",
    "input_info": [
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8xf32>) -> (tensor<8xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          8,
          7,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,7,6]. let\n    b:f32[7,7,6] = slice[\n      limit_indices=(8, 7, 6)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x7x6xf32>) -> (tensor<7x7x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:8, 0:7, 0:6] : (tensor<8x7x6xf32>) -> tensor<7x7x6xf32>\n    return %0 : tensor<7x7x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2,))",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>) -> (tensor<2xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          9,
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,5] b:f32[9,5]. let\n    c:f32[18,5] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x5xf32>, %arg1: tensor<9x5xf32>) -> (tensor<18x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<9x5xf32>, tensor<9x5xf32>) -> tensor<18x5xf32>\n    return %0 : tensor<18x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          9,
          9,
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9,
          9,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,9,3] b:f32[9,9,3]. let\n    c:f32[18,9,3] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x9x3xf32>, %arg1: tensor<9x9x3xf32>) -> (tensor<18x9x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<9x9x3xf32>, tensor<9x9x3xf32>) -> tensor<18x9x3xf32>\n    return %0 : tensor<18x9x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          10,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,10]. let\n    b:f32[10,10] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x10xf32>) -> (tensor<10x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<10x10xf32>) -> tensor<10x10xf32>\n    return %0 : tensor<10x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6] b:f32[6]. let\n    c:f32[1,6] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 6)\n      sharding=None\n    ] a\n    d:f32[1,6] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 6)\n      sharding=None\n    ] b\n    e:f32[2,6] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>, %arg1: tensor<6xf32>) -> (tensor<2x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<6xf32>) -> tensor<1x6xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<6xf32>) -> tensor<1x6xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x6xf32>, tensor<1x6xf32>) -> tensor<2x6xf32>\n    return %2 : tensor<2x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2] b:f32[2]. let\n    c:f32[4] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>, %arg1: tensor<2xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<2xf32>, tensor<2xf32>) -> tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8xf32>) -> (tensor<8xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          9,
          8,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,8,9]. let\n    b:f32[8,8,9] = slice[\n      limit_indices=(9, 8, 9)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x8x9xf32>) -> (tensor<8x8x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:9, 0:8, 0:9] : (tensor<9x8x9xf32>) -> tensor<8x8x9xf32>\n    return %0 : tensor<8x8x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>) -> (tensor<6xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          10,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,7]. let\n    b:f32[7,10] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x7xf32>) -> (tensor<7x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<10x7xf32>) -> tensor<7x10xf32>\n    return %0 : tensor<7x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          5,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,8]. let\n    b:f32[4,8] = slice[limit_indices=(5, 8) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x8xf32>) -> (tensor<4x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:5, 0:8] : (tensor<5x8xf32>) -> tensor<4x8xf32>\n    return %0 : tensor<4x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>) -> (tensor<5xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          10,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,10]. let\n    b:f32[10,10] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x10xf32>) -> (tensor<10x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<10x10xf32>) -> tensor<10x10xf32>\n    return %0 : tensor<10x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          6,
          5,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,5,4]. let\n    b:f32[4,5,6] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x5x4xf32>) -> (tensor<4x5x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<6x5x4xf32>) -> tensor<4x5x6xf32>\n    return %0 : tensor<4x5x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          4,
          8,
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4,
          8,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,8,5] b:f32[4,8,5]. let\n    c:f32[8,8,5] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x8x5xf32>, %arg1: tensor<4x8x5xf32>) -> (tensor<8x8x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<4x8x5xf32>, tensor<4x8x5xf32>) -> tensor<8x8x5xf32>\n    return %0 : tensor<8x8x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          10,
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,10] b:f32[10,10]. let\n    c:f32[20,10] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x10xf32>, %arg1: tensor<10x10xf32>) -> (tensor<20x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<10x10xf32>, tensor<10x10xf32>) -> tensor<20x10xf32>\n    return %0 : tensor<20x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (50,))",
    "input_info": [
      {
        "shape": [
          5,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,10]. let\n    b:f32[50] = reshape[dimensions=None new_sizes=(50,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x10xf32>) -> (tensor<50xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<5x10xf32>) -> tensor<50xf32>\n    return %0 : tensor<50xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          10,
          7,
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10,
          7,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,7,6] b:f32[10,7,6]. let\n    c:f32[20,7,6] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x7x6xf32>, %arg1: tensor<10x7x6xf32>) -> (tensor<20x7x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<10x7x6xf32>, tensor<10x7x6xf32>) -> tensor<20x7x6xf32>\n    return %0 : tensor<20x7x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5]. let\n    b:f32[4] = slice[limit_indices=(5,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:5] : (tensor<5xf32>) -> tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          8,
          6,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,6,3]. let\n    b:f32[7,6,3] = slice[\n      limit_indices=(8, 6, 3)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x6x3xf32>) -> (tensor<7x6x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:8, 0:6, 0:3] : (tensor<8x6x3xf32>) -> tensor<7x6x3xf32>\n    return %0 : tensor<7x6x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6] b:f32[6]. let\n    c:f32[1,6] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 6)\n      sharding=None\n    ] a\n    d:f32[1,6] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 6)\n      sharding=None\n    ] b\n    e:f32[2,6] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>, %arg1: tensor<6xf32>) -> (tensor<2x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<6xf32>) -> tensor<1x6xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<6xf32>) -> tensor<1x6xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x6xf32>, tensor<1x6xf32>) -> tensor<2x6xf32>\n    return %2 : tensor<2x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          8,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,8]. let\n    b:f32[8,8] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x8xf32>) -> (tensor<8x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<8x8xf32>) -> tensor<8x8xf32>\n    return %0 : tensor<8x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          2,
          3,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,3,4]. let\n    b:f32[4,3,2] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x3x4xf32>) -> (tensor<4x3x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<2x3x4xf32>) -> tensor<4x3x2xf32>\n    return %0 : tensor<4x3x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          2,
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,9] b:f32[2,9]. let\n    c:f32[1,2,9] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 2, 9)\n      sharding=None\n    ] a\n    d:f32[1,2,9] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 2, 9)\n      sharding=None\n    ] b\n    e:f32[2,2,9] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x9xf32>, %arg1: tensor<2x9xf32>) -> (tensor<2x2x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<2x9xf32>) -> tensor<1x2x9xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<2x9xf32>) -> tensor<1x2x9xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x2x9xf32>, tensor<1x2x9xf32>) -> tensor<2x2x9xf32>\n    return %2 : tensor<2x2x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          7,
          7,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,7,6]. let\n    b:f32[6,7,7] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x7x6xf32>) -> (tensor<6x7x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<7x7x6xf32>) -> tensor<6x7x7xf32>\n    return %0 : tensor<6x7x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          3,
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,3] b:f32[3,3]. let\n    c:f32[1,3,3] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 3, 3)\n      sharding=None\n    ] a\n    d:f32[1,3,3] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 3, 3)\n      sharding=None\n    ] b\n    e:f32[2,3,3] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x3xf32>, %arg1: tensor<3x3xf32>) -> (tensor<2x3x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<3x3xf32>) -> tensor<1x3x3xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<3x3xf32>) -> tensor<1x3x3xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x3x3xf32>, tensor<1x3x3xf32>) -> tensor<2x3x3xf32>\n    return %2 : tensor<2x3x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          7,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,5]. let\n    b:f32[6,5] = slice[limit_indices=(7, 5) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x5xf32>) -> (tensor<6x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:7, 0:5] : (tensor<7x5xf32>) -> tensor<6x5xf32>\n    return %0 : tensor<6x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4]. let\n    b:f32[3] = slice[limit_indices=(4,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:4] : (tensor<4xf32>) -> tensor<3xf32>\n    return %0 : tensor<3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          9,
          9,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,9,3]. let\n    b:f32[8,9,3] = slice[\n      limit_indices=(9, 9, 3)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x9x3xf32>) -> (tensor<8x9x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:9, 0:9, 0:3] : (tensor<9x9x3xf32>) -> tensor<8x9x3xf32>\n    return %0 : tensor<8x9x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          7,
          6,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,6,4]. let\n    b:f32[6,6,4] = slice[\n      limit_indices=(7, 6, 4)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x6x4xf32>) -> (tensor<6x6x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:7, 0:6, 0:4] : (tensor<7x6x4xf32>) -> tensor<6x6x4xf32>\n    return %0 : tensor<6x6x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          6,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,7]. let\n    b:f32[5,7] = slice[limit_indices=(6, 7) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x7xf32>) -> (tensor<5x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:6, 0:7] : (tensor<6x7xf32>) -> tensor<5x7xf32>\n    return %0 : tensor<5x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          5,
          10,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,10,7]. let\n    b:f32[7,10,5] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x10x7xf32>) -> (tensor<7x10x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<5x10x7xf32>) -> tensor<7x10x5xf32>\n    return %0 : tensor<7x10x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9] b:f32[9]. let\n    c:f32[18] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>, %arg1: tensor<9xf32>) -> (tensor<18xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<9xf32>, tensor<9xf32>) -> tensor<18xf32>\n    return %0 : tensor<18xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (5,))",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>) -> (tensor<5xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          2,
          2,
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2,
          2,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,2,9] b:f32[2,2,9]. let\n    c:f32[4,2,9] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x2x9xf32>, %arg1: tensor<2x2x9xf32>) -> (tensor<4x2x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<2x2x9xf32>, tensor<2x2x9xf32>) -> tensor<4x2x9xf32>\n    return %0 : tensor<4x2x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8] b:f32[8]. let\n    c:f32[16] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8xf32>, %arg1: tensor<8xf32>) -> (tensor<16xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<8xf32>, tensor<8xf32>) -> tensor<16xf32>\n    return %0 : tensor<16xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          7,
          7,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,7,6]. let\n    b:f32[6,7,7] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x7x6xf32>) -> (tensor<6x7x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<7x7x6xf32>) -> tensor<6x7x7xf32>\n    return %0 : tensor<6x7x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          9,
          9,
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9,
          9,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,9,5] b:f32[9,9,5]. let\n    c:f32[18,9,5] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x9x5xf32>, %arg1: tensor<9x9x5xf32>) -> (tensor<18x9x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<9x9x5xf32>, tensor<9x9x5xf32>) -> tensor<18x9x5xf32>\n    return %0 : tensor<18x9x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          6,
          8,
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6,
          8,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,8,6] b:f32[6,8,6]. let\n    c:f32[12,8,6] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x8x6xf32>, %arg1: tensor<6x8x6xf32>) -> (tensor<12x8x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<6x8x6xf32>, tensor<6x8x6xf32>) -> tensor<12x8x6xf32>\n    return %0 : tensor<12x8x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8xf32>) -> (tensor<8xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          9,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,6]. let\n    b:f32[8,6] = slice[limit_indices=(9, 6) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x6xf32>) -> (tensor<8x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:9, 0:6] : (tensor<9x6xf32>) -> tensor<8x6xf32>\n    return %0 : tensor<8x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10] b:f32[10]. let\n    c:f32[1,10] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 10)\n      sharding=None\n    ] a\n    d:f32[1,10] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 10)\n      sharding=None\n    ] b\n    e:f32[2,10] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>, %arg1: tensor<10xf32>) -> (tensor<2x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<10xf32>) -> tensor<1x10xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<10xf32>) -> tensor<1x10xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<2x10xf32>\n    return %2 : tensor<2x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          8,
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,7] b:f32[8,7]. let\n    c:f32[1,8,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 8, 7)\n      sharding=None\n    ] a\n    d:f32[1,8,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 8, 7)\n      sharding=None\n    ] b\n    e:f32[2,8,7] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x7xf32>, %arg1: tensor<8x7xf32>) -> (tensor<2x8x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<8x7xf32>) -> tensor<1x8x7xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<8x7xf32>) -> tensor<1x8x7xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x8x7xf32>, tensor<1x8x7xf32>) -> tensor<2x8x7xf32>\n    return %2 : tensor<2x8x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6] b:f32[6]. let\n    c:f32[1,6] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 6)\n      sharding=None\n    ] a\n    d:f32[1,6] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 6)\n      sharding=None\n    ] b\n    e:f32[2,6] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>, %arg1: tensor<6xf32>) -> (tensor<2x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<6xf32>) -> tensor<1x6xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<6xf32>) -> tensor<1x6xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x6xf32>, tensor<1x6xf32>) -> tensor<2x6xf32>\n    return %2 : tensor<2x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3] b:f32[3]. let\n    c:f32[1,3] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 3)\n      sharding=None\n    ] a\n    d:f32[1,3] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 3)\n      sharding=None\n    ] b\n    e:f32[2,3] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>, %arg1: tensor<3xf32>) -> (tensor<2x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<3xf32>) -> tensor<1x3xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<3xf32>) -> tensor<1x3xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x3xf32>, tensor<1x3xf32>) -> tensor<2x3xf32>\n    return %2 : tensor<2x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7] b:f32[7]. let\n    c:f32[14] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>, %arg1: tensor<7xf32>) -> (tensor<14xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<7xf32>, tensor<7xf32>) -> tensor<14xf32>\n    return %0 : tensor<14xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          4,
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,9] b:f32[4,9]. let\n    c:f32[8,9] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x9xf32>, %arg1: tensor<4x9xf32>) -> (tensor<8x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<4x9xf32>, tensor<4x9xf32>) -> tensor<8x9xf32>\n    return %0 : tensor<8x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (3, 3))",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9]. let\n    b:f32[3,3] = reshape[dimensions=None new_sizes=(3, 3) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>) -> (tensor<3x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<9xf32>) -> tensor<3x3xf32>\n    return %0 : tensor<3x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (3,))",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>) -> (tensor<3xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          10,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,7]. let\n    b:f32[9,7] = slice[limit_indices=(10, 7) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x7xf32>) -> (tensor<9x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:10, 0:7] : (tensor<10x7xf32>) -> tensor<9x7xf32>\n    return %0 : tensor<9x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3]. let\n    b:f32[2] = slice[limit_indices=(3,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>) -> (tensor<2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:3] : (tensor<3xf32>) -> tensor<2xf32>\n    return %0 : tensor<2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          10,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,10]. let\n    b:f32[9,10] = slice[\n      limit_indices=(10, 10)\n      start_indices=(1, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x10xf32>) -> (tensor<9x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:10, 0:10] : (tensor<10x10xf32>) -> tensor<9x10xf32>\n    return %0 : tensor<9x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          8,
          2,
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          2,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,2,3] b:f32[8,2,3]. let\n    c:f32[1,8,2,3] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 8, 2, 3)\n      sharding=None\n    ] a\n    d:f32[1,8,2,3] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 8, 2, 3)\n      sharding=None\n    ] b\n    e:f32[2,8,2,3] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x2x3xf32>, %arg1: tensor<8x2x3xf32>) -> (tensor<2x8x2x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<8x2x3xf32>) -> tensor<1x8x2x3xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<8x2x3xf32>) -> tensor<1x8x2x3xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x8x2x3xf32>, tensor<1x8x2x3xf32>) -> tensor<2x8x2x3xf32>\n    return %2 : tensor<2x8x2x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>) -> (tensor<2xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          2,
          7,
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2,
          7,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,7,5] b:f32[2,7,5]. let\n    c:f32[4,7,5] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x7x5xf32>, %arg1: tensor<2x7x5xf32>) -> (tensor<4x7x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<2x7x5xf32>, tensor<2x7x5xf32>) -> tensor<4x7x5xf32>\n    return %0 : tensor<4x7x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (126,))",
    "input_info": [
      {
        "shape": [
          2,
          9,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,9,7]. let\n    b:f32[126] = reshape[dimensions=None new_sizes=(126,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x9x7xf32>) -> (tensor<126xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<2x9x7xf32>) -> tensor<126xf32>\n    return %0 : tensor<126xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          9,
          9,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,9,3]. let\n    b:f32[3,9,9] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x9x3xf32>) -> (tensor<3x9x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<9x9x3xf32>) -> tensor<3x9x9xf32>\n    return %0 : tensor<3x9x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (24,))",
    "input_info": [
      {
        "shape": [
          2,
          6,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,6,2]. let\n    b:f32[24] = reshape[dimensions=None new_sizes=(24,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x6x2xf32>) -> (tensor<24xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<2x6x2xf32>) -> tensor<24xf32>\n    return %0 : tensor<24xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          4,
          9,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,9,6]. let\n    b:f32[3,9,6] = slice[\n      limit_indices=(4, 9, 6)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x9x6xf32>) -> (tensor<3x9x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:4, 0:9, 0:6] : (tensor<4x9x6xf32>) -> tensor<3x9x6xf32>\n    return %0 : tensor<3x9x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (10,))",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          5,
          3,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,3,6]. let\n    b:f32[6,3,5] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x3x6xf32>) -> (tensor<6x3x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<5x3x6xf32>) -> tensor<6x3x5xf32>\n    return %0 : tensor<6x3x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3]. let\n    b:f32[2] = slice[limit_indices=(3,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>) -> (tensor<2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:3] : (tensor<3xf32>) -> tensor<2xf32>\n    return %0 : tensor<2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          9,
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,7] b:f32[9,7]. let\n    c:f32[1,9,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 9, 7)\n      sharding=None\n    ] a\n    d:f32[1,9,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 9, 7)\n      sharding=None\n    ] b\n    e:f32[2,9,7] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x7xf32>, %arg1: tensor<9x7xf32>) -> (tensor<2x9x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<9x7xf32>) -> tensor<1x9x7xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<9x7xf32>) -> tensor<1x9x7xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x9x7xf32>, tensor<1x9x7xf32>) -> tensor<2x9x7xf32>\n    return %2 : tensor<2x9x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (7,))",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>) -> (tensor<7xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          8,
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,2] b:f32[8,2]. let\n    c:f32[16,2] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x2xf32>, %arg1: tensor<8x2xf32>) -> (tensor<16x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<8x2xf32>, tensor<8x2xf32>) -> tensor<16x2xf32>\n    return %0 : tensor<16x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (56,))",
    "input_info": [
      {
        "shape": [
          7,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,8]. let\n    b:f32[56] = reshape[dimensions=None new_sizes=(56,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x8xf32>) -> (tensor<56xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<7x8xf32>) -> tensor<56xf32>\n    return %0 : tensor<56xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 75))",
    "input_info": [
      {
        "shape": [
          3,
          10,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,10,5]. let\n    b:f32[2,75] = reshape[dimensions=None new_sizes=(2, 75) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x10x5xf32>) -> (tensor<2x75xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<3x10x5xf32>) -> tensor<2x75xf32>\n    return %0 : tensor<2x75xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (6,))",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>) -> (tensor<6xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          5,
          7,
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5,
          7,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,7,2] b:f32[5,7,2]. let\n    c:f32[1,5,7,2] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 5, 7, 2)\n      sharding=None\n    ] a\n    d:f32[1,5,7,2] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 5, 7, 2)\n      sharding=None\n    ] b\n    e:f32[2,5,7,2] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x7x2xf32>, %arg1: tensor<5x7x2xf32>) -> (tensor<2x5x7x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<5x7x2xf32>) -> tensor<1x5x7x2xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<5x7x2xf32>) -> tensor<1x5x7x2xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x5x7x2xf32>, tensor<1x5x7x2xf32>) -> tensor<2x5x7x2xf32>\n    return %2 : tensor<2x5x7x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4] b:f32[4]. let\n    c:f32[8] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> (tensor<8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<4xf32>, tensor<4xf32>) -> tensor<8xf32>\n    return %0 : tensor<8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6] b:f32[6]. let\n    c:f32[1,6] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 6)\n      sharding=None\n    ] a\n    d:f32[1,6] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 6)\n      sharding=None\n    ] b\n    e:f32[2,6] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>, %arg1: tensor<6xf32>) -> (tensor<2x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<6xf32>) -> tensor<1x6xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<6xf32>) -> tensor<1x6xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x6xf32>, tensor<1x6xf32>) -> tensor<2x6xf32>\n    return %2 : tensor<2x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          9,
          6,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,6,2]. let\n    b:f32[8,6,2] = slice[\n      limit_indices=(9, 6, 2)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x6x2xf32>) -> (tensor<8x6x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:9, 0:6, 0:2] : (tensor<9x6x2xf32>) -> tensor<8x6x2xf32>\n    return %0 : tensor<8x6x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>) -> (tensor<3xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (90,))",
    "input_info": [
      {
        "shape": [
          6,
          3,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,3,5]. let\n    b:f32[90] = reshape[dimensions=None new_sizes=(90,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x3x5xf32>) -> (tensor<90xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<6x3x5xf32>) -> tensor<90xf32>\n    return %0 : tensor<90xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2,))",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>) -> (tensor<2xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (7,))",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>) -> (tensor<7xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6] b:f32[6]. let\n    c:f32[12] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>, %arg1: tensor<6xf32>) -> (tensor<12xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<6xf32>, tensor<6xf32>) -> tensor<12xf32>\n    return %0 : tensor<12xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8]. let\n    b:f32[7] = slice[limit_indices=(8,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8xf32>) -> (tensor<7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:8] : (tensor<8xf32>) -> tensor<7xf32>\n    return %0 : tensor<7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          9,
          2,
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9,
          2,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,2,2] b:f32[9,2,2]. let\n    c:f32[1,9,2,2] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 9, 2, 2)\n      sharding=None\n    ] a\n    d:f32[1,9,2,2] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 9, 2, 2)\n      sharding=None\n    ] b\n    e:f32[2,9,2,2] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x2x2xf32>, %arg1: tensor<9x2x2xf32>) -> (tensor<2x9x2x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<9x2x2xf32>) -> tensor<1x9x2x2xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<9x2x2xf32>) -> tensor<1x9x2x2xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x9x2x2xf32>, tensor<1x9x2x2xf32>) -> tensor<2x9x2x2xf32>\n    return %2 : tensor<2x9x2x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2] b:f32[2]. let\n    c:f32[1,2] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 2)\n      sharding=None\n    ] a\n    d:f32[1,2] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 2)\n      sharding=None\n    ] b\n    e:f32[2,2] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>, %arg1: tensor<2xf32>) -> (tensor<2x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<2xf32>) -> tensor<1x2xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<2xf32>) -> tensor<1x2xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x2xf32>, tensor<1x2xf32>) -> tensor<2x2xf32>\n    return %2 : tensor<2x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          9,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,4]. let\n    b:f32[8,4] = slice[limit_indices=(9, 4) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x4xf32>) -> (tensor<8x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:9, 0:4] : (tensor<9x4xf32>) -> tensor<8x4xf32>\n    return %0 : tensor<8x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          9,
          2,
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9,
          2,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,2,6] b:f32[9,2,6]. let\n    c:f32[1,9,2,6] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 9, 2, 6)\n      sharding=None\n    ] a\n    d:f32[1,9,2,6] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 9, 2, 6)\n      sharding=None\n    ] b\n    e:f32[2,9,2,6] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x2x6xf32>, %arg1: tensor<9x2x6xf32>) -> (tensor<2x9x2x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<9x2x6xf32>) -> tensor<1x9x2x6xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<9x2x6xf32>) -> tensor<1x9x2x6xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x9x2x6xf32>, tensor<1x9x2x6xf32>) -> tensor<2x9x2x6xf32>\n    return %2 : tensor<2x9x2x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>) -> (tensor<7xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          9,
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,6] b:f32[9,6]. let\n    c:f32[1,9,6] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 9, 6)\n      sharding=None\n    ] a\n    d:f32[1,9,6] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 9, 6)\n      sharding=None\n    ] b\n    e:f32[2,9,6] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x6xf32>, %arg1: tensor<9x6xf32>) -> (tensor<2x9x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<9x6xf32>) -> tensor<1x9x6xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<9x6xf32>) -> tensor<1x9x6xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x9x6xf32>, tensor<1x9x6xf32>) -> tensor<2x9x6xf32>\n    return %2 : tensor<2x9x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (14,))",
    "input_info": [
      {
        "shape": [
          7,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,2]. let\n    b:f32[14] = reshape[dimensions=None new_sizes=(14,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x2xf32>) -> (tensor<14xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<7x2xf32>) -> tensor<14xf32>\n    return %0 : tensor<14xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (18,))",
    "input_info": [
      {
        "shape": [
          6,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,3]. let\n    b:f32[18] = reshape[dimensions=None new_sizes=(18,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x3xf32>) -> (tensor<18xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<6x3xf32>) -> tensor<18xf32>\n    return %0 : tensor<18xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          7,
          4,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,4,6]. let\n    b:f32[6,4,7] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x4x6xf32>) -> (tensor<6x4x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<7x4x6xf32>) -> tensor<6x4x7xf32>\n    return %0 : tensor<6x4x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2,))",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>) -> (tensor<2xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>) -> (tensor<7xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          8,
          8,
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          8,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,8,2] b:f32[8,8,2]. let\n    c:f32[1,8,8,2] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 8, 8, 2)\n      sharding=None\n    ] a\n    d:f32[1,8,8,2] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 8, 8, 2)\n      sharding=None\n    ] b\n    e:f32[2,8,8,2] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x8x2xf32>, %arg1: tensor<8x8x2xf32>) -> (tensor<2x8x8x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<8x8x2xf32>) -> tensor<1x8x8x2xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<8x8x2xf32>) -> tensor<1x8x8x2xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x8x8x2xf32>, tensor<1x8x8x2xf32>) -> tensor<2x8x8x2xf32>\n    return %2 : tensor<2x8x8x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10] b:f32[10]. let\n    c:f32[20] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>, %arg1: tensor<10xf32>) -> (tensor<20xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<10xf32>, tensor<10xf32>) -> tensor<20xf32>\n    return %0 : tensor<20xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2] b:f32[2]. let\n    c:f32[4] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>, %arg1: tensor<2xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<2xf32>, tensor<2xf32>) -> tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 2))",
    "input_info": [
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4]. let\n    b:f32[2,2] = reshape[dimensions=None new_sizes=(2, 2) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<2x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<4xf32>) -> tensor<2x2xf32>\n    return %0 : tensor<2x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (48,))",
    "input_info": [
      {
        "shape": [
          6,
          4,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,4,2]. let\n    b:f32[48] = reshape[dimensions=None new_sizes=(48,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x4x2xf32>) -> (tensor<48xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<6x4x2xf32>) -> tensor<48xf32>\n    return %0 : tensor<48xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>) -> (tensor<2xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>) -> (tensor<2xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (6,))",
    "input_info": [
      {
        "shape": [
          2,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,3]. let\n    b:f32[6] = reshape[dimensions=None new_sizes=(6,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x3xf32>) -> (tensor<6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<2x3xf32>) -> tensor<6xf32>\n    return %0 : tensor<6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          4,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,9]. let\n    b:f32[9,4] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x9xf32>) -> (tensor<9x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<4x9xf32>) -> tensor<9x4xf32>\n    return %0 : tensor<9x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          3,
          6,
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3,
          6,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,6,3] b:f32[3,6,3]. let\n    c:f32[6,6,3] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x6x3xf32>, %arg1: tensor<3x6x3xf32>) -> (tensor<6x6x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<3x6x3xf32>, tensor<3x6x3xf32>) -> tensor<6x6x3xf32>\n    return %0 : tensor<6x6x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5] b:f32[5]. let\n    c:f32[1,5] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 5)\n      sharding=None\n    ] a\n    d:f32[1,5] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 5)\n      sharding=None\n    ] b\n    e:f32[2,5] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>, %arg1: tensor<5xf32>) -> (tensor<2x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<5xf32>) -> tensor<1x5xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<5xf32>) -> tensor<1x5xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x5xf32>, tensor<1x5xf32>) -> tensor<2x5xf32>\n    return %2 : tensor<2x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          8,
          4,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,4,10]. let\n    b:f32[10,4,8] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x4x10xf32>) -> (tensor<10x4x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<8x4x10xf32>) -> tensor<10x4x8xf32>\n    return %0 : tensor<10x4x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          4,
          2,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,2,5]. let\n    b:f32[3,2,5] = slice[\n      limit_indices=(4, 2, 5)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x2x5xf32>) -> (tensor<3x2x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:4, 0:2, 0:5] : (tensor<4x2x5xf32>) -> tensor<3x2x5xf32>\n    return %0 : tensor<3x2x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>) -> (tensor<3xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7] b:f32[7]. let\n    c:f32[14] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>, %arg1: tensor<7xf32>) -> (tensor<14xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<7xf32>, tensor<7xf32>) -> tensor<14xf32>\n    return %0 : tensor<14xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9] b:f32[9]. let\n    c:f32[1,9] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 9)\n      sharding=None\n    ] a\n    d:f32[1,9] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 9)\n      sharding=None\n    ] b\n    e:f32[2,9] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>, %arg1: tensor<9xf32>) -> (tensor<2x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<9xf32>) -> tensor<1x9xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<9xf32>) -> tensor<1x9xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x9xf32>, tensor<1x9xf32>) -> tensor<2x9xf32>\n    return %2 : tensor<2x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          6,
          7,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,7,5]. let\n    b:f32[5,7,6] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x7x5xf32>) -> (tensor<5x7x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<6x7x5xf32>) -> tensor<5x7x6xf32>\n    return %0 : tensor<5x7x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5]. let\n    b:f32[4] = slice[limit_indices=(5,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:5] : (tensor<5xf32>) -> tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          7,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,10]. let\n    b:f32[6,10] = slice[limit_indices=(7, 10) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x10xf32>) -> (tensor<6x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:7, 0:10] : (tensor<7x10xf32>) -> tensor<6x10xf32>\n    return %0 : tensor<6x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          7,
          5,
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7,
          5,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,5,5] b:f32[7,5,5]. let\n    c:f32[1,7,5,5] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 7, 5, 5)\n      sharding=None\n    ] a\n    d:f32[1,7,5,5] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 7, 5, 5)\n      sharding=None\n    ] b\n    e:f32[2,7,5,5] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x5x5xf32>, %arg1: tensor<7x5x5xf32>) -> (tensor<2x7x5x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<7x5x5xf32>) -> tensor<1x7x5x5xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<7x5x5xf32>) -> tensor<1x7x5x5xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x7x5x5xf32>, tensor<1x7x5x5xf32>) -> tensor<2x7x5x5xf32>\n    return %2 : tensor<2x7x5x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (7,))",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>) -> (tensor<7xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          3,
          10,
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3,
          10,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,10,2] b:f32[3,10,2]. let\n    c:f32[1,3,10,2] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 3, 10, 2)\n      sharding=None\n    ] a\n    d:f32[1,3,10,2] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 3, 10, 2)\n      sharding=None\n    ] b\n    e:f32[2,3,10,2] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x10x2xf32>, %arg1: tensor<3x10x2xf32>) -> (tensor<2x3x10x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<3x10x2xf32>) -> tensor<1x3x10x2xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<3x10x2xf32>) -> tensor<1x3x10x2xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x3x10x2xf32>, tensor<1x3x10x2xf32>) -> tensor<2x3x10x2xf32>\n    return %2 : tensor<2x3x10x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          8,
          9,
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          9,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,9,5] b:f32[8,9,5]. let\n    c:f32[1,8,9,5] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 8, 9, 5)\n      sharding=None\n    ] a\n    d:f32[1,8,9,5] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 8, 9, 5)\n      sharding=None\n    ] b\n    e:f32[2,8,9,5] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x9x5xf32>, %arg1: tensor<8x9x5xf32>) -> (tensor<2x8x9x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<8x9x5xf32>) -> tensor<1x8x9x5xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<8x9x5xf32>) -> tensor<1x8x9x5xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x8x9x5xf32>, tensor<1x8x9x5xf32>) -> tensor<2x8x9x5xf32>\n    return %2 : tensor<2x8x9x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          6,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,7]. let\n    b:f32[5,7] = slice[limit_indices=(6, 7) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x7xf32>) -> (tensor<5x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:6, 0:7] : (tensor<6x7xf32>) -> tensor<5x7xf32>\n    return %0 : tensor<5x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (162,))",
    "input_info": [
      {
        "shape": [
          6,
          9,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,9,3]. let\n    b:f32[162] = reshape[dimensions=None new_sizes=(162,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x9x3xf32>) -> (tensor<162xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<6x9x3xf32>) -> tensor<162xf32>\n    return %0 : tensor<162xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (8,))",
    "input_info": [
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8xf32>) -> (tensor<8xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          5,
          6,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,6,7]. let\n    b:f32[7,6,5] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x6x7xf32>) -> (tensor<7x6x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<5x6x7xf32>) -> tensor<7x6x5xf32>\n    return %0 : tensor<7x6x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6] b:f32[6]. let\n    c:f32[1,6] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 6)\n      sharding=None\n    ] a\n    d:f32[1,6] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 6)\n      sharding=None\n    ] b\n    e:f32[2,6] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>, %arg1: tensor<6xf32>) -> (tensor<2x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<6xf32>) -> tensor<1x6xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<6xf32>) -> tensor<1x6xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x6xf32>, tensor<1x6xf32>) -> tensor<2x6xf32>\n    return %2 : tensor<2x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          8,
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,8] b:f32[8,8]. let\n    c:f32[16,8] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x8xf32>, %arg1: tensor<8x8xf32>) -> (tensor<16x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<8x8xf32>, tensor<8x8xf32>) -> tensor<16x8xf32>\n    return %0 : tensor<16x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          8,
          4,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,4,7]. let\n    b:f32[7,4,8] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x4x7xf32>) -> (tensor<7x4x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<8x4x7xf32>) -> tensor<7x4x8xf32>\n    return %0 : tensor<7x4x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8] b:f32[8]. let\n    c:f32[16] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8xf32>, %arg1: tensor<8xf32>) -> (tensor<16xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<8xf32>, tensor<8xf32>) -> tensor<16xf32>\n    return %0 : tensor<16xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 252))",
    "input_info": [
      {
        "shape": [
          7,
          8,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,8,9]. let\n    b:f32[2,252] = reshape[dimensions=None new_sizes=(2, 252) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x8x9xf32>) -> (tensor<2x252xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<7x8x9xf32>) -> tensor<2x252xf32>\n    return %0 : tensor<2x252xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6]. let\n    b:f32[5] = slice[limit_indices=(6,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>) -> (tensor<5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:6] : (tensor<6xf32>) -> tensor<5xf32>\n    return %0 : tensor<5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          10,
          5,
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10,
          5,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,5,2] b:f32[10,5,2]. let\n    c:f32[20,5,2] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x5x2xf32>, %arg1: tensor<10x5x2xf32>) -> (tensor<20x5x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<10x5x2xf32>, tensor<10x5x2xf32>) -> tensor<20x5x2xf32>\n    return %0 : tensor<20x5x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 84))",
    "input_info": [
      {
        "shape": [
          3,
          8,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,8,7]. let\n    b:f32[2,84] = reshape[dimensions=None new_sizes=(2, 84) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x8x7xf32>) -> (tensor<2x84xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<3x8x7xf32>) -> tensor<2x84xf32>\n    return %0 : tensor<2x84xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          2,
          4,
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2,
          4,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,4,2] b:f32[2,4,2]. let\n    c:f32[4,4,2] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x4x2xf32>, %arg1: tensor<2x4x2xf32>) -> (tensor<4x4x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<2x4x2xf32>, tensor<2x4x2xf32>) -> tensor<4x4x2xf32>\n    return %0 : tensor<4x4x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (7,))",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>) -> (tensor<7xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          5,
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,10] b:f32[5,10]. let\n    c:f32[10,10] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x10xf32>, %arg1: tensor<5x10xf32>) -> (tensor<10x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<5x10xf32>, tensor<5x10xf32>) -> tensor<10x10xf32>\n    return %0 : tensor<10x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8] b:f32[8]. let\n    c:f32[1,8] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 8)\n      sharding=None\n    ] a\n    d:f32[1,8] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 8)\n      sharding=None\n    ] b\n    e:f32[2,8] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8xf32>, %arg1: tensor<8xf32>) -> (tensor<2x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<8xf32>) -> tensor<1x8xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<8xf32>) -> tensor<1x8xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x8xf32>, tensor<1x8xf32>) -> tensor<2x8xf32>\n    return %2 : tensor<2x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8xf32>) -> (tensor<8xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8] b:f32[8]. let\n    c:f32[16] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8xf32>, %arg1: tensor<8xf32>) -> (tensor<16xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<8xf32>, tensor<8xf32>) -> tensor<16xf32>\n    return %0 : tensor<16xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3] b:f32[3]. let\n    c:f32[1,3] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 3)\n      sharding=None\n    ] a\n    d:f32[1,3] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 3)\n      sharding=None\n    ] b\n    e:f32[2,3] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>, %arg1: tensor<3xf32>) -> (tensor<2x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<3xf32>) -> tensor<1x3xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<3xf32>) -> tensor<1x3xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x3xf32>, tensor<1x3xf32>) -> tensor<2x3xf32>\n    return %2 : tensor<2x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 7))",
    "input_info": [
      {
        "shape": [
          7,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,2]. let\n    b:f32[2,7] = reshape[dimensions=None new_sizes=(2, 7) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x2xf32>) -> (tensor<2x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<7x2xf32>) -> tensor<2x7xf32>\n    return %0 : tensor<2x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6]. let\n    b:f32[5] = slice[limit_indices=(6,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>) -> (tensor<5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:6] : (tensor<6xf32>) -> tensor<5xf32>\n    return %0 : tensor<5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          2,
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,7] b:f32[2,7]. let\n    c:f32[4,7] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x7xf32>, %arg1: tensor<2x7xf32>) -> (tensor<4x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<2x7xf32>, tensor<2x7xf32>) -> tensor<4x7xf32>\n    return %0 : tensor<4x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3]. let\n    b:f32[2] = slice[limit_indices=(3,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>) -> (tensor<2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:3] : (tensor<3xf32>) -> tensor<2xf32>\n    return %0 : tensor<2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2,))",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>) -> (tensor<2xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3]. let\n    b:f32[2] = slice[limit_indices=(3,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>) -> (tensor<2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:3] : (tensor<3xf32>) -> tensor<2xf32>\n    return %0 : tensor<2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          6,
          3,
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6,
          3,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,3,10] b:f32[6,3,10]. let\n    c:f32[12,3,10] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x3x10xf32>, %arg1: tensor<6x3x10xf32>) -> (tensor<12x3x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<6x3x10xf32>, tensor<6x3x10xf32>) -> tensor<12x3x10xf32>\n    return %0 : tensor<12x3x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          5,
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,6] b:f32[5,6]. let\n    c:f32[1,5,6] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 5, 6)\n      sharding=None\n    ] a\n    d:f32[1,5,6] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 5, 6)\n      sharding=None\n    ] b\n    e:f32[2,5,6] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x6xf32>, %arg1: tensor<5x6xf32>) -> (tensor<2x5x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<5x6xf32>) -> tensor<1x5x6xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<5x6xf32>) -> tensor<1x5x6xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x5x6xf32>, tensor<1x5x6xf32>) -> tensor<2x5x6xf32>\n    return %2 : tensor<2x5x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4] b:f32[4]. let\n    c:f32[1,4] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 4)\n      sharding=None\n    ] a\n    d:f32[1,4] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 4)\n      sharding=None\n    ] b\n    e:f32[2,4] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> (tensor<2x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<4xf32>) -> tensor<1x4xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<4xf32>) -> tensor<1x4xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x4xf32>, tensor<1x4xf32>) -> tensor<2x4xf32>\n    return %2 : tensor<2x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          9,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,2]. let\n    b:f32[2,9] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x2xf32>) -> (tensor<2x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<9x2xf32>) -> tensor<2x9xf32>\n    return %0 : tensor<2x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          7,
          3,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,3,2]. let\n    b:f32[2,3,7] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x3x2xf32>) -> (tensor<2x3x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<7x3x2xf32>) -> tensor<2x3x7xf32>\n    return %0 : tensor<2x3x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          5,
          2,
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5,
          2,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,2,9] b:f32[5,2,9]. let\n    c:f32[10,2,9] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x2x9xf32>, %arg1: tensor<5x2x9xf32>) -> (tensor<10x2x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<5x2x9xf32>, tensor<5x2x9xf32>) -> tensor<10x2x9xf32>\n    return %0 : tensor<10x2x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5]. let\n    b:f32[4] = slice[limit_indices=(5,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:5] : (tensor<5xf32>) -> tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          7,
          6,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,6,3]. let\n    b:f32[3,6,7] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x6x3xf32>) -> (tensor<3x6x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<7x6x3xf32>) -> tensor<3x6x7xf32>\n    return %0 : tensor<3x6x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          10,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,6]. let\n    b:f32[9,6] = slice[limit_indices=(10, 6) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x6xf32>) -> (tensor<9x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:10, 0:6] : (tensor<10x6xf32>) -> tensor<9x6xf32>\n    return %0 : tensor<9x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 3))",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6]. let\n    b:f32[2,3] = reshape[dimensions=None new_sizes=(2, 3) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>) -> (tensor<2x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<6xf32>) -> tensor<2x3xf32>\n    return %0 : tensor<2x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          6,
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,10] b:f32[6,10]. let\n    c:f32[1,6,10] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 6, 10)\n      sharding=None\n    ] a\n    d:f32[1,6,10] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 6, 10)\n      sharding=None\n    ] b\n    e:f32[2,6,10] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x10xf32>, %arg1: tensor<6x10xf32>) -> (tensor<2x6x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<6x10xf32>) -> tensor<1x6x10xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<6x10xf32>) -> tensor<1x6x10xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x6x10xf32>, tensor<1x6x10xf32>) -> tensor<2x6x10xf32>\n    return %2 : tensor<2x6x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          3,
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,2] b:f32[3,2]. let\n    c:f32[6,2] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x2xf32>, %arg1: tensor<3x2xf32>) -> (tensor<6x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<3x2xf32>, tensor<3x2xf32>) -> tensor<6x2xf32>\n    return %0 : tensor<6x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          6,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,5]. let\n    b:f32[5,5] = slice[limit_indices=(6, 5) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x5xf32>) -> (tensor<5x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:6, 0:5] : (tensor<6x5xf32>) -> tensor<5x5xf32>\n    return %0 : tensor<5x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 3))",
    "input_info": [
      {
        "shape": [
          3,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,2]. let\n    b:f32[2,3] = reshape[dimensions=None new_sizes=(2, 3) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x2xf32>) -> (tensor<2x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<3x2xf32>) -> tensor<2x3xf32>\n    return %0 : tensor<2x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 350))",
    "input_info": [
      {
        "shape": [
          7,
          10,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,10,10]. let\n    b:f32[2,350] = reshape[dimensions=None new_sizes=(2, 350) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x10x10xf32>) -> (tensor<2x350xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<7x10x10xf32>) -> tensor<2x350xf32>\n    return %0 : tensor<2x350xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 90))",
    "input_info": [
      {
        "shape": [
          6,
          10,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,10,3]. let\n    b:f32[2,90] = reshape[dimensions=None new_sizes=(2, 90) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x10x3xf32>) -> (tensor<2x90xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<6x10x3xf32>) -> tensor<2x90xf32>\n    return %0 : tensor<2x90xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6] b:f32[6]. let\n    c:f32[12] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>, %arg1: tensor<6xf32>) -> (tensor<12xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<6xf32>, tensor<6xf32>) -> tensor<12xf32>\n    return %0 : tensor<12xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 75))",
    "input_info": [
      {
        "shape": [
          5,
          3,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,3,10]. let\n    b:f32[2,75] = reshape[dimensions=None new_sizes=(2, 75) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x3x10xf32>) -> (tensor<2x75xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<5x3x10xf32>) -> tensor<2x75xf32>\n    return %0 : tensor<2x75xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          6,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,10]. let\n    b:f32[5,10] = slice[limit_indices=(6, 10) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x10xf32>) -> (tensor<5x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:6, 0:10] : (tensor<6x10xf32>) -> tensor<5x10xf32>\n    return %0 : tensor<5x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (250,))",
    "input_info": [
      {
        "shape": [
          10,
          5,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,5,5]. let\n    b:f32[250] = reshape[dimensions=None new_sizes=(250,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x5x5xf32>) -> (tensor<250xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<10x5x5xf32>) -> tensor<250xf32>\n    return %0 : tensor<250xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          10,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,7]. let\n    b:f32[9,7] = slice[limit_indices=(10, 7) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x7xf32>) -> (tensor<9x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:10, 0:7] : (tensor<10x7xf32>) -> tensor<9x7xf32>\n    return %0 : tensor<9x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (216,))",
    "input_info": [
      {
        "shape": [
          6,
          6,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,6,6]. let\n    b:f32[216] = reshape[dimensions=None new_sizes=(216,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x6x6xf32>) -> (tensor<216xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<6x6x6xf32>) -> tensor<216xf32>\n    return %0 : tensor<216xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          5,
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,2] b:f32[5,2]. let\n    c:f32[10,2] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x2xf32>, %arg1: tensor<5x2xf32>) -> (tensor<10x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<5x2xf32>, tensor<5x2xf32>) -> tensor<10x2xf32>\n    return %0 : tensor<10x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2] b:f32[2]. let\n    c:f32[4] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>, %arg1: tensor<2xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<2xf32>, tensor<2xf32>) -> tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 3))",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6]. let\n    b:f32[2,3] = reshape[dimensions=None new_sizes=(2, 3) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>) -> (tensor<2x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<6xf32>) -> tensor<2x3xf32>\n    return %0 : tensor<2x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 27))",
    "input_info": [
      {
        "shape": [
          9,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,6]. let\n    b:f32[2,27] = reshape[dimensions=None new_sizes=(2, 27) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x6xf32>) -> (tensor<2x27xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<9x6xf32>) -> tensor<2x27xf32>\n    return %0 : tensor<2x27xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2,))",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>) -> (tensor<2xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8]. let\n    b:f32[7] = slice[limit_indices=(8,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8xf32>) -> (tensor<7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:8] : (tensor<8xf32>) -> tensor<7xf32>\n    return %0 : tensor<7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          5,
          10,
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5,
          10,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,10,7] b:f32[5,10,7]. let\n    c:f32[10,10,7] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x10x7xf32>, %arg1: tensor<5x10x7xf32>) -> (tensor<10x10x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<5x10x7xf32>, tensor<5x10x7xf32>) -> tensor<10x10x7xf32>\n    return %0 : tensor<10x10x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          2,
          6,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,6,2]. let\n    b:f32[2,6,2] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x6x2xf32>) -> (tensor<2x6x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<2x6x2xf32>) -> tensor<2x6x2xf32>\n    return %0 : tensor<2x6x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          6,
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,7] b:f32[6,7]. let\n    c:f32[12,7] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x7xf32>, %arg1: tensor<6x7xf32>) -> (tensor<12x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<6x7xf32>, tensor<6x7xf32>) -> tensor<12x7xf32>\n    return %0 : tensor<12x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10] b:f32[10]. let\n    c:f32[20] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>, %arg1: tensor<10xf32>) -> (tensor<20xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<10xf32>, tensor<10xf32>) -> tensor<20xf32>\n    return %0 : tensor<20xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          10,
          8,
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10,
          8,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,8,6] b:f32[10,8,6]. let\n    c:f32[1,10,8,6] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 10, 8, 6)\n      sharding=None\n    ] a\n    d:f32[1,10,8,6] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 10, 8, 6)\n      sharding=None\n    ] b\n    e:f32[2,10,8,6] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x8x6xf32>, %arg1: tensor<10x8x6xf32>) -> (tensor<2x10x8x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<10x8x6xf32>) -> tensor<1x10x8x6xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<10x8x6xf32>) -> tensor<1x10x8x6xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x10x8x6xf32>, tensor<1x10x8x6xf32>) -> tensor<2x10x8x6xf32>\n    return %2 : tensor<2x10x8x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5]. let\n    b:f32[4] = slice[limit_indices=(5,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:5] : (tensor<5xf32>) -> tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 72))",
    "input_info": [
      {
        "shape": [
          4,
          6,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,6,6]. let\n    b:f32[2,72] = reshape[dimensions=None new_sizes=(2, 72) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x6x6xf32>) -> (tensor<2x72xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<4x6x6xf32>) -> tensor<2x72xf32>\n    return %0 : tensor<2x72xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9] b:f32[9]. let\n    c:f32[1,9] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 9)\n      sharding=None\n    ] a\n    d:f32[1,9] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 9)\n      sharding=None\n    ] b\n    e:f32[2,9] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>, %arg1: tensor<9xf32>) -> (tensor<2x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<9xf32>) -> tensor<1x9xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<9xf32>) -> tensor<1x9xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x9xf32>, tensor<1x9xf32>) -> tensor<2x9xf32>\n    return %2 : tensor<2x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3]. let\n    b:f32[2] = slice[limit_indices=(3,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>) -> (tensor<2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:3] : (tensor<3xf32>) -> tensor<2xf32>\n    return %0 : tensor<2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (20,))",
    "input_info": [
      {
        "shape": [
          10,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,2]. let\n    b:f32[20] = reshape[dimensions=None new_sizes=(20,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x2xf32>) -> (tensor<20xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<10x2xf32>) -> tensor<20xf32>\n    return %0 : tensor<20xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          10,
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,9] b:f32[10,9]. let\n    c:f32[1,10,9] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 10, 9)\n      sharding=None\n    ] a\n    d:f32[1,10,9] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 10, 9)\n      sharding=None\n    ] b\n    e:f32[2,10,9] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x9xf32>, %arg1: tensor<10x9xf32>) -> (tensor<2x10x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<10x9xf32>) -> tensor<1x10x9xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<10x9xf32>) -> tensor<1x10x9xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x10x9xf32>, tensor<1x10x9xf32>) -> tensor<2x10x9xf32>\n    return %2 : tensor<2x10x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          9,
          8,
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9,
          8,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,8,7] b:f32[9,8,7]. let\n    c:f32[18,8,7] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x8x7xf32>, %arg1: tensor<9x8x7xf32>) -> (tensor<18x8x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<9x8x7xf32>, tensor<9x8x7xf32>) -> tensor<18x8x7xf32>\n    return %0 : tensor<18x8x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          9,
          8,
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9,
          8,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,8,2] b:f32[9,8,2]. let\n    c:f32[18,8,2] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x8x2xf32>, %arg1: tensor<9x8x2xf32>) -> (tensor<18x8x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<9x8x2xf32>, tensor<9x8x2xf32>) -> tensor<18x8x2xf32>\n    return %0 : tensor<18x8x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          6,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,10]. let\n    b:f32[10,6] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x10xf32>) -> (tensor<10x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<6x10xf32>) -> tensor<10x6xf32>\n    return %0 : tensor<10x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          5,
          6,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,6,8]. let\n    b:f32[4,6,8] = slice[\n      limit_indices=(5, 6, 8)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x6x8xf32>) -> (tensor<4x6x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:5, 0:6, 0:8] : (tensor<5x6x8xf32>) -> tensor<4x6x8xf32>\n    return %0 : tensor<4x6x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (45,))",
    "input_info": [
      {
        "shape": [
          9,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,5]. let\n    b:f32[45] = reshape[dimensions=None new_sizes=(45,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x5xf32>) -> (tensor<45xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<9x5xf32>) -> tensor<45xf32>\n    return %0 : tensor<45xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          7,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,10]. let\n    b:f32[6,10] = slice[limit_indices=(7, 10) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x10xf32>) -> (tensor<6x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:7, 0:10] : (tensor<7x10xf32>) -> tensor<6x10xf32>\n    return %0 : tensor<6x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          2,
          4,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,4,9]. let\n    b:f32[1,4,9] = slice[\n      limit_indices=(2, 4, 9)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x4x9xf32>) -> (tensor<1x4x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:2, 0:4, 0:9] : (tensor<2x4x9xf32>) -> tensor<1x4x9xf32>\n    return %0 : tensor<1x4x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9] b:f32[9]. let\n    c:f32[1,9] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 9)\n      sharding=None\n    ] a\n    d:f32[1,9] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 9)\n      sharding=None\n    ] b\n    e:f32[2,9] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>, %arg1: tensor<9xf32>) -> (tensor<2x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<9xf32>) -> tensor<1x9xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<9xf32>) -> tensor<1x9xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x9xf32>, tensor<1x9xf32>) -> tensor<2x9xf32>\n    return %2 : tensor<2x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>) -> (tensor<5xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 72))",
    "input_info": [
      {
        "shape": [
          8,
          6,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,6,3]. let\n    b:f32[2,72] = reshape[dimensions=None new_sizes=(2, 72) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x6x3xf32>) -> (tensor<2x72xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<8x6x3xf32>) -> tensor<2x72xf32>\n    return %0 : tensor<2x72xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7] b:f32[7]. let\n    c:f32[1,7] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 7)\n      sharding=None\n    ] a\n    d:f32[1,7] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 7)\n      sharding=None\n    ] b\n    e:f32[2,7] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>, %arg1: tensor<7xf32>) -> (tensor<2x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<7xf32>) -> tensor<1x7xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<7xf32>) -> tensor<1x7xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x7xf32>, tensor<1x7xf32>) -> tensor<2x7xf32>\n    return %2 : tensor<2x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (18,))",
    "input_info": [
      {
        "shape": [
          3,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,6]. let\n    b:f32[18] = reshape[dimensions=None new_sizes=(18,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x6xf32>) -> (tensor<18xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<3x6xf32>) -> tensor<18xf32>\n    return %0 : tensor<18xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (100,))",
    "input_info": [
      {
        "shape": [
          10,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,10]. let\n    b:f32[100] = reshape[dimensions=None new_sizes=(100,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x10xf32>) -> (tensor<100xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<10x10xf32>) -> tensor<100xf32>\n    return %0 : tensor<100xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8xf32>) -> (tensor<8xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6] b:f32[6]. let\n    c:f32[1,6] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 6)\n      sharding=None\n    ] a\n    d:f32[1,6] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 6)\n      sharding=None\n    ] b\n    e:f32[2,6] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>, %arg1: tensor<6xf32>) -> (tensor<2x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<6xf32>) -> tensor<1x6xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<6xf32>) -> tensor<1x6xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x6xf32>, tensor<1x6xf32>) -> tensor<2x6xf32>\n    return %2 : tensor<2x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          7,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,8]. let\n    b:f32[6,8] = slice[limit_indices=(7, 8) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x8xf32>) -> (tensor<6x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:7, 0:8] : (tensor<7x8xf32>) -> tensor<6x8xf32>\n    return %0 : tensor<6x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9]. let\n    b:f32[8] = slice[limit_indices=(9,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>) -> (tensor<8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:9] : (tensor<9xf32>) -> tensor<8xf32>\n    return %0 : tensor<8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 56))",
    "input_info": [
      {
        "shape": [
          7,
          8,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,8,2]. let\n    b:f32[2,56] = reshape[dimensions=None new_sizes=(2, 56) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x8x2xf32>) -> (tensor<2x56xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<7x8x2xf32>) -> tensor<2x56xf32>\n    return %0 : tensor<2x56xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          9,
          3,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,3,10]. let\n    b:f32[10,3,9] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x3x10xf32>) -> (tensor<10x3x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<9x3x10xf32>) -> tensor<10x3x9xf32>\n    return %0 : tensor<10x3x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (320,))",
    "input_info": [
      {
        "shape": [
          4,
          8,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,8,10]. let\n    b:f32[320] = reshape[dimensions=None new_sizes=(320,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x8x10xf32>) -> (tensor<320xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<4x8x10xf32>) -> tensor<320xf32>\n    return %0 : tensor<320xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          8,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,3]. let\n    b:f32[7,3] = slice[limit_indices=(8, 3) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x3xf32>) -> (tensor<7x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:8, 0:3] : (tensor<8x3xf32>) -> tensor<7x3xf32>\n    return %0 : tensor<7x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (100,))",
    "input_info": [
      {
        "shape": [
          10,
          5,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,5,2]. let\n    b:f32[100] = reshape[dimensions=None new_sizes=(100,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x5x2xf32>) -> (tensor<100xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<10x5x2xf32>) -> tensor<100xf32>\n    return %0 : tensor<100xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8xf32>) -> (tensor<8xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4]. let\n    b:f32[3] = slice[limit_indices=(4,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:4] : (tensor<4xf32>) -> tensor<3xf32>\n    return %0 : tensor<3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          4,
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,3] b:f32[4,3]. let\n    c:f32[1,4,3] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 4, 3)\n      sharding=None\n    ] a\n    d:f32[1,4,3] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 4, 3)\n      sharding=None\n    ] b\n    e:f32[2,4,3] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x3xf32>, %arg1: tensor<4x3xf32>) -> (tensor<2x4x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<4x3xf32>) -> tensor<1x4x3xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<4x3xf32>) -> tensor<1x4x3xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x4x3xf32>, tensor<1x4x3xf32>) -> tensor<2x4x3xf32>\n    return %2 : tensor<2x4x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2,))",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>) -> (tensor<2xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (16,))",
    "input_info": [
      {
        "shape": [
          8,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,2]. let\n    b:f32[16] = reshape[dimensions=None new_sizes=(16,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x2xf32>) -> (tensor<16xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<8x2xf32>) -> tensor<16xf32>\n    return %0 : tensor<16xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          9,
          9,
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9,
          9,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,9,5] b:f32[9,9,5]. let\n    c:f32[1,9,9,5] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 9, 9, 5)\n      sharding=None\n    ] a\n    d:f32[1,9,9,5] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 9, 9, 5)\n      sharding=None\n    ] b\n    e:f32[2,9,9,5] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x9x5xf32>, %arg1: tensor<9x9x5xf32>) -> (tensor<2x9x9x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<9x9x5xf32>) -> tensor<1x9x9x5xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<9x9x5xf32>) -> tensor<1x9x9x5xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x9x9x5xf32>, tensor<1x9x9x5xf32>) -> tensor<2x9x9x5xf32>\n    return %2 : tensor<2x9x9x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          4,
          10,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,10,4]. let\n    b:f32[4,10,4] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x10x4xf32>) -> (tensor<4x10x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<4x10x4xf32>) -> tensor<4x10x4xf32>\n    return %0 : tensor<4x10x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 45))",
    "input_info": [
      {
        "shape": [
          10,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,9]. let\n    b:f32[2,45] = reshape[dimensions=None new_sizes=(2, 45) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x9xf32>) -> (tensor<2x45xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<10x9xf32>) -> tensor<2x45xf32>\n    return %0 : tensor<2x45xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          10,
          8,
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10,
          8,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,8,7] b:f32[10,8,7]. let\n    c:f32[20,8,7] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x8x7xf32>, %arg1: tensor<10x8x7xf32>) -> (tensor<20x8x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<10x8x7xf32>, tensor<10x8x7xf32>) -> tensor<20x8x7xf32>\n    return %0 : tensor<20x8x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (36,))",
    "input_info": [
      {
        "shape": [
          3,
          2,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,2,6]. let\n    b:f32[36] = reshape[dimensions=None new_sizes=(36,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x2x6xf32>) -> (tensor<36xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<3x2x6xf32>) -> tensor<36xf32>\n    return %0 : tensor<36xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (12,))",
    "input_info": [
      {
        "shape": [
          3,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,4]. let\n    b:f32[12] = reshape[dimensions=None new_sizes=(12,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x4xf32>) -> (tensor<12xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<3x4xf32>) -> tensor<12xf32>\n    return %0 : tensor<12xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (18,))",
    "input_info": [
      {
        "shape": [
          6,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,3]. let\n    b:f32[18] = reshape[dimensions=None new_sizes=(18,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x3xf32>) -> (tensor<18xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<6x3xf32>) -> tensor<18xf32>\n    return %0 : tensor<18xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6] b:f32[6]. let\n    c:f32[12] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>, %arg1: tensor<6xf32>) -> (tensor<12xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<6xf32>, tensor<6xf32>) -> tensor<12xf32>\n    return %0 : tensor<12xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          4,
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,7] b:f32[4,7]. let\n    c:f32[1,4,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 4, 7)\n      sharding=None\n    ] a\n    d:f32[1,4,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 4, 7)\n      sharding=None\n    ] b\n    e:f32[2,4,7] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x7xf32>, %arg1: tensor<4x7xf32>) -> (tensor<2x4x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<4x7xf32>) -> tensor<1x4x7xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<4x7xf32>) -> tensor<1x4x7xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x4x7xf32>, tensor<1x4x7xf32>) -> tensor<2x4x7xf32>\n    return %2 : tensor<2x4x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4]. let\n    b:f32[3] = slice[limit_indices=(4,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:4] : (tensor<4xf32>) -> tensor<3xf32>\n    return %0 : tensor<3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          4,
          5,
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4,
          5,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,5,3] b:f32[4,5,3]. let\n    c:f32[1,4,5,3] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 4, 5, 3)\n      sharding=None\n    ] a\n    d:f32[1,4,5,3] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 4, 5, 3)\n      sharding=None\n    ] b\n    e:f32[2,4,5,3] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x5x3xf32>, %arg1: tensor<4x5x3xf32>) -> (tensor<2x4x5x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<4x5x3xf32>) -> tensor<1x4x5x3xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<4x5x3xf32>) -> tensor<1x4x5x3xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x4x5x3xf32>, tensor<1x4x5x3xf32>) -> tensor<2x4x5x3xf32>\n    return %2 : tensor<2x4x5x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10]. let\n    b:f32[9] = slice[limit_indices=(10,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:10] : (tensor<10xf32>) -> tensor<9xf32>\n    return %0 : tensor<9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2,))",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>) -> (tensor<2xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          5,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,10]. let\n    b:f32[10,5] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x10xf32>) -> (tensor<10x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<5x10xf32>) -> tensor<10x5xf32>\n    return %0 : tensor<10x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          6,
          2,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,2,6]. let\n    b:f32[6,2,6] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x2x6xf32>) -> (tensor<6x2x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<6x2x6xf32>) -> tensor<6x2x6xf32>\n    return %0 : tensor<6x2x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7] b:f32[7]. let\n    c:f32[1,7] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 7)\n      sharding=None\n    ] a\n    d:f32[1,7] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 7)\n      sharding=None\n    ] b\n    e:f32[2,7] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>, %arg1: tensor<7xf32>) -> (tensor<2x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<7xf32>) -> tensor<1x7xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<7xf32>) -> tensor<1x7xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x7xf32>, tensor<1x7xf32>) -> tensor<2x7xf32>\n    return %2 : tensor<2x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          6,
          8,
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6,
          8,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,8,6] b:f32[6,8,6]. let\n    c:f32[12,8,6] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x8x6xf32>, %arg1: tensor<6x8x6xf32>) -> (tensor<12x8x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<6x8x6xf32>, tensor<6x8x6xf32>) -> tensor<12x8x6xf32>\n    return %0 : tensor<12x8x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          3,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,6]. let\n    b:f32[6,3] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x6xf32>) -> (tensor<6x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<3x6xf32>) -> tensor<6x3xf32>\n    return %0 : tensor<6x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (4,))",
    "input_info": [
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>) -> (tensor<3xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          8,
          10,
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          10,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,10,8] b:f32[8,10,8]. let\n    c:f32[1,8,10,8] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 8, 10, 8)\n      sharding=None\n    ] a\n    d:f32[1,8,10,8] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 8, 10, 8)\n      sharding=None\n    ] b\n    e:f32[2,8,10,8] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x10x8xf32>, %arg1: tensor<8x10x8xf32>) -> (tensor<2x8x10x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<8x10x8xf32>) -> tensor<1x8x10x8xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<8x10x8xf32>) -> tensor<1x8x10x8xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x8x10x8xf32>, tensor<1x8x10x8xf32>) -> tensor<2x8x10x8xf32>\n    return %2 : tensor<2x8x10x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>) -> (tensor<2xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          5,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,4]. let\n    b:f32[4,4] = slice[limit_indices=(5, 4) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x4xf32>) -> (tensor<4x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:5, 0:4] : (tensor<5x4xf32>) -> tensor<4x4xf32>\n    return %0 : tensor<4x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          8,
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,9] b:f32[8,9]. let\n    c:f32[1,8,9] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 8, 9)\n      sharding=None\n    ] a\n    d:f32[1,8,9] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 8, 9)\n      sharding=None\n    ] b\n    e:f32[2,8,9] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x9xf32>, %arg1: tensor<8x9xf32>) -> (tensor<2x8x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<8x9xf32>) -> tensor<1x8x9xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<8x9xf32>) -> tensor<1x8x9xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x8x9xf32>, tensor<1x8x9xf32>) -> tensor<2x8x9xf32>\n    return %2 : tensor<2x8x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          4,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,5]. let\n    b:f32[5,4] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x5xf32>) -> (tensor<5x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<4x5xf32>) -> tensor<5x4xf32>\n    return %0 : tensor<5x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          6,
          9,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,9,7]. let\n    b:f32[5,9,7] = slice[\n      limit_indices=(6, 9, 7)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x9x7xf32>) -> (tensor<5x9x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:6, 0:9, 0:7] : (tensor<6x9x7xf32>) -> tensor<5x9x7xf32>\n    return %0 : tensor<5x9x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          9,
          8,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,8,10]. let\n    b:f32[10,8,9] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x8x10xf32>) -> (tensor<10x8x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<9x8x10xf32>) -> tensor<10x8x9xf32>\n    return %0 : tensor<10x8x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3] b:f32[3]. let\n    c:f32[1,3] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 3)\n      sharding=None\n    ] a\n    d:f32[1,3] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 3)\n      sharding=None\n    ] b\n    e:f32[2,3] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>, %arg1: tensor<3xf32>) -> (tensor<2x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<3xf32>) -> tensor<1x3xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<3xf32>) -> tensor<1x3xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x3xf32>, tensor<1x3xf32>) -> tensor<2x3xf32>\n    return %2 : tensor<2x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          8,
          7,
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          7,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,7,10] b:f32[8,7,10]. let\n    c:f32[1,8,7,10] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 8, 7, 10)\n      sharding=None\n    ] a\n    d:f32[1,8,7,10] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 8, 7, 10)\n      sharding=None\n    ] b\n    e:f32[2,8,7,10] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x7x10xf32>, %arg1: tensor<8x7x10xf32>) -> (tensor<2x8x7x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<8x7x10xf32>) -> tensor<1x8x7x10xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<8x7x10xf32>) -> tensor<1x8x7x10xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x8x7x10xf32>, tensor<1x8x7x10xf32>) -> tensor<2x8x7x10xf32>\n    return %2 : tensor<2x8x7x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          10,
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,4] b:f32[10,4]. let\n    c:f32[1,10,4] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 10, 4)\n      sharding=None\n    ] a\n    d:f32[1,10,4] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 10, 4)\n      sharding=None\n    ] b\n    e:f32[2,10,4] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x4xf32>, %arg1: tensor<10x4xf32>) -> (tensor<2x10x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<10x4xf32>) -> tensor<1x10x4xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<10x4xf32>) -> tensor<1x10x4xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x10x4xf32>, tensor<1x10x4xf32>) -> tensor<2x10x4xf32>\n    return %2 : tensor<2x10x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          4,
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,7] b:f32[4,7]. let\n    c:f32[8,7] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x7xf32>, %arg1: tensor<4x7xf32>) -> (tensor<8x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<4x7xf32>, tensor<4x7xf32>) -> tensor<8x7xf32>\n    return %0 : tensor<8x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          9,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,9]. let\n    b:f32[9,9] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x9xf32>) -> (tensor<9x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<9x9xf32>) -> tensor<9x9xf32>\n    return %0 : tensor<9x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>) -> (tensor<5xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6] b:f32[6]. let\n    c:f32[12] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>, %arg1: tensor<6xf32>) -> (tensor<12xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<6xf32>, tensor<6xf32>) -> tensor<12xf32>\n    return %0 : tensor<12xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3]. let\n    b:f32[2] = slice[limit_indices=(3,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>) -> (tensor<2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:3] : (tensor<3xf32>) -> tensor<2xf32>\n    return %0 : tensor<2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4]. let\n    b:f32[3] = slice[limit_indices=(4,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:4] : (tensor<4xf32>) -> tensor<3xf32>\n    return %0 : tensor<3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          4,
          7,
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4,
          7,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,7,3] b:f32[4,7,3]. let\n    c:f32[8,7,3] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x7x3xf32>, %arg1: tensor<4x7x3xf32>) -> (tensor<8x7x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<4x7x3xf32>, tensor<4x7x3xf32>) -> tensor<8x7x3xf32>\n    return %0 : tensor<8x7x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          6,
          5,
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6,
          5,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,5,8] b:f32[6,5,8]. let\n    c:f32[1,6,5,8] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 6, 5, 8)\n      sharding=None\n    ] a\n    d:f32[1,6,5,8] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 6, 5, 8)\n      sharding=None\n    ] b\n    e:f32[2,6,5,8] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x5x8xf32>, %arg1: tensor<6x5x8xf32>) -> (tensor<2x6x5x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<6x5x8xf32>) -> tensor<1x6x5x8xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<6x5x8xf32>) -> tensor<1x6x5x8xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x6x5x8xf32>, tensor<1x6x5x8xf32>) -> tensor<2x6x5x8xf32>\n    return %2 : tensor<2x6x5x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7]. let\n    b:f32[6] = slice[limit_indices=(7,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>) -> (tensor<6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:7] : (tensor<7xf32>) -> tensor<6xf32>\n    return %0 : tensor<6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          10,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,8]. let\n    b:f32[8,10] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x8xf32>) -> (tensor<8x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<10x8xf32>) -> tensor<8x10xf32>\n    return %0 : tensor<8x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          5,
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,2] b:f32[5,2]. let\n    c:f32[1,5,2] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 5, 2)\n      sharding=None\n    ] a\n    d:f32[1,5,2] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 5, 2)\n      sharding=None\n    ] b\n    e:f32[2,5,2] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x2xf32>, %arg1: tensor<5x2xf32>) -> (tensor<2x5x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<5x2xf32>) -> tensor<1x5x2xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<5x2xf32>) -> tensor<1x5x2xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x5x2xf32>, tensor<1x5x2xf32>) -> tensor<2x5x2xf32>\n    return %2 : tensor<2x5x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          10,
          5,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,5,5]. let\n    b:f32[5,5,10] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x5x5xf32>) -> (tensor<5x5x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<10x5x5xf32>) -> tensor<5x5x10xf32>\n    return %0 : tensor<5x5x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          4,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,10]. let\n    b:f32[3,10] = slice[limit_indices=(4, 10) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x10xf32>) -> (tensor<3x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:4, 0:10] : (tensor<4x10xf32>) -> tensor<3x10xf32>\n    return %0 : tensor<3x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>) -> (tensor<3xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          9,
          4,
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9,
          4,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,4,9] b:f32[9,4,9]. let\n    c:f32[18,4,9] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x4x9xf32>, %arg1: tensor<9x4x9xf32>) -> (tensor<18x4x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<9x4x9xf32>, tensor<9x4x9xf32>) -> tensor<18x4x9xf32>\n    return %0 : tensor<18x4x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          6,
          7,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,7,9]. let\n    b:f32[9,7,6] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x7x9xf32>) -> (tensor<9x7x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<6x7x9xf32>) -> tensor<9x7x6xf32>\n    return %0 : tensor<9x7x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          10,
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,7] b:f32[10,7]. let\n    c:f32[1,10,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 10, 7)\n      sharding=None\n    ] a\n    d:f32[1,10,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 10, 7)\n      sharding=None\n    ] b\n    e:f32[2,10,7] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x7xf32>, %arg1: tensor<10x7xf32>) -> (tensor<2x10x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<10x7xf32>) -> tensor<1x10x7xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<10x7xf32>) -> tensor<1x10x7xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x10x7xf32>, tensor<1x10x7xf32>) -> tensor<2x10x7xf32>\n    return %2 : tensor<2x10x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (240,))",
    "input_info": [
      {
        "shape": [
          10,
          4,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,4,6]. let\n    b:f32[240] = reshape[dimensions=None new_sizes=(240,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x4x6xf32>) -> (tensor<240xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<10x4x6xf32>) -> tensor<240xf32>\n    return %0 : tensor<240xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          2,
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,5] b:f32[2,5]. let\n    c:f32[4,5] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x5xf32>, %arg1: tensor<2x5xf32>) -> (tensor<4x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<2x5xf32>, tensor<2x5xf32>) -> tensor<4x5xf32>\n    return %0 : tensor<4x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          9,
          9,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,9,6]. let\n    b:f32[6,9,9] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x9x6xf32>) -> (tensor<6x9x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<9x9x6xf32>) -> tensor<6x9x9xf32>\n    return %0 : tensor<6x9x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 10))",
    "input_info": [
      {
        "shape": [
          5,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,4]. let\n    b:f32[2,10] = reshape[dimensions=None new_sizes=(2, 10) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x4xf32>) -> (tensor<2x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<5x4xf32>) -> tensor<2x10xf32>\n    return %0 : tensor<2x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          10,
          6,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,6,10]. let\n    b:f32[10,6,10] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x6x10xf32>) -> (tensor<10x6x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<10x6x10xf32>) -> tensor<10x6x10xf32>\n    return %0 : tensor<10x6x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          7,
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,2] b:f32[7,2]. let\n    c:f32[1,7,2] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 7, 2)\n      sharding=None\n    ] a\n    d:f32[1,7,2] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 7, 2)\n      sharding=None\n    ] b\n    e:f32[2,7,2] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x2xf32>, %arg1: tensor<7x2xf32>) -> (tensor<2x7x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<7x2xf32>) -> tensor<1x7x2xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<7x2xf32>) -> tensor<1x7x2xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x7x2xf32>, tensor<1x7x2xf32>) -> tensor<2x7x2xf32>\n    return %2 : tensor<2x7x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>) -> (tensor<7xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          7,
          2,
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7,
          2,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,2,6] b:f32[7,2,6]. let\n    c:f32[1,7,2,6] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 7, 2, 6)\n      sharding=None\n    ] a\n    d:f32[1,7,2,6] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 7, 2, 6)\n      sharding=None\n    ] b\n    e:f32[2,7,2,6] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x2x6xf32>, %arg1: tensor<7x2x6xf32>) -> (tensor<2x7x2x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<7x2x6xf32>) -> tensor<1x7x2x6xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<7x2x6xf32>) -> tensor<1x7x2x6xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x7x2x6xf32>, tensor<1x7x2x6xf32>) -> tensor<2x7x2x6xf32>\n    return %2 : tensor<2x7x2x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          2,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,10]. let\n    b:f32[1,10] = slice[limit_indices=(2, 10) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x10xf32>) -> (tensor<1x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:2, 0:10] : (tensor<2x10xf32>) -> tensor<1x10xf32>\n    return %0 : tensor<1x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          2,
          7,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,7,10]. let\n    b:f32[10,7,2] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x7x10xf32>) -> (tensor<10x7x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<2x7x10xf32>) -> tensor<10x7x2xf32>\n    return %0 : tensor<10x7x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          7,
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,10] b:f32[7,10]. let\n    c:f32[1,7,10] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 7, 10)\n      sharding=None\n    ] a\n    d:f32[1,7,10] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 7, 10)\n      sharding=None\n    ] b\n    e:f32[2,7,10] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x10xf32>, %arg1: tensor<7x10xf32>) -> (tensor<2x7x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<7x10xf32>) -> tensor<1x7x10xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<7x10xf32>) -> tensor<1x7x10xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x7x10xf32>, tensor<1x7x10xf32>) -> tensor<2x7x10xf32>\n    return %2 : tensor<2x7x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (4,))",
    "input_info": [
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9] b:f32[9]. let\n    c:f32[1,9] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 9)\n      sharding=None\n    ] a\n    d:f32[1,9] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 9)\n      sharding=None\n    ] b\n    e:f32[2,9] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>, %arg1: tensor<9xf32>) -> (tensor<2x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<9xf32>) -> tensor<1x9xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<9xf32>) -> tensor<1x9xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x9xf32>, tensor<1x9xf32>) -> tensor<2x9xf32>\n    return %2 : tensor<2x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          6,
          4,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,4,10]. let\n    b:f32[10,4,6] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x4x10xf32>) -> (tensor<10x4x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<6x4x10xf32>) -> tensor<10x4x6xf32>\n    return %0 : tensor<10x4x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          6,
          2,
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6,
          2,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,2,10] b:f32[6,2,10]. let\n    c:f32[12,2,10] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x2x10xf32>, %arg1: tensor<6x2x10xf32>) -> (tensor<12x2x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<6x2x10xf32>, tensor<6x2x10xf32>) -> tensor<12x2x10xf32>\n    return %0 : tensor<12x2x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          4,
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,6] b:f32[4,6]. let\n    c:f32[1,4,6] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 4, 6)\n      sharding=None\n    ] a\n    d:f32[1,4,6] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 4, 6)\n      sharding=None\n    ] b\n    e:f32[2,4,6] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x6xf32>, %arg1: tensor<4x6xf32>) -> (tensor<2x4x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<4x6xf32>) -> tensor<1x4x6xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<4x6xf32>) -> tensor<1x4x6xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x4x6xf32>, tensor<1x4x6xf32>) -> tensor<2x4x6xf32>\n    return %2 : tensor<2x4x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          9,
          6,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,6,6]. let\n    b:f32[6,6,9] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x6x6xf32>) -> (tensor<6x6x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<9x6x6xf32>) -> tensor<6x6x9xf32>\n    return %0 : tensor<6x6x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          8,
          5,
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          5,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,5,3] b:f32[8,5,3]. let\n    c:f32[1,8,5,3] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 8, 5, 3)\n      sharding=None\n    ] a\n    d:f32[1,8,5,3] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 8, 5, 3)\n      sharding=None\n    ] b\n    e:f32[2,8,5,3] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x5x3xf32>, %arg1: tensor<8x5x3xf32>) -> (tensor<2x8x5x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<8x5x3xf32>) -> tensor<1x8x5x3xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<8x5x3xf32>) -> tensor<1x8x5x3xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x8x5x3xf32>, tensor<1x8x5x3xf32>) -> tensor<2x8x5x3xf32>\n    return %2 : tensor<2x8x5x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          7,
          7,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,7,6]. let\n    b:f32[6,7,6] = slice[\n      limit_indices=(7, 7, 6)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x7x6xf32>) -> (tensor<6x7x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:7, 0:7, 0:6] : (tensor<7x7x6xf32>) -> tensor<6x7x6xf32>\n    return %0 : tensor<6x7x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          7,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,9]. let\n    b:f32[9,7] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x9xf32>) -> (tensor<9x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<7x9xf32>) -> tensor<9x7xf32>\n    return %0 : tensor<9x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 72))",
    "input_info": [
      {
        "shape": [
          8,
          2,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,2,9]. let\n    b:f32[2,72] = reshape[dimensions=None new_sizes=(2, 72) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x2x9xf32>) -> (tensor<2x72xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<8x2x9xf32>) -> tensor<2x72xf32>\n    return %0 : tensor<2x72xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          8,
          2,
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          2,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,2,8] b:f32[8,2,8]. let\n    c:f32[1,8,2,8] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 8, 2, 8)\n      sharding=None\n    ] a\n    d:f32[1,8,2,8] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 8, 2, 8)\n      sharding=None\n    ] b\n    e:f32[2,8,2,8] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x2x8xf32>, %arg1: tensor<8x2x8xf32>) -> (tensor<2x8x2x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<8x2x8xf32>) -> tensor<1x8x2x8xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<8x2x8xf32>) -> tensor<1x8x2x8xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x8x2x8xf32>, tensor<1x8x2x8xf32>) -> tensor<2x8x2x8xf32>\n    return %2 : tensor<2x8x2x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (30,))",
    "input_info": [
      {
        "shape": [
          3,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,10]. let\n    b:f32[30] = reshape[dimensions=None new_sizes=(30,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x10xf32>) -> (tensor<30xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<3x10xf32>) -> tensor<30xf32>\n    return %0 : tensor<30xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9] b:f32[9]. let\n    c:f32[18] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>, %arg1: tensor<9xf32>) -> (tensor<18xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<9xf32>, tensor<9xf32>) -> tensor<18xf32>\n    return %0 : tensor<18xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 20))",
    "input_info": [
      {
        "shape": [
          4,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,10]. let\n    b:f32[2,20] = reshape[dimensions=None new_sizes=(2, 20) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x10xf32>) -> (tensor<2x20xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<4x10xf32>) -> tensor<2x20xf32>\n    return %0 : tensor<2x20xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          2,
          4,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,4,2]. let\n    b:f32[2,4,2] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x4x2xf32>) -> (tensor<2x4x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<2x4x2xf32>) -> tensor<2x4x2xf32>\n    return %0 : tensor<2x4x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10] b:f32[10]. let\n    c:f32[20] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>, %arg1: tensor<10xf32>) -> (tensor<20xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<10xf32>, tensor<10xf32>) -> tensor<20xf32>\n    return %0 : tensor<20xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          2,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,5]. let\n    b:f32[1,5] = slice[limit_indices=(2, 5) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x5xf32>) -> (tensor<1x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:2, 0:5] : (tensor<2x5xf32>) -> tensor<1x5xf32>\n    return %0 : tensor<1x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (50,))",
    "input_info": [
      {
        "shape": [
          10,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,5]. let\n    b:f32[50] = reshape[dimensions=None new_sizes=(50,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x5xf32>) -> (tensor<50xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<10x5xf32>) -> tensor<50xf32>\n    return %0 : tensor<50xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          6,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,6]. let\n    b:f32[5,6] = slice[limit_indices=(6, 6) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x6xf32>) -> (tensor<5x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:6, 0:6] : (tensor<6x6xf32>) -> tensor<5x6xf32>\n    return %0 : tensor<5x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          5,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,2]. let\n    b:f32[4,2] = slice[limit_indices=(5, 2) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x2xf32>) -> (tensor<4x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:5, 0:2] : (tensor<5x2xf32>) -> tensor<4x2xf32>\n    return %0 : tensor<4x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          9,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,5]. let\n    b:f32[8,5] = slice[limit_indices=(9, 5) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x5xf32>) -> (tensor<8x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:9, 0:5] : (tensor<9x5xf32>) -> tensor<8x5xf32>\n    return %0 : tensor<8x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2] b:f32[2]. let\n    c:f32[4] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>, %arg1: tensor<2xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<2xf32>, tensor<2xf32>) -> tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>) -> (tensor<3xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5] b:f32[5]. let\n    c:f32[1,5] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 5)\n      sharding=None\n    ] a\n    d:f32[1,5] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 5)\n      sharding=None\n    ] b\n    e:f32[2,5] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>, %arg1: tensor<5xf32>) -> (tensor<2x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<5xf32>) -> tensor<1x5xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<5xf32>) -> tensor<1x5xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x5xf32>, tensor<1x5xf32>) -> tensor<2x5xf32>\n    return %2 : tensor<2x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          8,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,2]. let\n    b:f32[7,2] = slice[limit_indices=(8, 2) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x2xf32>) -> (tensor<7x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:8, 0:2] : (tensor<8x2xf32>) -> tensor<7x2xf32>\n    return %0 : tensor<7x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 3))",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6]. let\n    b:f32[2,3] = reshape[dimensions=None new_sizes=(2, 3) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>) -> (tensor<2x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<6xf32>) -> tensor<2x3xf32>\n    return %0 : tensor<2x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5]. let\n    b:f32[4] = slice[limit_indices=(5,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:5] : (tensor<5xf32>) -> tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          3,
          2,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,2,5]. let\n    b:f32[2,2,5] = slice[\n      limit_indices=(3, 2, 5)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x2x5xf32>) -> (tensor<2x2x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:3, 0:2, 0:5] : (tensor<3x2x5xf32>) -> tensor<2x2x5xf32>\n    return %0 : tensor<2x2x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5]. let\n    b:f32[4] = slice[limit_indices=(5,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:5] : (tensor<5xf32>) -> tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          2,
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,7] b:f32[2,7]. let\n    c:f32[1,2,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 2, 7)\n      sharding=None\n    ] a\n    d:f32[1,2,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 2, 7)\n      sharding=None\n    ] b\n    e:f32[2,2,7] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x7xf32>, %arg1: tensor<2x7xf32>) -> (tensor<2x2x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<2x7xf32>) -> tensor<1x2x7xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<2x7xf32>) -> tensor<1x2x7xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x2x7xf32>, tensor<1x2x7xf32>) -> tensor<2x2x7xf32>\n    return %2 : tensor<2x2x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7] b:f32[7]. let\n    c:f32[14] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>, %arg1: tensor<7xf32>) -> (tensor<14xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<7xf32>, tensor<7xf32>) -> tensor<14xf32>\n    return %0 : tensor<14xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (24,))",
    "input_info": [
      {
        "shape": [
          3,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,8]. let\n    b:f32[24] = reshape[dimensions=None new_sizes=(24,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x8xf32>) -> (tensor<24xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<3x8xf32>) -> tensor<24xf32>\n    return %0 : tensor<24xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          6,
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,3] b:f32[6,3]. let\n    c:f32[1,6,3] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 6, 3)\n      sharding=None\n    ] a\n    d:f32[1,6,3] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 6, 3)\n      sharding=None\n    ] b\n    e:f32[2,6,3] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x3xf32>, %arg1: tensor<6x3xf32>) -> (tensor<2x6x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<6x3xf32>) -> tensor<1x6x3xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<6x3xf32>) -> tensor<1x6x3xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x6x3xf32>, tensor<1x6x3xf32>) -> tensor<2x6x3xf32>\n    return %2 : tensor<2x6x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          7,
          3,
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7,
          3,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,3,9] b:f32[7,3,9]. let\n    c:f32[1,7,3,9] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 7, 3, 9)\n      sharding=None\n    ] a\n    d:f32[1,7,3,9] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 7, 3, 9)\n      sharding=None\n    ] b\n    e:f32[2,7,3,9] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x3x9xf32>, %arg1: tensor<7x3x9xf32>) -> (tensor<2x7x3x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<7x3x9xf32>) -> tensor<1x7x3x9xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<7x3x9xf32>) -> tensor<1x7x3x9xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x7x3x9xf32>, tensor<1x7x3x9xf32>) -> tensor<2x7x3x9xf32>\n    return %2 : tensor<2x7x3x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 3))",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6]. let\n    b:f32[2,3] = reshape[dimensions=None new_sizes=(2, 3) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>) -> (tensor<2x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<6xf32>) -> tensor<2x3xf32>\n    return %0 : tensor<2x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          8,
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,8] b:f32[8,8]. let\n    c:f32[16,8] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x8xf32>, %arg1: tensor<8x8xf32>) -> (tensor<16x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<8x8xf32>, tensor<8x8xf32>) -> tensor<16x8xf32>\n    return %0 : tensor<16x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (3, 3))",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9]. let\n    b:f32[3,3] = reshape[dimensions=None new_sizes=(3, 3) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>) -> (tensor<3x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<9xf32>) -> tensor<3x3xf32>\n    return %0 : tensor<3x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          7,
          8,
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7,
          8,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,8,8] b:f32[7,8,8]. let\n    c:f32[1,7,8,8] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 7, 8, 8)\n      sharding=None\n    ] a\n    d:f32[1,7,8,8] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 7, 8, 8)\n      sharding=None\n    ] b\n    e:f32[2,7,8,8] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x8x8xf32>, %arg1: tensor<7x8x8xf32>) -> (tensor<2x7x8x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<7x8x8xf32>) -> tensor<1x7x8x8xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<7x8x8xf32>) -> tensor<1x7x8x8xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x7x8x8xf32>, tensor<1x7x8x8xf32>) -> tensor<2x7x8x8xf32>\n    return %2 : tensor<2x7x8x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          2,
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,5] b:f32[2,5]. let\n    c:f32[1,2,5] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 2, 5)\n      sharding=None\n    ] a\n    d:f32[1,2,5] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 2, 5)\n      sharding=None\n    ] b\n    e:f32[2,2,5] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x5xf32>, %arg1: tensor<2x5xf32>) -> (tensor<2x2x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<2x5xf32>) -> tensor<1x2x5xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<2x5xf32>) -> tensor<1x2x5xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x2x5xf32>, tensor<1x2x5xf32>) -> tensor<2x2x5xf32>\n    return %2 : tensor<2x2x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          10,
          8,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,8,7]. let\n    b:f32[9,8,7] = slice[\n      limit_indices=(10, 8, 7)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x8x7xf32>) -> (tensor<9x8x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:10, 0:8, 0:7] : (tensor<10x8x7xf32>) -> tensor<9x8x7xf32>\n    return %0 : tensor<9x8x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          9,
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,9] b:f32[9,9]. let\n    c:f32[18,9] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x9xf32>, %arg1: tensor<9x9xf32>) -> (tensor<18x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<9x9xf32>, tensor<9x9xf32>) -> tensor<18x9xf32>\n    return %0 : tensor<18x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (9,))",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>) -> (tensor<9xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          6,
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,6] b:f32[6,6]. let\n    c:f32[12,6] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x6xf32>, %arg1: tensor<6x6xf32>) -> (tensor<12x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<6x6xf32>, tensor<6x6xf32>) -> tensor<12x6xf32>\n    return %0 : tensor<12x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          8,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,10]. let\n    b:f32[10,8] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x10xf32>) -> (tensor<10x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<8x10xf32>) -> tensor<10x8xf32>\n    return %0 : tensor<10x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          8,
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,6] b:f32[8,6]. let\n    c:f32[1,8,6] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 8, 6)\n      sharding=None\n    ] a\n    d:f32[1,8,6] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 8, 6)\n      sharding=None\n    ] b\n    e:f32[2,8,6] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x6xf32>, %arg1: tensor<8x6xf32>) -> (tensor<2x8x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<8x6xf32>) -> tensor<1x8x6xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<8x6xf32>) -> tensor<1x8x6xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x8x6xf32>, tensor<1x8x6xf32>) -> tensor<2x8x6xf32>\n    return %2 : tensor<2x8x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (3, 105))",
    "input_info": [
      {
        "shape": [
          9,
          5,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,5,7]. let\n    b:f32[3,105] = reshape[dimensions=None new_sizes=(3, 105) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x5x7xf32>) -> (tensor<3x105xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<9x5x7xf32>) -> tensor<3x105xf32>\n    return %0 : tensor<3x105xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          4,
          5,
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4,
          5,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,5,2] b:f32[4,5,2]. let\n    c:f32[8,5,2] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x5x2xf32>, %arg1: tensor<4x5x2xf32>) -> (tensor<8x5x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<4x5x2xf32>, tensor<4x5x2xf32>) -> tensor<8x5x2xf32>\n    return %0 : tensor<8x5x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          10,
          3,
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10,
          3,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,3,4] b:f32[10,3,4]. let\n    c:f32[20,3,4] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x3x4xf32>, %arg1: tensor<10x3x4xf32>) -> (tensor<20x3x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<10x3x4xf32>, tensor<10x3x4xf32>) -> tensor<20x3x4xf32>\n    return %0 : tensor<20x3x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          6,
          9,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,9,3]. let\n    b:f32[5,9,3] = slice[\n      limit_indices=(6, 9, 3)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x9x3xf32>) -> (tensor<5x9x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:6, 0:9, 0:3] : (tensor<6x9x3xf32>) -> tensor<5x9x3xf32>\n    return %0 : tensor<5x9x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2] b:f32[2]. let\n    c:f32[4] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>, %arg1: tensor<2xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<2xf32>, tensor<2xf32>) -> tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          4,
          6,
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4,
          6,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,6,3] b:f32[4,6,3]. let\n    c:f32[1,4,6,3] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 4, 6, 3)\n      sharding=None\n    ] a\n    d:f32[1,4,6,3] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 4, 6, 3)\n      sharding=None\n    ] b\n    e:f32[2,4,6,3] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x6x3xf32>, %arg1: tensor<4x6x3xf32>) -> (tensor<2x4x6x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<4x6x3xf32>) -> tensor<1x4x6x3xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<4x6x3xf32>) -> tensor<1x4x6x3xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x4x6x3xf32>, tensor<1x4x6x3xf32>) -> tensor<2x4x6x3xf32>\n    return %2 : tensor<2x4x6x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 3))",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6]. let\n    b:f32[2,3] = reshape[dimensions=None new_sizes=(2, 3) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>) -> (tensor<2x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<6xf32>) -> tensor<2x3xf32>\n    return %0 : tensor<2x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (7,))",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>) -> (tensor<7xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9] b:f32[9]. let\n    c:f32[18] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>, %arg1: tensor<9xf32>) -> (tensor<18xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<9xf32>, tensor<9xf32>) -> tensor<18xf32>\n    return %0 : tensor<18xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          7,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,8]. let\n    b:f32[8,7] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x8xf32>) -> (tensor<8x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<7x8xf32>) -> tensor<8x7xf32>\n    return %0 : tensor<8x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10] b:f32[10]. let\n    c:f32[1,10] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 10)\n      sharding=None\n    ] a\n    d:f32[1,10] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 10)\n      sharding=None\n    ] b\n    e:f32[2,10] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>, %arg1: tensor<10xf32>) -> (tensor<2x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<10xf32>) -> tensor<1x10xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<10xf32>) -> tensor<1x10xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<2x10xf32>\n    return %2 : tensor<2x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3]. let\n    b:f32[2] = slice[limit_indices=(3,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>) -> (tensor<2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:3] : (tensor<3xf32>) -> tensor<2xf32>\n    return %0 : tensor<2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3]. let\n    b:f32[2] = slice[limit_indices=(3,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>) -> (tensor<2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:3] : (tensor<3xf32>) -> tensor<2xf32>\n    return %0 : tensor<2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9]. let\n    b:f32[8] = slice[limit_indices=(9,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>) -> (tensor<8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:9] : (tensor<9xf32>) -> tensor<8xf32>\n    return %0 : tensor<8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          9,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,6]. let\n    b:f32[8,6] = slice[limit_indices=(9, 6) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x6xf32>) -> (tensor<8x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:9, 0:6] : (tensor<9x6xf32>) -> tensor<8x6xf32>\n    return %0 : tensor<8x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          8,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,4]. let\n    b:f32[7,4] = slice[limit_indices=(8, 4) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x4xf32>) -> (tensor<7x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:8, 0:4] : (tensor<8x4xf32>) -> tensor<7x4xf32>\n    return %0 : tensor<7x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (5,))",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>) -> (tensor<5xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (3, 27))",
    "input_info": [
      {
        "shape": [
          9,
          3,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,3,3]. let\n    b:f32[3,27] = reshape[dimensions=None new_sizes=(3, 27) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x3x3xf32>) -> (tensor<3x27xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<9x3x3xf32>) -> tensor<3x27xf32>\n    return %0 : tensor<3x27xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6] b:f32[6]. let\n    c:f32[1,6] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 6)\n      sharding=None\n    ] a\n    d:f32[1,6] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 6)\n      sharding=None\n    ] b\n    e:f32[2,6] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>, %arg1: tensor<6xf32>) -> (tensor<2x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<6xf32>) -> tensor<1x6xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<6xf32>) -> tensor<1x6xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x6xf32>, tensor<1x6xf32>) -> tensor<2x6xf32>\n    return %2 : tensor<2x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>) -> (tensor<7xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>) -> (tensor<2xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          10,
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,5] b:f32[10,5]. let\n    c:f32[1,10,5] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 10, 5)\n      sharding=None\n    ] a\n    d:f32[1,10,5] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 10, 5)\n      sharding=None\n    ] b\n    e:f32[2,10,5] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x5xf32>, %arg1: tensor<10x5xf32>) -> (tensor<2x10x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<10x5xf32>) -> tensor<1x10x5xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<10x5xf32>) -> tensor<1x10x5xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x10x5xf32>, tensor<1x10x5xf32>) -> tensor<2x10x5xf32>\n    return %2 : tensor<2x10x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 70))",
    "input_info": [
      {
        "shape": [
          4,
          5,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,5,7]. let\n    b:f32[2,70] = reshape[dimensions=None new_sizes=(2, 70) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x5x7xf32>) -> (tensor<2x70xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<4x5x7xf32>) -> tensor<2x70xf32>\n    return %0 : tensor<2x70xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (3,))",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>) -> (tensor<3xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6] b:f32[6]. let\n    c:f32[1,6] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 6)\n      sharding=None\n    ] a\n    d:f32[1,6] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 6)\n      sharding=None\n    ] b\n    e:f32[2,6] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>, %arg1: tensor<6xf32>) -> (tensor<2x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<6xf32>) -> tensor<1x6xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<6xf32>) -> tensor<1x6xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x6xf32>, tensor<1x6xf32>) -> tensor<2x6xf32>\n    return %2 : tensor<2x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          5,
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,8] b:f32[5,8]. let\n    c:f32[1,5,8] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 5, 8)\n      sharding=None\n    ] a\n    d:f32[1,5,8] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 5, 8)\n      sharding=None\n    ] b\n    e:f32[2,5,8] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x8xf32>, %arg1: tensor<5x8xf32>) -> (tensor<2x5x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<5x8xf32>) -> tensor<1x5x8xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<5x8xf32>) -> tensor<1x5x8xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x5x8xf32>, tensor<1x5x8xf32>) -> tensor<2x5x8xf32>\n    return %2 : tensor<2x5x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          8,
          2,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,2,10]. let\n    b:f32[10,2,8] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x2x10xf32>) -> (tensor<10x2x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<8x2x10xf32>) -> tensor<10x2x8xf32>\n    return %0 : tensor<10x2x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          4,
          6,
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4,
          6,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,6,9] b:f32[4,6,9]. let\n    c:f32[8,6,9] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x6x9xf32>, %arg1: tensor<4x6x9xf32>) -> (tensor<8x6x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<4x6x9xf32>, tensor<4x6x9xf32>) -> tensor<8x6x9xf32>\n    return %0 : tensor<8x6x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          3,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,3]. let\n    b:f32[3,3] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x3xf32>) -> (tensor<3x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<3x3xf32>) -> tensor<3x3xf32>\n    return %0 : tensor<3x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          2,
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,7] b:f32[2,7]. let\n    c:f32[4,7] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x7xf32>, %arg1: tensor<2x7xf32>) -> (tensor<4x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<2x7xf32>, tensor<2x7xf32>) -> tensor<4x7xf32>\n    return %0 : tensor<4x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5] b:f32[5]. let\n    c:f32[1,5] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 5)\n      sharding=None\n    ] a\n    d:f32[1,5] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 5)\n      sharding=None\n    ] b\n    e:f32[2,5] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>, %arg1: tensor<5xf32>) -> (tensor<2x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<5xf32>) -> tensor<1x5xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<5xf32>) -> tensor<1x5xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x5xf32>, tensor<1x5xf32>) -> tensor<2x5xf32>\n    return %2 : tensor<2x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8xf32>) -> (tensor<8xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7] b:f32[7]. let\n    c:f32[14] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>, %arg1: tensor<7xf32>) -> (tensor<14xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<7xf32>, tensor<7xf32>) -> tensor<14xf32>\n    return %0 : tensor<14xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          3,
          10,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,10,2]. let\n    b:f32[2,10,2] = slice[\n      limit_indices=(3, 10, 2)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x10x2xf32>) -> (tensor<2x10x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:3, 0:10, 0:2] : (tensor<3x10x2xf32>) -> tensor<2x10x2xf32>\n    return %0 : tensor<2x10x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7]. let\n    b:f32[6] = slice[limit_indices=(7,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>) -> (tensor<6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:7] : (tensor<7xf32>) -> tensor<6xf32>\n    return %0 : tensor<6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          6,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,2]. let\n    b:f32[2,6] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x2xf32>) -> (tensor<2x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<6x2xf32>) -> tensor<2x6xf32>\n    return %0 : tensor<2x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>) -> (tensor<2xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 24))",
    "input_info": [
      {
        "shape": [
          3,
          2,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,2,8]. let\n    b:f32[2,24] = reshape[dimensions=None new_sizes=(2, 24) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x2x8xf32>) -> (tensor<2x24xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<3x2x8xf32>) -> tensor<2x24xf32>\n    return %0 : tensor<2x24xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (60,))",
    "input_info": [
      {
        "shape": [
          2,
          10,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,10,3]. let\n    b:f32[60] = reshape[dimensions=None new_sizes=(60,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x10x3xf32>) -> (tensor<60xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<2x10x3xf32>) -> tensor<60xf32>\n    return %0 : tensor<60xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          7,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,9]. let\n    b:f32[6,9] = slice[limit_indices=(7, 9) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x9xf32>) -> (tensor<6x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:7, 0:9] : (tensor<7x9xf32>) -> tensor<6x9xf32>\n    return %0 : tensor<6x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          3,
          5,
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3,
          5,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,5,2] b:f32[3,5,2]. let\n    c:f32[6,5,2] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x5x2xf32>, %arg1: tensor<3x5x2xf32>) -> (tensor<6x5x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<3x5x2xf32>, tensor<3x5x2xf32>) -> tensor<6x5x2xf32>\n    return %0 : tensor<6x5x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>) -> (tensor<2xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          6,
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,3] b:f32[6,3]. let\n    c:f32[1,6,3] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 6, 3)\n      sharding=None\n    ] a\n    d:f32[1,6,3] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 6, 3)\n      sharding=None\n    ] b\n    e:f32[2,6,3] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x3xf32>, %arg1: tensor<6x3xf32>) -> (tensor<2x6x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<6x3xf32>) -> tensor<1x6x3xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<6x3xf32>) -> tensor<1x6x3xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x6x3xf32>, tensor<1x6x3xf32>) -> tensor<2x6x3xf32>\n    return %2 : tensor<2x6x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (200,))",
    "input_info": [
      {
        "shape": [
          5,
          5,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,5,8]. let\n    b:f32[200] = reshape[dimensions=None new_sizes=(200,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x5x8xf32>) -> (tensor<200xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<5x5x8xf32>) -> tensor<200xf32>\n    return %0 : tensor<200xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          6,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,6]. let\n    b:f32[6,6] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x6xf32>) -> (tensor<6x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<6x6xf32>) -> tensor<6x6xf32>\n    return %0 : tensor<6x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          2,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,6]. let\n    b:f32[1,6] = slice[limit_indices=(2, 6) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x6xf32>) -> (tensor<1x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:2, 0:6] : (tensor<2x6xf32>) -> tensor<1x6xf32>\n    return %0 : tensor<1x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          10,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,4]. let\n    b:f32[9,4] = slice[limit_indices=(10, 4) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x4xf32>) -> (tensor<9x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:10, 0:4] : (tensor<10x4xf32>) -> tensor<9x4xf32>\n    return %0 : tensor<9x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (12,))",
    "input_info": [
      {
        "shape": [
          6,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,2]. let\n    b:f32[12] = reshape[dimensions=None new_sizes=(12,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x2xf32>) -> (tensor<12xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<6x2xf32>) -> tensor<12xf32>\n    return %0 : tensor<12xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3] b:f32[3]. let\n    c:f32[1,3] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 3)\n      sharding=None\n    ] a\n    d:f32[1,3] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 3)\n      sharding=None\n    ] b\n    e:f32[2,3] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>, %arg1: tensor<3xf32>) -> (tensor<2x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<3xf32>) -> tensor<1x3xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<3xf32>) -> tensor<1x3xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x3xf32>, tensor<1x3xf32>) -> tensor<2x3xf32>\n    return %2 : tensor<2x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          6,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,9]. let\n    b:f32[5,9] = slice[limit_indices=(6, 9) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x9xf32>) -> (tensor<5x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:6, 0:9] : (tensor<6x9xf32>) -> tensor<5x9xf32>\n    return %0 : tensor<5x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          2,
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,5] b:f32[2,5]. let\n    c:f32[4,5] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x5xf32>, %arg1: tensor<2x5xf32>) -> (tensor<4x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<2x5xf32>, tensor<2x5xf32>) -> tensor<4x5xf32>\n    return %0 : tensor<4x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2] b:f32[2]. let\n    c:f32[4] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>, %arg1: tensor<2xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<2xf32>, tensor<2xf32>) -> tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10] b:f32[10]. let\n    c:f32[1,10] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 10)\n      sharding=None\n    ] a\n    d:f32[1,10] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 10)\n      sharding=None\n    ] b\n    e:f32[2,10] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>, %arg1: tensor<10xf32>) -> (tensor<2x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<10xf32>) -> tensor<1x10xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<10xf32>) -> tensor<1x10xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<2x10xf32>\n    return %2 : tensor<2x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          9,
          2,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,2,5]. let\n    b:f32[5,2,9] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x2x5xf32>) -> (tensor<5x2x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<9x2x5xf32>) -> tensor<5x2x9xf32>\n    return %0 : tensor<5x2x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8] b:f32[8]. let\n    c:f32[16] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8xf32>, %arg1: tensor<8xf32>) -> (tensor<16xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<8xf32>, tensor<8xf32>) -> tensor<16xf32>\n    return %0 : tensor<16xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          4,
          7,
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4,
          7,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,7,2] b:f32[4,7,2]. let\n    c:f32[1,4,7,2] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 4, 7, 2)\n      sharding=None\n    ] a\n    d:f32[1,4,7,2] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 4, 7, 2)\n      sharding=None\n    ] b\n    e:f32[2,4,7,2] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x7x2xf32>, %arg1: tensor<4x7x2xf32>) -> (tensor<2x4x7x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<4x7x2xf32>) -> tensor<1x4x7x2xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<4x7x2xf32>) -> tensor<1x4x7x2xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x4x7x2xf32>, tensor<1x4x7x2xf32>) -> tensor<2x4x7x2xf32>\n    return %2 : tensor<2x4x7x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9] b:f32[9]. let\n    c:f32[18] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>, %arg1: tensor<9xf32>) -> (tensor<18xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<9xf32>, tensor<9xf32>) -> tensor<18xf32>\n    return %0 : tensor<18xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9]. let\n    b:f32[8] = slice[limit_indices=(9,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>) -> (tensor<8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:9] : (tensor<9xf32>) -> tensor<8xf32>\n    return %0 : tensor<8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          3,
          7,
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3,
          7,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,7,6] b:f32[3,7,6]. let\n    c:f32[6,7,6] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x7x6xf32>, %arg1: tensor<3x7x6xf32>) -> (tensor<6x7x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<3x7x6xf32>, tensor<3x7x6xf32>) -> tensor<6x7x6xf32>\n    return %0 : tensor<6x7x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          8,
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,3] b:f32[8,3]. let\n    c:f32[16,3] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x3xf32>, %arg1: tensor<8x3xf32>) -> (tensor<16x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<8x3xf32>, tensor<8x3xf32>) -> tensor<16x3xf32>\n    return %0 : tensor<16x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          4,
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,9] b:f32[4,9]. let\n    c:f32[8,9] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x9xf32>, %arg1: tensor<4x9xf32>) -> (tensor<8x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<4x9xf32>, tensor<4x9xf32>) -> tensor<8x9xf32>\n    return %0 : tensor<8x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (210,))",
    "input_info": [
      {
        "shape": [
          10,
          7,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,7,3]. let\n    b:f32[210] = reshape[dimensions=None new_sizes=(210,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x7x3xf32>) -> (tensor<210xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<10x7x3xf32>) -> tensor<210xf32>\n    return %0 : tensor<210xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          4,
          2,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,2,10]. let\n    b:f32[10,2,4] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x2x10xf32>) -> (tensor<10x2x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<4x2x10xf32>) -> tensor<10x2x4xf32>\n    return %0 : tensor<10x2x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>) -> (tensor<3xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          10,
          5,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,5,2]. let\n    b:f32[2,5,10] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x5x2xf32>) -> (tensor<2x5x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<10x5x2xf32>) -> tensor<2x5x10xf32>\n    return %0 : tensor<2x5x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          5,
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,3] b:f32[5,3]. let\n    c:f32[1,5,3] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 5, 3)\n      sharding=None\n    ] a\n    d:f32[1,5,3] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 5, 3)\n      sharding=None\n    ] b\n    e:f32[2,5,3] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x3xf32>, %arg1: tensor<5x3xf32>) -> (tensor<2x5x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<5x3xf32>) -> tensor<1x5x3xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<5x3xf32>) -> tensor<1x5x3xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x5x3xf32>, tensor<1x5x3xf32>) -> tensor<2x5x3xf32>\n    return %2 : tensor<2x5x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          2,
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,5] b:f32[2,5]. let\n    c:f32[4,5] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x5xf32>, %arg1: tensor<2x5xf32>) -> (tensor<4x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<2x5xf32>, tensor<2x5xf32>) -> tensor<4x5xf32>\n    return %0 : tensor<4x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8xf32>) -> (tensor<8xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          3,
          5,
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3,
          5,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,5,2] b:f32[3,5,2]. let\n    c:f32[1,3,5,2] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 3, 5, 2)\n      sharding=None\n    ] a\n    d:f32[1,3,5,2] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 3, 5, 2)\n      sharding=None\n    ] b\n    e:f32[2,3,5,2] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x5x2xf32>, %arg1: tensor<3x5x2xf32>) -> (tensor<2x3x5x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<3x5x2xf32>) -> tensor<1x3x5x2xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<3x5x2xf32>) -> tensor<1x3x5x2xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x3x5x2xf32>, tensor<1x3x5x2xf32>) -> tensor<2x3x5x2xf32>\n    return %2 : tensor<2x3x5x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (27,))",
    "input_info": [
      {
        "shape": [
          9,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,3]. let\n    b:f32[27] = reshape[dimensions=None new_sizes=(27,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x3xf32>) -> (tensor<27xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<9x3xf32>) -> tensor<27xf32>\n    return %0 : tensor<27xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (8,))",
    "input_info": [
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8xf32>) -> (tensor<8xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8]. let\n    b:f32[7] = slice[limit_indices=(8,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8xf32>) -> (tensor<7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:8] : (tensor<8xf32>) -> tensor<7xf32>\n    return %0 : tensor<7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          9,
          10,
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9,
          10,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,10,8] b:f32[9,10,8]. let\n    c:f32[1,9,10,8] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 9, 10, 8)\n      sharding=None\n    ] a\n    d:f32[1,9,10,8] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 9, 10, 8)\n      sharding=None\n    ] b\n    e:f32[2,9,10,8] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x10x8xf32>, %arg1: tensor<9x10x8xf32>) -> (tensor<2x9x10x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<9x10x8xf32>) -> tensor<1x9x10x8xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<9x10x8xf32>) -> tensor<1x9x10x8xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x9x10x8xf32>, tensor<1x9x10x8xf32>) -> tensor<2x9x10x8xf32>\n    return %2 : tensor<2x9x10x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          5,
          6,
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5,
          6,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,6,4] b:f32[5,6,4]. let\n    c:f32[1,5,6,4] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 5, 6, 4)\n      sharding=None\n    ] a\n    d:f32[1,5,6,4] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 5, 6, 4)\n      sharding=None\n    ] b\n    e:f32[2,5,6,4] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x6x4xf32>, %arg1: tensor<5x6x4xf32>) -> (tensor<2x5x6x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<5x6x4xf32>) -> tensor<1x5x6x4xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<5x6x4xf32>) -> tensor<1x5x6x4xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x5x6x4xf32>, tensor<1x5x6x4xf32>) -> tensor<2x5x6x4xf32>\n    return %2 : tensor<2x5x6x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          3,
          7,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,7,4]. let\n    b:f32[2,7,4] = slice[\n      limit_indices=(3, 7, 4)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x7x4xf32>) -> (tensor<2x7x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:3, 0:7, 0:4] : (tensor<3x7x4xf32>) -> tensor<2x7x4xf32>\n    return %0 : tensor<2x7x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8xf32>) -> (tensor<8xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10]. let\n    b:f32[9] = slice[limit_indices=(10,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:10] : (tensor<10xf32>) -> tensor<9xf32>\n    return %0 : tensor<9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          6,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,9]. let\n    b:f32[5,9] = slice[limit_indices=(6, 9) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x9xf32>) -> (tensor<5x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:6, 0:9] : (tensor<6x9xf32>) -> tensor<5x9xf32>\n    return %0 : tensor<5x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>) -> (tensor<3xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          7,
          8,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,8,5]. let\n    b:f32[5,8,7] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x8x5xf32>) -> (tensor<5x8x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<7x8x5xf32>) -> tensor<5x8x7xf32>\n    return %0 : tensor<5x8x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          5,
          8,
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5,
          8,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,8,8] b:f32[5,8,8]. let\n    c:f32[1,5,8,8] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 5, 8, 8)\n      sharding=None\n    ] a\n    d:f32[1,5,8,8] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 5, 8, 8)\n      sharding=None\n    ] b\n    e:f32[2,5,8,8] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x8x8xf32>, %arg1: tensor<5x8x8xf32>) -> (tensor<2x5x8x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<5x8x8xf32>) -> tensor<1x5x8x8xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<5x8x8xf32>) -> tensor<1x5x8x8xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x5x8x8xf32>, tensor<1x5x8x8xf32>) -> tensor<2x5x8x8xf32>\n    return %2 : tensor<2x5x8x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          2,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,7]. let\n    b:f32[7,2] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x7xf32>) -> (tensor<7x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<2x7xf32>) -> tensor<7x2xf32>\n    return %0 : tensor<7x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>) -> (tensor<5xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          7,
          5,
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7,
          5,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,5,4] b:f32[7,5,4]. let\n    c:f32[1,7,5,4] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 7, 5, 4)\n      sharding=None\n    ] a\n    d:f32[1,7,5,4] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 7, 5, 4)\n      sharding=None\n    ] b\n    e:f32[2,7,5,4] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x5x4xf32>, %arg1: tensor<7x5x4xf32>) -> (tensor<2x7x5x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<7x5x4xf32>) -> tensor<1x7x5x4xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<7x5x4xf32>) -> tensor<1x7x5x4xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x7x5x4xf32>, tensor<1x7x5x4xf32>) -> tensor<2x7x5x4xf32>\n    return %2 : tensor<2x7x5x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          10,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,7]. let\n    b:f32[7,10] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x7xf32>) -> (tensor<7x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<10x7xf32>) -> tensor<7x10xf32>\n    return %0 : tensor<7x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          6,
          9,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,9,3]. let\n    b:f32[3,9,6] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x9x3xf32>) -> (tensor<3x9x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<6x9x3xf32>) -> tensor<3x9x6xf32>\n    return %0 : tensor<3x9x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          5,
          7,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,7,10]. let\n    b:f32[4,7,10] = slice[\n      limit_indices=(5, 7, 10)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x7x10xf32>) -> (tensor<4x7x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:5, 0:7, 0:10] : (tensor<5x7x10xf32>) -> tensor<4x7x10xf32>\n    return %0 : tensor<4x7x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          6,
          8,
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6,
          8,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,8,8] b:f32[6,8,8]. let\n    c:f32[1,6,8,8] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 6, 8, 8)\n      sharding=None\n    ] a\n    d:f32[1,6,8,8] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 6, 8, 8)\n      sharding=None\n    ] b\n    e:f32[2,6,8,8] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x8x8xf32>, %arg1: tensor<6x8x8xf32>) -> (tensor<2x6x8x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<6x8x8xf32>) -> tensor<1x6x8x8xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<6x8x8xf32>) -> tensor<1x6x8x8xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x6x8x8xf32>, tensor<1x6x8x8xf32>) -> tensor<2x6x8x8xf32>\n    return %2 : tensor<2x6x8x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 5))",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10]. let\n    b:f32[2,5] = reshape[dimensions=None new_sizes=(2, 5) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<2x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<10xf32>) -> tensor<2x5xf32>\n    return %0 : tensor<2x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4] b:f32[4]. let\n    c:f32[8] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> (tensor<8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<4xf32>, tensor<4xf32>) -> tensor<8xf32>\n    return %0 : tensor<8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          10,
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,4] b:f32[10,4]. let\n    c:f32[1,10,4] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 10, 4)\n      sharding=None\n    ] a\n    d:f32[1,10,4] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 10, 4)\n      sharding=None\n    ] b\n    e:f32[2,10,4] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x4xf32>, %arg1: tensor<10x4xf32>) -> (tensor<2x10x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<10x4xf32>) -> tensor<1x10x4xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<10x4xf32>) -> tensor<1x10x4xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x10x4xf32>, tensor<1x10x4xf32>) -> tensor<2x10x4xf32>\n    return %2 : tensor<2x10x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          3,
          8,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,8,2]. let\n    b:f32[2,8,3] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x8x2xf32>) -> (tensor<2x8x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<3x8x2xf32>) -> tensor<2x8x3xf32>\n    return %0 : tensor<2x8x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (120,))",
    "input_info": [
      {
        "shape": [
          10,
          3,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,3,4]. let\n    b:f32[120] = reshape[dimensions=None new_sizes=(120,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x3x4xf32>) -> (tensor<120xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<10x3x4xf32>) -> tensor<120xf32>\n    return %0 : tensor<120xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10] b:f32[10]. let\n    c:f32[20] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>, %arg1: tensor<10xf32>) -> (tensor<20xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<10xf32>, tensor<10xf32>) -> tensor<20xf32>\n    return %0 : tensor<20xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (3, 21))",
    "input_info": [
      {
        "shape": [
          9,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,7]. let\n    b:f32[3,21] = reshape[dimensions=None new_sizes=(3, 21) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x7xf32>) -> (tensor<3x21xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<9x7xf32>) -> tensor<3x21xf32>\n    return %0 : tensor<3x21xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          3,
          2,
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3,
          2,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,2,10] b:f32[3,2,10]. let\n    c:f32[1,3,2,10] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 3, 2, 10)\n      sharding=None\n    ] a\n    d:f32[1,3,2,10] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 3, 2, 10)\n      sharding=None\n    ] b\n    e:f32[2,3,2,10] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x2x10xf32>, %arg1: tensor<3x2x10xf32>) -> (tensor<2x3x2x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<3x2x10xf32>) -> tensor<1x3x2x10xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<3x2x10xf32>) -> tensor<1x3x2x10xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x3x2x10xf32>, tensor<1x3x2x10xf32>) -> tensor<2x3x2x10xf32>\n    return %2 : tensor<2x3x2x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          7,
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,9] b:f32[7,9]. let\n    c:f32[14,9] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x9xf32>, %arg1: tensor<7x9xf32>) -> (tensor<14x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<7x9xf32>, tensor<7x9xf32>) -> tensor<14x9xf32>\n    return %0 : tensor<14x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 45))",
    "input_info": [
      {
        "shape": [
          2,
          9,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,9,5]. let\n    b:f32[2,45] = reshape[dimensions=None new_sizes=(2, 45) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x9x5xf32>) -> (tensor<2x45xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<2x9x5xf32>) -> tensor<2x45xf32>\n    return %0 : tensor<2x45xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10] b:f32[10]. let\n    c:f32[20] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>, %arg1: tensor<10xf32>) -> (tensor<20xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<10xf32>, tensor<10xf32>) -> tensor<20xf32>\n    return %0 : tensor<20xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          10,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,10]. let\n    b:f32[10,10] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x10xf32>) -> (tensor<10x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<10x10xf32>) -> tensor<10x10xf32>\n    return %0 : tensor<10x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          5,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,9]. let\n    b:f32[9,5] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x9xf32>) -> (tensor<9x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<5x9xf32>) -> tensor<9x5xf32>\n    return %0 : tensor<9x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          5,
          2,
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5,
          2,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,2,4] b:f32[5,2,4]. let\n    c:f32[10,2,4] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x2x4xf32>, %arg1: tensor<5x2x4xf32>) -> (tensor<10x2x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<5x2x4xf32>, tensor<5x2x4xf32>) -> tensor<10x2x4xf32>\n    return %0 : tensor<10x2x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          9,
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,4] b:f32[9,4]. let\n    c:f32[1,9,4] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 9, 4)\n      sharding=None\n    ] a\n    d:f32[1,9,4] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 9, 4)\n      sharding=None\n    ] b\n    e:f32[2,9,4] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x4xf32>, %arg1: tensor<9x4xf32>) -> (tensor<2x9x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<9x4xf32>) -> tensor<1x9x4xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<9x4xf32>) -> tensor<1x9x4xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x9x4xf32>, tensor<1x9x4xf32>) -> tensor<2x9x4xf32>\n    return %2 : tensor<2x9x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          8,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,9]. let\n    b:f32[7,9] = slice[limit_indices=(8, 9) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x9xf32>) -> (tensor<7x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:8, 0:9] : (tensor<8x9xf32>) -> tensor<7x9xf32>\n    return %0 : tensor<7x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          5,
          2,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,2,2]. let\n    b:f32[4,2,2] = slice[\n      limit_indices=(5, 2, 2)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x2x2xf32>) -> (tensor<4x2x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:5, 0:2, 0:2] : (tensor<5x2x2xf32>) -> tensor<4x2x2xf32>\n    return %0 : tensor<4x2x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          5,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,6]. let\n    b:f32[4,6] = slice[limit_indices=(5, 6) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x6xf32>) -> (tensor<4x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:5, 0:6] : (tensor<5x6xf32>) -> tensor<4x6xf32>\n    return %0 : tensor<4x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8]. let\n    b:f32[7] = slice[limit_indices=(8,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8xf32>) -> (tensor<7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:8] : (tensor<8xf32>) -> tensor<7xf32>\n    return %0 : tensor<7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 270))",
    "input_info": [
      {
        "shape": [
          6,
          9,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,9,10]. let\n    b:f32[2,270] = reshape[dimensions=None new_sizes=(2, 270) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x9x10xf32>) -> (tensor<2x270xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<6x9x10xf32>) -> tensor<2x270xf32>\n    return %0 : tensor<2x270xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          7,
          8,
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7,
          8,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,8,8] b:f32[7,8,8]. let\n    c:f32[14,8,8] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x8x8xf32>, %arg1: tensor<7x8x8xf32>) -> (tensor<14x8x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<7x8x8xf32>, tensor<7x8x8xf32>) -> tensor<14x8x8xf32>\n    return %0 : tensor<14x8x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          4,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,3]. let\n    b:f32[3,4] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x3xf32>) -> (tensor<3x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<4x3xf32>) -> tensor<3x4xf32>\n    return %0 : tensor<3x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          10,
          3,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,3,8]. let\n    b:f32[8,3,10] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x3x8xf32>) -> (tensor<8x3x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<10x3x8xf32>) -> tensor<8x3x10xf32>\n    return %0 : tensor<8x3x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          3,
          10,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,10,8]. let\n    b:f32[8,10,3] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x10x8xf32>) -> (tensor<8x10x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<3x10x8xf32>) -> tensor<8x10x3xf32>\n    return %0 : tensor<8x10x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          4,
          10,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,10,10]. let\n    b:f32[3,10,10] = slice[\n      limit_indices=(4, 10, 10)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x10x10xf32>) -> (tensor<3x10x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:4, 0:10, 0:10] : (tensor<4x10x10xf32>) -> tensor<3x10x10xf32>\n    return %0 : tensor<3x10x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2,))",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>) -> (tensor<2xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (3, 3))",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9]. let\n    b:f32[3,3] = reshape[dimensions=None new_sizes=(3, 3) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>) -> (tensor<3x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<9xf32>) -> tensor<3x3xf32>\n    return %0 : tensor<3x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          2,
          6,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,6,4]. let\n    b:f32[4,6,2] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x6x4xf32>) -> (tensor<4x6x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<2x6x4xf32>) -> tensor<4x6x2xf32>\n    return %0 : tensor<4x6x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8]. let\n    b:f32[7] = slice[limit_indices=(8,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8xf32>) -> (tensor<7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:8] : (tensor<8xf32>) -> tensor<7xf32>\n    return %0 : tensor<7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          2,
          4,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,4,5]. let\n    b:f32[1,4,5] = slice[\n      limit_indices=(2, 4, 5)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x4x5xf32>) -> (tensor<1x4x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:2, 0:4, 0:5] : (tensor<2x4x5xf32>) -> tensor<1x4x5xf32>\n    return %0 : tensor<1x4x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          4,
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,7] b:f32[4,7]. let\n    c:f32[1,4,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 4, 7)\n      sharding=None\n    ] a\n    d:f32[1,4,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 4, 7)\n      sharding=None\n    ] b\n    e:f32[2,4,7] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x7xf32>, %arg1: tensor<4x7xf32>) -> (tensor<2x4x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<4x7xf32>) -> tensor<1x4x7xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<4x7xf32>) -> tensor<1x4x7xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x4x7xf32>, tensor<1x4x7xf32>) -> tensor<2x4x7xf32>\n    return %2 : tensor<2x4x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7]. let\n    b:f32[6] = slice[limit_indices=(7,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>) -> (tensor<6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:7] : (tensor<7xf32>) -> tensor<6xf32>\n    return %0 : tensor<6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9] b:f32[9]. let\n    c:f32[18] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>, %arg1: tensor<9xf32>) -> (tensor<18xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<9xf32>, tensor<9xf32>) -> tensor<18xf32>\n    return %0 : tensor<18xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          2,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,10]. let\n    b:f32[10,2] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x10xf32>) -> (tensor<10x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<2x10xf32>) -> tensor<10x2xf32>\n    return %0 : tensor<10x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (3, 3))",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9]. let\n    b:f32[3,3] = reshape[dimensions=None new_sizes=(3, 3) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>) -> (tensor<3x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<9xf32>) -> tensor<3x3xf32>\n    return %0 : tensor<3x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          9,
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,2] b:f32[9,2]. let\n    c:f32[1,9,2] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 9, 2)\n      sharding=None\n    ] a\n    d:f32[1,9,2] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 9, 2)\n      sharding=None\n    ] b\n    e:f32[2,9,2] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x2xf32>, %arg1: tensor<9x2xf32>) -> (tensor<2x9x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<9x2xf32>) -> tensor<1x9x2xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<9x2xf32>) -> tensor<1x9x2xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x9x2xf32>, tensor<1x9x2xf32>) -> tensor<2x9x2xf32>\n    return %2 : tensor<2x9x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          2,
          4,
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2,
          4,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,4,6] b:f32[2,4,6]. let\n    c:f32[1,2,4,6] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 2, 4, 6)\n      sharding=None\n    ] a\n    d:f32[1,2,4,6] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 2, 4, 6)\n      sharding=None\n    ] b\n    e:f32[2,2,4,6] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x4x6xf32>, %arg1: tensor<2x4x6xf32>) -> (tensor<2x2x4x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<2x4x6xf32>) -> tensor<1x2x4x6xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<2x4x6xf32>) -> tensor<1x2x4x6xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x2x4x6xf32>, tensor<1x2x4x6xf32>) -> tensor<2x2x4x6xf32>\n    return %2 : tensor<2x2x4x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          8,
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,7] b:f32[8,7]. let\n    c:f32[1,8,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 8, 7)\n      sharding=None\n    ] a\n    d:f32[1,8,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 8, 7)\n      sharding=None\n    ] b\n    e:f32[2,8,7] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x7xf32>, %arg1: tensor<8x7xf32>) -> (tensor<2x8x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<8x7xf32>) -> tensor<1x8x7xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<8x7xf32>) -> tensor<1x8x7xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x8x7xf32>, tensor<1x8x7xf32>) -> tensor<2x8x7xf32>\n    return %2 : tensor<2x8x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          2,
          7,
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2,
          7,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,7,10] b:f32[2,7,10]. let\n    c:f32[1,2,7,10] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 2, 7, 10)\n      sharding=None\n    ] a\n    d:f32[1,2,7,10] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 2, 7, 10)\n      sharding=None\n    ] b\n    e:f32[2,2,7,10] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x7x10xf32>, %arg1: tensor<2x7x10xf32>) -> (tensor<2x2x7x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<2x7x10xf32>) -> tensor<1x2x7x10xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<2x7x10xf32>) -> tensor<1x2x7x10xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x2x7x10xf32>, tensor<1x2x7x10xf32>) -> tensor<2x2x7x10xf32>\n    return %2 : tensor<2x2x7x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 63))",
    "input_info": [
      {
        "shape": [
          9,
          2,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,2,7]. let\n    b:f32[2,63] = reshape[dimensions=None new_sizes=(2, 63) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x2x7xf32>) -> (tensor<2x63xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<9x2x7xf32>) -> tensor<2x63xf32>\n    return %0 : tensor<2x63xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 25))",
    "input_info": [
      {
        "shape": [
          10,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,5]. let\n    b:f32[2,25] = reshape[dimensions=None new_sizes=(2, 25) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x5xf32>) -> (tensor<2x25xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<10x5xf32>) -> tensor<2x25xf32>\n    return %0 : tensor<2x25xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          5,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,6]. let\n    b:f32[6,5] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x6xf32>) -> (tensor<6x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<5x6xf32>) -> tensor<6x5xf32>\n    return %0 : tensor<6x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          7,
          8,
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7,
          8,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,8,2] b:f32[7,8,2]. let\n    c:f32[1,7,8,2] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 7, 8, 2)\n      sharding=None\n    ] a\n    d:f32[1,7,8,2] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 7, 8, 2)\n      sharding=None\n    ] b\n    e:f32[2,7,8,2] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x8x2xf32>, %arg1: tensor<7x8x2xf32>) -> (tensor<2x7x8x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<7x8x2xf32>) -> tensor<1x7x8x2xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<7x8x2xf32>) -> tensor<1x7x8x2xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x7x8x2xf32>, tensor<1x7x8x2xf32>) -> tensor<2x7x8x2xf32>\n    return %2 : tensor<2x7x8x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          4,
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,8] b:f32[4,8]. let\n    c:f32[8,8] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x8xf32>, %arg1: tensor<4x8xf32>) -> (tensor<8x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<4x8xf32>, tensor<4x8xf32>) -> tensor<8x8xf32>\n    return %0 : tensor<8x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          6,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,2]. let\n    b:f32[5,2] = slice[limit_indices=(6, 2) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x2xf32>) -> (tensor<5x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:6, 0:2] : (tensor<6x2xf32>) -> tensor<5x2xf32>\n    return %0 : tensor<5x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          8,
          9,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,9,9]. let\n    b:f32[9,9,8] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x9x9xf32>) -> (tensor<9x9x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<8x9x9xf32>) -> tensor<9x9x8xf32>\n    return %0 : tensor<9x9x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          9,
          3,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,3,6]. let\n    b:f32[8,3,6] = slice[\n      limit_indices=(9, 3, 6)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x3x6xf32>) -> (tensor<8x3x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:9, 0:3, 0:6] : (tensor<9x3x6xf32>) -> tensor<8x3x6xf32>\n    return %0 : tensor<8x3x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4] b:f32[4]. let\n    c:f32[8] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> (tensor<8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<4xf32>, tensor<4xf32>) -> tensor<8xf32>\n    return %0 : tensor<8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7] b:f32[7]. let\n    c:f32[1,7] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 7)\n      sharding=None\n    ] a\n    d:f32[1,7] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 7)\n      sharding=None\n    ] b\n    e:f32[2,7] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>, %arg1: tensor<7xf32>) -> (tensor<2x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<7xf32>) -> tensor<1x7xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<7xf32>) -> tensor<1x7xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x7xf32>, tensor<1x7xf32>) -> tensor<2x7xf32>\n    return %2 : tensor<2x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          3,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,8]. let\n    b:f32[8,3] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x8xf32>) -> (tensor<8x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<3x8xf32>) -> tensor<8x3xf32>\n    return %0 : tensor<8x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9] b:f32[9]. let\n    c:f32[1,9] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 9)\n      sharding=None\n    ] a\n    d:f32[1,9] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 9)\n      sharding=None\n    ] b\n    e:f32[2,9] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>, %arg1: tensor<9xf32>) -> (tensor<2x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<9xf32>) -> tensor<1x9xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<9xf32>) -> tensor<1x9xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x9xf32>, tensor<1x9xf32>) -> tensor<2x9xf32>\n    return %2 : tensor<2x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          3,
          10,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,10,2]. let\n    b:f32[2,10,3] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x10x2xf32>) -> (tensor<2x10x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<3x10x2xf32>) -> tensor<2x10x3xf32>\n    return %0 : tensor<2x10x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5]. let\n    b:f32[4] = slice[limit_indices=(5,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:5] : (tensor<5xf32>) -> tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          8,
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,2] b:f32[8,2]. let\n    c:f32[16,2] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x2xf32>, %arg1: tensor<8x2xf32>) -> (tensor<16x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<8x2xf32>, tensor<8x2xf32>) -> tensor<16x2xf32>\n    return %0 : tensor<16x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9] b:f32[9]. let\n    c:f32[18] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>, %arg1: tensor<9xf32>) -> (tensor<18xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<9xf32>, tensor<9xf32>) -> tensor<18xf32>\n    return %0 : tensor<18xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          5,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,7]. let\n    b:f32[7,5] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x7xf32>) -> (tensor<7x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<5x7xf32>) -> tensor<7x5xf32>\n    return %0 : tensor<7x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          7,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,4]. let\n    b:f32[4,7] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x4xf32>) -> (tensor<4x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<7x4xf32>) -> tensor<4x7xf32>\n    return %0 : tensor<4x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          4,
          4,
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4,
          4,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,4,8] b:f32[4,4,8]. let\n    c:f32[1,4,4,8] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 4, 4, 8)\n      sharding=None\n    ] a\n    d:f32[1,4,4,8] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 4, 4, 8)\n      sharding=None\n    ] b\n    e:f32[2,4,4,8] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x4x8xf32>, %arg1: tensor<4x4x8xf32>) -> (tensor<2x4x4x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<4x4x8xf32>) -> tensor<1x4x4x8xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<4x4x8xf32>) -> tensor<1x4x4x8xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x4x4x8xf32>, tensor<1x4x4x8xf32>) -> tensor<2x4x4x8xf32>\n    return %2 : tensor<2x4x4x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          7,
          2,
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7,
          2,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,2,5] b:f32[7,2,5]. let\n    c:f32[14,2,5] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x2x5xf32>, %arg1: tensor<7x2x5xf32>) -> (tensor<14x2x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<7x2x5xf32>, tensor<7x2x5xf32>) -> tensor<14x2x5xf32>\n    return %0 : tensor<14x2x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9] b:f32[9]. let\n    c:f32[1,9] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 9)\n      sharding=None\n    ] a\n    d:f32[1,9] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 9)\n      sharding=None\n    ] b\n    e:f32[2,9] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>, %arg1: tensor<9xf32>) -> (tensor<2x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<9xf32>) -> tensor<1x9xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<9xf32>) -> tensor<1x9xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x9xf32>, tensor<1x9xf32>) -> tensor<2x9xf32>\n    return %2 : tensor<2x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4] b:f32[4]. let\n    c:f32[1,4] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 4)\n      sharding=None\n    ] a\n    d:f32[1,4] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 4)\n      sharding=None\n    ] b\n    e:f32[2,4] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> (tensor<2x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<4xf32>) -> tensor<1x4xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<4xf32>) -> tensor<1x4xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x4xf32>, tensor<1x4xf32>) -> tensor<2x4xf32>\n    return %2 : tensor<2x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          4,
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,4] b:f32[4,4]. let\n    c:f32[8,4] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x4xf32>, %arg1: tensor<4x4xf32>) -> (tensor<8x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<4x4xf32>, tensor<4x4xf32>) -> tensor<8x4xf32>\n    return %0 : tensor<8x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          5,
          9,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,9,7]. let\n    b:f32[7,9,5] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x9x7xf32>) -> (tensor<7x9x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<5x9x7xf32>) -> tensor<7x9x5xf32>\n    return %0 : tensor<7x9x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (5,))",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>) -> (tensor<5xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (10,))",
    "input_info": [
      {
        "shape": [
          2,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,5]. let\n    b:f32[10] = reshape[dimensions=None new_sizes=(10,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x5xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<2x5xf32>) -> tensor<10xf32>\n    return %0 : tensor<10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2]. let\n    b:f32[1] = slice[limit_indices=(2,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>) -> (tensor<1xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:2] : (tensor<2xf32>) -> tensor<1xf32>\n    return %0 : tensor<1xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          6,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,3]. let\n    b:f32[5,3] = slice[limit_indices=(6, 3) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x3xf32>) -> (tensor<5x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:6, 0:3] : (tensor<6x3xf32>) -> tensor<5x3xf32>\n    return %0 : tensor<5x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          7,
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,7] b:f32[7,7]. let\n    c:f32[14,7] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x7xf32>, %arg1: tensor<7x7xf32>) -> (tensor<14x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<7x7xf32>, tensor<7x7xf32>) -> tensor<14x7xf32>\n    return %0 : tensor<14x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2] b:f32[2]. let\n    c:f32[4] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>, %arg1: tensor<2xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<2xf32>, tensor<2xf32>) -> tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 5))",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10]. let\n    b:f32[2,5] = reshape[dimensions=None new_sizes=(2, 5) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<2x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<10xf32>) -> tensor<2x5xf32>\n    return %0 : tensor<2x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5] b:f32[5]. let\n    c:f32[10] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>, %arg1: tensor<5xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<5xf32>, tensor<5xf32>) -> tensor<10xf32>\n    return %0 : tensor<10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5] b:f32[5]. let\n    c:f32[1,5] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 5)\n      sharding=None\n    ] a\n    d:f32[1,5] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 5)\n      sharding=None\n    ] b\n    e:f32[2,5] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>, %arg1: tensor<5xf32>) -> (tensor<2x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<5xf32>) -> tensor<1x5xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<5xf32>) -> tensor<1x5xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x5xf32>, tensor<1x5xf32>) -> tensor<2x5xf32>\n    return %2 : tensor<2x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (3, 3))",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9]. let\n    b:f32[3,3] = reshape[dimensions=None new_sizes=(3, 3) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>) -> (tensor<3x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<9xf32>) -> tensor<3x3xf32>\n    return %0 : tensor<3x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          3,
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,5] b:f32[3,5]. let\n    c:f32[6,5] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x5xf32>, %arg1: tensor<3x5xf32>) -> (tensor<6x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<3x5xf32>, tensor<3x5xf32>) -> tensor<6x5xf32>\n    return %0 : tensor<6x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8] b:f32[8]. let\n    c:f32[16] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8xf32>, %arg1: tensor<8xf32>) -> (tensor<16xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<8xf32>, tensor<8xf32>) -> tensor<16xf32>\n    return %0 : tensor<16xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          9,
          7,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,7,9]. let\n    b:f32[8,7,9] = slice[\n      limit_indices=(9, 7, 9)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x7x9xf32>) -> (tensor<8x7x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:9, 0:7, 0:9] : (tensor<9x7x9xf32>) -> tensor<8x7x9xf32>\n    return %0 : tensor<8x7x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          4,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,3]. let\n    b:f32[3,4] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x3xf32>) -> (tensor<3x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<4x3xf32>) -> tensor<3x4xf32>\n    return %0 : tensor<3x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (5,))",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>) -> (tensor<5xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7] b:f32[7]. let\n    c:f32[14] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>, %arg1: tensor<7xf32>) -> (tensor<14xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<7xf32>, tensor<7xf32>) -> tensor<14xf32>\n    return %0 : tensor<14xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          3,
          10,
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3,
          10,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,10,10] b:f32[3,10,10]. let\n    c:f32[6,10,10] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x10x10xf32>, %arg1: tensor<3x10x10xf32>) -> (tensor<6x10x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<3x10x10xf32>, tensor<3x10x10xf32>) -> tensor<6x10x10xf32>\n    return %0 : tensor<6x10x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          10,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,7]. let\n    b:f32[9,7] = slice[limit_indices=(10, 7) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x7xf32>) -> (tensor<9x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:10, 0:7] : (tensor<10x7xf32>) -> tensor<9x7xf32>\n    return %0 : tensor<9x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10] b:f32[10]. let\n    c:f32[20] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>, %arg1: tensor<10xf32>) -> (tensor<20xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<10xf32>, tensor<10xf32>) -> tensor<20xf32>\n    return %0 : tensor<20xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (96,))",
    "input_info": [
      {
        "shape": [
          6,
          2,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,2,8]. let\n    b:f32[96] = reshape[dimensions=None new_sizes=(96,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x2x8xf32>) -> (tensor<96xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<6x2x8xf32>) -> tensor<96xf32>\n    return %0 : tensor<96xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2] b:f32[2]. let\n    c:f32[1,2] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 2)\n      sharding=None\n    ] a\n    d:f32[1,2] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 2)\n      sharding=None\n    ] b\n    e:f32[2,2] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>, %arg1: tensor<2xf32>) -> (tensor<2x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<2xf32>) -> tensor<1x2xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<2xf32>) -> tensor<1x2xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x2xf32>, tensor<1x2xf32>) -> tensor<2x2xf32>\n    return %2 : tensor<2x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          10,
          3,
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10,
          3,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,3,2] b:f32[10,3,2]. let\n    c:f32[20,3,2] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x3x2xf32>, %arg1: tensor<10x3x2xf32>) -> (tensor<20x3x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<10x3x2xf32>, tensor<10x3x2xf32>) -> tensor<20x3x2xf32>\n    return %0 : tensor<20x3x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          4,
          3,
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4,
          3,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,3,6] b:f32[4,3,6]. let\n    c:f32[8,3,6] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x3x6xf32>, %arg1: tensor<4x3x6xf32>) -> (tensor<8x3x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<4x3x6xf32>, tensor<4x3x6xf32>) -> tensor<8x3x6xf32>\n    return %0 : tensor<8x3x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          9,
          8,
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9,
          8,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,8,4] b:f32[9,8,4]. let\n    c:f32[1,9,8,4] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 9, 8, 4)\n      sharding=None\n    ] a\n    d:f32[1,9,8,4] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 9, 8, 4)\n      sharding=None\n    ] b\n    e:f32[2,9,8,4] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x8x4xf32>, %arg1: tensor<9x8x4xf32>) -> (tensor<2x9x8x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<9x8x4xf32>) -> tensor<1x9x8x4xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<9x8x4xf32>) -> tensor<1x9x8x4xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x9x8x4xf32>, tensor<1x9x8x4xf32>) -> tensor<2x9x8x4xf32>\n    return %2 : tensor<2x9x8x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          5,
          7,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,7,4]. let\n    b:f32[4,7,5] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x7x4xf32>) -> (tensor<4x7x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<5x7x4xf32>) -> tensor<4x7x5xf32>\n    return %0 : tensor<4x7x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          10,
          9,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,9,10]. let\n    b:f32[10,9,10] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x9x10xf32>) -> (tensor<10x9x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<10x9x10xf32>) -> tensor<10x9x10xf32>\n    return %0 : tensor<10x9x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          5,
          6,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,6,8]. let\n    b:f32[4,6,8] = slice[\n      limit_indices=(5, 6, 8)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x6x8xf32>) -> (tensor<4x6x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:5, 0:6, 0:8] : (tensor<5x6x8xf32>) -> tensor<4x6x8xf32>\n    return %0 : tensor<4x6x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          4,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,5]. let\n    b:f32[5,4] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x5xf32>) -> (tensor<5x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<4x5xf32>) -> tensor<5x4xf32>\n    return %0 : tensor<5x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          4,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,7]. let\n    b:f32[7,4] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x7xf32>) -> (tensor<7x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<4x7xf32>) -> tensor<7x4xf32>\n    return %0 : tensor<7x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 360))",
    "input_info": [
      {
        "shape": [
          8,
          9,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,9,10]. let\n    b:f32[2,360] = reshape[dimensions=None new_sizes=(2, 360) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x9x10xf32>) -> (tensor<2x360xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<8x9x10xf32>) -> tensor<2x360xf32>\n    return %0 : tensor<2x360xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          6,
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,4] b:f32[6,4]. let\n    c:f32[1,6,4] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 6, 4)\n      sharding=None\n    ] a\n    d:f32[1,6,4] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 6, 4)\n      sharding=None\n    ] b\n    e:f32[2,6,4] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x4xf32>, %arg1: tensor<6x4xf32>) -> (tensor<2x6x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<6x4xf32>) -> tensor<1x6x4xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<6x4xf32>) -> tensor<1x6x4xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x6x4xf32>, tensor<1x6x4xf32>) -> tensor<2x6x4xf32>\n    return %2 : tensor<2x6x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8] b:f32[8]. let\n    c:f32[1,8] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 8)\n      sharding=None\n    ] a\n    d:f32[1,8] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 8)\n      sharding=None\n    ] b\n    e:f32[2,8] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8xf32>, %arg1: tensor<8xf32>) -> (tensor<2x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<8xf32>) -> tensor<1x8xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<8xf32>) -> tensor<1x8xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x8xf32>, tensor<1x8xf32>) -> tensor<2x8xf32>\n    return %2 : tensor<2x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          8,
          4,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,4,3]. let\n    b:f32[7,4,3] = slice[\n      limit_indices=(8, 4, 3)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x4x3xf32>) -> (tensor<7x4x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:8, 0:4, 0:3] : (tensor<8x4x3xf32>) -> tensor<7x4x3xf32>\n    return %0 : tensor<7x4x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 72))",
    "input_info": [
      {
        "shape": [
          9,
          8,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,8,2]. let\n    b:f32[2,72] = reshape[dimensions=None new_sizes=(2, 72) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x8x2xf32>) -> (tensor<2x72xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<9x8x2xf32>) -> tensor<2x72xf32>\n    return %0 : tensor<2x72xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          10,
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,2] b:f32[10,2]. let\n    c:f32[1,10,2] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 10, 2)\n      sharding=None\n    ] a\n    d:f32[1,10,2] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 10, 2)\n      sharding=None\n    ] b\n    e:f32[2,10,2] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x2xf32>, %arg1: tensor<10x2xf32>) -> (tensor<2x10x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<10x2xf32>) -> tensor<1x10x2xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<10x2xf32>) -> tensor<1x10x2xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x10x2xf32>, tensor<1x10x2xf32>) -> tensor<2x10x2xf32>\n    return %2 : tensor<2x10x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (189,))",
    "input_info": [
      {
        "shape": [
          7,
          9,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,9,3]. let\n    b:f32[189] = reshape[dimensions=None new_sizes=(189,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x9x3xf32>) -> (tensor<189xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<7x9x3xf32>) -> tensor<189xf32>\n    return %0 : tensor<189xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          4,
          7,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,7,8]. let\n    b:f32[8,7,4] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x7x8xf32>) -> (tensor<8x7x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<4x7x8xf32>) -> tensor<8x7x4xf32>\n    return %0 : tensor<8x7x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9] b:f32[9]. let\n    c:f32[18] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>, %arg1: tensor<9xf32>) -> (tensor<18xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<9xf32>, tensor<9xf32>) -> tensor<18xf32>\n    return %0 : tensor<18xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          10,
          7,
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10,
          7,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,7,3] b:f32[10,7,3]. let\n    c:f32[20,7,3] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x7x3xf32>, %arg1: tensor<10x7x3xf32>) -> (tensor<20x7x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<10x7x3xf32>, tensor<10x7x3xf32>) -> tensor<20x7x3xf32>\n    return %0 : tensor<20x7x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>) -> (tensor<5xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (72,))",
    "input_info": [
      {
        "shape": [
          9,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,8]. let\n    b:f32[72] = reshape[dimensions=None new_sizes=(72,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x8xf32>) -> (tensor<72xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<9x8xf32>) -> tensor<72xf32>\n    return %0 : tensor<72xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          9,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,8]. let\n    b:f32[8,8] = slice[limit_indices=(9, 8) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x8xf32>) -> (tensor<8x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:9, 0:8] : (tensor<9x8xf32>) -> tensor<8x8xf32>\n    return %0 : tensor<8x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (3,))",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>) -> (tensor<3xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5] b:f32[5]. let\n    c:f32[10] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>, %arg1: tensor<5xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<5xf32>, tensor<5xf32>) -> tensor<10xf32>\n    return %0 : tensor<10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10]. let\n    b:f32[9] = slice[limit_indices=(10,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:10] : (tensor<10xf32>) -> tensor<9xf32>\n    return %0 : tensor<9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          9,
          2,
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9,
          2,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,2,3] b:f32[9,2,3]. let\n    c:f32[18,2,3] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x2x3xf32>, %arg1: tensor<9x2x3xf32>) -> (tensor<18x2x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<9x2x3xf32>, tensor<9x2x3xf32>) -> tensor<18x2x3xf32>\n    return %0 : tensor<18x2x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          6,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,2]. let\n    b:f32[5,2] = slice[limit_indices=(6, 2) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x2xf32>) -> (tensor<5x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:6, 0:2] : (tensor<6x2xf32>) -> tensor<5x2xf32>\n    return %0 : tensor<5x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          8,
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,9] b:f32[8,9]. let\n    c:f32[1,8,9] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 8, 9)\n      sharding=None\n    ] a\n    d:f32[1,8,9] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 8, 9)\n      sharding=None\n    ] b\n    e:f32[2,8,9] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x9xf32>, %arg1: tensor<8x9xf32>) -> (tensor<2x8x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<8x9xf32>) -> tensor<1x8x9xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<8x9xf32>) -> tensor<1x8x9xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x8x9xf32>, tensor<1x8x9xf32>) -> tensor<2x8x9xf32>\n    return %2 : tensor<2x8x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5]. let\n    b:f32[4] = slice[limit_indices=(5,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:5] : (tensor<5xf32>) -> tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2,))",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>) -> (tensor<2xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          2,
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,3] b:f32[2,3]. let\n    c:f32[4,3] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x3xf32>, %arg1: tensor<2x3xf32>) -> (tensor<4x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<2x3xf32>, tensor<2x3xf32>) -> tensor<4x3xf32>\n    return %0 : tensor<4x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          9,
          2,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,2,10]. let\n    b:f32[10,2,9] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x2x10xf32>) -> (tensor<10x2x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<9x2x10xf32>) -> tensor<10x2x9xf32>\n    return %0 : tensor<10x2x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          8,
          8,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,8,6]. let\n    b:f32[6,8,8] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x8x6xf32>) -> (tensor<6x8x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<8x8x6xf32>) -> tensor<6x8x8xf32>\n    return %0 : tensor<6x8x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3] b:f32[3]. let\n    c:f32[6] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>, %arg1: tensor<3xf32>) -> (tensor<6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<3xf32>, tensor<3xf32>) -> tensor<6xf32>\n    return %0 : tensor<6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 15))",
    "input_info": [
      {
        "shape": [
          5,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,6]. let\n    b:f32[2,15] = reshape[dimensions=None new_sizes=(2, 15) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x6xf32>) -> (tensor<2x15xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<5x6xf32>) -> tensor<2x15xf32>\n    return %0 : tensor<2x15xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7]. let\n    b:f32[6] = slice[limit_indices=(7,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>) -> (tensor<6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:7] : (tensor<7xf32>) -> tensor<6xf32>\n    return %0 : tensor<6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          8,
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,4] b:f32[8,4]. let\n    c:f32[1,8,4] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 8, 4)\n      sharding=None\n    ] a\n    d:f32[1,8,4] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 8, 4)\n      sharding=None\n    ] b\n    e:f32[2,8,4] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x4xf32>, %arg1: tensor<8x4xf32>) -> (tensor<2x8x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<8x4xf32>) -> tensor<1x8x4xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<8x4xf32>) -> tensor<1x8x4xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x8x4xf32>, tensor<1x8x4xf32>) -> tensor<2x8x4xf32>\n    return %2 : tensor<2x8x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          2,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,2]. let\n    b:f32[1,2] = slice[limit_indices=(2, 2) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x2xf32>) -> (tensor<1x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:2, 0:2] : (tensor<2x2xf32>) -> tensor<1x2xf32>\n    return %0 : tensor<1x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2]. let\n    b:f32[1] = slice[limit_indices=(2,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>) -> (tensor<1xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:2] : (tensor<2xf32>) -> tensor<1xf32>\n    return %0 : tensor<1xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (256,))",
    "input_info": [
      {
        "shape": [
          8,
          4,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,4,8]. let\n    b:f32[256] = reshape[dimensions=None new_sizes=(256,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x4x8xf32>) -> (tensor<256xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<8x4x8xf32>) -> tensor<256xf32>\n    return %0 : tensor<256xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7]. let\n    b:f32[6] = slice[limit_indices=(7,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>) -> (tensor<6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:7] : (tensor<7xf32>) -> tensor<6xf32>\n    return %0 : tensor<6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          8,
          9,
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          9,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,9,6] b:f32[8,9,6]. let\n    c:f32[16,9,6] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x9x6xf32>, %arg1: tensor<8x9x6xf32>) -> (tensor<16x9x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<8x9x6xf32>, tensor<8x9x6xf32>) -> tensor<16x9x6xf32>\n    return %0 : tensor<16x9x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          10,
          6,
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10,
          6,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,6,6] b:f32[10,6,6]. let\n    c:f32[1,10,6,6] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 10, 6, 6)\n      sharding=None\n    ] a\n    d:f32[1,10,6,6] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 10, 6, 6)\n      sharding=None\n    ] b\n    e:f32[2,10,6,6] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x6x6xf32>, %arg1: tensor<10x6x6xf32>) -> (tensor<2x10x6x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<10x6x6xf32>) -> tensor<1x10x6x6xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<10x6x6xf32>) -> tensor<1x10x6x6xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x10x6x6xf32>, tensor<1x10x6x6xf32>) -> tensor<2x10x6x6xf32>\n    return %2 : tensor<2x10x6x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6] b:f32[6]. let\n    c:f32[12] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>, %arg1: tensor<6xf32>) -> (tensor<12xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<6xf32>, tensor<6xf32>) -> tensor<12xf32>\n    return %0 : tensor<12xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          5,
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,2] b:f32[5,2]. let\n    c:f32[10,2] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x2xf32>, %arg1: tensor<5x2xf32>) -> (tensor<10x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<5x2xf32>, tensor<5x2xf32>) -> tensor<10x2xf32>\n    return %0 : tensor<10x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7] b:f32[7]. let\n    c:f32[14] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>, %arg1: tensor<7xf32>) -> (tensor<14xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<7xf32>, tensor<7xf32>) -> tensor<14xf32>\n    return %0 : tensor<14xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (3,))",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>) -> (tensor<3xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          9,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,7]. let\n    b:f32[7,9] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x7xf32>) -> (tensor<7x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<9x7xf32>) -> tensor<7x9xf32>\n    return %0 : tensor<7x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          2,
          6,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,6,2]. let\n    b:f32[2,6,2] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x6x2xf32>) -> (tensor<2x6x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<2x6x2xf32>) -> tensor<2x6x2xf32>\n    return %0 : tensor<2x6x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>) -> (tensor<3xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          10,
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,10] b:f32[10,10]. let\n    c:f32[1,10,10] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 10, 10)\n      sharding=None\n    ] a\n    d:f32[1,10,10] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 10, 10)\n      sharding=None\n    ] b\n    e:f32[2,10,10] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x10xf32>, %arg1: tensor<10x10xf32>) -> (tensor<2x10x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<10x10xf32>) -> tensor<1x10x10xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<10x10xf32>) -> tensor<1x10x10xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x10x10xf32>, tensor<1x10x10xf32>) -> tensor<2x10x10xf32>\n    return %2 : tensor<2x10x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          3,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,2]. let\n    b:f32[2,3] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x2xf32>) -> (tensor<2x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<3x2xf32>) -> tensor<2x3xf32>\n    return %0 : tensor<2x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9] b:f32[9]. let\n    c:f32[1,9] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 9)\n      sharding=None\n    ] a\n    d:f32[1,9] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 9)\n      sharding=None\n    ] b\n    e:f32[2,9] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>, %arg1: tensor<9xf32>) -> (tensor<2x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<9xf32>) -> tensor<1x9xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<9xf32>) -> tensor<1x9xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x9xf32>, tensor<1x9xf32>) -> tensor<2x9xf32>\n    return %2 : tensor<2x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          2,
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,3] b:f32[2,3]. let\n    c:f32[1,2,3] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 2, 3)\n      sharding=None\n    ] a\n    d:f32[1,2,3] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 2, 3)\n      sharding=None\n    ] b\n    e:f32[2,2,3] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x3xf32>, %arg1: tensor<2x3xf32>) -> (tensor<2x2x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<2x3xf32>) -> tensor<1x2x3xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<2x3xf32>) -> tensor<1x2x3xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x2x3xf32>, tensor<1x2x3xf32>) -> tensor<2x2x3xf32>\n    return %2 : tensor<2x2x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          5,
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,9] b:f32[5,9]. let\n    c:f32[10,9] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x9xf32>, %arg1: tensor<5x9xf32>) -> (tensor<10x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<5x9xf32>, tensor<5x9xf32>) -> tensor<10x9xf32>\n    return %0 : tensor<10x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 4))",
    "input_info": [
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8]. let\n    b:f32[2,4] = reshape[dimensions=None new_sizes=(2, 4) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8xf32>) -> (tensor<2x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<8xf32>) -> tensor<2x4xf32>\n    return %0 : tensor<2x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          5,
          6,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,6,2]. let\n    b:f32[4,6,2] = slice[\n      limit_indices=(5, 6, 2)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x6x2xf32>) -> (tensor<4x6x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:5, 0:6, 0:2] : (tensor<5x6x2xf32>) -> tensor<4x6x2xf32>\n    return %0 : tensor<4x6x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          8,
          2,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,2,10]. let\n    b:f32[7,2,10] = slice[\n      limit_indices=(8, 2, 10)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x2x10xf32>) -> (tensor<7x2x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:8, 0:2, 0:10] : (tensor<8x2x10xf32>) -> tensor<7x2x10xf32>\n    return %0 : tensor<7x2x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3] b:f32[3]. let\n    c:f32[6] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>, %arg1: tensor<3xf32>) -> (tensor<6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<3xf32>, tensor<3xf32>) -> tensor<6xf32>\n    return %0 : tensor<6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          6,
          9,
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6,
          9,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,9,8] b:f32[6,9,8]. let\n    c:f32[12,9,8] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x9x8xf32>, %arg1: tensor<6x9x8xf32>) -> (tensor<12x9x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<6x9x8xf32>, tensor<6x9x8xf32>) -> tensor<12x9x8xf32>\n    return %0 : tensor<12x9x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10]. let\n    b:f32[9] = slice[limit_indices=(10,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:10] : (tensor<10xf32>) -> tensor<9xf32>\n    return %0 : tensor<9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          3,
          8,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,8,9]. let\n    b:f32[9,8,3] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x8x9xf32>) -> (tensor<9x8x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<3x8x9xf32>) -> tensor<9x8x3xf32>\n    return %0 : tensor<9x8x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          4,
          4,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,4,5]. let\n    b:f32[5,4,4] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x4x5xf32>) -> (tensor<5x4x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<4x4x5xf32>) -> tensor<5x4x4xf32>\n    return %0 : tensor<5x4x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          3,
          8,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,8,2]. let\n    b:f32[2,8,3] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x8x2xf32>) -> (tensor<2x8x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<3x8x2xf32>) -> tensor<2x8x3xf32>\n    return %0 : tensor<2x8x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (9,))",
    "input_info": [
      {
        "shape": [
          3,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,3]. let\n    b:f32[9] = reshape[dimensions=None new_sizes=(9,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x3xf32>) -> (tensor<9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<3x3xf32>) -> tensor<9xf32>\n    return %0 : tensor<9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          7,
          3,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,3,3]. let\n    b:f32[3,3,7] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x3x3xf32>) -> (tensor<3x3x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<7x3x3xf32>) -> tensor<3x3x7xf32>\n    return %0 : tensor<3x3x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (15,))",
    "input_info": [
      {
        "shape": [
          3,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,5]. let\n    b:f32[15] = reshape[dimensions=None new_sizes=(15,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x5xf32>) -> (tensor<15xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<3x5xf32>) -> tensor<15xf32>\n    return %0 : tensor<15xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 10))",
    "input_info": [
      {
        "shape": [
          2,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,10]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x10xf32>) -> (tensor<2x10xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<2x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          10,
          4,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,4,10]. let\n    b:f32[10,4,10] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x4x10xf32>) -> (tensor<10x4x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<10x4x10xf32>) -> tensor<10x4x10xf32>\n    return %0 : tensor<10x4x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>) -> (tensor<7xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          7,
          4,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,4,3]. let\n    b:f32[6,4,3] = slice[\n      limit_indices=(7, 4, 3)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x4x3xf32>) -> (tensor<6x4x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:7, 0:4, 0:3] : (tensor<7x4x3xf32>) -> tensor<6x4x3xf32>\n    return %0 : tensor<6x4x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          6,
          3,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,3,10]. let\n    b:f32[10,3,6] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x3x10xf32>) -> (tensor<10x3x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<6x3x10xf32>) -> tensor<10x3x6xf32>\n    return %0 : tensor<10x3x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          3,
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,6] b:f32[3,6]. let\n    c:f32[1,3,6] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 3, 6)\n      sharding=None\n    ] a\n    d:f32[1,3,6] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 3, 6)\n      sharding=None\n    ] b\n    e:f32[2,3,6] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x6xf32>, %arg1: tensor<3x6xf32>) -> (tensor<2x3x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<3x6xf32>) -> tensor<1x3x6xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<3x6xf32>) -> tensor<1x3x6xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x3x6xf32>, tensor<1x3x6xf32>) -> tensor<2x3x6xf32>\n    return %2 : tensor<2x3x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          3,
          4,
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3,
          4,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,4,8] b:f32[3,4,8]. let\n    c:f32[6,4,8] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x4x8xf32>, %arg1: tensor<3x4x8xf32>) -> (tensor<6x4x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<3x4x8xf32>, tensor<3x4x8xf32>) -> tensor<6x4x8xf32>\n    return %0 : tensor<6x4x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (5,))",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>) -> (tensor<5xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          8,
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,7] b:f32[8,7]. let\n    c:f32[16,7] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x7xf32>, %arg1: tensor<8x7xf32>) -> (tensor<16x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<8x7xf32>, tensor<8x7xf32>) -> tensor<16x7xf32>\n    return %0 : tensor<16x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          10,
          6,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,6,6]. let\n    b:f32[9,6,6] = slice[\n      limit_indices=(10, 6, 6)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x6x6xf32>) -> (tensor<9x6x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:10, 0:6, 0:6] : (tensor<10x6x6xf32>) -> tensor<9x6x6xf32>\n    return %0 : tensor<9x6x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          8,
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,10] b:f32[8,10]. let\n    c:f32[1,8,10] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 8, 10)\n      sharding=None\n    ] a\n    d:f32[1,8,10] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 8, 10)\n      sharding=None\n    ] b\n    e:f32[2,8,10] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x10xf32>, %arg1: tensor<8x10xf32>) -> (tensor<2x8x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<8x10xf32>) -> tensor<1x8x10xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<8x10xf32>) -> tensor<1x8x10xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x8x10xf32>, tensor<1x8x10xf32>) -> tensor<2x8x10xf32>\n    return %2 : tensor<2x8x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          3,
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,4] b:f32[3,4]. let\n    c:f32[1,3,4] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 3, 4)\n      sharding=None\n    ] a\n    d:f32[1,3,4] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 3, 4)\n      sharding=None\n    ] b\n    e:f32[2,3,4] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x4xf32>, %arg1: tensor<3x4xf32>) -> (tensor<2x3x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<3x4xf32>) -> tensor<1x3x4xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<3x4xf32>) -> tensor<1x3x4xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x3x4xf32>, tensor<1x3x4xf32>) -> tensor<2x3x4xf32>\n    return %2 : tensor<2x3x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (5,))",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>) -> (tensor<5xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          3,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,5]. let\n    b:f32[5,3] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x5xf32>) -> (tensor<5x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<3x5xf32>) -> tensor<5x3xf32>\n    return %0 : tensor<5x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2]. let\n    b:f32[1] = slice[limit_indices=(2,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>) -> (tensor<1xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:2] : (tensor<2xf32>) -> tensor<1xf32>\n    return %0 : tensor<1xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          7,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,7]. let\n    b:f32[6,7] = slice[limit_indices=(7, 7) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x7xf32>) -> (tensor<6x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:7, 0:7] : (tensor<7x7xf32>) -> tensor<6x7xf32>\n    return %0 : tensor<6x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2]. let\n    b:f32[1] = slice[limit_indices=(2,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>) -> (tensor<1xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:2] : (tensor<2xf32>) -> tensor<1xf32>\n    return %0 : tensor<1xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9] b:f32[9]. let\n    c:f32[18] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>, %arg1: tensor<9xf32>) -> (tensor<18xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<9xf32>, tensor<9xf32>) -> tensor<18xf32>\n    return %0 : tensor<18xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2] b:f32[2]. let\n    c:f32[4] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>, %arg1: tensor<2xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<2xf32>, tensor<2xf32>) -> tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8xf32>) -> (tensor<8xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (30,))",
    "input_info": [
      {
        "shape": [
          5,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,6]. let\n    b:f32[30] = reshape[dimensions=None new_sizes=(30,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x6xf32>) -> (tensor<30xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<5x6xf32>) -> tensor<30xf32>\n    return %0 : tensor<30xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 12))",
    "input_info": [
      {
        "shape": [
          3,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,8]. let\n    b:f32[2,12] = reshape[dimensions=None new_sizes=(2, 12) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x8xf32>) -> (tensor<2x12xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<3x8xf32>) -> tensor<2x12xf32>\n    return %0 : tensor<2x12xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          8,
          5,
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          5,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,5,10] b:f32[8,5,10]. let\n    c:f32[1,8,5,10] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 8, 5, 10)\n      sharding=None\n    ] a\n    d:f32[1,8,5,10] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 8, 5, 10)\n      sharding=None\n    ] b\n    e:f32[2,8,5,10] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x5x10xf32>, %arg1: tensor<8x5x10xf32>) -> (tensor<2x8x5x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<8x5x10xf32>) -> tensor<1x8x5x10xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<8x5x10xf32>) -> tensor<1x8x5x10xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x8x5x10xf32>, tensor<1x8x5x10xf32>) -> tensor<2x8x5x10xf32>\n    return %2 : tensor<2x8x5x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          3,
          6,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,6,2]. let\n    b:f32[2,6,2] = slice[\n      limit_indices=(3, 6, 2)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x6x2xf32>) -> (tensor<2x6x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:3, 0:6, 0:2] : (tensor<3x6x2xf32>) -> tensor<2x6x2xf32>\n    return %0 : tensor<2x6x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6] b:f32[6]. let\n    c:f32[1,6] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 6)\n      sharding=None\n    ] a\n    d:f32[1,6] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 6)\n      sharding=None\n    ] b\n    e:f32[2,6] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>, %arg1: tensor<6xf32>) -> (tensor<2x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<6xf32>) -> tensor<1x6xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<6xf32>) -> tensor<1x6xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x6xf32>, tensor<1x6xf32>) -> tensor<2x6xf32>\n    return %2 : tensor<2x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>) -> (tensor<3xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          10,
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,6] b:f32[10,6]. let\n    c:f32[20,6] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x6xf32>, %arg1: tensor<10x6xf32>) -> (tensor<20x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<10x6xf32>, tensor<10x6xf32>) -> tensor<20x6xf32>\n    return %0 : tensor<20x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6] b:f32[6]. let\n    c:f32[1,6] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 6)\n      sharding=None\n    ] a\n    d:f32[1,6] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 6)\n      sharding=None\n    ] b\n    e:f32[2,6] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>, %arg1: tensor<6xf32>) -> (tensor<2x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<6xf32>) -> tensor<1x6xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<6xf32>) -> tensor<1x6xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x6xf32>, tensor<1x6xf32>) -> tensor<2x6xf32>\n    return %2 : tensor<2x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 245))",
    "input_info": [
      {
        "shape": [
          10,
          7,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,7,7]. let\n    b:f32[2,245] = reshape[dimensions=None new_sizes=(2, 245) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x7x7xf32>) -> (tensor<2x245xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<10x7x7xf32>) -> tensor<2x245xf32>\n    return %0 : tensor<2x245xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          7,
          3,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,3,2]. let\n    b:f32[6,3,2] = slice[\n      limit_indices=(7, 3, 2)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x3x2xf32>) -> (tensor<6x3x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:7, 0:3, 0:2] : (tensor<7x3x2xf32>) -> tensor<6x3x2xf32>\n    return %0 : tensor<6x3x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 70))",
    "input_info": [
      {
        "shape": [
          5,
          7,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,7,4]. let\n    b:f32[2,70] = reshape[dimensions=None new_sizes=(2, 70) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x7x4xf32>) -> (tensor<2x70xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<5x7x4xf32>) -> tensor<2x70xf32>\n    return %0 : tensor<2x70xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3]. let\n    b:f32[2] = slice[limit_indices=(3,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>) -> (tensor<2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:3] : (tensor<3xf32>) -> tensor<2xf32>\n    return %0 : tensor<2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          6,
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,9] b:f32[6,9]. let\n    c:f32[12,9] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x9xf32>, %arg1: tensor<6x9xf32>) -> (tensor<12x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<6x9xf32>, tensor<6x9xf32>) -> tensor<12x9xf32>\n    return %0 : tensor<12x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          2,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,4]. let\n    b:f32[4,2] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x4xf32>) -> (tensor<4x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<2x4xf32>) -> tensor<4x2xf32>\n    return %0 : tensor<4x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6]. let\n    b:f32[5] = slice[limit_indices=(6,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>) -> (tensor<5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:6] : (tensor<6xf32>) -> tensor<5xf32>\n    return %0 : tensor<5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          7,
          2,
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7,
          2,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,2,10] b:f32[7,2,10]. let\n    c:f32[14,2,10] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x2x10xf32>, %arg1: tensor<7x2x10xf32>) -> (tensor<14x2x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<7x2x10xf32>, tensor<7x2x10xf32>) -> tensor<14x2x10xf32>\n    return %0 : tensor<14x2x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          8,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,4]. let\n    b:f32[7,4] = slice[limit_indices=(8, 4) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x4xf32>) -> (tensor<7x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:8, 0:4] : (tensor<8x4xf32>) -> tensor<7x4xf32>\n    return %0 : tensor<7x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (400,))",
    "input_info": [
      {
        "shape": [
          10,
          10,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,10,4]. let\n    b:f32[400] = reshape[dimensions=None new_sizes=(400,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x10x4xf32>) -> (tensor<400xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<10x10x4xf32>) -> tensor<400xf32>\n    return %0 : tensor<400xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          3,
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,4] b:f32[3,4]. let\n    c:f32[6,4] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x4xf32>, %arg1: tensor<3x4xf32>) -> (tensor<6x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<3x4xf32>, tensor<3x4xf32>) -> tensor<6x4xf32>\n    return %0 : tensor<6x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>) -> (tensor<2xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (72,))",
    "input_info": [
      {
        "shape": [
          8,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,9]. let\n    b:f32[72] = reshape[dimensions=None new_sizes=(72,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x9xf32>) -> (tensor<72xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<8x9xf32>) -> tensor<72xf32>\n    return %0 : tensor<72xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          6,
          4,
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6,
          4,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,4,7] b:f32[6,4,7]. let\n    c:f32[12,4,7] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x4x7xf32>, %arg1: tensor<6x4x7xf32>) -> (tensor<12x4x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<6x4x7xf32>, tensor<6x4x7xf32>) -> tensor<12x4x7xf32>\n    return %0 : tensor<12x4x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5] b:f32[5]. let\n    c:f32[10] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>, %arg1: tensor<5xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<5xf32>, tensor<5xf32>) -> tensor<10xf32>\n    return %0 : tensor<10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          2,
          7,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,7,4]. let\n    b:f32[4,7,2] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x7x4xf32>) -> (tensor<4x7x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<2x7x4xf32>) -> tensor<4x7x2xf32>\n    return %0 : tensor<4x7x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (320,))",
    "input_info": [
      {
        "shape": [
          10,
          8,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,8,4]. let\n    b:f32[320] = reshape[dimensions=None new_sizes=(320,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x8x4xf32>) -> (tensor<320xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<10x8x4xf32>) -> tensor<320xf32>\n    return %0 : tensor<320xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4] b:f32[4]. let\n    c:f32[1,4] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 4)\n      sharding=None\n    ] a\n    d:f32[1,4] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 4)\n      sharding=None\n    ] b\n    e:f32[2,4] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> (tensor<2x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<4xf32>) -> tensor<1x4xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<4xf32>) -> tensor<1x4xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x4xf32>, tensor<1x4xf32>) -> tensor<2x4xf32>\n    return %2 : tensor<2x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          3,
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,7] b:f32[3,7]. let\n    c:f32[1,3,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 3, 7)\n      sharding=None\n    ] a\n    d:f32[1,3,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 3, 7)\n      sharding=None\n    ] b\n    e:f32[2,3,7] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x7xf32>, %arg1: tensor<3x7xf32>) -> (tensor<2x3x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<3x7xf32>) -> tensor<1x3x7xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<3x7xf32>) -> tensor<1x3x7xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x3x7xf32>, tensor<1x3x7xf32>) -> tensor<2x3x7xf32>\n    return %2 : tensor<2x3x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          6,
          3,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,3,10]. let\n    b:f32[10,3,6] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x3x10xf32>) -> (tensor<10x3x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<6x3x10xf32>) -> tensor<10x3x6xf32>\n    return %0 : tensor<10x3x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7] b:f32[7]. let\n    c:f32[14] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>, %arg1: tensor<7xf32>) -> (tensor<14xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<7xf32>, tensor<7xf32>) -> tensor<14xf32>\n    return %0 : tensor<14xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3] b:f32[3]. let\n    c:f32[6] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>, %arg1: tensor<3xf32>) -> (tensor<6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<3xf32>, tensor<3xf32>) -> tensor<6xf32>\n    return %0 : tensor<6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>) -> (tensor<6xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4]. let\n    b:f32[3] = slice[limit_indices=(4,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:4] : (tensor<4xf32>) -> tensor<3xf32>\n    return %0 : tensor<3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>) -> (tensor<6xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6] b:f32[6]. let\n    c:f32[12] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>, %arg1: tensor<6xf32>) -> (tensor<12xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<6xf32>, tensor<6xf32>) -> tensor<12xf32>\n    return %0 : tensor<12xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 40))",
    "input_info": [
      {
        "shape": [
          10,
          4,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,4,2]. let\n    b:f32[2,40] = reshape[dimensions=None new_sizes=(2, 40) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x4x2xf32>) -> (tensor<2x40xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<10x4x2xf32>) -> tensor<2x40xf32>\n    return %0 : tensor<2x40xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10] b:f32[10]. let\n    c:f32[20] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>, %arg1: tensor<10xf32>) -> (tensor<20xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<10xf32>, tensor<10xf32>) -> tensor<20xf32>\n    return %0 : tensor<20xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          10,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,8]. let\n    b:f32[9,8] = slice[limit_indices=(10, 8) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x8xf32>) -> (tensor<9x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:10, 0:8] : (tensor<10x8xf32>) -> tensor<9x8xf32>\n    return %0 : tensor<9x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 320))",
    "input_info": [
      {
        "shape": [
          8,
          10,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,10,8]. let\n    b:f32[2,320] = reshape[dimensions=None new_sizes=(2, 320) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x10x8xf32>) -> (tensor<2x320xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<8x10x8xf32>) -> tensor<2x320xf32>\n    return %0 : tensor<2x320xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          3,
          5,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,5,7]. let\n    b:f32[2,5,7] = slice[\n      limit_indices=(3, 5, 7)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x5x7xf32>) -> (tensor<2x5x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:3, 0:5, 0:7] : (tensor<3x5x7xf32>) -> tensor<2x5x7xf32>\n    return %0 : tensor<2x5x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          7,
          3,
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7,
          3,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,3,7] b:f32[7,3,7]. let\n    c:f32[1,7,3,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 7, 3, 7)\n      sharding=None\n    ] a\n    d:f32[1,7,3,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 7, 3, 7)\n      sharding=None\n    ] b\n    e:f32[2,7,3,7] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x3x7xf32>, %arg1: tensor<7x3x7xf32>) -> (tensor<2x7x3x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<7x3x7xf32>) -> tensor<1x7x3x7xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<7x3x7xf32>) -> tensor<1x7x3x7xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x7x3x7xf32>, tensor<1x7x3x7xf32>) -> tensor<2x7x3x7xf32>\n    return %2 : tensor<2x7x3x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          7,
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,6] b:f32[7,6]. let\n    c:f32[14,6] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x6xf32>, %arg1: tensor<7x6xf32>) -> (tensor<14x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<7x6xf32>, tensor<7x6xf32>) -> tensor<14x6xf32>\n    return %0 : tensor<14x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          3,
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,9] b:f32[3,9]. let\n    c:f32[6,9] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x9xf32>, %arg1: tensor<3x9xf32>) -> (tensor<6x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<3x9xf32>, tensor<3x9xf32>) -> tensor<6x9xf32>\n    return %0 : tensor<6x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (5,))",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>) -> (tensor<5xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          8,
          8,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,8,3]. let\n    b:f32[7,8,3] = slice[\n      limit_indices=(8, 8, 3)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x8x3xf32>) -> (tensor<7x8x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:8, 0:8, 0:3] : (tensor<8x8x3xf32>) -> tensor<7x8x3xf32>\n    return %0 : tensor<7x8x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 14))",
    "input_info": [
      {
        "shape": [
          7,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,4]. let\n    b:f32[2,14] = reshape[dimensions=None new_sizes=(2, 14) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x4xf32>) -> (tensor<2x14xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<7x4xf32>) -> tensor<2x14xf32>\n    return %0 : tensor<2x14xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (5,))",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>) -> (tensor<5xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          6,
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,4] b:f32[6,4]. let\n    c:f32[1,6,4] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 6, 4)\n      sharding=None\n    ] a\n    d:f32[1,6,4] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 6, 4)\n      sharding=None\n    ] b\n    e:f32[2,6,4] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x4xf32>, %arg1: tensor<6x4xf32>) -> (tensor<2x6x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<6x4xf32>) -> tensor<1x6x4xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<6x4xf32>) -> tensor<1x6x4xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x6x4xf32>, tensor<1x6x4xf32>) -> tensor<2x6x4xf32>\n    return %2 : tensor<2x6x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          2,
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,2] b:f32[2,2]. let\n    c:f32[4,2] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x2xf32>, %arg1: tensor<2x2xf32>) -> (tensor<4x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<2x2xf32>, tensor<2x2xf32>) -> tensor<4x2xf32>\n    return %0 : tensor<4x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          4,
          8,
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4,
          8,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,8,5] b:f32[4,8,5]. let\n    c:f32[1,4,8,5] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 4, 8, 5)\n      sharding=None\n    ] a\n    d:f32[1,4,8,5] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 4, 8, 5)\n      sharding=None\n    ] b\n    e:f32[2,4,8,5] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x8x5xf32>, %arg1: tensor<4x8x5xf32>) -> (tensor<2x4x8x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<4x8x5xf32>) -> tensor<1x4x8x5xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<4x8x5xf32>) -> tensor<1x4x8x5xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x4x8x5xf32>, tensor<1x4x8x5xf32>) -> tensor<2x4x8x5xf32>\n    return %2 : tensor<2x4x8x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          9,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,4]. let\n    b:f32[8,4] = slice[limit_indices=(9, 4) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x4xf32>) -> (tensor<8x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:9, 0:4] : (tensor<9x4xf32>) -> tensor<8x4xf32>\n    return %0 : tensor<8x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          9,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,8]. let\n    b:f32[8,9] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x8xf32>) -> (tensor<8x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<9x8xf32>) -> tensor<8x9xf32>\n    return %0 : tensor<8x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          10,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,6]. let\n    b:f32[6,10] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x6xf32>) -> (tensor<6x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<10x6xf32>) -> tensor<6x10xf32>\n    return %0 : tensor<6x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          4,
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,8] b:f32[4,8]. let\n    c:f32[8,8] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x8xf32>, %arg1: tensor<4x8xf32>) -> (tensor<8x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<4x8xf32>, tensor<4x8xf32>) -> tensor<8x8xf32>\n    return %0 : tensor<8x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10] b:f32[10]. let\n    c:f32[20] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>, %arg1: tensor<10xf32>) -> (tensor<20xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<10xf32>, tensor<10xf32>) -> tensor<20xf32>\n    return %0 : tensor<20xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          8,
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,4] b:f32[8,4]. let\n    c:f32[16,4] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x4xf32>, %arg1: tensor<8x4xf32>) -> (tensor<16x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<8x4xf32>, tensor<8x4xf32>) -> tensor<16x4xf32>\n    return %0 : tensor<16x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8xf32>) -> (tensor<8xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          2,
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,6] b:f32[2,6]. let\n    c:f32[1,2,6] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 2, 6)\n      sharding=None\n    ] a\n    d:f32[1,2,6] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 2, 6)\n      sharding=None\n    ] b\n    e:f32[2,2,6] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x6xf32>, %arg1: tensor<2x6xf32>) -> (tensor<2x2x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<2x6xf32>) -> tensor<1x2x6xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<2x6xf32>) -> tensor<1x2x6xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x2x6xf32>, tensor<1x2x6xf32>) -> tensor<2x2x6xf32>\n    return %2 : tensor<2x2x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          4,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,8]. let\n    b:f32[8,4] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x8xf32>) -> (tensor<8x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<4x8xf32>) -> tensor<8x4xf32>\n    return %0 : tensor<8x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          9,
          8,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,8,7]. let\n    b:f32[8,8,7] = slice[\n      limit_indices=(9, 8, 7)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x8x7xf32>) -> (tensor<8x8x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:9, 0:8, 0:7] : (tensor<9x8x7xf32>) -> tensor<8x8x7xf32>\n    return %0 : tensor<8x8x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          10,
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,2] b:f32[10,2]. let\n    c:f32[1,10,2] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 10, 2)\n      sharding=None\n    ] a\n    d:f32[1,10,2] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 10, 2)\n      sharding=None\n    ] b\n    e:f32[2,10,2] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x2xf32>, %arg1: tensor<10x2xf32>) -> (tensor<2x10x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<10x2xf32>) -> tensor<1x10x2xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<10x2xf32>) -> tensor<1x10x2xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x10x2xf32>, tensor<1x10x2xf32>) -> tensor<2x10x2xf32>\n    return %2 : tensor<2x10x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          2,
          10,
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2,
          10,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,10,2] b:f32[2,10,2]. let\n    c:f32[4,10,2] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x10x2xf32>, %arg1: tensor<2x10x2xf32>) -> (tensor<4x10x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<2x10x2xf32>, tensor<2x10x2xf32>) -> tensor<4x10x2xf32>\n    return %0 : tensor<4x10x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          9,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,6]. let\n    b:f32[6,9] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x6xf32>) -> (tensor<6x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<9x6xf32>) -> tensor<6x9xf32>\n    return %0 : tensor<6x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4]. let\n    b:f32[3] = slice[limit_indices=(4,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:4] : (tensor<4xf32>) -> tensor<3xf32>\n    return %0 : tensor<3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          6,
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,4] b:f32[6,4]. let\n    c:f32[12,4] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x4xf32>, %arg1: tensor<6x4xf32>) -> (tensor<12x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<6x4xf32>, tensor<6x4xf32>) -> tensor<12x4xf32>\n    return %0 : tensor<12x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9] b:f32[9]. let\n    c:f32[18] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>, %arg1: tensor<9xf32>) -> (tensor<18xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<9xf32>, tensor<9xf32>) -> tensor<18xf32>\n    return %0 : tensor<18xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          5,
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,5] b:f32[5,5]. let\n    c:f32[1,5,5] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 5, 5)\n      sharding=None\n    ] a\n    d:f32[1,5,5] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 5, 5)\n      sharding=None\n    ] b\n    e:f32[2,5,5] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x5xf32>, %arg1: tensor<5x5xf32>) -> (tensor<2x5x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<5x5xf32>) -> tensor<1x5x5xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<5x5xf32>) -> tensor<1x5x5xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x5x5xf32>, tensor<1x5x5xf32>) -> tensor<2x5x5xf32>\n    return %2 : tensor<2x5x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>) -> (tensor<3xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          4,
          9,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,9,10]. let\n    b:f32[3,9,10] = slice[\n      limit_indices=(4, 9, 10)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x9x10xf32>) -> (tensor<3x9x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:4, 0:9, 0:10] : (tensor<4x9x10xf32>) -> tensor<3x9x10xf32>\n    return %0 : tensor<3x9x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          4,
          5,
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4,
          5,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,5,7] b:f32[4,5,7]. let\n    c:f32[1,4,5,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 4, 5, 7)\n      sharding=None\n    ] a\n    d:f32[1,4,5,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 4, 5, 7)\n      sharding=None\n    ] b\n    e:f32[2,4,5,7] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x5x7xf32>, %arg1: tensor<4x5x7xf32>) -> (tensor<2x4x5x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<4x5x7xf32>) -> tensor<1x4x5x7xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<4x5x7xf32>) -> tensor<1x4x5x7xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x4x5x7xf32>, tensor<1x4x5x7xf32>) -> tensor<2x4x5x7xf32>\n    return %2 : tensor<2x4x5x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 16))",
    "input_info": [
      {
        "shape": [
          8,
          2,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,2,2]. let\n    b:f32[2,16] = reshape[dimensions=None new_sizes=(2, 16) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x2x2xf32>) -> (tensor<2x16xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<8x2x2xf32>) -> tensor<2x16xf32>\n    return %0 : tensor<2x16xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (24,))",
    "input_info": [
      {
        "shape": [
          4,
          3,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,3,2]. let\n    b:f32[24] = reshape[dimensions=None new_sizes=(24,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x3x2xf32>) -> (tensor<24xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<4x3x2xf32>) -> tensor<24xf32>\n    return %0 : tensor<24xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          9,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,4]. let\n    b:f32[8,4] = slice[limit_indices=(9, 4) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x4xf32>) -> (tensor<8x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:9, 0:4] : (tensor<9x4xf32>) -> tensor<8x4xf32>\n    return %0 : tensor<8x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5] b:f32[5]. let\n    c:f32[1,5] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 5)\n      sharding=None\n    ] a\n    d:f32[1,5] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 5)\n      sharding=None\n    ] b\n    e:f32[2,5] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>, %arg1: tensor<5xf32>) -> (tensor<2x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<5xf32>) -> tensor<1x5xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<5xf32>) -> tensor<1x5xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x5xf32>, tensor<1x5xf32>) -> tensor<2x5xf32>\n    return %2 : tensor<2x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          8,
          2,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,2,8]. let\n    b:f32[8,2,8] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x2x8xf32>) -> (tensor<8x2x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<8x2x8xf32>) -> tensor<8x2x8xf32>\n    return %0 : tensor<8x2x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 180))",
    "input_info": [
      {
        "shape": [
          9,
          8,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,8,5]. let\n    b:f32[2,180] = reshape[dimensions=None new_sizes=(2, 180) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x8x5xf32>) -> (tensor<2x180xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<9x8x5xf32>) -> tensor<2x180xf32>\n    return %0 : tensor<2x180xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          8,
          6,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,6,9]. let\n    b:f32[9,6,8] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x6x9xf32>) -> (tensor<9x6x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<8x6x9xf32>) -> tensor<9x6x8xf32>\n    return %0 : tensor<9x6x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          7,
          4,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,4,7]. let\n    b:f32[7,4,7] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x4x7xf32>) -> (tensor<7x4x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<7x4x7xf32>) -> tensor<7x4x7xf32>\n    return %0 : tensor<7x4x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          9,
          9,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,9,2]. let\n    b:f32[2,9,9] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x9x2xf32>) -> (tensor<2x9x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<9x9x2xf32>) -> tensor<2x9x9xf32>\n    return %0 : tensor<2x9x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          6,
          3,
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6,
          3,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,3,4] b:f32[6,3,4]. let\n    c:f32[1,6,3,4] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 6, 3, 4)\n      sharding=None\n    ] a\n    d:f32[1,6,3,4] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 6, 3, 4)\n      sharding=None\n    ] b\n    e:f32[2,6,3,4] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x3x4xf32>, %arg1: tensor<6x3x4xf32>) -> (tensor<2x6x3x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<6x3x4xf32>) -> tensor<1x6x3x4xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<6x3x4xf32>) -> tensor<1x6x3x4xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x6x3x4xf32>, tensor<1x6x3x4xf32>) -> tensor<2x6x3x4xf32>\n    return %2 : tensor<2x6x3x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8] b:f32[8]. let\n    c:f32[16] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8xf32>, %arg1: tensor<8xf32>) -> (tensor<16xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<8xf32>, tensor<8xf32>) -> tensor<16xf32>\n    return %0 : tensor<16xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          2,
          3,
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2,
          3,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,3,9] b:f32[2,3,9]. let\n    c:f32[4,3,9] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x3x9xf32>, %arg1: tensor<2x3x9xf32>) -> (tensor<4x3x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<2x3x9xf32>, tensor<2x3x9xf32>) -> tensor<4x3x9xf32>\n    return %0 : tensor<4x3x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          4,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,3]. let\n    b:f32[3,4] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x3xf32>) -> (tensor<3x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<4x3xf32>) -> tensor<3x4xf32>\n    return %0 : tensor<3x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          9,
          3,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,3,4]. let\n    b:f32[8,3,4] = slice[\n      limit_indices=(9, 3, 4)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x3x4xf32>) -> (tensor<8x3x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:9, 0:3, 0:4] : (tensor<9x3x4xf32>) -> tensor<8x3x4xf32>\n    return %0 : tensor<8x3x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          10,
          10,
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10,
          10,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,10,3] b:f32[10,10,3]. let\n    c:f32[20,10,3] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x10x3xf32>, %arg1: tensor<10x10x3xf32>) -> (tensor<20x10x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<10x10x3xf32>, tensor<10x10x3xf32>) -> tensor<20x10x3xf32>\n    return %0 : tensor<20x10x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          3,
          8,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,8,4]. let\n    b:f32[4,8,3] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x8x4xf32>) -> (tensor<4x8x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<3x8x4xf32>) -> tensor<4x8x3xf32>\n    return %0 : tensor<4x8x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 20))",
    "input_info": [
      {
        "shape": [
          5,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,8]. let\n    b:f32[2,20] = reshape[dimensions=None new_sizes=(2, 20) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x8xf32>) -> (tensor<2x20xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<5x8xf32>) -> tensor<2x20xf32>\n    return %0 : tensor<2x20xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5] b:f32[5]. let\n    c:f32[10] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>, %arg1: tensor<5xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<5xf32>, tensor<5xf32>) -> tensor<10xf32>\n    return %0 : tensor<10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          8,
          5,
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          5,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,5,3] b:f32[8,5,3]. let\n    c:f32[16,5,3] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x5x3xf32>, %arg1: tensor<8x5x3xf32>) -> (tensor<16x5x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<8x5x3xf32>, tensor<8x5x3xf32>) -> tensor<16x5x3xf32>\n    return %0 : tensor<16x5x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 25))",
    "input_info": [
      {
        "shape": [
          5,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,10]. let\n    b:f32[2,25] = reshape[dimensions=None new_sizes=(2, 25) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x10xf32>) -> (tensor<2x25xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<5x10xf32>) -> tensor<2x25xf32>\n    return %0 : tensor<2x25xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          9,
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,8] b:f32[9,8]. let\n    c:f32[18,8] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x8xf32>, %arg1: tensor<9x8xf32>) -> (tensor<18x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<9x8xf32>, tensor<9x8xf32>) -> tensor<18x8xf32>\n    return %0 : tensor<18x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          2,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,9]. let\n    b:f32[1,9] = slice[limit_indices=(2, 9) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x9xf32>) -> (tensor<1x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:2, 0:9] : (tensor<2x9xf32>) -> tensor<1x9xf32>\n    return %0 : tensor<1x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          4,
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,5] b:f32[4,5]. let\n    c:f32[8,5] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x5xf32>, %arg1: tensor<4x5xf32>) -> (tensor<8x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<4x5xf32>, tensor<4x5xf32>) -> tensor<8x5xf32>\n    return %0 : tensor<8x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          6,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,4]. let\n    b:f32[5,4] = slice[limit_indices=(6, 4) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x4xf32>) -> (tensor<5x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:6, 0:4] : (tensor<6x4xf32>) -> tensor<5x4xf32>\n    return %0 : tensor<5x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          4,
          2,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,2,4]. let\n    b:f32[3,2,4] = slice[\n      limit_indices=(4, 2, 4)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x2x4xf32>) -> (tensor<3x2x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:4, 0:2, 0:4] : (tensor<4x2x4xf32>) -> tensor<3x2x4xf32>\n    return %0 : tensor<3x2x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (5,))",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>) -> (tensor<5xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          10,
          7,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,7,9]. let\n    b:f32[9,7,10] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x7x9xf32>) -> (tensor<9x7x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<10x7x9xf32>) -> tensor<9x7x10xf32>\n    return %0 : tensor<9x7x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          3,
          8,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,8,9]. let\n    b:f32[9,8,3] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x8x9xf32>) -> (tensor<9x8x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<3x8x9xf32>) -> tensor<9x8x3xf32>\n    return %0 : tensor<9x8x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3] b:f32[3]. let\n    c:f32[1,3] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 3)\n      sharding=None\n    ] a\n    d:f32[1,3] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 3)\n      sharding=None\n    ] b\n    e:f32[2,3] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>, %arg1: tensor<3xf32>) -> (tensor<2x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<3xf32>) -> tensor<1x3xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<3xf32>) -> tensor<1x3xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x3xf32>, tensor<1x3xf32>) -> tensor<2x3xf32>\n    return %2 : tensor<2x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          8,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,10]. let\n    b:f32[10,8] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x10xf32>) -> (tensor<10x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<8x10xf32>) -> tensor<10x8xf32>\n    return %0 : tensor<10x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          4,
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,10] b:f32[4,10]. let\n    c:f32[1,4,10] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 4, 10)\n      sharding=None\n    ] a\n    d:f32[1,4,10] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 4, 10)\n      sharding=None\n    ] b\n    e:f32[2,4,10] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x10xf32>, %arg1: tensor<4x10xf32>) -> (tensor<2x4x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<4x10xf32>) -> tensor<1x4x10xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<4x10xf32>) -> tensor<1x4x10xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x4x10xf32>, tensor<1x4x10xf32>) -> tensor<2x4x10xf32>\n    return %2 : tensor<2x4x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (80,))",
    "input_info": [
      {
        "shape": [
          2,
          8,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,8,5]. let\n    b:f32[80] = reshape[dimensions=None new_sizes=(80,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x8x5xf32>) -> (tensor<80xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<2x8x5xf32>) -> tensor<80xf32>\n    return %0 : tensor<80xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          7,
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,6] b:f32[7,6]. let\n    c:f32[14,6] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x6xf32>, %arg1: tensor<7x6xf32>) -> (tensor<14x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<7x6xf32>, tensor<7x6xf32>) -> tensor<14x6xf32>\n    return %0 : tensor<14x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          7,
          6,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,6,10]. let\n    b:f32[6,6,10] = slice[\n      limit_indices=(7, 6, 10)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x6x10xf32>) -> (tensor<6x6x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:7, 0:6, 0:10] : (tensor<7x6x10xf32>) -> tensor<6x6x10xf32>\n    return %0 : tensor<6x6x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          2,
          3,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,3,6]. let\n    b:f32[6,3,2] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x3x6xf32>) -> (tensor<6x3x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<2x3x6xf32>) -> tensor<6x3x2xf32>\n    return %0 : tensor<6x3x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7]. let\n    b:f32[6] = slice[limit_indices=(7,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>) -> (tensor<6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:7] : (tensor<7xf32>) -> tensor<6xf32>\n    return %0 : tensor<6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 6))",
    "input_info": [
      {
        "shape": [
          3,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,4]. let\n    b:f32[2,6] = reshape[dimensions=None new_sizes=(2, 6) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x4xf32>) -> (tensor<2x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<3x4xf32>) -> tensor<2x6xf32>\n    return %0 : tensor<2x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          2,
          7,
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2,
          7,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,7,6] b:f32[2,7,6]. let\n    c:f32[1,2,7,6] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 2, 7, 6)\n      sharding=None\n    ] a\n    d:f32[1,2,7,6] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 2, 7, 6)\n      sharding=None\n    ] b\n    e:f32[2,2,7,6] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x7x6xf32>, %arg1: tensor<2x7x6xf32>) -> (tensor<2x2x7x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<2x7x6xf32>) -> tensor<1x2x7x6xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<2x7x6xf32>) -> tensor<1x2x7x6xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x2x7x6xf32>, tensor<1x2x7x6xf32>) -> tensor<2x2x7x6xf32>\n    return %2 : tensor<2x2x7x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          4,
          9,
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4,
          9,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,9,2] b:f32[4,9,2]. let\n    c:f32[8,9,2] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x9x2xf32>, %arg1: tensor<4x9x2xf32>) -> (tensor<8x9x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<4x9x2xf32>, tensor<4x9x2xf32>) -> tensor<8x9x2xf32>\n    return %0 : tensor<8x9x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (3,))",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>) -> (tensor<3xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (3,))",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>) -> (tensor<3xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          3,
          6,
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3,
          6,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,6,5] b:f32[3,6,5]. let\n    c:f32[1,3,6,5] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 3, 6, 5)\n      sharding=None\n    ] a\n    d:f32[1,3,6,5] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 3, 6, 5)\n      sharding=None\n    ] b\n    e:f32[2,3,6,5] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x6x5xf32>, %arg1: tensor<3x6x5xf32>) -> (tensor<2x3x6x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<3x6x5xf32>) -> tensor<1x3x6x5xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<3x6x5xf32>) -> tensor<1x3x6x5xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x3x6x5xf32>, tensor<1x3x6x5xf32>) -> tensor<2x3x6x5xf32>\n    return %2 : tensor<2x3x6x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          3,
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,3] b:f32[3,3]. let\n    c:f32[6,3] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x3xf32>, %arg1: tensor<3x3xf32>) -> (tensor<6x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<3x3xf32>, tensor<3x3xf32>) -> tensor<6x3xf32>\n    return %0 : tensor<6x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          10,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,10]. let\n    b:f32[10,10] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x10xf32>) -> (tensor<10x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<10x10xf32>) -> tensor<10x10xf32>\n    return %0 : tensor<10x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          9,
          3,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,3,4]. let\n    b:f32[8,3,4] = slice[\n      limit_indices=(9, 3, 4)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x3x4xf32>) -> (tensor<8x3x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:9, 0:3, 0:4] : (tensor<9x3x4xf32>) -> tensor<8x3x4xf32>\n    return %0 : tensor<8x3x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          5,
          8,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,8,2]. let\n    b:f32[2,8,5] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x8x2xf32>) -> (tensor<2x8x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<5x8x2xf32>) -> tensor<2x8x5xf32>\n    return %0 : tensor<2x8x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3] b:f32[3]. let\n    c:f32[6] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>, %arg1: tensor<3xf32>) -> (tensor<6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<3xf32>, tensor<3xf32>) -> tensor<6xf32>\n    return %0 : tensor<6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          10,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,2]. let\n    b:f32[9,2] = slice[limit_indices=(10, 2) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x2xf32>) -> (tensor<9x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:10, 0:2] : (tensor<10x2xf32>) -> tensor<9x2xf32>\n    return %0 : tensor<9x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5] b:f32[5]. let\n    c:f32[1,5] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 5)\n      sharding=None\n    ] a\n    d:f32[1,5] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 5)\n      sharding=None\n    ] b\n    e:f32[2,5] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>, %arg1: tensor<5xf32>) -> (tensor<2x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<5xf32>) -> tensor<1x5xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<5xf32>) -> tensor<1x5xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x5xf32>, tensor<1x5xf32>) -> tensor<2x5xf32>\n    return %2 : tensor<2x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (9,))",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>) -> (tensor<9xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          9,
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,10] b:f32[9,10]. let\n    c:f32[1,9,10] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 9, 10)\n      sharding=None\n    ] a\n    d:f32[1,9,10] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 9, 10)\n      sharding=None\n    ] b\n    e:f32[2,9,10] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x10xf32>, %arg1: tensor<9x10xf32>) -> (tensor<2x9x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<9x10xf32>) -> tensor<1x9x10xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<9x10xf32>) -> tensor<1x9x10xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x9x10xf32>, tensor<1x9x10xf32>) -> tensor<2x9x10xf32>\n    return %2 : tensor<2x9x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          6,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,10]. let\n    b:f32[10,6] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x10xf32>) -> (tensor<10x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<6x10xf32>) -> tensor<10x6xf32>\n    return %0 : tensor<10x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          3,
          2,
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3,
          2,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,2,5] b:f32[3,2,5]. let\n    c:f32[6,2,5] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x2x5xf32>, %arg1: tensor<3x2x5xf32>) -> (tensor<6x2x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<3x2x5xf32>, tensor<3x2x5xf32>) -> tensor<6x2x5xf32>\n    return %0 : tensor<6x2x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          8,
          8,
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          8,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,8,6] b:f32[8,8,6]. let\n    c:f32[16,8,6] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x8x6xf32>, %arg1: tensor<8x8x6xf32>) -> (tensor<16x8x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<8x8x6xf32>, tensor<8x8x6xf32>) -> tensor<16x8x6xf32>\n    return %0 : tensor<16x8x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          2,
          5,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,5,5]. let\n    b:f32[1,5,5] = slice[\n      limit_indices=(2, 5, 5)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x5x5xf32>) -> (tensor<1x5x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:2, 0:5, 0:5] : (tensor<2x5x5xf32>) -> tensor<1x5x5xf32>\n    return %0 : tensor<1x5x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 20))",
    "input_info": [
      {
        "shape": [
          5,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,8]. let\n    b:f32[2,20] = reshape[dimensions=None new_sizes=(2, 20) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x8xf32>) -> (tensor<2x20xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<5x8xf32>) -> tensor<2x20xf32>\n    return %0 : tensor<2x20xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (3,))",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>) -> (tensor<3xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>) -> (tensor<9xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          10,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,8]. let\n    b:f32[8,10] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x8xf32>) -> (tensor<8x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<10x8xf32>) -> tensor<8x10xf32>\n    return %0 : tensor<8x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          6,
          3,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,3,10]. let\n    b:f32[5,3,10] = slice[\n      limit_indices=(6, 3, 10)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x3x10xf32>) -> (tensor<5x3x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:6, 0:3, 0:10] : (tensor<6x3x10xf32>) -> tensor<5x3x10xf32>\n    return %0 : tensor<5x3x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          7,
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,8] b:f32[7,8]. let\n    c:f32[14,8] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x8xf32>, %arg1: tensor<7x8xf32>) -> (tensor<14x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<7x8xf32>, tensor<7x8xf32>) -> tensor<14x8xf32>\n    return %0 : tensor<14x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          9,
          7,
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9,
          7,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,7,8] b:f32[9,7,8]. let\n    c:f32[1,9,7,8] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 9, 7, 8)\n      sharding=None\n    ] a\n    d:f32[1,9,7,8] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 9, 7, 8)\n      sharding=None\n    ] b\n    e:f32[2,9,7,8] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x7x8xf32>, %arg1: tensor<9x7x8xf32>) -> (tensor<2x9x7x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<9x7x8xf32>) -> tensor<1x9x7x8xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<9x7x8xf32>) -> tensor<1x9x7x8xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x9x7x8xf32>, tensor<1x9x7x8xf32>) -> tensor<2x9x7x8xf32>\n    return %2 : tensor<2x9x7x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10]. let\n    b:f32[9] = slice[limit_indices=(10,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:10] : (tensor<10xf32>) -> tensor<9xf32>\n    return %0 : tensor<9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (12,))",
    "input_info": [
      {
        "shape": [
          2,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,6]. let\n    b:f32[12] = reshape[dimensions=None new_sizes=(12,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x6xf32>) -> (tensor<12xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<2x6xf32>) -> tensor<12xf32>\n    return %0 : tensor<12xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (5,))",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>) -> (tensor<5xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (7,))",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>) -> (tensor<7xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>) -> (tensor<9xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7] b:f32[7]. let\n    c:f32[14] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>, %arg1: tensor<7xf32>) -> (tensor<14xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<7xf32>, tensor<7xf32>) -> tensor<14xf32>\n    return %0 : tensor<14xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (3,))",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>) -> (tensor<3xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9]. let\n    b:f32[8] = slice[limit_indices=(9,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>) -> (tensor<8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:9] : (tensor<9xf32>) -> tensor<8xf32>\n    return %0 : tensor<8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          5,
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,3] b:f32[5,3]. let\n    c:f32[1,5,3] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 5, 3)\n      sharding=None\n    ] a\n    d:f32[1,5,3] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 5, 3)\n      sharding=None\n    ] b\n    e:f32[2,5,3] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x3xf32>, %arg1: tensor<5x3xf32>) -> (tensor<2x5x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<5x3xf32>) -> tensor<1x5x3xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<5x3xf32>) -> tensor<1x5x3xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x5x3xf32>, tensor<1x5x3xf32>) -> tensor<2x5x3xf32>\n    return %2 : tensor<2x5x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (7,))",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>) -> (tensor<7xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (90,))",
    "input_info": [
      {
        "shape": [
          10,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,9]. let\n    b:f32[90] = reshape[dimensions=None new_sizes=(90,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x9xf32>) -> (tensor<90xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<10x9xf32>) -> tensor<90xf32>\n    return %0 : tensor<90xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          8,
          8,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,8,5]. let\n    b:f32[7,8,5] = slice[\n      limit_indices=(8, 8, 5)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x8x5xf32>) -> (tensor<7x8x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:8, 0:8, 0:5] : (tensor<8x8x5xf32>) -> tensor<7x8x5xf32>\n    return %0 : tensor<7x8x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          8,
          3,
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          3,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,3,10] b:f32[8,3,10]. let\n    c:f32[16,3,10] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x3x10xf32>, %arg1: tensor<8x3x10xf32>) -> (tensor<16x3x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<8x3x10xf32>, tensor<8x3x10xf32>) -> tensor<16x3x10xf32>\n    return %0 : tensor<16x3x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          5,
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,3] b:f32[5,3]. let\n    c:f32[1,5,3] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 5, 3)\n      sharding=None\n    ] a\n    d:f32[1,5,3] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 5, 3)\n      sharding=None\n    ] b\n    e:f32[2,5,3] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x3xf32>, %arg1: tensor<5x3xf32>) -> (tensor<2x5x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<5x3xf32>) -> tensor<1x5x3xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<5x3xf32>) -> tensor<1x5x3xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x5x3xf32>, tensor<1x5x3xf32>) -> tensor<2x5x3xf32>\n    return %2 : tensor<2x5x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 5))",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10]. let\n    b:f32[2,5] = reshape[dimensions=None new_sizes=(2, 5) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<2x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<10xf32>) -> tensor<2x5xf32>\n    return %0 : tensor<2x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          6,
          6,
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6,
          6,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,6,8] b:f32[6,6,8]. let\n    c:f32[1,6,6,8] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 6, 6, 8)\n      sharding=None\n    ] a\n    d:f32[1,6,6,8] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 6, 6, 8)\n      sharding=None\n    ] b\n    e:f32[2,6,6,8] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x6x8xf32>, %arg1: tensor<6x6x8xf32>) -> (tensor<2x6x6x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<6x6x8xf32>) -> tensor<1x6x6x8xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<6x6x8xf32>) -> tensor<1x6x6x8xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x6x6x8xf32>, tensor<1x6x6x8xf32>) -> tensor<2x6x6x8xf32>\n    return %2 : tensor<2x6x6x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          9,
          5,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,5,9]. let\n    b:f32[8,5,9] = slice[\n      limit_indices=(9, 5, 9)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x5x9xf32>) -> (tensor<8x5x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:9, 0:5, 0:9] : (tensor<9x5x9xf32>) -> tensor<8x5x9xf32>\n    return %0 : tensor<8x5x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 2))",
    "input_info": [
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4]. let\n    b:f32[2,2] = reshape[dimensions=None new_sizes=(2, 2) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<2x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<4xf32>) -> tensor<2x2xf32>\n    return %0 : tensor<2x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (10,))",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8]. let\n    b:f32[7] = slice[limit_indices=(8,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8xf32>) -> (tensor<7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:8] : (tensor<8xf32>) -> tensor<7xf32>\n    return %0 : tensor<7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          7,
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,7] b:f32[7,7]. let\n    c:f32[1,7,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 7, 7)\n      sharding=None\n    ] a\n    d:f32[1,7,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 7, 7)\n      sharding=None\n    ] b\n    e:f32[2,7,7] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x7xf32>, %arg1: tensor<7x7xf32>) -> (tensor<2x7x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<7x7xf32>) -> tensor<1x7x7xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<7x7xf32>) -> tensor<1x7x7xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x7x7xf32>, tensor<1x7x7xf32>) -> tensor<2x7x7xf32>\n    return %2 : tensor<2x7x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          4,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,6]. let\n    b:f32[6,4] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x6xf32>) -> (tensor<6x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<4x6xf32>) -> tensor<6x4xf32>\n    return %0 : tensor<6x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          6,
          2,
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6,
          2,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,2,6] b:f32[6,2,6]. let\n    c:f32[12,2,6] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x2x6xf32>, %arg1: tensor<6x2x6xf32>) -> (tensor<12x2x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<6x2x6xf32>, tensor<6x2x6xf32>) -> tensor<12x2x6xf32>\n    return %0 : tensor<12x2x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          9,
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,5] b:f32[9,5]. let\n    c:f32[18,5] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x5xf32>, %arg1: tensor<9x5xf32>) -> (tensor<18x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<9x5xf32>, tensor<9x5xf32>) -> tensor<18x5xf32>\n    return %0 : tensor<18x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (21,))",
    "input_info": [
      {
        "shape": [
          7,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,3]. let\n    b:f32[21] = reshape[dimensions=None new_sizes=(21,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x3xf32>) -> (tensor<21xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<7x3xf32>) -> tensor<21xf32>\n    return %0 : tensor<21xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>) -> (tensor<9xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (360,))",
    "input_info": [
      {
        "shape": [
          4,
          9,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,9,10]. let\n    b:f32[360] = reshape[dimensions=None new_sizes=(360,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x9x10xf32>) -> (tensor<360xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<4x9x10xf32>) -> tensor<360xf32>\n    return %0 : tensor<360xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4] b:f32[4]. let\n    c:f32[8] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> (tensor<8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<4xf32>, tensor<4xf32>) -> tensor<8xf32>\n    return %0 : tensor<8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 72))",
    "input_info": [
      {
        "shape": [
          8,
          2,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,2,9]. let\n    b:f32[2,72] = reshape[dimensions=None new_sizes=(2, 72) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x2x9xf32>) -> (tensor<2x72xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<8x2x9xf32>) -> tensor<2x72xf32>\n    return %0 : tensor<2x72xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 450))",
    "input_info": [
      {
        "shape": [
          10,
          10,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,10,9]. let\n    b:f32[2,450] = reshape[dimensions=None new_sizes=(2, 450) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x10x9xf32>) -> (tensor<2x450xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<10x10x9xf32>) -> tensor<2x450xf32>\n    return %0 : tensor<2x450xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          6,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,7]. let\n    b:f32[7,6] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x7xf32>) -> (tensor<7x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<6x7xf32>) -> tensor<7x6xf32>\n    return %0 : tensor<7x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10] b:f32[10]. let\n    c:f32[1,10] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 10)\n      sharding=None\n    ] a\n    d:f32[1,10] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 10)\n      sharding=None\n    ] b\n    e:f32[2,10] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>, %arg1: tensor<10xf32>) -> (tensor<2x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<10xf32>) -> tensor<1x10xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<10xf32>) -> tensor<1x10xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<2x10xf32>\n    return %2 : tensor<2x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (630,))",
    "input_info": [
      {
        "shape": [
          9,
          7,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,7,10]. let\n    b:f32[630] = reshape[dimensions=None new_sizes=(630,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x7x10xf32>) -> (tensor<630xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<9x7x10xf32>) -> tensor<630xf32>\n    return %0 : tensor<630xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 8))",
    "input_info": [
      {
        "shape": [
          4,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,4]. let\n    b:f32[2,8] = reshape[dimensions=None new_sizes=(2, 8) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x4xf32>) -> (tensor<2x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<4x4xf32>) -> tensor<2x8xf32>\n    return %0 : tensor<2x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          7,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,4]. let\n    b:f32[6,4] = slice[limit_indices=(7, 4) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x4xf32>) -> (tensor<6x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:7, 0:4] : (tensor<7x4xf32>) -> tensor<6x4xf32>\n    return %0 : tensor<6x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          10,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,3]. let\n    b:f32[3,10] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x3xf32>) -> (tensor<3x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<10x3xf32>) -> tensor<3x10xf32>\n    return %0 : tensor<3x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>) -> (tensor<5xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>) -> (tensor<5xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7] b:f32[7]. let\n    c:f32[14] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>, %arg1: tensor<7xf32>) -> (tensor<14xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<7xf32>, tensor<7xf32>) -> tensor<14xf32>\n    return %0 : tensor<14xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2]. let\n    b:f32[1] = slice[limit_indices=(2,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>) -> (tensor<1xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:2] : (tensor<2xf32>) -> tensor<1xf32>\n    return %0 : tensor<1xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          2,
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,8] b:f32[2,8]. let\n    c:f32[4,8] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x8xf32>, %arg1: tensor<2x8xf32>) -> (tensor<4x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<2x8xf32>, tensor<2x8xf32>) -> tensor<4x8xf32>\n    return %0 : tensor<4x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (3,))",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>) -> (tensor<3xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2]. let\n    b:f32[1] = slice[limit_indices=(2,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>) -> (tensor<1xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:2] : (tensor<2xf32>) -> tensor<1xf32>\n    return %0 : tensor<1xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          7,
          2,
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7,
          2,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,2,5] b:f32[7,2,5]. let\n    c:f32[1,7,2,5] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 7, 2, 5)\n      sharding=None\n    ] a\n    d:f32[1,7,2,5] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 7, 2, 5)\n      sharding=None\n    ] b\n    e:f32[2,7,2,5] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x2x5xf32>, %arg1: tensor<7x2x5xf32>) -> (tensor<2x7x2x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<7x2x5xf32>) -> tensor<1x7x2x5xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<7x2x5xf32>) -> tensor<1x7x2x5xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x7x2x5xf32>, tensor<1x7x2x5xf32>) -> tensor<2x7x2x5xf32>\n    return %2 : tensor<2x7x2x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          4,
          2,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,2,4]. let\n    b:f32[3,2,4] = slice[\n      limit_indices=(4, 2, 4)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x2x4xf32>) -> (tensor<3x2x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:4, 0:2, 0:4] : (tensor<4x2x4xf32>) -> tensor<3x2x4xf32>\n    return %0 : tensor<3x2x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6] b:f32[6]. let\n    c:f32[12] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>, %arg1: tensor<6xf32>) -> (tensor<12xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<6xf32>, tensor<6xf32>) -> tensor<12xf32>\n    return %0 : tensor<12xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>) -> (tensor<9xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          10,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,8]. let\n    b:f32[8,10] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x8xf32>) -> (tensor<8x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<10x8xf32>) -> tensor<8x10xf32>\n    return %0 : tensor<8x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (27,))",
    "input_info": [
      {
        "shape": [
          3,
          3,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,3,3]. let\n    b:f32[27] = reshape[dimensions=None new_sizes=(27,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x3x3xf32>) -> (tensor<27xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<3x3x3xf32>) -> tensor<27xf32>\n    return %0 : tensor<27xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          6,
          7,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,7,2]. let\n    b:f32[5,7,2] = slice[\n      limit_indices=(6, 7, 2)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x7x2xf32>) -> (tensor<5x7x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:6, 0:7, 0:2] : (tensor<6x7x2xf32>) -> tensor<5x7x2xf32>\n    return %0 : tensor<5x7x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (18,))",
    "input_info": [
      {
        "shape": [
          2,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,9]. let\n    b:f32[18] = reshape[dimensions=None new_sizes=(18,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x9xf32>) -> (tensor<18xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<2x9xf32>) -> tensor<18xf32>\n    return %0 : tensor<18xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          6,
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,7] b:f32[6,7]. let\n    c:f32[12,7] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x7xf32>, %arg1: tensor<6x7xf32>) -> (tensor<12x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<6x7xf32>, tensor<6x7xf32>) -> tensor<12x7xf32>\n    return %0 : tensor<12x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          7,
          6,
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7,
          6,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,6,5] b:f32[7,6,5]. let\n    c:f32[1,7,6,5] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 7, 6, 5)\n      sharding=None\n    ] a\n    d:f32[1,7,6,5] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 7, 6, 5)\n      sharding=None\n    ] b\n    e:f32[2,7,6,5] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x6x5xf32>, %arg1: tensor<7x6x5xf32>) -> (tensor<2x7x6x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<7x6x5xf32>) -> tensor<1x7x6x5xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<7x6x5xf32>) -> tensor<1x7x6x5xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x7x6x5xf32>, tensor<1x7x6x5xf32>) -> tensor<2x7x6x5xf32>\n    return %2 : tensor<2x7x6x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          5,
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,5] b:f32[5,5]. let\n    c:f32[10,5] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x5xf32>, %arg1: tensor<5x5xf32>) -> (tensor<10x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<5x5xf32>, tensor<5x5xf32>) -> tensor<10x5xf32>\n    return %0 : tensor<10x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          10,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,7]. let\n    b:f32[9,7] = slice[limit_indices=(10, 7) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x7xf32>) -> (tensor<9x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:10, 0:7] : (tensor<10x7xf32>) -> tensor<9x7xf32>\n    return %0 : tensor<9x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          3,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,6]. let\n    b:f32[2,6] = slice[limit_indices=(3, 6) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x6xf32>) -> (tensor<2x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:3, 0:6] : (tensor<3x6xf32>) -> tensor<2x6xf32>\n    return %0 : tensor<2x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6] b:f32[6]. let\n    c:f32[1,6] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 6)\n      sharding=None\n    ] a\n    d:f32[1,6] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 6)\n      sharding=None\n    ] b\n    e:f32[2,6] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>, %arg1: tensor<6xf32>) -> (tensor<2x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<6xf32>) -> tensor<1x6xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<6xf32>) -> tensor<1x6xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x6xf32>, tensor<1x6xf32>) -> tensor<2x6xf32>\n    return %2 : tensor<2x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          8,
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,2] b:f32[8,2]. let\n    c:f32[16,2] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x2xf32>, %arg1: tensor<8x2xf32>) -> (tensor<16x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<8x2xf32>, tensor<8x2xf32>) -> tensor<16x2xf32>\n    return %0 : tensor<16x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          6,
          2,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,2,7]. let\n    b:f32[7,2,6] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x2x7xf32>) -> (tensor<7x2x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<6x2x7xf32>) -> tensor<7x2x6xf32>\n    return %0 : tensor<7x2x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 25))",
    "input_info": [
      {
        "shape": [
          10,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,5]. let\n    b:f32[2,25] = reshape[dimensions=None new_sizes=(2, 25) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x5xf32>) -> (tensor<2x25xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<10x5xf32>) -> tensor<2x25xf32>\n    return %0 : tensor<2x25xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (648,))",
    "input_info": [
      {
        "shape": [
          9,
          8,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,8,9]. let\n    b:f32[648] = reshape[dimensions=None new_sizes=(648,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x8x9xf32>) -> (tensor<648xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<9x8x9xf32>) -> tensor<648xf32>\n    return %0 : tensor<648xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          2,
          7,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,7,3]. let\n    b:f32[3,7,2] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x7x3xf32>) -> (tensor<3x7x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<2x7x3xf32>) -> tensor<3x7x2xf32>\n    return %0 : tensor<3x7x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2] b:f32[2]. let\n    c:f32[1,2] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 2)\n      sharding=None\n    ] a\n    d:f32[1,2] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 2)\n      sharding=None\n    ] b\n    e:f32[2,2] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>, %arg1: tensor<2xf32>) -> (tensor<2x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<2xf32>) -> tensor<1x2xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<2xf32>) -> tensor<1x2xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x2xf32>, tensor<1x2xf32>) -> tensor<2x2xf32>\n    return %2 : tensor<2x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6] b:f32[6]. let\n    c:f32[12] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>, %arg1: tensor<6xf32>) -> (tensor<12xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<6xf32>, tensor<6xf32>) -> tensor<12xf32>\n    return %0 : tensor<12xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>) -> (tensor<6xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          5,
          8,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,8,6]. let\n    b:f32[4,8,6] = slice[\n      limit_indices=(5, 8, 6)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x8x6xf32>) -> (tensor<4x8x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:5, 0:8, 0:6] : (tensor<5x8x6xf32>) -> tensor<4x8x6xf32>\n    return %0 : tensor<4x8x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8] b:f32[8]. let\n    c:f32[16] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8xf32>, %arg1: tensor<8xf32>) -> (tensor<16xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<8xf32>, tensor<8xf32>) -> tensor<16xf32>\n    return %0 : tensor<16xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          2,
          3,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,3,6]. let\n    b:f32[1,3,6] = slice[\n      limit_indices=(2, 3, 6)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x3x6xf32>) -> (tensor<1x3x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:2, 0:3, 0:6] : (tensor<2x3x6xf32>) -> tensor<1x3x6xf32>\n    return %0 : tensor<1x3x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          9,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,10]. let\n    b:f32[8,10] = slice[limit_indices=(9, 10) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x10xf32>) -> (tensor<8x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:9, 0:10] : (tensor<9x10xf32>) -> tensor<8x10xf32>\n    return %0 : tensor<8x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          10,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,2]. let\n    b:f32[2,10] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x2xf32>) -> (tensor<2x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<10x2xf32>) -> tensor<2x10xf32>\n    return %0 : tensor<2x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          5,
          7,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,7,3]. let\n    b:f32[3,7,5] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x7x3xf32>) -> (tensor<3x7x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<5x7x3xf32>) -> tensor<3x7x5xf32>\n    return %0 : tensor<3x7x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          2,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,8]. let\n    b:f32[8,2] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x8xf32>) -> (tensor<8x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<2x8xf32>) -> tensor<8x2xf32>\n    return %0 : tensor<8x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6] b:f32[6]. let\n    c:f32[12] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>, %arg1: tensor<6xf32>) -> (tensor<12xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<6xf32>, tensor<6xf32>) -> tensor<12xf32>\n    return %0 : tensor<12xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 14))",
    "input_info": [
      {
        "shape": [
          7,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,4]. let\n    b:f32[2,14] = reshape[dimensions=None new_sizes=(2, 14) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x4xf32>) -> (tensor<2x14xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<7x4xf32>) -> tensor<2x14xf32>\n    return %0 : tensor<2x14xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 25))",
    "input_info": [
      {
        "shape": [
          10,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,5]. let\n    b:f32[2,25] = reshape[dimensions=None new_sizes=(2, 25) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x5xf32>) -> (tensor<2x25xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<10x5xf32>) -> tensor<2x25xf32>\n    return %0 : tensor<2x25xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          6,
          8,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,8,5]. let\n    b:f32[5,8,6] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x8x5xf32>) -> (tensor<5x8x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<6x8x5xf32>) -> tensor<5x8x6xf32>\n    return %0 : tensor<5x8x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10]. let\n    b:f32[9] = slice[limit_indices=(10,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:10] : (tensor<10xf32>) -> tensor<9xf32>\n    return %0 : tensor<9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          3,
          5,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,5,2]. let\n    b:f32[2,5,3] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x5x2xf32>) -> (tensor<2x5x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<3x5x2xf32>) -> tensor<2x5x3xf32>\n    return %0 : tensor<2x5x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8]. let\n    b:f32[7] = slice[limit_indices=(8,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8xf32>) -> (tensor<7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:8] : (tensor<8xf32>) -> tensor<7xf32>\n    return %0 : tensor<7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8xf32>) -> (tensor<8xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          3,
          5,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,5,2]. let\n    b:f32[2,5,3] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x5x2xf32>) -> (tensor<2x5x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<3x5x2xf32>) -> tensor<2x5x3xf32>\n    return %0 : tensor<2x5x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4] b:f32[4]. let\n    c:f32[1,4] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 4)\n      sharding=None\n    ] a\n    d:f32[1,4] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 4)\n      sharding=None\n    ] b\n    e:f32[2,4] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> (tensor<2x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<4xf32>) -> tensor<1x4xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<4xf32>) -> tensor<1x4xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x4xf32>, tensor<1x4xf32>) -> tensor<2x4xf32>\n    return %2 : tensor<2x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9] b:f32[9]. let\n    c:f32[1,9] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 9)\n      sharding=None\n    ] a\n    d:f32[1,9] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 9)\n      sharding=None\n    ] b\n    e:f32[2,9] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>, %arg1: tensor<9xf32>) -> (tensor<2x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<9xf32>) -> tensor<1x9xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<9xf32>) -> tensor<1x9xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x9xf32>, tensor<1x9xf32>) -> tensor<2x9xf32>\n    return %2 : tensor<2x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          6,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,2]. let\n    b:f32[5,2] = slice[limit_indices=(6, 2) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x2xf32>) -> (tensor<5x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:6, 0:2] : (tensor<6x2xf32>) -> tensor<5x2xf32>\n    return %0 : tensor<5x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4] b:f32[4]. let\n    c:f32[1,4] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 4)\n      sharding=None\n    ] a\n    d:f32[1,4] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 4)\n      sharding=None\n    ] b\n    e:f32[2,4] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> (tensor<2x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<4xf32>) -> tensor<1x4xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<4xf32>) -> tensor<1x4xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x4xf32>, tensor<1x4xf32>) -> tensor<2x4xf32>\n    return %2 : tensor<2x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          10,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,10]. let\n    b:f32[10,10] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x10xf32>) -> (tensor<10x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<10x10xf32>) -> tensor<10x10xf32>\n    return %0 : tensor<10x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          9,
          7,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,7,2]. let\n    b:f32[2,7,9] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x7x2xf32>) -> (tensor<2x7x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<9x7x2xf32>) -> tensor<2x7x9xf32>\n    return %0 : tensor<2x7x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          2,
          7,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,7,4]. let\n    b:f32[4,7,2] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x7x4xf32>) -> (tensor<4x7x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<2x7x4xf32>) -> tensor<4x7x2xf32>\n    return %0 : tensor<4x7x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10] b:f32[10]. let\n    c:f32[20] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>, %arg1: tensor<10xf32>) -> (tensor<20xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<10xf32>, tensor<10xf32>) -> tensor<20xf32>\n    return %0 : tensor<20xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          7,
          5,
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7,
          5,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,5,7] b:f32[7,5,7]. let\n    c:f32[14,5,7] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x5x7xf32>, %arg1: tensor<7x5x7xf32>) -> (tensor<14x5x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<7x5x7xf32>, tensor<7x5x7xf32>) -> tensor<14x5x7xf32>\n    return %0 : tensor<14x5x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          6,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,4]. let\n    b:f32[4,6] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x4xf32>) -> (tensor<4x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<6x4xf32>) -> tensor<4x6xf32>\n    return %0 : tensor<4x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (3, 63))",
    "input_info": [
      {
        "shape": [
          9,
          7,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,7,3]. let\n    b:f32[3,63] = reshape[dimensions=None new_sizes=(3, 63) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x7x3xf32>) -> (tensor<3x63xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<9x7x3xf32>) -> tensor<3x63xf32>\n    return %0 : tensor<3x63xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7] b:f32[7]. let\n    c:f32[1,7] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 7)\n      sharding=None\n    ] a\n    d:f32[1,7] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 7)\n      sharding=None\n    ] b\n    e:f32[2,7] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>, %arg1: tensor<7xf32>) -> (tensor<2x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<7xf32>) -> tensor<1x7xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<7xf32>) -> tensor<1x7xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x7xf32>, tensor<1x7xf32>) -> tensor<2x7xf32>\n    return %2 : tensor<2x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4]. let\n    b:f32[3] = slice[limit_indices=(4,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:4] : (tensor<4xf32>) -> tensor<3xf32>\n    return %0 : tensor<3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          2,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,8]. let\n    b:f32[8,2] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x8xf32>) -> (tensor<8x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<2x8xf32>) -> tensor<8x2xf32>\n    return %0 : tensor<8x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6] b:f32[6]. let\n    c:f32[12] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>, %arg1: tensor<6xf32>) -> (tensor<12xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<6xf32>, tensor<6xf32>) -> tensor<12xf32>\n    return %0 : tensor<12xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          8,
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,9] b:f32[8,9]. let\n    c:f32[1,8,9] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 8, 9)\n      sharding=None\n    ] a\n    d:f32[1,8,9] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 8, 9)\n      sharding=None\n    ] b\n    e:f32[2,8,9] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x9xf32>, %arg1: tensor<8x9xf32>) -> (tensor<2x8x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<8x9xf32>) -> tensor<1x8x9xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<8x9xf32>) -> tensor<1x8x9xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x8x9xf32>, tensor<1x8x9xf32>) -> tensor<2x8x9xf32>\n    return %2 : tensor<2x8x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          2,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,6]. let\n    b:f32[1,6] = slice[limit_indices=(2, 6) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x6xf32>) -> (tensor<1x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:2, 0:6] : (tensor<2x6xf32>) -> tensor<1x6xf32>\n    return %0 : tensor<1x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 36))",
    "input_info": [
      {
        "shape": [
          9,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,8]. let\n    b:f32[2,36] = reshape[dimensions=None new_sizes=(2, 36) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x8xf32>) -> (tensor<2x36xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<9x8xf32>) -> tensor<2x36xf32>\n    return %0 : tensor<2x36xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          6,
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,7] b:f32[6,7]. let\n    c:f32[12,7] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x7xf32>, %arg1: tensor<6x7xf32>) -> (tensor<12x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<6x7xf32>, tensor<6x7xf32>) -> tensor<12x7xf32>\n    return %0 : tensor<12x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 98))",
    "input_info": [
      {
        "shape": [
          7,
          4,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,4,7]. let\n    b:f32[2,98] = reshape[dimensions=None new_sizes=(2, 98) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x4x7xf32>) -> (tensor<2x98xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<7x4x7xf32>) -> tensor<2x98xf32>\n    return %0 : tensor<2x98xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (48,))",
    "input_info": [
      {
        "shape": [
          8,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,6]. let\n    b:f32[48] = reshape[dimensions=None new_sizes=(48,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x6xf32>) -> (tensor<48xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<8x6xf32>) -> tensor<48xf32>\n    return %0 : tensor<48xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          4,
          7,
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4,
          7,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,7,6] b:f32[4,7,6]. let\n    c:f32[1,4,7,6] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 4, 7, 6)\n      sharding=None\n    ] a\n    d:f32[1,4,7,6] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 4, 7, 6)\n      sharding=None\n    ] b\n    e:f32[2,4,7,6] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x7x6xf32>, %arg1: tensor<4x7x6xf32>) -> (tensor<2x4x7x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<4x7x6xf32>) -> tensor<1x4x7x6xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<4x7x6xf32>) -> tensor<1x4x7x6xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x4x7x6xf32>, tensor<1x4x7x6xf32>) -> tensor<2x4x7x6xf32>\n    return %2 : tensor<2x4x7x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 15))",
    "input_info": [
      {
        "shape": [
          10,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,3]. let\n    b:f32[2,15] = reshape[dimensions=None new_sizes=(2, 15) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x3xf32>) -> (tensor<2x15xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<10x3xf32>) -> tensor<2x15xf32>\n    return %0 : tensor<2x15xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 60))",
    "input_info": [
      {
        "shape": [
          6,
          5,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,5,4]. let\n    b:f32[2,60] = reshape[dimensions=None new_sizes=(2, 60) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x5x4xf32>) -> (tensor<2x60xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<6x5x4xf32>) -> tensor<2x60xf32>\n    return %0 : tensor<2x60xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3]. let\n    b:f32[2] = slice[limit_indices=(3,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>) -> (tensor<2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:3] : (tensor<3xf32>) -> tensor<2xf32>\n    return %0 : tensor<2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9]. let\n    b:f32[8] = slice[limit_indices=(9,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>) -> (tensor<8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:9] : (tensor<9xf32>) -> tensor<8xf32>\n    return %0 : tensor<8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5]. let\n    b:f32[4] = slice[limit_indices=(5,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:5] : (tensor<5xf32>) -> tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6] b:f32[6]. let\n    c:f32[1,6] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 6)\n      sharding=None\n    ] a\n    d:f32[1,6] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 6)\n      sharding=None\n    ] b\n    e:f32[2,6] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>, %arg1: tensor<6xf32>) -> (tensor<2x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<6xf32>) -> tensor<1x6xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<6xf32>) -> tensor<1x6xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x6xf32>, tensor<1x6xf32>) -> tensor<2x6xf32>\n    return %2 : tensor<2x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10]. let\n    b:f32[9] = slice[limit_indices=(10,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:10] : (tensor<10xf32>) -> tensor<9xf32>\n    return %0 : tensor<9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6] b:f32[6]. let\n    c:f32[12] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>, %arg1: tensor<6xf32>) -> (tensor<12xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<6xf32>, tensor<6xf32>) -> tensor<12xf32>\n    return %0 : tensor<12xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2] b:f32[2]. let\n    c:f32[1,2] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 2)\n      sharding=None\n    ] a\n    d:f32[1,2] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 2)\n      sharding=None\n    ] b\n    e:f32[2,2] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>, %arg1: tensor<2xf32>) -> (tensor<2x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<2xf32>) -> tensor<1x2xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<2xf32>) -> tensor<1x2xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x2xf32>, tensor<1x2xf32>) -> tensor<2x2xf32>\n    return %2 : tensor<2x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>) -> (tensor<7xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          5,
          7,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,7,3]. let\n    b:f32[4,7,3] = slice[\n      limit_indices=(5, 7, 3)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x7x3xf32>) -> (tensor<4x7x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:5, 0:7, 0:3] : (tensor<5x7x3xf32>) -> tensor<4x7x3xf32>\n    return %0 : tensor<4x7x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          3,
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,6] b:f32[3,6]. let\n    c:f32[6,6] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x6xf32>, %arg1: tensor<3x6xf32>) -> (tensor<6x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<3x6xf32>, tensor<3x6xf32>) -> tensor<6x6xf32>\n    return %0 : tensor<6x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          9,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,2]. let\n    b:f32[8,2] = slice[limit_indices=(9, 2) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x2xf32>) -> (tensor<8x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:9, 0:2] : (tensor<9x2xf32>) -> tensor<8x2xf32>\n    return %0 : tensor<8x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8] b:f32[8]. let\n    c:f32[16] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8xf32>, %arg1: tensor<8xf32>) -> (tensor<16xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<8xf32>, tensor<8xf32>) -> tensor<16xf32>\n    return %0 : tensor<16xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          10,
          5,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,5,9]. let\n    b:f32[9,5,9] = slice[\n      limit_indices=(10, 5, 9)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x5x9xf32>) -> (tensor<9x5x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:10, 0:5, 0:9] : (tensor<10x5x9xf32>) -> tensor<9x5x9xf32>\n    return %0 : tensor<9x5x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9] b:f32[9]. let\n    c:f32[18] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>, %arg1: tensor<9xf32>) -> (tensor<18xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<9xf32>, tensor<9xf32>) -> tensor<18xf32>\n    return %0 : tensor<18xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 8))",
    "input_info": [
      {
        "shape": [
          8,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,2]. let\n    b:f32[2,8] = reshape[dimensions=None new_sizes=(2, 8) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x2xf32>) -> (tensor<2x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<8x2xf32>) -> tensor<2x8xf32>\n    return %0 : tensor<2x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 35))",
    "input_info": [
      {
        "shape": [
          10,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,7]. let\n    b:f32[2,35] = reshape[dimensions=None new_sizes=(2, 35) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x7xf32>) -> (tensor<2x35xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<10x7xf32>) -> tensor<2x35xf32>\n    return %0 : tensor<2x35xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          6,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,9]. let\n    b:f32[5,9] = slice[limit_indices=(6, 9) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x9xf32>) -> (tensor<5x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:6, 0:9] : (tensor<6x9xf32>) -> tensor<5x9xf32>\n    return %0 : tensor<5x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3] b:f32[3]. let\n    c:f32[6] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>, %arg1: tensor<3xf32>) -> (tensor<6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<3xf32>, tensor<3xf32>) -> tensor<6xf32>\n    return %0 : tensor<6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 50))",
    "input_info": [
      {
        "shape": [
          2,
          10,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,10,5]. let\n    b:f32[2,50] = reshape[dimensions=None new_sizes=(2, 50) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x10x5xf32>) -> (tensor<2x50xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<2x10x5xf32>) -> tensor<2x50xf32>\n    return %0 : tensor<2x50xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>) -> (tensor<6xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          2,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,9]. let\n    b:f32[1,9] = slice[limit_indices=(2, 9) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x9xf32>) -> (tensor<1x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:2, 0:9] : (tensor<2x9xf32>) -> tensor<1x9xf32>\n    return %0 : tensor<1x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (3, 3))",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9]. let\n    b:f32[3,3] = reshape[dimensions=None new_sizes=(3, 3) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>) -> (tensor<3x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<9xf32>) -> tensor<3x3xf32>\n    return %0 : tensor<3x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3] b:f32[3]. let\n    c:f32[6] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>, %arg1: tensor<3xf32>) -> (tensor<6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<3xf32>, tensor<3xf32>) -> tensor<6xf32>\n    return %0 : tensor<6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          8,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,3]. let\n    b:f32[7,3] = slice[limit_indices=(8, 3) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x3xf32>) -> (tensor<7x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:8, 0:3] : (tensor<8x3xf32>) -> tensor<7x3xf32>\n    return %0 : tensor<7x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (336,))",
    "input_info": [
      {
        "shape": [
          6,
          7,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,7,8]. let\n    b:f32[336] = reshape[dimensions=None new_sizes=(336,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x7x8xf32>) -> (tensor<336xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<6x7x8xf32>) -> tensor<336xf32>\n    return %0 : tensor<336xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4]. let\n    b:f32[3] = slice[limit_indices=(4,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:4] : (tensor<4xf32>) -> tensor<3xf32>\n    return %0 : tensor<3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          2,
          10,
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2,
          10,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,10,4] b:f32[2,10,4]. let\n    c:f32[1,2,10,4] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 2, 10, 4)\n      sharding=None\n    ] a\n    d:f32[1,2,10,4] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 2, 10, 4)\n      sharding=None\n    ] b\n    e:f32[2,2,10,4] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x10x4xf32>, %arg1: tensor<2x10x4xf32>) -> (tensor<2x2x10x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<2x10x4xf32>) -> tensor<1x2x10x4xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<2x10x4xf32>) -> tensor<1x2x10x4xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x2x10x4xf32>, tensor<1x2x10x4xf32>) -> tensor<2x2x10x4xf32>\n    return %2 : tensor<2x2x10x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          6,
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,2] b:f32[6,2]. let\n    c:f32[12,2] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x2xf32>, %arg1: tensor<6x2xf32>) -> (tensor<12x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<6x2xf32>, tensor<6x2xf32>) -> tensor<12x2xf32>\n    return %0 : tensor<12x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          3,
          8,
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3,
          8,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,8,4] b:f32[3,8,4]. let\n    c:f32[1,3,8,4] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 3, 8, 4)\n      sharding=None\n    ] a\n    d:f32[1,3,8,4] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 3, 8, 4)\n      sharding=None\n    ] b\n    e:f32[2,3,8,4] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x8x4xf32>, %arg1: tensor<3x8x4xf32>) -> (tensor<2x3x8x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<3x8x4xf32>) -> tensor<1x3x8x4xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<3x8x4xf32>) -> tensor<1x3x8x4xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x3x8x4xf32>, tensor<1x3x8x4xf32>) -> tensor<2x3x8x4xf32>\n    return %2 : tensor<2x3x8x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          10,
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,9] b:f32[10,9]. let\n    c:f32[20,9] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x9xf32>, %arg1: tensor<10x9xf32>) -> (tensor<20x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<10x9xf32>, tensor<10x9xf32>) -> tensor<20x9xf32>\n    return %0 : tensor<20x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>) -> (tensor<6xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          8,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,2]. let\n    b:f32[7,2] = slice[limit_indices=(8, 2) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x2xf32>) -> (tensor<7x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:8, 0:2] : (tensor<8x2xf32>) -> tensor<7x2xf32>\n    return %0 : tensor<7x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (35,))",
    "input_info": [
      {
        "shape": [
          5,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,7]. let\n    b:f32[35] = reshape[dimensions=None new_sizes=(35,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x7xf32>) -> (tensor<35xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<5x7xf32>) -> tensor<35xf32>\n    return %0 : tensor<35xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          7,
          6,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,6,6]. let\n    b:f32[6,6,7] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x6x6xf32>) -> (tensor<6x6x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<7x6x6xf32>) -> tensor<6x6x7xf32>\n    return %0 : tensor<6x6x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          8,
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,7] b:f32[8,7]. let\n    c:f32[1,8,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 8, 7)\n      sharding=None\n    ] a\n    d:f32[1,8,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 8, 7)\n      sharding=None\n    ] b\n    e:f32[2,8,7] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x7xf32>, %arg1: tensor<8x7xf32>) -> (tensor<2x8x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<8x7xf32>) -> tensor<1x8x7xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<8x7xf32>) -> tensor<1x8x7xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x8x7xf32>, tensor<1x8x7xf32>) -> tensor<2x8x7xf32>\n    return %2 : tensor<2x8x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10]. let\n    b:f32[9] = slice[limit_indices=(10,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:10] : (tensor<10xf32>) -> tensor<9xf32>\n    return %0 : tensor<9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>) -> (tensor<9xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          4,
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,4] b:f32[4,4]. let\n    c:f32[1,4,4] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 4, 4)\n      sharding=None\n    ] a\n    d:f32[1,4,4] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 4, 4)\n      sharding=None\n    ] b\n    e:f32[2,4,4] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x4xf32>, %arg1: tensor<4x4xf32>) -> (tensor<2x4x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<4x4xf32>) -> tensor<1x4x4xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<4x4xf32>) -> tensor<1x4x4xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x4x4xf32>, tensor<1x4x4xf32>) -> tensor<2x4x4xf32>\n    return %2 : tensor<2x4x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (3, 35))",
    "input_info": [
      {
        "shape": [
          5,
          7,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,7,3]. let\n    b:f32[3,35] = reshape[dimensions=None new_sizes=(3, 35) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x7x3xf32>) -> (tensor<3x35xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<5x7x3xf32>) -> tensor<3x35xf32>\n    return %0 : tensor<3x35xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          8,
          2,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,2,6]. let\n    b:f32[7,2,6] = slice[\n      limit_indices=(8, 2, 6)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x2x6xf32>) -> (tensor<7x2x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:8, 0:2, 0:6] : (tensor<8x2x6xf32>) -> tensor<7x2x6xf32>\n    return %0 : tensor<7x2x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (448,))",
    "input_info": [
      {
        "shape": [
          7,
          8,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,8,8]. let\n    b:f32[448] = reshape[dimensions=None new_sizes=(448,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x8x8xf32>) -> (tensor<448xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<7x8x8xf32>) -> tensor<448xf32>\n    return %0 : tensor<448xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (5,))",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>) -> (tensor<5xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          6,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,6]. let\n    b:f32[6,6] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x6xf32>) -> (tensor<6x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<6x6xf32>) -> tensor<6x6xf32>\n    return %0 : tensor<6x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          2,
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,5] b:f32[2,5]. let\n    c:f32[4,5] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x5xf32>, %arg1: tensor<2x5xf32>) -> (tensor<4x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<2x5xf32>, tensor<2x5xf32>) -> tensor<4x5xf32>\n    return %0 : tensor<4x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 5))",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10]. let\n    b:f32[2,5] = reshape[dimensions=None new_sizes=(2, 5) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<2x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<10xf32>) -> tensor<2x5xf32>\n    return %0 : tensor<2x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>) -> (tensor<9xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          9,
          9,
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9,
          9,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,9,3] b:f32[9,9,3]. let\n    c:f32[1,9,9,3] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 9, 9, 3)\n      sharding=None\n    ] a\n    d:f32[1,9,9,3] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 9, 9, 3)\n      sharding=None\n    ] b\n    e:f32[2,9,9,3] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x9x3xf32>, %arg1: tensor<9x9x3xf32>) -> (tensor<2x9x9x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<9x9x3xf32>) -> tensor<1x9x9x3xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<9x9x3xf32>) -> tensor<1x9x9x3xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x9x9x3xf32>, tensor<1x9x9x3xf32>) -> tensor<2x9x9x3xf32>\n    return %2 : tensor<2x9x9x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          6,
          6,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,6,5]. let\n    b:f32[5,6,5] = slice[\n      limit_indices=(6, 6, 5)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x6x5xf32>) -> (tensor<5x6x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:6, 0:6, 0:5] : (tensor<6x6x5xf32>) -> tensor<5x6x5xf32>\n    return %0 : tensor<5x6x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          10,
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,3] b:f32[10,3]. let\n    c:f32[1,10,3] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 10, 3)\n      sharding=None\n    ] a\n    d:f32[1,10,3] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 10, 3)\n      sharding=None\n    ] b\n    e:f32[2,10,3] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x3xf32>, %arg1: tensor<10x3xf32>) -> (tensor<2x10x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<10x3xf32>) -> tensor<1x10x3xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<10x3xf32>) -> tensor<1x10x3xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x10x3xf32>, tensor<1x10x3xf32>) -> tensor<2x10x3xf32>\n    return %2 : tensor<2x10x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          10,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,9]. let\n    b:f32[9,10] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x9xf32>) -> (tensor<9x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<10x9xf32>) -> tensor<9x10xf32>\n    return %0 : tensor<9x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          2,
          4,
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2,
          4,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,4,7] b:f32[2,4,7]. let\n    c:f32[1,2,4,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 2, 4, 7)\n      sharding=None\n    ] a\n    d:f32[1,2,4,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 2, 4, 7)\n      sharding=None\n    ] b\n    e:f32[2,2,4,7] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x4x7xf32>, %arg1: tensor<2x4x7xf32>) -> (tensor<2x2x4x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<2x4x7xf32>) -> tensor<1x2x4x7xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<2x4x7xf32>) -> tensor<1x2x4x7xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x2x4x7xf32>, tensor<1x2x4x7xf32>) -> tensor<2x2x4x7xf32>\n    return %2 : tensor<2x2x4x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          7,
          6,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,6,7]. let\n    b:f32[7,6,7] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x6x7xf32>) -> (tensor<7x6x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<7x6x7xf32>) -> tensor<7x6x7xf32>\n    return %0 : tensor<7x6x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          8,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,6]. let\n    b:f32[7,6] = slice[limit_indices=(8, 6) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x6xf32>) -> (tensor<7x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:8, 0:6] : (tensor<8x6xf32>) -> tensor<7x6xf32>\n    return %0 : tensor<7x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6] b:f32[6]. let\n    c:f32[12] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>, %arg1: tensor<6xf32>) -> (tensor<12xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<6xf32>, tensor<6xf32>) -> tensor<12xf32>\n    return %0 : tensor<12xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          4,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,10]. let\n    b:f32[3,10] = slice[limit_indices=(4, 10) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x10xf32>) -> (tensor<3x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:4, 0:10] : (tensor<4x10xf32>) -> tensor<3x10xf32>\n    return %0 : tensor<3x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          3,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,4]. let\n    b:f32[4,3] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x4xf32>) -> (tensor<4x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<3x4xf32>) -> tensor<4x3xf32>\n    return %0 : tensor<4x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 6))",
    "input_info": [
      {
        "shape": [
          3,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,4]. let\n    b:f32[2,6] = reshape[dimensions=None new_sizes=(2, 6) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x4xf32>) -> (tensor<2x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<3x4xf32>) -> tensor<2x6xf32>\n    return %0 : tensor<2x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          5,
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,3] b:f32[5,3]. let\n    c:f32[10,3] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x3xf32>, %arg1: tensor<5x3xf32>) -> (tensor<10x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<5x3xf32>, tensor<5x3xf32>) -> tensor<10x3xf32>\n    return %0 : tensor<10x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>) -> (tensor<7xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (16,))",
    "input_info": [
      {
        "shape": [
          4,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,4]. let\n    b:f32[16] = reshape[dimensions=None new_sizes=(16,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x4xf32>) -> (tensor<16xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<4x4xf32>) -> tensor<16xf32>\n    return %0 : tensor<16xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9] b:f32[9]. let\n    c:f32[1,9] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 9)\n      sharding=None\n    ] a\n    d:f32[1,9] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 9)\n      sharding=None\n    ] b\n    e:f32[2,9] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>, %arg1: tensor<9xf32>) -> (tensor<2x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<9xf32>) -> tensor<1x9xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<9xf32>) -> tensor<1x9xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x9xf32>, tensor<1x9xf32>) -> tensor<2x9xf32>\n    return %2 : tensor<2x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3] b:f32[3]. let\n    c:f32[6] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>, %arg1: tensor<3xf32>) -> (tensor<6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<3xf32>, tensor<3xf32>) -> tensor<6xf32>\n    return %0 : tensor<6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          7,
          7,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,7,6]. let\n    b:f32[6,7,7] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x7x6xf32>) -> (tensor<6x7x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<7x7x6xf32>) -> tensor<6x7x7xf32>\n    return %0 : tensor<6x7x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8xf32>) -> (tensor<8xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (243,))",
    "input_info": [
      {
        "shape": [
          9,
          9,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,9,3]. let\n    b:f32[243] = reshape[dimensions=None new_sizes=(243,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x9x3xf32>) -> (tensor<243xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<9x9x3xf32>) -> tensor<243xf32>\n    return %0 : tensor<243xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          4,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,10]. let\n    b:f32[10,4] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x10xf32>) -> (tensor<10x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<4x10xf32>) -> tensor<10x4xf32>\n    return %0 : tensor<10x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          2,
          4,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,4,4]. let\n    b:f32[4,4,2] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x4x4xf32>) -> (tensor<4x4x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<2x4x4xf32>) -> tensor<4x4x2xf32>\n    return %0 : tensor<4x4x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          5,
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,7] b:f32[5,7]. let\n    c:f32[1,5,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 5, 7)\n      sharding=None\n    ] a\n    d:f32[1,5,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 5, 7)\n      sharding=None\n    ] b\n    e:f32[2,5,7] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x7xf32>, %arg1: tensor<5x7xf32>) -> (tensor<2x5x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<5x7xf32>) -> tensor<1x5x7xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<5x7xf32>) -> tensor<1x5x7xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x5x7xf32>, tensor<1x5x7xf32>) -> tensor<2x5x7xf32>\n    return %2 : tensor<2x5x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          2,
          3,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,3,7]. let\n    b:f32[1,3,7] = slice[\n      limit_indices=(2, 3, 7)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x3x7xf32>) -> (tensor<1x3x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:2, 0:3, 0:7] : (tensor<2x3x7xf32>) -> tensor<1x3x7xf32>\n    return %0 : tensor<1x3x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          8,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,3]. let\n    b:f32[7,3] = slice[limit_indices=(8, 3) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x3xf32>) -> (tensor<7x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:8, 0:3] : (tensor<8x3xf32>) -> tensor<7x3xf32>\n    return %0 : tensor<7x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 30))",
    "input_info": [
      {
        "shape": [
          5,
          3,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,3,4]. let\n    b:f32[2,30] = reshape[dimensions=None new_sizes=(2, 30) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x3x4xf32>) -> (tensor<2x30xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<5x3x4xf32>) -> tensor<2x30xf32>\n    return %0 : tensor<2x30xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          9,
          3,
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9,
          3,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,3,4] b:f32[9,3,4]. let\n    c:f32[1,9,3,4] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 9, 3, 4)\n      sharding=None\n    ] a\n    d:f32[1,9,3,4] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 9, 3, 4)\n      sharding=None\n    ] b\n    e:f32[2,9,3,4] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x3x4xf32>, %arg1: tensor<9x3x4xf32>) -> (tensor<2x9x3x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<9x3x4xf32>) -> tensor<1x9x3x4xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<9x3x4xf32>) -> tensor<1x9x3x4xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x9x3x4xf32>, tensor<1x9x3x4xf32>) -> tensor<2x9x3x4xf32>\n    return %2 : tensor<2x9x3x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          7,
          10,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,10,7]. let\n    b:f32[6,10,7] = slice[\n      limit_indices=(7, 10, 7)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x10x7xf32>) -> (tensor<6x10x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:7, 0:10, 0:7] : (tensor<7x10x7xf32>) -> tensor<6x10x7xf32>\n    return %0 : tensor<6x10x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          8,
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,10] b:f32[8,10]. let\n    c:f32[16,10] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x10xf32>, %arg1: tensor<8x10xf32>) -> (tensor<16x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<8x10xf32>, tensor<8x10xf32>) -> tensor<16x10xf32>\n    return %0 : tensor<16x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          6,
          8,
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6,
          8,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,8,8] b:f32[6,8,8]. let\n    c:f32[12,8,8] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x8x8xf32>, %arg1: tensor<6x8x8xf32>) -> (tensor<12x8x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<6x8x8xf32>, tensor<6x8x8xf32>) -> tensor<12x8x8xf32>\n    return %0 : tensor<12x8x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          6,
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,6] b:f32[6,6]. let\n    c:f32[12,6] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x6xf32>, %arg1: tensor<6x6xf32>) -> (tensor<12x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<6x6xf32>, tensor<6x6xf32>) -> tensor<12x6xf32>\n    return %0 : tensor<12x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          5,
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,7] b:f32[5,7]. let\n    c:f32[10,7] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x7xf32>, %arg1: tensor<5x7xf32>) -> (tensor<10x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<5x7xf32>, tensor<5x7xf32>) -> tensor<10x7xf32>\n    return %0 : tensor<10x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          3,
          9,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,9,4]. let\n    b:f32[4,9,3] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x9x4xf32>) -> (tensor<4x9x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<3x9x4xf32>) -> tensor<4x9x3xf32>\n    return %0 : tensor<4x9x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 6))",
    "input_info": [
      {
        "shape": [
          3,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,4]. let\n    b:f32[2,6] = reshape[dimensions=None new_sizes=(2, 6) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x4xf32>) -> (tensor<2x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<3x4xf32>) -> tensor<2x6xf32>\n    return %0 : tensor<2x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          9,
          10,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,10,6]. let\n    b:f32[8,10,6] = slice[\n      limit_indices=(9, 10, 6)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x10x6xf32>) -> (tensor<8x10x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:9, 0:10, 0:6] : (tensor<9x10x6xf32>) -> tensor<8x10x6xf32>\n    return %0 : tensor<8x10x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          4,
          8,
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4,
          8,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,8,8] b:f32[4,8,8]. let\n    c:f32[8,8,8] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x8x8xf32>, %arg1: tensor<4x8x8xf32>) -> (tensor<8x8x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<4x8x8xf32>, tensor<4x8x8xf32>) -> tensor<8x8x8xf32>\n    return %0 : tensor<8x8x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (63,))",
    "input_info": [
      {
        "shape": [
          3,
          7,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,7,3]. let\n    b:f32[63] = reshape[dimensions=None new_sizes=(63,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x7x3xf32>) -> (tensor<63xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<3x7x3xf32>) -> tensor<63xf32>\n    return %0 : tensor<63xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          3,
          10,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,10,5]. let\n    b:f32[2,10,5] = slice[\n      limit_indices=(3, 10, 5)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x10x5xf32>) -> (tensor<2x10x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:3, 0:10, 0:5] : (tensor<3x10x5xf32>) -> tensor<2x10x5xf32>\n    return %0 : tensor<2x10x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          7,
          7,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,7,4]. let\n    b:f32[6,7,4] = slice[\n      limit_indices=(7, 7, 4)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x7x4xf32>) -> (tensor<6x7x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:7, 0:7, 0:4] : (tensor<7x7x4xf32>) -> tensor<6x7x4xf32>\n    return %0 : tensor<6x7x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          3,
          2,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,2,10]. let\n    b:f32[10,2,3] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x2x10xf32>) -> (tensor<10x2x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<3x2x10xf32>) -> tensor<10x2x3xf32>\n    return %0 : tensor<10x2x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          7,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,3]. let\n    b:f32[6,3] = slice[limit_indices=(7, 3) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x3xf32>) -> (tensor<6x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:7, 0:3] : (tensor<7x3xf32>) -> tensor<6x3xf32>\n    return %0 : tensor<6x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10] b:f32[10]. let\n    c:f32[1,10] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 10)\n      sharding=None\n    ] a\n    d:f32[1,10] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 10)\n      sharding=None\n    ] b\n    e:f32[2,10] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>, %arg1: tensor<10xf32>) -> (tensor<2x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<10xf32>) -> tensor<1x10xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<10xf32>) -> tensor<1x10xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<2x10xf32>\n    return %2 : tensor<2x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          5,
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,8] b:f32[5,8]. let\n    c:f32[1,5,8] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 5, 8)\n      sharding=None\n    ] a\n    d:f32[1,5,8] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 5, 8)\n      sharding=None\n    ] b\n    e:f32[2,5,8] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x8xf32>, %arg1: tensor<5x8xf32>) -> (tensor<2x5x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<5x8xf32>) -> tensor<1x5x8xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<5x8xf32>) -> tensor<1x5x8xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x5x8xf32>, tensor<1x5x8xf32>) -> tensor<2x5x8xf32>\n    return %2 : tensor<2x5x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          3,
          8,
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3,
          8,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,8,7] b:f32[3,8,7]. let\n    c:f32[6,8,7] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x8x7xf32>, %arg1: tensor<3x8x7xf32>) -> (tensor<6x8x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<3x8x7xf32>, tensor<3x8x7xf32>) -> tensor<6x8x7xf32>\n    return %0 : tensor<6x8x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          4,
          7,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,7,5]. let\n    b:f32[5,7,4] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x7x5xf32>) -> (tensor<5x7x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<4x7x5xf32>) -> tensor<5x7x4xf32>\n    return %0 : tensor<5x7x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          7,
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,4] b:f32[7,4]. let\n    c:f32[1,7,4] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 7, 4)\n      sharding=None\n    ] a\n    d:f32[1,7,4] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 7, 4)\n      sharding=None\n    ] b\n    e:f32[2,7,4] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x4xf32>, %arg1: tensor<7x4xf32>) -> (tensor<2x7x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<7x4xf32>) -> tensor<1x7x4xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<7x4xf32>) -> tensor<1x7x4xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x7x4xf32>, tensor<1x7x4xf32>) -> tensor<2x7x4xf32>\n    return %2 : tensor<2x7x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          5,
          8,
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5,
          8,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,8,6] b:f32[5,8,6]. let\n    c:f32[1,5,8,6] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 5, 8, 6)\n      sharding=None\n    ] a\n    d:f32[1,5,8,6] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 5, 8, 6)\n      sharding=None\n    ] b\n    e:f32[2,5,8,6] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x8x6xf32>, %arg1: tensor<5x8x6xf32>) -> (tensor<2x5x8x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<5x8x6xf32>) -> tensor<1x5x8x6xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<5x8x6xf32>) -> tensor<1x5x8x6xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x5x8x6xf32>, tensor<1x5x8x6xf32>) -> tensor<2x5x8x6xf32>\n    return %2 : tensor<2x5x8x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          4,
          2,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,2,3]. let\n    b:f32[3,2,4] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x2x3xf32>) -> (tensor<3x2x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<4x2x3xf32>) -> tensor<3x2x4xf32>\n    return %0 : tensor<3x2x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          6,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,6]. let\n    b:f32[6,6] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x6xf32>) -> (tensor<6x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<6x6xf32>) -> tensor<6x6xf32>\n    return %0 : tensor<6x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          2,
          6,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,6,3]. let\n    b:f32[3,6,2] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x6x3xf32>) -> (tensor<3x6x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<2x6x3xf32>) -> tensor<3x6x2xf32>\n    return %0 : tensor<3x6x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 20))",
    "input_info": [
      {
        "shape": [
          4,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,10]. let\n    b:f32[2,20] = reshape[dimensions=None new_sizes=(2, 20) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x10xf32>) -> (tensor<2x20xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<4x10xf32>) -> tensor<2x20xf32>\n    return %0 : tensor<2x20xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          7,
          7,
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7,
          7,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,7,10] b:f32[7,7,10]. let\n    c:f32[1,7,7,10] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 7, 7, 10)\n      sharding=None\n    ] a\n    d:f32[1,7,7,10] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 7, 7, 10)\n      sharding=None\n    ] b\n    e:f32[2,7,7,10] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x7x10xf32>, %arg1: tensor<7x7x10xf32>) -> (tensor<2x7x7x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<7x7x10xf32>) -> tensor<1x7x7x10xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<7x7x10xf32>) -> tensor<1x7x7x10xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x7x7x10xf32>, tensor<1x7x7x10xf32>) -> tensor<2x7x7x10xf32>\n    return %2 : tensor<2x7x7x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          8,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,9]. let\n    b:f32[7,9] = slice[limit_indices=(8, 9) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x9xf32>) -> (tensor<7x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:8, 0:9] : (tensor<8x9xf32>) -> tensor<7x9xf32>\n    return %0 : tensor<7x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          7,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,2]. let\n    b:f32[6,2] = slice[limit_indices=(7, 2) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x2xf32>) -> (tensor<6x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:7, 0:2] : (tensor<7x2xf32>) -> tensor<6x2xf32>\n    return %0 : tensor<6x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          4,
          5,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,5,4]. let\n    b:f32[4,5,4] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x5x4xf32>) -> (tensor<4x5x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<4x5x4xf32>) -> tensor<4x5x4xf32>\n    return %0 : tensor<4x5x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          8,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,9]. let\n    b:f32[9,8] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x9xf32>) -> (tensor<9x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<8x9xf32>) -> tensor<9x8xf32>\n    return %0 : tensor<9x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3] b:f32[3]. let\n    c:f32[6] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>, %arg1: tensor<3xf32>) -> (tensor<6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<3xf32>, tensor<3xf32>) -> tensor<6xf32>\n    return %0 : tensor<6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (5,))",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>) -> (tensor<5xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (48,))",
    "input_info": [
      {
        "shape": [
          6,
          4,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,4,2]. let\n    b:f32[48] = reshape[dimensions=None new_sizes=(48,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x4x2xf32>) -> (tensor<48xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<6x4x2xf32>) -> tensor<48xf32>\n    return %0 : tensor<48xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8]. let\n    b:f32[7] = slice[limit_indices=(8,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8xf32>) -> (tensor<7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:8] : (tensor<8xf32>) -> tensor<7xf32>\n    return %0 : tensor<7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          6,
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,6] b:f32[6,6]. let\n    c:f32[1,6,6] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 6, 6)\n      sharding=None\n    ] a\n    d:f32[1,6,6] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 6, 6)\n      sharding=None\n    ] b\n    e:f32[2,6,6] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x6xf32>, %arg1: tensor<6x6xf32>) -> (tensor<2x6x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<6x6xf32>) -> tensor<1x6x6xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<6x6xf32>) -> tensor<1x6x6xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x6x6xf32>, tensor<1x6x6xf32>) -> tensor<2x6x6xf32>\n    return %2 : tensor<2x6x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10]. let\n    b:f32[9] = slice[limit_indices=(10,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:10] : (tensor<10xf32>) -> tensor<9xf32>\n    return %0 : tensor<9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          10,
          4,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,4,8]. let\n    b:f32[9,4,8] = slice[\n      limit_indices=(10, 4, 8)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x4x8xf32>) -> (tensor<9x4x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:10, 0:4, 0:8] : (tensor<10x4x8xf32>) -> tensor<9x4x8xf32>\n    return %0 : tensor<9x4x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          7,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,7]. let\n    b:f32[7,7] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x7xf32>) -> (tensor<7x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<7x7xf32>) -> tensor<7x7xf32>\n    return %0 : tensor<7x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 5))",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10]. let\n    b:f32[2,5] = reshape[dimensions=None new_sizes=(2, 5) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<2x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<10xf32>) -> tensor<2x5xf32>\n    return %0 : tensor<2x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          5,
          6,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,6,3]. let\n    b:f32[3,6,5] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x6x3xf32>) -> (tensor<3x6x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<5x6x3xf32>) -> tensor<3x6x5xf32>\n    return %0 : tensor<3x6x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10]. let\n    b:f32[9] = slice[limit_indices=(10,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:10] : (tensor<10xf32>) -> tensor<9xf32>\n    return %0 : tensor<9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          9,
          10,
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9,
          10,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,10,10] b:f32[9,10,10]. let\n    c:f32[18,10,10] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x10x10xf32>, %arg1: tensor<9x10x10xf32>) -> (tensor<18x10x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<9x10x10xf32>, tensor<9x10x10xf32>) -> tensor<18x10x10xf32>\n    return %0 : tensor<18x10x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 4))",
    "input_info": [
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8]. let\n    b:f32[2,4] = reshape[dimensions=None new_sizes=(2, 4) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8xf32>) -> (tensor<2x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<8xf32>) -> tensor<2x4xf32>\n    return %0 : tensor<2x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8] b:f32[8]. let\n    c:f32[16] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8xf32>, %arg1: tensor<8xf32>) -> (tensor<16xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<8xf32>, tensor<8xf32>) -> tensor<16xf32>\n    return %0 : tensor<16xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>) -> (tensor<5xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5]. let\n    b:f32[4] = slice[limit_indices=(5,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:5] : (tensor<5xf32>) -> tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2,))",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>) -> (tensor<2xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 210))",
    "input_info": [
      {
        "shape": [
          10,
          7,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,7,6]. let\n    b:f32[2,210] = reshape[dimensions=None new_sizes=(2, 210) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x7x6xf32>) -> (tensor<2x210xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<10x7x6xf32>) -> tensor<2x210xf32>\n    return %0 : tensor<2x210xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7] b:f32[7]. let\n    c:f32[14] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>, %arg1: tensor<7xf32>) -> (tensor<14xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<7xf32>, tensor<7xf32>) -> tensor<14xf32>\n    return %0 : tensor<14xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          2,
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,5] b:f32[2,5]. let\n    c:f32[1,2,5] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 2, 5)\n      sharding=None\n    ] a\n    d:f32[1,2,5] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 2, 5)\n      sharding=None\n    ] b\n    e:f32[2,2,5] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x5xf32>, %arg1: tensor<2x5xf32>) -> (tensor<2x2x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<2x5xf32>) -> tensor<1x2x5xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<2x5xf32>) -> tensor<1x2x5xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x2x5xf32>, tensor<1x2x5xf32>) -> tensor<2x2x5xf32>\n    return %2 : tensor<2x2x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (320,))",
    "input_info": [
      {
        "shape": [
          4,
          10,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,10,8]. let\n    b:f32[320] = reshape[dimensions=None new_sizes=(320,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x10x8xf32>) -> (tensor<320xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<4x10x8xf32>) -> tensor<320xf32>\n    return %0 : tensor<320xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (300,))",
    "input_info": [
      {
        "shape": [
          3,
          10,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,10,10]. let\n    b:f32[300] = reshape[dimensions=None new_sizes=(300,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x10x10xf32>) -> (tensor<300xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<3x10x10xf32>) -> tensor<300xf32>\n    return %0 : tensor<300xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          5,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,10]. let\n    b:f32[10,5] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x10xf32>) -> (tensor<10x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<5x10xf32>) -> tensor<10x5xf32>\n    return %0 : tensor<10x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          8,
          9,
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          9,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,9,4] b:f32[8,9,4]. let\n    c:f32[1,8,9,4] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 8, 9, 4)\n      sharding=None\n    ] a\n    d:f32[1,8,9,4] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 8, 9, 4)\n      sharding=None\n    ] b\n    e:f32[2,8,9,4] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x9x4xf32>, %arg1: tensor<8x9x4xf32>) -> (tensor<2x8x9x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<8x9x4xf32>) -> tensor<1x8x9x4xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<8x9x4xf32>) -> tensor<1x8x9x4xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x8x9x4xf32>, tensor<1x8x9x4xf32>) -> tensor<2x8x9x4xf32>\n    return %2 : tensor<2x8x9x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          6,
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,3] b:f32[6,3]. let\n    c:f32[1,6,3] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 6, 3)\n      sharding=None\n    ] a\n    d:f32[1,6,3] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 6, 3)\n      sharding=None\n    ] b\n    e:f32[2,6,3] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x3xf32>, %arg1: tensor<6x3xf32>) -> (tensor<2x6x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<6x3xf32>) -> tensor<1x6x3xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<6x3xf32>) -> tensor<1x6x3xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x6x3xf32>, tensor<1x6x3xf32>) -> tensor<2x6x3xf32>\n    return %2 : tensor<2x6x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7] b:f32[7]. let\n    c:f32[1,7] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 7)\n      sharding=None\n    ] a\n    d:f32[1,7] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 7)\n      sharding=None\n    ] b\n    e:f32[2,7] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>, %arg1: tensor<7xf32>) -> (tensor<2x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<7xf32>) -> tensor<1x7xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<7xf32>) -> tensor<1x7xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x7xf32>, tensor<1x7xf32>) -> tensor<2x7xf32>\n    return %2 : tensor<2x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          6,
          7,
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6,
          7,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,7,2] b:f32[6,7,2]. let\n    c:f32[12,7,2] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x7x2xf32>, %arg1: tensor<6x7x2xf32>) -> (tensor<12x7x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<6x7x2xf32>, tensor<6x7x2xf32>) -> tensor<12x7x2xf32>\n    return %0 : tensor<12x7x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 5))",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10]. let\n    b:f32[2,5] = reshape[dimensions=None new_sizes=(2, 5) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<2x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<10xf32>) -> tensor<2x5xf32>\n    return %0 : tensor<2x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4] b:f32[4]. let\n    c:f32[8] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> (tensor<8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<4xf32>, tensor<4xf32>) -> tensor<8xf32>\n    return %0 : tensor<8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          6,
          2,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,2,2]. let\n    b:f32[2,2,6] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x2x2xf32>) -> (tensor<2x2x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<6x2x2xf32>) -> tensor<2x2x6xf32>\n    return %0 : tensor<2x2x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          5,
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,6] b:f32[5,6]. let\n    c:f32[10,6] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x6xf32>, %arg1: tensor<5x6xf32>) -> (tensor<10x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<5x6xf32>, tensor<5x6xf32>) -> tensor<10x6xf32>\n    return %0 : tensor<10x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          9,
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,5] b:f32[9,5]. let\n    c:f32[18,5] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x5xf32>, %arg1: tensor<9x5xf32>) -> (tensor<18x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<9x5xf32>, tensor<9x5xf32>) -> tensor<18x5xf32>\n    return %0 : tensor<18x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          10,
          3,
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10,
          3,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,3,5] b:f32[10,3,5]. let\n    c:f32[1,10,3,5] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 10, 3, 5)\n      sharding=None\n    ] a\n    d:f32[1,10,3,5] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 10, 3, 5)\n      sharding=None\n    ] b\n    e:f32[2,10,3,5] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x3x5xf32>, %arg1: tensor<10x3x5xf32>) -> (tensor<2x10x3x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<10x3x5xf32>) -> tensor<1x10x3x5xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<10x3x5xf32>) -> tensor<1x10x3x5xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x10x3x5xf32>, tensor<1x10x3x5xf32>) -> tensor<2x10x3x5xf32>\n    return %2 : tensor<2x10x3x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6] b:f32[6]. let\n    c:f32[1,6] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 6)\n      sharding=None\n    ] a\n    d:f32[1,6] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 6)\n      sharding=None\n    ] b\n    e:f32[2,6] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>, %arg1: tensor<6xf32>) -> (tensor<2x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<6xf32>) -> tensor<1x6xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<6xf32>) -> tensor<1x6xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x6xf32>, tensor<1x6xf32>) -> tensor<2x6xf32>\n    return %2 : tensor<2x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7] b:f32[7]. let\n    c:f32[1,7] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 7)\n      sharding=None\n    ] a\n    d:f32[1,7] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 7)\n      sharding=None\n    ] b\n    e:f32[2,7] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>, %arg1: tensor<7xf32>) -> (tensor<2x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<7xf32>) -> tensor<1x7xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<7xf32>) -> tensor<1x7xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x7xf32>, tensor<1x7xf32>) -> tensor<2x7xf32>\n    return %2 : tensor<2x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          4,
          7,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,7,8]. let\n    b:f32[8,7,4] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x7x8xf32>) -> (tensor<8x7x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<4x7x8xf32>) -> tensor<8x7x4xf32>\n    return %0 : tensor<8x7x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9]. let\n    b:f32[8] = slice[limit_indices=(9,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>) -> (tensor<8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:9] : (tensor<9xf32>) -> tensor<8xf32>\n    return %0 : tensor<8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          5,
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,2] b:f32[5,2]. let\n    c:f32[1,5,2] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 5, 2)\n      sharding=None\n    ] a\n    d:f32[1,5,2] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 5, 2)\n      sharding=None\n    ] b\n    e:f32[2,5,2] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x2xf32>, %arg1: tensor<5x2xf32>) -> (tensor<2x5x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<5x2xf32>) -> tensor<1x5x2xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<5x2xf32>) -> tensor<1x5x2xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x5x2xf32>, tensor<1x5x2xf32>) -> tensor<2x5x2xf32>\n    return %2 : tensor<2x5x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (3, 7))",
    "input_info": [
      {
        "shape": [
          7,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,3]. let\n    b:f32[3,7] = reshape[dimensions=None new_sizes=(3, 7) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x3xf32>) -> (tensor<3x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<7x3xf32>) -> tensor<3x7xf32>\n    return %0 : tensor<3x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10] b:f32[10]. let\n    c:f32[20] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>, %arg1: tensor<10xf32>) -> (tensor<20xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<10xf32>, tensor<10xf32>) -> tensor<20xf32>\n    return %0 : tensor<20xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          2,
          10,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,10,7]. let\n    b:f32[7,10,2] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x10x7xf32>) -> (tensor<7x10x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<2x10x7xf32>) -> tensor<7x10x2xf32>\n    return %0 : tensor<7x10x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          7,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,3]. let\n    b:f32[3,7] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x3xf32>) -> (tensor<3x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<7x3xf32>) -> tensor<3x7xf32>\n    return %0 : tensor<3x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          7,
          4,
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7,
          4,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,4,3] b:f32[7,4,3]. let\n    c:f32[14,4,3] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x4x3xf32>, %arg1: tensor<7x4x3xf32>) -> (tensor<14x4x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<7x4x3xf32>, tensor<7x4x3xf32>) -> tensor<14x4x3xf32>\n    return %0 : tensor<14x4x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          7,
          4,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,4,3]. let\n    b:f32[3,4,7] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x4x3xf32>) -> (tensor<3x4x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<7x4x3xf32>) -> tensor<3x4x7xf32>\n    return %0 : tensor<3x4x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          5,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,10]. let\n    b:f32[10,5] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x10xf32>) -> (tensor<10x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<5x10xf32>) -> tensor<10x5xf32>\n    return %0 : tensor<10x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          9,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,6]. let\n    b:f32[8,6] = slice[limit_indices=(9, 6) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x6xf32>) -> (tensor<8x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:9, 0:6] : (tensor<9x6xf32>) -> tensor<8x6xf32>\n    return %0 : tensor<8x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          7,
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,9] b:f32[7,9]. let\n    c:f32[14,9] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x9xf32>, %arg1: tensor<7x9xf32>) -> (tensor<14x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<7x9xf32>, tensor<7x9xf32>) -> tensor<14x9xf32>\n    return %0 : tensor<14x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          6,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,8]. let\n    b:f32[8,6] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x8xf32>) -> (tensor<8x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<6x8xf32>) -> tensor<8x6xf32>\n    return %0 : tensor<8x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7] b:f32[7]. let\n    c:f32[1,7] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 7)\n      sharding=None\n    ] a\n    d:f32[1,7] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 7)\n      sharding=None\n    ] b\n    e:f32[2,7] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>, %arg1: tensor<7xf32>) -> (tensor<2x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<7xf32>) -> tensor<1x7xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<7xf32>) -> tensor<1x7xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x7xf32>, tensor<1x7xf32>) -> tensor<2x7xf32>\n    return %2 : tensor<2x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          3,
          10,
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3,
          10,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,10,10] b:f32[3,10,10]. let\n    c:f32[1,3,10,10] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 3, 10, 10)\n      sharding=None\n    ] a\n    d:f32[1,3,10,10] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 3, 10, 10)\n      sharding=None\n    ] b\n    e:f32[2,3,10,10] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x10x10xf32>, %arg1: tensor<3x10x10xf32>) -> (tensor<2x3x10x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<3x10x10xf32>) -> tensor<1x3x10x10xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<3x10x10xf32>) -> tensor<1x3x10x10xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x3x10x10xf32>, tensor<1x3x10x10xf32>) -> tensor<2x3x10x10xf32>\n    return %2 : tensor<2x3x10x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          8,
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,10] b:f32[8,10]. let\n    c:f32[16,10] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x10xf32>, %arg1: tensor<8x10xf32>) -> (tensor<16x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<8x10xf32>, tensor<8x10xf32>) -> tensor<16x10xf32>\n    return %0 : tensor<16x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          4,
          7,
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4,
          7,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,7,5] b:f32[4,7,5]. let\n    c:f32[1,4,7,5] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 4, 7, 5)\n      sharding=None\n    ] a\n    d:f32[1,4,7,5] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 4, 7, 5)\n      sharding=None\n    ] b\n    e:f32[2,4,7,5] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x7x5xf32>, %arg1: tensor<4x7x5xf32>) -> (tensor<2x4x7x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<4x7x5xf32>) -> tensor<1x4x7x5xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<4x7x5xf32>) -> tensor<1x4x7x5xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x4x7x5xf32>, tensor<1x4x7x5xf32>) -> tensor<2x4x7x5xf32>\n    return %2 : tensor<2x4x7x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (32,))",
    "input_info": [
      {
        "shape": [
          8,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,4]. let\n    b:f32[32] = reshape[dimensions=None new_sizes=(32,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x4xf32>) -> (tensor<32xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<8x4xf32>) -> tensor<32xf32>\n    return %0 : tensor<32xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          9,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,8]. let\n    b:f32[8,9] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x8xf32>) -> (tensor<8x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<9x8xf32>) -> tensor<8x9xf32>\n    return %0 : tensor<8x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (80,))",
    "input_info": [
      {
        "shape": [
          10,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,8]. let\n    b:f32[80] = reshape[dimensions=None new_sizes=(80,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x8xf32>) -> (tensor<80xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<10x8xf32>) -> tensor<80xf32>\n    return %0 : tensor<80xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (192,))",
    "input_info": [
      {
        "shape": [
          8,
          6,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,6,4]. let\n    b:f32[192] = reshape[dimensions=None new_sizes=(192,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x6x4xf32>) -> (tensor<192xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<8x6x4xf32>) -> tensor<192xf32>\n    return %0 : tensor<192xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          2,
          6,
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2,
          6,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,6,9] b:f32[2,6,9]. let\n    c:f32[1,2,6,9] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 2, 6, 9)\n      sharding=None\n    ] a\n    d:f32[1,2,6,9] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 2, 6, 9)\n      sharding=None\n    ] b\n    e:f32[2,2,6,9] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x6x9xf32>, %arg1: tensor<2x6x9xf32>) -> (tensor<2x2x6x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<2x6x9xf32>) -> tensor<1x2x6x9xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<2x6x9xf32>) -> tensor<1x2x6x9xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x2x6x9xf32>, tensor<1x2x6x9xf32>) -> tensor<2x2x6x9xf32>\n    return %2 : tensor<2x2x6x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          3,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,5]. let\n    b:f32[2,5] = slice[limit_indices=(3, 5) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x5xf32>) -> (tensor<2x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:3, 0:5] : (tensor<3x5xf32>) -> tensor<2x5xf32>\n    return %0 : tensor<2x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          9,
          6,
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9,
          6,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,6,2] b:f32[9,6,2]. let\n    c:f32[18,6,2] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x6x2xf32>, %arg1: tensor<9x6x2xf32>) -> (tensor<18x6x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<9x6x2xf32>, tensor<9x6x2xf32>) -> tensor<18x6x2xf32>\n    return %0 : tensor<18x6x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          4,
          4,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,4,5]. let\n    b:f32[3,4,5] = slice[\n      limit_indices=(4, 4, 5)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x4x5xf32>) -> (tensor<3x4x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:4, 0:4, 0:5] : (tensor<4x4x5xf32>) -> tensor<3x4x5xf32>\n    return %0 : tensor<3x4x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          6,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,3]. let\n    b:f32[5,3] = slice[limit_indices=(6, 3) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x3xf32>) -> (tensor<5x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:6, 0:3] : (tensor<6x3xf32>) -> tensor<5x3xf32>\n    return %0 : tensor<5x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 350))",
    "input_info": [
      {
        "shape": [
          10,
          10,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,10,7]. let\n    b:f32[2,350] = reshape[dimensions=None new_sizes=(2, 350) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x10x7xf32>) -> (tensor<2x350xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<10x10x7xf32>) -> tensor<2x350xf32>\n    return %0 : tensor<2x350xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          2,
          3,
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2,
          3,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,3,5] b:f32[2,3,5]. let\n    c:f32[4,3,5] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x3x5xf32>, %arg1: tensor<2x3x5xf32>) -> (tensor<4x3x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<2x3x5xf32>, tensor<2x3x5xf32>) -> tensor<4x3x5xf32>\n    return %0 : tensor<4x3x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10] b:f32[10]. let\n    c:f32[20] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>, %arg1: tensor<10xf32>) -> (tensor<20xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<10xf32>, tensor<10xf32>) -> tensor<20xf32>\n    return %0 : tensor<20xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 6))",
    "input_info": [
      {
        "shape": [
          3,
          2,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,2,2]. let\n    b:f32[2,6] = reshape[dimensions=None new_sizes=(2, 6) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x2x2xf32>) -> (tensor<2x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<3x2x2xf32>) -> tensor<2x6xf32>\n    return %0 : tensor<2x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          6,
          8,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,8,10]. let\n    b:f32[5,8,10] = slice[\n      limit_indices=(6, 8, 10)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x8x10xf32>) -> (tensor<5x8x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:6, 0:8, 0:10] : (tensor<6x8x10xf32>) -> tensor<5x8x10xf32>\n    return %0 : tensor<5x8x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (7,))",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>) -> (tensor<7xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          9,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,4]. let\n    b:f32[8,4] = slice[limit_indices=(9, 4) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x4xf32>) -> (tensor<8x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:9, 0:4] : (tensor<9x4xf32>) -> tensor<8x4xf32>\n    return %0 : tensor<8x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          8,
          5,
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          5,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,5,2] b:f32[8,5,2]. let\n    c:f32[1,8,5,2] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 8, 5, 2)\n      sharding=None\n    ] a\n    d:f32[1,8,5,2] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 8, 5, 2)\n      sharding=None\n    ] b\n    e:f32[2,8,5,2] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x5x2xf32>, %arg1: tensor<8x5x2xf32>) -> (tensor<2x8x5x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<8x5x2xf32>) -> tensor<1x8x5x2xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<8x5x2xf32>) -> tensor<1x8x5x2xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x8x5x2xf32>, tensor<1x8x5x2xf32>) -> tensor<2x8x5x2xf32>\n    return %2 : tensor<2x8x5x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8xf32>) -> (tensor<8xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 20))",
    "input_info": [
      {
        "shape": [
          2,
          2,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,2,10]. let\n    b:f32[2,20] = reshape[dimensions=None new_sizes=(2, 20) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x2x10xf32>) -> (tensor<2x20xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<2x2x10xf32>) -> tensor<2x20xf32>\n    return %0 : tensor<2x20xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          9,
          9,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,9,5]. let\n    b:f32[5,9,9] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x9x5xf32>) -> (tensor<5x9x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<9x9x5xf32>) -> tensor<5x9x9xf32>\n    return %0 : tensor<5x9x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3] b:f32[3]. let\n    c:f32[1,3] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 3)\n      sharding=None\n    ] a\n    d:f32[1,3] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 3)\n      sharding=None\n    ] b\n    e:f32[2,3] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>, %arg1: tensor<3xf32>) -> (tensor<2x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<3xf32>) -> tensor<1x3xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<3xf32>) -> tensor<1x3xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x3xf32>, tensor<1x3xf32>) -> tensor<2x3xf32>\n    return %2 : tensor<2x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          9,
          2,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,2,3]. let\n    b:f32[3,2,9] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x2x3xf32>) -> (tensor<3x2x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<9x2x3xf32>) -> tensor<3x2x9xf32>\n    return %0 : tensor<3x2x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          3,
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,7] b:f32[3,7]. let\n    c:f32[1,3,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 3, 7)\n      sharding=None\n    ] a\n    d:f32[1,3,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 3, 7)\n      sharding=None\n    ] b\n    e:f32[2,3,7] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x7xf32>, %arg1: tensor<3x7xf32>) -> (tensor<2x3x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<3x7xf32>) -> tensor<1x3x7xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<3x7xf32>) -> tensor<1x3x7xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x3x7xf32>, tensor<1x3x7xf32>) -> tensor<2x3x7xf32>\n    return %2 : tensor<2x3x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6]. let\n    b:f32[5] = slice[limit_indices=(6,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>) -> (tensor<5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:6] : (tensor<6xf32>) -> tensor<5xf32>\n    return %0 : tensor<5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          4,
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,10] b:f32[4,10]. let\n    c:f32[8,10] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x10xf32>, %arg1: tensor<4x10xf32>) -> (tensor<8x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<4x10xf32>, tensor<4x10xf32>) -> tensor<8x10xf32>\n    return %0 : tensor<8x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 25))",
    "input_info": [
      {
        "shape": [
          5,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,10]. let\n    b:f32[2,25] = reshape[dimensions=None new_sizes=(2, 25) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x10xf32>) -> (tensor<2x25xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<5x10xf32>) -> tensor<2x25xf32>\n    return %0 : tensor<2x25xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          8,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,8]. let\n    b:f32[7,8] = slice[limit_indices=(8, 8) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x8xf32>) -> (tensor<7x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:8, 0:8] : (tensor<8x8xf32>) -> tensor<7x8xf32>\n    return %0 : tensor<7x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>) -> (tensor<9xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 72))",
    "input_info": [
      {
        "shape": [
          9,
          4,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,4,4]. let\n    b:f32[2,72] = reshape[dimensions=None new_sizes=(2, 72) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x4x4xf32>) -> (tensor<2x72xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<9x4x4xf32>) -> tensor<2x72xf32>\n    return %0 : tensor<2x72xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3] b:f32[3]. let\n    c:f32[6] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>, %arg1: tensor<3xf32>) -> (tensor<6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<3xf32>, tensor<3xf32>) -> tensor<6xf32>\n    return %0 : tensor<6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          8,
          9,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,9,6]. let\n    b:f32[6,9,8] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x9x6xf32>) -> (tensor<6x9x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<8x9x6xf32>) -> tensor<6x9x8xf32>\n    return %0 : tensor<6x9x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9] b:f32[9]. let\n    c:f32[18] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>, %arg1: tensor<9xf32>) -> (tensor<18xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<9xf32>, tensor<9xf32>) -> tensor<18xf32>\n    return %0 : tensor<18xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2]. let\n    b:f32[1] = slice[limit_indices=(2,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>) -> (tensor<1xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:2] : (tensor<2xf32>) -> tensor<1xf32>\n    return %0 : tensor<1xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          6,
          6,
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6,
          6,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,6,9] b:f32[6,6,9]. let\n    c:f32[12,6,9] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x6x9xf32>, %arg1: tensor<6x6x9xf32>) -> (tensor<12x6x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<6x6x9xf32>, tensor<6x6x9xf32>) -> tensor<12x6x9xf32>\n    return %0 : tensor<12x6x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          6,
          3,
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6,
          3,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,3,9] b:f32[6,3,9]. let\n    c:f32[12,3,9] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x3x9xf32>, %arg1: tensor<6x3x9xf32>) -> (tensor<12x3x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<6x3x9xf32>, tensor<6x3x9xf32>) -> tensor<12x3x9xf32>\n    return %0 : tensor<12x3x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          2,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,8]. let\n    b:f32[8,2] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x8xf32>) -> (tensor<8x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<2x8xf32>) -> tensor<8x2xf32>\n    return %0 : tensor<8x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (14,))",
    "input_info": [
      {
        "shape": [
          2,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,7]. let\n    b:f32[14] = reshape[dimensions=None new_sizes=(14,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x7xf32>) -> (tensor<14xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<2x7xf32>) -> tensor<14xf32>\n    return %0 : tensor<14xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3] b:f32[3]. let\n    c:f32[6] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>, %arg1: tensor<3xf32>) -> (tensor<6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<3xf32>, tensor<3xf32>) -> tensor<6xf32>\n    return %0 : tensor<6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (6,))",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>) -> (tensor<6xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          8,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,7]. let\n    b:f32[7,8] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x7xf32>) -> (tensor<7x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<8x7xf32>) -> tensor<7x8xf32>\n    return %0 : tensor<7x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 84))",
    "input_info": [
      {
        "shape": [
          7,
          8,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,8,3]. let\n    b:f32[2,84] = reshape[dimensions=None new_sizes=(2, 84) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x8x3xf32>) -> (tensor<2x84xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<7x8x3xf32>) -> tensor<2x84xf32>\n    return %0 : tensor<2x84xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          4,
          5,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,5,8]. let\n    b:f32[3,5,8] = slice[\n      limit_indices=(4, 5, 8)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x5x8xf32>) -> (tensor<3x5x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:4, 0:5, 0:8] : (tensor<4x5x8xf32>) -> tensor<3x5x8xf32>\n    return %0 : tensor<3x5x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          10,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,4]. let\n    b:f32[9,4] = slice[limit_indices=(10, 4) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x4xf32>) -> (tensor<9x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:10, 0:4] : (tensor<10x4xf32>) -> tensor<9x4xf32>\n    return %0 : tensor<9x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7]. let\n    b:f32[6] = slice[limit_indices=(7,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>) -> (tensor<6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:7] : (tensor<7xf32>) -> tensor<6xf32>\n    return %0 : tensor<6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          3,
          7,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,7,5]. let\n    b:f32[2,7,5] = slice[\n      limit_indices=(3, 7, 5)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x7x5xf32>) -> (tensor<2x7x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:3, 0:7, 0:5] : (tensor<3x7x5xf32>) -> tensor<2x7x5xf32>\n    return %0 : tensor<2x7x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          5,
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,7] b:f32[5,7]. let\n    c:f32[10,7] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x7xf32>, %arg1: tensor<5x7xf32>) -> (tensor<10x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<5x7xf32>, tensor<5x7xf32>) -> tensor<10x7xf32>\n    return %0 : tensor<10x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (7,))",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>) -> (tensor<7xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          4,
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,2] b:f32[4,2]. let\n    c:f32[8,2] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x2xf32>, %arg1: tensor<4x2xf32>) -> (tensor<8x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<4x2xf32>, tensor<4x2xf32>) -> tensor<8x2xf32>\n    return %0 : tensor<8x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (3, 3))",
    "input_info": [
      {
        "shape": [
          3,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,3]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x3xf32>) -> (tensor<3x3xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<3x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          4,
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,7] b:f32[4,7]. let\n    c:f32[8,7] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x7xf32>, %arg1: tensor<4x7xf32>) -> (tensor<8x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<4x7xf32>, tensor<4x7xf32>) -> tensor<8x7xf32>\n    return %0 : tensor<8x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          6,
          10,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,10,8]. let\n    b:f32[8,10,6] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x10x8xf32>) -> (tensor<8x10x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<6x10x8xf32>) -> tensor<8x10x6xf32>\n    return %0 : tensor<8x10x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6] b:f32[6]. let\n    c:f32[1,6] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 6)\n      sharding=None\n    ] a\n    d:f32[1,6] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 6)\n      sharding=None\n    ] b\n    e:f32[2,6] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>, %arg1: tensor<6xf32>) -> (tensor<2x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<6xf32>) -> tensor<1x6xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<6xf32>) -> tensor<1x6xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x6xf32>, tensor<1x6xf32>) -> tensor<2x6xf32>\n    return %2 : tensor<2x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5] b:f32[5]. let\n    c:f32[10] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>, %arg1: tensor<5xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<5xf32>, tensor<5xf32>) -> tensor<10xf32>\n    return %0 : tensor<10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (3, 45))",
    "input_info": [
      {
        "shape": [
          9,
          3,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,3,5]. let\n    b:f32[3,45] = reshape[dimensions=None new_sizes=(3, 45) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x3x5xf32>) -> (tensor<3x45xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<9x3x5xf32>) -> tensor<3x45xf32>\n    return %0 : tensor<3x45xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          10,
          10,
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10,
          10,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,10,8] b:f32[10,10,8]. let\n    c:f32[1,10,10,8] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 10, 10, 8)\n      sharding=None\n    ] a\n    d:f32[1,10,10,8] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 10, 10, 8)\n      sharding=None\n    ] b\n    e:f32[2,10,10,8] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x10x8xf32>, %arg1: tensor<10x10x8xf32>) -> (tensor<2x10x10x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<10x10x8xf32>) -> tensor<1x10x10x8xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<10x10x8xf32>) -> tensor<1x10x10x8xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x10x10x8xf32>, tensor<1x10x10x8xf32>) -> tensor<2x10x10x8xf32>\n    return %2 : tensor<2x10x10x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          7,
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,5] b:f32[7,5]. let\n    c:f32[1,7,5] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 7, 5)\n      sharding=None\n    ] a\n    d:f32[1,7,5] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 7, 5)\n      sharding=None\n    ] b\n    e:f32[2,7,5] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x5xf32>, %arg1: tensor<7x5xf32>) -> (tensor<2x7x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<7x5xf32>) -> tensor<1x7x5xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<7x5xf32>) -> tensor<1x7x5xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x7x5xf32>, tensor<1x7x5xf32>) -> tensor<2x7x5xf32>\n    return %2 : tensor<2x7x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          4,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,5]. let\n    b:f32[5,4] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x5xf32>) -> (tensor<5x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<4x5xf32>) -> tensor<5x4xf32>\n    return %0 : tensor<5x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          9,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,5]. let\n    b:f32[8,5] = slice[limit_indices=(9, 5) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x5xf32>) -> (tensor<8x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:9, 0:5] : (tensor<9x5xf32>) -> tensor<8x5xf32>\n    return %0 : tensor<8x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (120,))",
    "input_info": [
      {
        "shape": [
          10,
          2,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,2,6]. let\n    b:f32[120] = reshape[dimensions=None new_sizes=(120,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x2x6xf32>) -> (tensor<120xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<10x2x6xf32>) -> tensor<120xf32>\n    return %0 : tensor<120xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          5,
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,7] b:f32[5,7]. let\n    c:f32[1,5,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 5, 7)\n      sharding=None\n    ] a\n    d:f32[1,5,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 5, 7)\n      sharding=None\n    ] b\n    e:f32[2,5,7] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x7xf32>, %arg1: tensor<5x7xf32>) -> (tensor<2x5x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<5x7xf32>) -> tensor<1x5x7xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<5x7xf32>) -> tensor<1x5x7xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x5x7xf32>, tensor<1x5x7xf32>) -> tensor<2x5x7xf32>\n    return %2 : tensor<2x5x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          8,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,10]. let\n    b:f32[7,10] = slice[limit_indices=(8, 10) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x10xf32>) -> (tensor<7x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:8, 0:10] : (tensor<8x10xf32>) -> tensor<7x10xf32>\n    return %0 : tensor<7x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          10,
          8,
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10,
          8,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,8,6] b:f32[10,8,6]. let\n    c:f32[20,8,6] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x8x6xf32>, %arg1: tensor<10x8x6xf32>) -> (tensor<20x8x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<10x8x6xf32>, tensor<10x8x6xf32>) -> tensor<20x8x6xf32>\n    return %0 : tensor<20x8x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          9,
          10,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,10,5]. let\n    b:f32[5,10,9] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x10x5xf32>) -> (tensor<5x10x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<9x10x5xf32>) -> tensor<5x10x9xf32>\n    return %0 : tensor<5x10x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          9,
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,4] b:f32[9,4]. let\n    c:f32[1,9,4] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 9, 4)\n      sharding=None\n    ] a\n    d:f32[1,9,4] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 9, 4)\n      sharding=None\n    ] b\n    e:f32[2,9,4] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x4xf32>, %arg1: tensor<9x4xf32>) -> (tensor<2x9x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<9x4xf32>) -> tensor<1x9x4xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<9x4xf32>) -> tensor<1x9x4xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x9x4xf32>, tensor<1x9x4xf32>) -> tensor<2x9x4xf32>\n    return %2 : tensor<2x9x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          7,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,8]. let\n    b:f32[6,8] = slice[limit_indices=(7, 8) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x8xf32>) -> (tensor<6x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:7, 0:8] : (tensor<7x8xf32>) -> tensor<6x8xf32>\n    return %0 : tensor<6x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          7,
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,10] b:f32[7,10]. let\n    c:f32[1,7,10] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 7, 10)\n      sharding=None\n    ] a\n    d:f32[1,7,10] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 7, 10)\n      sharding=None\n    ] b\n    e:f32[2,7,10] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x10xf32>, %arg1: tensor<7x10xf32>) -> (tensor<2x7x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<7x10xf32>) -> tensor<1x7x10xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<7x10xf32>) -> tensor<1x7x10xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x7x10xf32>, tensor<1x7x10xf32>) -> tensor<2x7x10xf32>\n    return %2 : tensor<2x7x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (3,))",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>) -> (tensor<3xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          6,
          8,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,8,7]. let\n    b:f32[7,8,6] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x8x7xf32>) -> (tensor<7x8x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<6x8x7xf32>) -> tensor<7x8x6xf32>\n    return %0 : tensor<7x8x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          8,
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,7] b:f32[8,7]. let\n    c:f32[1,8,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 8, 7)\n      sharding=None\n    ] a\n    d:f32[1,8,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 8, 7)\n      sharding=None\n    ] b\n    e:f32[2,8,7] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x7xf32>, %arg1: tensor<8x7xf32>) -> (tensor<2x8x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<8x7xf32>) -> tensor<1x8x7xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<8x7xf32>) -> tensor<1x8x7xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x8x7xf32>, tensor<1x8x7xf32>) -> tensor<2x8x7xf32>\n    return %2 : tensor<2x8x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          2,
          2,
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2,
          2,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,2,4] b:f32[2,2,4]. let\n    c:f32[1,2,2,4] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 2, 2, 4)\n      sharding=None\n    ] a\n    d:f32[1,2,2,4] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 2, 2, 4)\n      sharding=None\n    ] b\n    e:f32[2,2,2,4] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x2x4xf32>, %arg1: tensor<2x2x4xf32>) -> (tensor<2x2x2x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<2x2x4xf32>) -> tensor<1x2x2x4xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<2x2x4xf32>) -> tensor<1x2x2x4xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x2x2x4xf32>, tensor<1x2x2x4xf32>) -> tensor<2x2x2x4xf32>\n    return %2 : tensor<2x2x2x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10]. let\n    b:f32[9] = slice[limit_indices=(10,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:10] : (tensor<10xf32>) -> tensor<9xf32>\n    return %0 : tensor<9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          9,
          5,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,5,2]. let\n    b:f32[8,5,2] = slice[\n      limit_indices=(9, 5, 2)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x5x2xf32>) -> (tensor<8x5x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:9, 0:5, 0:2] : (tensor<9x5x2xf32>) -> tensor<8x5x2xf32>\n    return %0 : tensor<8x5x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (6,))",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>) -> (tensor<6xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7] b:f32[7]. let\n    c:f32[14] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>, %arg1: tensor<7xf32>) -> (tensor<14xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<7xf32>, tensor<7xf32>) -> tensor<14xf32>\n    return %0 : tensor<14xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          2,
          2,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,2,7]. let\n    b:f32[7,2,2] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x2x7xf32>) -> (tensor<7x2x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<2x2x7xf32>) -> tensor<7x2x2xf32>\n    return %0 : tensor<7x2x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          6,
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,2] b:f32[6,2]. let\n    c:f32[1,6,2] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 6, 2)\n      sharding=None\n    ] a\n    d:f32[1,6,2] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 6, 2)\n      sharding=None\n    ] b\n    e:f32[2,6,2] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x2xf32>, %arg1: tensor<6x2xf32>) -> (tensor<2x6x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<6x2xf32>) -> tensor<1x6x2xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<6x2xf32>) -> tensor<1x6x2xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x6x2xf32>, tensor<1x6x2xf32>) -> tensor<2x6x2xf32>\n    return %2 : tensor<2x6x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (400,))",
    "input_info": [
      {
        "shape": [
          10,
          8,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,8,5]. let\n    b:f32[400] = reshape[dimensions=None new_sizes=(400,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x8x5xf32>) -> (tensor<400xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<10x8x5xf32>) -> tensor<400xf32>\n    return %0 : tensor<400xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          8,
          10,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,10,10]. let\n    b:f32[10,10,8] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x10x10xf32>) -> (tensor<10x10x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<8x10x10xf32>) -> tensor<10x10x8xf32>\n    return %0 : tensor<10x10x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (36,))",
    "input_info": [
      {
        "shape": [
          6,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,6]. let\n    b:f32[36] = reshape[dimensions=None new_sizes=(36,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x6xf32>) -> (tensor<36xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<6x6xf32>) -> tensor<36xf32>\n    return %0 : tensor<36xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          3,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,7]. let\n    b:f32[2,7] = slice[limit_indices=(3, 7) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x7xf32>) -> (tensor<2x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:3, 0:7] : (tensor<3x7xf32>) -> tensor<2x7xf32>\n    return %0 : tensor<2x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (60,))",
    "input_info": [
      {
        "shape": [
          2,
          10,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,10,3]. let\n    b:f32[60] = reshape[dimensions=None new_sizes=(60,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x10x3xf32>) -> (tensor<60xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<2x10x3xf32>) -> tensor<60xf32>\n    return %0 : tensor<60xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          10,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,7]. let\n    b:f32[9,7] = slice[limit_indices=(10, 7) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x7xf32>) -> (tensor<9x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:10, 0:7] : (tensor<10x7xf32>) -> tensor<9x7xf32>\n    return %0 : tensor<9x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          2,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,2]. let\n    b:f32[2,2] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x2xf32>) -> (tensor<2x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<2x2xf32>) -> tensor<2x2xf32>\n    return %0 : tensor<2x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4] b:f32[4]. let\n    c:f32[1,4] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 4)\n      sharding=None\n    ] a\n    d:f32[1,4] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 4)\n      sharding=None\n    ] b\n    e:f32[2,4] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> (tensor<2x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<4xf32>) -> tensor<1x4xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<4xf32>) -> tensor<1x4xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x4xf32>, tensor<1x4xf32>) -> tensor<2x4xf32>\n    return %2 : tensor<2x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (90,))",
    "input_info": [
      {
        "shape": [
          9,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,10]. let\n    b:f32[90] = reshape[dimensions=None new_sizes=(90,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x10xf32>) -> (tensor<90xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<9x10xf32>) -> tensor<90xf32>\n    return %0 : tensor<90xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (5,))",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>) -> (tensor<5xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          8,
          5,
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          5,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,5,4] b:f32[8,5,4]. let\n    c:f32[16,5,4] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x5x4xf32>, %arg1: tensor<8x5x4xf32>) -> (tensor<16x5x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<8x5x4xf32>, tensor<8x5x4xf32>) -> tensor<16x5x4xf32>\n    return %0 : tensor<16x5x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          2,
          9,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,9,9]. let\n    b:f32[1,9,9] = slice[\n      limit_indices=(2, 9, 9)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x9x9xf32>) -> (tensor<1x9x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:2, 0:9, 0:9] : (tensor<2x9x9xf32>) -> tensor<1x9x9xf32>\n    return %0 : tensor<1x9x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9] b:f32[9]. let\n    c:f32[18] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>, %arg1: tensor<9xf32>) -> (tensor<18xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<9xf32>, tensor<9xf32>) -> tensor<18xf32>\n    return %0 : tensor<18xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9]. let\n    b:f32[8] = slice[limit_indices=(9,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>) -> (tensor<8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:9] : (tensor<9xf32>) -> tensor<8xf32>\n    return %0 : tensor<8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9]. let\n    b:f32[8] = slice[limit_indices=(9,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>) -> (tensor<8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:9] : (tensor<9xf32>) -> tensor<8xf32>\n    return %0 : tensor<8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10] b:f32[10]. let\n    c:f32[20] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>, %arg1: tensor<10xf32>) -> (tensor<20xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<10xf32>, tensor<10xf32>) -> tensor<20xf32>\n    return %0 : tensor<20xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3]. let\n    b:f32[2] = slice[limit_indices=(3,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>) -> (tensor<2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:3] : (tensor<3xf32>) -> tensor<2xf32>\n    return %0 : tensor<2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (140,))",
    "input_info": [
      {
        "shape": [
          4,
          7,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,7,5]. let\n    b:f32[140] = reshape[dimensions=None new_sizes=(140,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x7x5xf32>) -> (tensor<140xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<4x7x5xf32>) -> tensor<140xf32>\n    return %0 : tensor<140xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          8,
          3,
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          3,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,3,3] b:f32[8,3,3]. let\n    c:f32[16,3,3] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x3x3xf32>, %arg1: tensor<8x3x3xf32>) -> (tensor<16x3x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<8x3x3xf32>, tensor<8x3x3xf32>) -> tensor<16x3x3xf32>\n    return %0 : tensor<16x3x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          2,
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,9] b:f32[2,9]. let\n    c:f32[4,9] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x9xf32>, %arg1: tensor<2x9xf32>) -> (tensor<4x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<2x9xf32>, tensor<2x9xf32>) -> tensor<4x9xf32>\n    return %0 : tensor<4x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 320))",
    "input_info": [
      {
        "shape": [
          10,
          8,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,8,8]. let\n    b:f32[2,320] = reshape[dimensions=None new_sizes=(2, 320) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x8x8xf32>) -> (tensor<2x320xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<10x8x8xf32>) -> tensor<2x320xf32>\n    return %0 : tensor<2x320xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10]. let\n    b:f32[9] = slice[limit_indices=(10,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:10] : (tensor<10xf32>) -> tensor<9xf32>\n    return %0 : tensor<9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          3,
          10,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,10,3]. let\n    b:f32[2,10,3] = slice[\n      limit_indices=(3, 10, 3)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x10x3xf32>) -> (tensor<2x10x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:3, 0:10, 0:3] : (tensor<3x10x3xf32>) -> tensor<2x10x3xf32>\n    return %0 : tensor<2x10x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          7,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,2]. let\n    b:f32[2,7] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x2xf32>) -> (tensor<2x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<7x2xf32>) -> tensor<2x7xf32>\n    return %0 : tensor<2x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (10,))",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (7,))",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>) -> (tensor<7xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          9,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,3]. let\n    b:f32[8,3] = slice[limit_indices=(9, 3) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x3xf32>) -> (tensor<8x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:9, 0:3] : (tensor<9x3xf32>) -> tensor<8x3xf32>\n    return %0 : tensor<8x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          4,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,5]. let\n    b:f32[5,4] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x5xf32>) -> (tensor<5x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<4x5xf32>) -> tensor<5x4xf32>\n    return %0 : tensor<5x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 30))",
    "input_info": [
      {
        "shape": [
          10,
          3,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,3,2]. let\n    b:f32[2,30] = reshape[dimensions=None new_sizes=(2, 30) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x3x2xf32>) -> (tensor<2x30xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<10x3x2xf32>) -> tensor<2x30xf32>\n    return %0 : tensor<2x30xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          10,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,9]. let\n    b:f32[9,10] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x9xf32>) -> (tensor<9x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<10x9xf32>) -> tensor<9x10xf32>\n    return %0 : tensor<9x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          10,
          8,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,8,8]. let\n    b:f32[8,8,10] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x8x8xf32>) -> (tensor<8x8x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<10x8x8xf32>) -> tensor<8x8x10xf32>\n    return %0 : tensor<8x8x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          8,
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,2] b:f32[8,2]. let\n    c:f32[1,8,2] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 8, 2)\n      sharding=None\n    ] a\n    d:f32[1,8,2] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 8, 2)\n      sharding=None\n    ] b\n    e:f32[2,8,2] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x2xf32>, %arg1: tensor<8x2xf32>) -> (tensor<2x8x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<8x2xf32>) -> tensor<1x8x2xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<8x2xf32>) -> tensor<1x8x2xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x8x2xf32>, tensor<1x8x2xf32>) -> tensor<2x8x2xf32>\n    return %2 : tensor<2x8x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          6,
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,9] b:f32[6,9]. let\n    c:f32[1,6,9] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 6, 9)\n      sharding=None\n    ] a\n    d:f32[1,6,9] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 6, 9)\n      sharding=None\n    ] b\n    e:f32[2,6,9] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x9xf32>, %arg1: tensor<6x9xf32>) -> (tensor<2x6x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<6x9xf32>) -> tensor<1x6x9xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<6x9xf32>) -> tensor<1x6x9xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x6x9xf32>, tensor<1x6x9xf32>) -> tensor<2x6x9xf32>\n    return %2 : tensor<2x6x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (630,))",
    "input_info": [
      {
        "shape": [
          10,
          7,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,7,9]. let\n    b:f32[630] = reshape[dimensions=None new_sizes=(630,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x7x9xf32>) -> (tensor<630xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<10x7x9xf32>) -> tensor<630xf32>\n    return %0 : tensor<630xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>) -> (tensor<6xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          5,
          4,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,4,4]. let\n    b:f32[4,4,5] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x4x4xf32>) -> (tensor<4x4x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<5x4x4xf32>) -> tensor<4x4x5xf32>\n    return %0 : tensor<4x4x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          7,
          6,
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7,
          6,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,6,2] b:f32[7,6,2]. let\n    c:f32[14,6,2] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x6x2xf32>, %arg1: tensor<7x6x2xf32>) -> (tensor<14x6x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<7x6x2xf32>, tensor<7x6x2xf32>) -> tensor<14x6x2xf32>\n    return %0 : tensor<14x6x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (90,))",
    "input_info": [
      {
        "shape": [
          3,
          3,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,3,10]. let\n    b:f32[90] = reshape[dimensions=None new_sizes=(90,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x3x10xf32>) -> (tensor<90xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<3x3x10xf32>) -> tensor<90xf32>\n    return %0 : tensor<90xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          7,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,3]. let\n    b:f32[3,7] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x3xf32>) -> (tensor<3x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<7x3xf32>) -> tensor<3x7xf32>\n    return %0 : tensor<3x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          8,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,4]. let\n    b:f32[7,4] = slice[limit_indices=(8, 4) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x4xf32>) -> (tensor<7x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:8, 0:4] : (tensor<8x4xf32>) -> tensor<7x4xf32>\n    return %0 : tensor<7x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          3,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,5]. let\n    b:f32[5,3] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x5xf32>) -> (tensor<5x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<3x5xf32>) -> tensor<5x3xf32>\n    return %0 : tensor<5x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>) -> (tensor<2xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          7,
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,2] b:f32[7,2]. let\n    c:f32[14,2] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x2xf32>, %arg1: tensor<7x2xf32>) -> (tensor<14x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<7x2xf32>, tensor<7x2xf32>) -> tensor<14x2xf32>\n    return %0 : tensor<14x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          9,
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,7] b:f32[9,7]. let\n    c:f32[18,7] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x7xf32>, %arg1: tensor<9x7xf32>) -> (tensor<18x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<9x7xf32>, tensor<9x7xf32>) -> tensor<18x7xf32>\n    return %0 : tensor<18x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (216,))",
    "input_info": [
      {
        "shape": [
          9,
          3,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,3,8]. let\n    b:f32[216] = reshape[dimensions=None new_sizes=(216,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x3x8xf32>) -> (tensor<216xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<9x3x8xf32>) -> tensor<216xf32>\n    return %0 : tensor<216xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4] b:f32[4]. let\n    c:f32[1,4] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 4)\n      sharding=None\n    ] a\n    d:f32[1,4] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 4)\n      sharding=None\n    ] b\n    e:f32[2,4] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> (tensor<2x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<4xf32>) -> tensor<1x4xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<4xf32>) -> tensor<1x4xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x4xf32>, tensor<1x4xf32>) -> tensor<2x4xf32>\n    return %2 : tensor<2x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (10,))",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          3,
          4,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,4,7]. let\n    b:f32[2,4,7] = slice[\n      limit_indices=(3, 4, 7)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x4x7xf32>) -> (tensor<2x4x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:3, 0:4, 0:7] : (tensor<3x4x7xf32>) -> tensor<2x4x7xf32>\n    return %0 : tensor<2x4x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6] b:f32[6]. let\n    c:f32[12] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>, %arg1: tensor<6xf32>) -> (tensor<12xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<6xf32>, tensor<6xf32>) -> tensor<12xf32>\n    return %0 : tensor<12xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (27,))",
    "input_info": [
      {
        "shape": [
          3,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,9]. let\n    b:f32[27] = reshape[dimensions=None new_sizes=(27,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x9xf32>) -> (tensor<27xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<3x9xf32>) -> tensor<27xf32>\n    return %0 : tensor<27xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (3, 27))",
    "input_info": [
      {
        "shape": [
          9,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,9]. let\n    b:f32[3,27] = reshape[dimensions=None new_sizes=(3, 27) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x9xf32>) -> (tensor<3x27xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<9x9xf32>) -> tensor<3x27xf32>\n    return %0 : tensor<3x27xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          9,
          7,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,7,3]. let\n    b:f32[3,7,9] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x7x3xf32>) -> (tensor<3x7x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<9x7x3xf32>) -> tensor<3x7x9xf32>\n    return %0 : tensor<3x7x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          2,
          9,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,9,8]. let\n    b:f32[1,9,8] = slice[\n      limit_indices=(2, 9, 8)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x9x8xf32>) -> (tensor<1x9x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:2, 0:9, 0:8] : (tensor<2x9x8xf32>) -> tensor<1x9x8xf32>\n    return %0 : tensor<1x9x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          3,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,5]. let\n    b:f32[2,5] = slice[limit_indices=(3, 5) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x5xf32>) -> (tensor<2x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:3, 0:5] : (tensor<3x5xf32>) -> tensor<2x5xf32>\n    return %0 : tensor<2x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          8,
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,8] b:f32[8,8]. let\n    c:f32[16,8] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x8xf32>, %arg1: tensor<8x8xf32>) -> (tensor<16x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<8x8xf32>, tensor<8x8xf32>) -> tensor<16x8xf32>\n    return %0 : tensor<16x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (576,))",
    "input_info": [
      {
        "shape": [
          8,
          9,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,9,8]. let\n    b:f32[576] = reshape[dimensions=None new_sizes=(576,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x9x8xf32>) -> (tensor<576xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<8x9x8xf32>) -> tensor<576xf32>\n    return %0 : tensor<576xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2]. let\n    b:f32[1] = slice[limit_indices=(2,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>) -> (tensor<1xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:2] : (tensor<2xf32>) -> tensor<1xf32>\n    return %0 : tensor<1xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          8,
          4,
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          4,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,4,3] b:f32[8,4,3]. let\n    c:f32[16,4,3] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x4x3xf32>, %arg1: tensor<8x4x3xf32>) -> (tensor<16x4x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<8x4x3xf32>, tensor<8x4x3xf32>) -> tensor<16x4x3xf32>\n    return %0 : tensor<16x4x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (90,))",
    "input_info": [
      {
        "shape": [
          10,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,9]. let\n    b:f32[90] = reshape[dimensions=None new_sizes=(90,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x9xf32>) -> (tensor<90xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<10x9xf32>) -> tensor<90xf32>\n    return %0 : tensor<90xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          5,
          2,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,2,4]. let\n    b:f32[4,2,5] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x2x4xf32>) -> (tensor<4x2x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<5x2x4xf32>) -> tensor<4x2x5xf32>\n    return %0 : tensor<4x2x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          4,
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,7] b:f32[4,7]. let\n    c:f32[8,7] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x7xf32>, %arg1: tensor<4x7xf32>) -> (tensor<8x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<4x7xf32>, tensor<4x7xf32>) -> tensor<8x7xf32>\n    return %0 : tensor<8x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          2,
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,8] b:f32[2,8]. let\n    c:f32[1,2,8] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 2, 8)\n      sharding=None\n    ] a\n    d:f32[1,2,8] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 2, 8)\n      sharding=None\n    ] b\n    e:f32[2,2,8] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x8xf32>, %arg1: tensor<2x8xf32>) -> (tensor<2x2x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<2x8xf32>) -> tensor<1x2x8xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<2x8xf32>) -> tensor<1x2x8xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x2x8xf32>, tensor<1x2x8xf32>) -> tensor<2x2x8xf32>\n    return %2 : tensor<2x2x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>) -> (tensor<7xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          8,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,2]. let\n    b:f32[2,8] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x2xf32>) -> (tensor<2x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<8x2xf32>) -> tensor<2x8xf32>\n    return %0 : tensor<2x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          8,
          5,
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          5,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,5,8] b:f32[8,5,8]. let\n    c:f32[1,8,5,8] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 8, 5, 8)\n      sharding=None\n    ] a\n    d:f32[1,8,5,8] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 8, 5, 8)\n      sharding=None\n    ] b\n    e:f32[2,8,5,8] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x5x8xf32>, %arg1: tensor<8x5x8xf32>) -> (tensor<2x8x5x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<8x5x8xf32>) -> tensor<1x8x5x8xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<8x5x8xf32>) -> tensor<1x8x5x8xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x8x5x8xf32>, tensor<1x8x5x8xf32>) -> tensor<2x8x5x8xf32>\n    return %2 : tensor<2x8x5x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6]. let\n    b:f32[5] = slice[limit_indices=(6,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>) -> (tensor<5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:6] : (tensor<6xf32>) -> tensor<5xf32>\n    return %0 : tensor<5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          9,
          10,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,10,3]. let\n    b:f32[8,10,3] = slice[\n      limit_indices=(9, 10, 3)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x10x3xf32>) -> (tensor<8x10x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:9, 0:10, 0:3] : (tensor<9x10x3xf32>) -> tensor<8x10x3xf32>\n    return %0 : tensor<8x10x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 162))",
    "input_info": [
      {
        "shape": [
          9,
          6,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,6,6]. let\n    b:f32[2,162] = reshape[dimensions=None new_sizes=(2, 162) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x6x6xf32>) -> (tensor<2x162xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<9x6x6xf32>) -> tensor<2x162xf32>\n    return %0 : tensor<2x162xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          9,
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,2] b:f32[9,2]. let\n    c:f32[18,2] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x2xf32>, %arg1: tensor<9x2xf32>) -> (tensor<18x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<9x2xf32>, tensor<9x2xf32>) -> tensor<18x2xf32>\n    return %0 : tensor<18x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          2,
          7,
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2,
          7,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,7,4] b:f32[2,7,4]. let\n    c:f32[1,2,7,4] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 2, 7, 4)\n      sharding=None\n    ] a\n    d:f32[1,2,7,4] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 2, 7, 4)\n      sharding=None\n    ] b\n    e:f32[2,2,7,4] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x7x4xf32>, %arg1: tensor<2x7x4xf32>) -> (tensor<2x2x7x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<2x7x4xf32>) -> tensor<1x2x7x4xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<2x7x4xf32>) -> tensor<1x2x7x4xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x2x7x4xf32>, tensor<1x2x7x4xf32>) -> tensor<2x2x7x4xf32>\n    return %2 : tensor<2x2x7x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          5,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,2]. let\n    b:f32[4,2] = slice[limit_indices=(5, 2) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x2xf32>) -> (tensor<4x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:5, 0:2] : (tensor<5x2xf32>) -> tensor<4x2xf32>\n    return %0 : tensor<4x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2] b:f32[2]. let\n    c:f32[4] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>, %arg1: tensor<2xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<2xf32>, tensor<2xf32>) -> tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (3, 3))",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9]. let\n    b:f32[3,3] = reshape[dimensions=None new_sizes=(3, 3) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>) -> (tensor<3x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<9xf32>) -> tensor<3x3xf32>\n    return %0 : tensor<3x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 140))",
    "input_info": [
      {
        "shape": [
          7,
          10,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,10,4]. let\n    b:f32[2,140] = reshape[dimensions=None new_sizes=(2, 140) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x10x4xf32>) -> (tensor<2x140xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<7x10x4xf32>) -> tensor<2x140xf32>\n    return %0 : tensor<2x140xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          8,
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,9] b:f32[8,9]. let\n    c:f32[1,8,9] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 8, 9)\n      sharding=None\n    ] a\n    d:f32[1,8,9] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 8, 9)\n      sharding=None\n    ] b\n    e:f32[2,8,9] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x9xf32>, %arg1: tensor<8x9xf32>) -> (tensor<2x8x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<8x9xf32>) -> tensor<1x8x9xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<8x9xf32>) -> tensor<1x8x9xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x8x9xf32>, tensor<1x8x9xf32>) -> tensor<2x8x9xf32>\n    return %2 : tensor<2x8x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          3,
          5,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,5,4]. let\n    b:f32[4,5,3] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x5x4xf32>) -> (tensor<4x5x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<3x5x4xf32>) -> tensor<4x5x3xf32>\n    return %0 : tensor<4x5x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>) -> (tensor<5xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          6,
          7,
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6,
          7,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,7,7] b:f32[6,7,7]. let\n    c:f32[1,6,7,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 6, 7, 7)\n      sharding=None\n    ] a\n    d:f32[1,6,7,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 6, 7, 7)\n      sharding=None\n    ] b\n    e:f32[2,6,7,7] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x7x7xf32>, %arg1: tensor<6x7x7xf32>) -> (tensor<2x6x7x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<6x7x7xf32>) -> tensor<1x6x7x7xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<6x7x7xf32>) -> tensor<1x6x7x7xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x6x7x7xf32>, tensor<1x6x7x7xf32>) -> tensor<2x6x7x7xf32>\n    return %2 : tensor<2x6x7x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          5,
          8,
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5,
          8,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,8,7] b:f32[5,8,7]. let\n    c:f32[10,8,7] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x8x7xf32>, %arg1: tensor<5x8x7xf32>) -> (tensor<10x8x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<5x8x7xf32>, tensor<5x8x7xf32>) -> tensor<10x8x7xf32>\n    return %0 : tensor<10x8x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>) -> (tensor<6xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (3, 49))",
    "input_info": [
      {
        "shape": [
          7,
          7,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,7,3]. let\n    b:f32[3,49] = reshape[dimensions=None new_sizes=(3, 49) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x7x3xf32>) -> (tensor<3x49xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<7x7x3xf32>) -> tensor<3x49xf32>\n    return %0 : tensor<3x49xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>) -> (tensor<9xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          7,
          3,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,3,8]. let\n    b:f32[6,3,8] = slice[\n      limit_indices=(7, 3, 8)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x3x8xf32>) -> (tensor<6x3x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:7, 0:3, 0:8] : (tensor<7x3x8xf32>) -> tensor<6x3x8xf32>\n    return %0 : tensor<6x3x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 9))",
    "input_info": [
      {
        "shape": [
          6,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,3]. let\n    b:f32[2,9] = reshape[dimensions=None new_sizes=(2, 9) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x3xf32>) -> (tensor<2x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<6x3xf32>) -> tensor<2x9xf32>\n    return %0 : tensor<2x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6] b:f32[6]. let\n    c:f32[12] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>, %arg1: tensor<6xf32>) -> (tensor<12xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<6xf32>, tensor<6xf32>) -> tensor<12xf32>\n    return %0 : tensor<12xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7]. let\n    b:f32[6] = slice[limit_indices=(7,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>) -> (tensor<6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:7] : (tensor<7xf32>) -> tensor<6xf32>\n    return %0 : tensor<6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          5,
          2,
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5,
          2,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,2,9] b:f32[5,2,9]. let\n    c:f32[1,5,2,9] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 5, 2, 9)\n      sharding=None\n    ] a\n    d:f32[1,5,2,9] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 5, 2, 9)\n      sharding=None\n    ] b\n    e:f32[2,5,2,9] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x2x9xf32>, %arg1: tensor<5x2x9xf32>) -> (tensor<2x5x2x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<5x2x9xf32>) -> tensor<1x5x2x9xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<5x2x9xf32>) -> tensor<1x5x2x9xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x5x2x9xf32>, tensor<1x5x2x9xf32>) -> tensor<2x5x2x9xf32>\n    return %2 : tensor<2x5x2x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          7,
          5,
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7,
          5,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,5,2] b:f32[7,5,2]. let\n    c:f32[1,7,5,2] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 7, 5, 2)\n      sharding=None\n    ] a\n    d:f32[1,7,5,2] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 7, 5, 2)\n      sharding=None\n    ] b\n    e:f32[2,7,5,2] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x5x2xf32>, %arg1: tensor<7x5x2xf32>) -> (tensor<2x7x5x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<7x5x2xf32>) -> tensor<1x7x5x2xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<7x5x2xf32>) -> tensor<1x7x5x2xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x7x5x2xf32>, tensor<1x7x5x2xf32>) -> tensor<2x7x5x2xf32>\n    return %2 : tensor<2x7x5x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          5,
          8,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,8,3]. let\n    b:f32[4,8,3] = slice[\n      limit_indices=(5, 8, 3)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x8x3xf32>) -> (tensor<4x8x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:5, 0:8, 0:3] : (tensor<5x8x3xf32>) -> tensor<4x8x3xf32>\n    return %0 : tensor<4x8x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          7,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,9]. let\n    b:f32[6,9] = slice[limit_indices=(7, 9) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x9xf32>) -> (tensor<6x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:7, 0:9] : (tensor<7x9xf32>) -> tensor<6x9xf32>\n    return %0 : tensor<6x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          2,
          7,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,7,9]. let\n    b:f32[9,7,2] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x7x9xf32>) -> (tensor<9x7x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<2x7x9xf32>) -> tensor<9x7x2xf32>\n    return %0 : tensor<9x7x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          2,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,5]. let\n    b:f32[1,5] = slice[limit_indices=(2, 5) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x5xf32>) -> (tensor<1x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:2, 0:5] : (tensor<2x5xf32>) -> tensor<1x5xf32>\n    return %0 : tensor<1x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          2,
          9,
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2,
          9,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,9,6] b:f32[2,9,6]. let\n    c:f32[4,9,6] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x9x6xf32>, %arg1: tensor<2x9x6xf32>) -> (tensor<4x9x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<2x9x6xf32>, tensor<2x9x6xf32>) -> tensor<4x9x6xf32>\n    return %0 : tensor<4x9x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10] b:f32[10]. let\n    c:f32[20] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>, %arg1: tensor<10xf32>) -> (tensor<20xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<10xf32>, tensor<10xf32>) -> tensor<20xf32>\n    return %0 : tensor<20xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (5,))",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>) -> (tensor<5xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          6,
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,6] b:f32[6,6]. let\n    c:f32[1,6,6] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 6, 6)\n      sharding=None\n    ] a\n    d:f32[1,6,6] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 6, 6)\n      sharding=None\n    ] b\n    e:f32[2,6,6] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x6xf32>, %arg1: tensor<6x6xf32>) -> (tensor<2x6x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<6x6xf32>) -> tensor<1x6x6xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<6x6xf32>) -> tensor<1x6x6xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x6x6xf32>, tensor<1x6x6xf32>) -> tensor<2x6x6xf32>\n    return %2 : tensor<2x6x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7]. let\n    b:f32[6] = slice[limit_indices=(7,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>) -> (tensor<6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:7] : (tensor<7xf32>) -> tensor<6xf32>\n    return %0 : tensor<6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          4,
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,9] b:f32[4,9]. let\n    c:f32[8,9] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x9xf32>, %arg1: tensor<4x9xf32>) -> (tensor<8x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<4x9xf32>, tensor<4x9xf32>) -> tensor<8x9xf32>\n    return %0 : tensor<8x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          6,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,9]. let\n    b:f32[5,9] = slice[limit_indices=(6, 9) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x9xf32>) -> (tensor<5x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:6, 0:9] : (tensor<6x9xf32>) -> tensor<5x9xf32>\n    return %0 : tensor<5x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          7,
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,6] b:f32[7,6]. let\n    c:f32[14,6] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x6xf32>, %arg1: tensor<7x6xf32>) -> (tensor<14x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<7x6xf32>, tensor<7x6xf32>) -> tensor<14x6xf32>\n    return %0 : tensor<14x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2] b:f32[2]. let\n    c:f32[1,2] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 2)\n      sharding=None\n    ] a\n    d:f32[1,2] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 2)\n      sharding=None\n    ] b\n    e:f32[2,2] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>, %arg1: tensor<2xf32>) -> (tensor<2x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<2xf32>) -> tensor<1x2xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<2xf32>) -> tensor<1x2xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x2xf32>, tensor<1x2xf32>) -> tensor<2x2xf32>\n    return %2 : tensor<2x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          10,
          4,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,4,2]. let\n    b:f32[2,4,10] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x4x2xf32>) -> (tensor<2x4x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<10x4x2xf32>) -> tensor<2x4x10xf32>\n    return %0 : tensor<2x4x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          3,
          7,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,7,7]. let\n    b:f32[2,7,7] = slice[\n      limit_indices=(3, 7, 7)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x7x7xf32>) -> (tensor<2x7x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:3, 0:7, 0:7] : (tensor<3x7x7xf32>) -> tensor<2x7x7xf32>\n    return %0 : tensor<2x7x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          6,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,7]. let\n    b:f32[5,7] = slice[limit_indices=(6, 7) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x7xf32>) -> (tensor<5x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:6, 0:7] : (tensor<6x7xf32>) -> tensor<5x7xf32>\n    return %0 : tensor<5x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>) -> (tensor<9xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          7,
          5,
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7,
          5,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,5,3] b:f32[7,5,3]. let\n    c:f32[14,5,3] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x5x3xf32>, %arg1: tensor<7x5x3xf32>) -> (tensor<14x5x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<7x5x3xf32>, tensor<7x5x3xf32>) -> tensor<14x5x3xf32>\n    return %0 : tensor<14x5x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          10,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,9]. let\n    b:f32[9,10] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x9xf32>) -> (tensor<9x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<10x9xf32>) -> tensor<9x10xf32>\n    return %0 : tensor<9x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          7,
          6,
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7,
          6,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,6,9] b:f32[7,6,9]. let\n    c:f32[1,7,6,9] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 7, 6, 9)\n      sharding=None\n    ] a\n    d:f32[1,7,6,9] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 7, 6, 9)\n      sharding=None\n    ] b\n    e:f32[2,7,6,9] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x6x9xf32>, %arg1: tensor<7x6x9xf32>) -> (tensor<2x7x6x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<7x6x9xf32>) -> tensor<1x7x6x9xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<7x6x9xf32>) -> tensor<1x7x6x9xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x7x6x9xf32>, tensor<1x7x6x9xf32>) -> tensor<2x7x6x9xf32>\n    return %2 : tensor<2x7x6x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          10,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,4]. let\n    b:f32[9,4] = slice[limit_indices=(10, 4) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x4xf32>) -> (tensor<9x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:10, 0:4] : (tensor<10x4xf32>) -> tensor<9x4xf32>\n    return %0 : tensor<9x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          3,
          8,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,8,6]. let\n    b:f32[2,8,6] = slice[\n      limit_indices=(3, 8, 6)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x8x6xf32>) -> (tensor<2x8x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:3, 0:8, 0:6] : (tensor<3x8x6xf32>) -> tensor<2x8x6xf32>\n    return %0 : tensor<2x8x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          6,
          3,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,3,2]. let\n    b:f32[5,3,2] = slice[\n      limit_indices=(6, 3, 2)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x3x2xf32>) -> (tensor<5x3x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:6, 0:3, 0:2] : (tensor<6x3x2xf32>) -> tensor<5x3x2xf32>\n    return %0 : tensor<5x3x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (7,))",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>) -> (tensor<7xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          2,
          2,
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2,
          2,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,2,9] b:f32[2,2,9]. let\n    c:f32[4,2,9] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x2x9xf32>, %arg1: tensor<2x2x9xf32>) -> (tensor<4x2x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<2x2x9xf32>, tensor<2x2x9xf32>) -> tensor<4x2x9xf32>\n    return %0 : tensor<4x2x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          3,
          9,
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3,
          9,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,9,2] b:f32[3,9,2]. let\n    c:f32[6,9,2] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x9x2xf32>, %arg1: tensor<3x9x2xf32>) -> (tensor<6x9x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<3x9x2xf32>, tensor<3x9x2xf32>) -> tensor<6x9x2xf32>\n    return %0 : tensor<6x9x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (5,))",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>) -> (tensor<5xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (42,))",
    "input_info": [
      {
        "shape": [
          7,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,6]. let\n    b:f32[42] = reshape[dimensions=None new_sizes=(42,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x6xf32>) -> (tensor<42xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<7x6xf32>) -> tensor<42xf32>\n    return %0 : tensor<42xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          7,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,6]. let\n    b:f32[6,6] = slice[limit_indices=(7, 6) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x6xf32>) -> (tensor<6x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:7, 0:6] : (tensor<7x6xf32>) -> tensor<6x6xf32>\n    return %0 : tensor<6x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          7,
          8,
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7,
          8,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,8,2] b:f32[7,8,2]. let\n    c:f32[14,8,2] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x8x2xf32>, %arg1: tensor<7x8x2xf32>) -> (tensor<14x8x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<7x8x2xf32>, tensor<7x8x2xf32>) -> tensor<14x8x2xf32>\n    return %0 : tensor<14x8x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5] b:f32[5]. let\n    c:f32[1,5] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 5)\n      sharding=None\n    ] a\n    d:f32[1,5] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 5)\n      sharding=None\n    ] b\n    e:f32[2,5] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>, %arg1: tensor<5xf32>) -> (tensor<2x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<5xf32>) -> tensor<1x5xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<5xf32>) -> tensor<1x5xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x5xf32>, tensor<1x5xf32>) -> tensor<2x5xf32>\n    return %2 : tensor<2x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          9,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,4]. let\n    b:f32[8,4] = slice[limit_indices=(9, 4) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x4xf32>) -> (tensor<8x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:9, 0:4] : (tensor<9x4xf32>) -> tensor<8x4xf32>\n    return %0 : tensor<8x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          5,
          3,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,3,7]. let\n    b:f32[4,3,7] = slice[\n      limit_indices=(5, 3, 7)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x3x7xf32>) -> (tensor<4x3x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:5, 0:3, 0:7] : (tensor<5x3x7xf32>) -> tensor<4x3x7xf32>\n    return %0 : tensor<4x3x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          9,
          8,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,8,4]. let\n    b:f32[4,8,9] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x8x4xf32>) -> (tensor<4x8x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<9x8x4xf32>) -> tensor<4x8x9xf32>\n    return %0 : tensor<4x8x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6] b:f32[6]. let\n    c:f32[1,6] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 6)\n      sharding=None\n    ] a\n    d:f32[1,6] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 6)\n      sharding=None\n    ] b\n    e:f32[2,6] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>, %arg1: tensor<6xf32>) -> (tensor<2x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<6xf32>) -> tensor<1x6xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<6xf32>) -> tensor<1x6xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x6xf32>, tensor<1x6xf32>) -> tensor<2x6xf32>\n    return %2 : tensor<2x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9] b:f32[9]. let\n    c:f32[18] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>, %arg1: tensor<9xf32>) -> (tensor<18xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<9xf32>, tensor<9xf32>) -> tensor<18xf32>\n    return %0 : tensor<18xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          2,
          3,
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2,
          3,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,3,9] b:f32[2,3,9]. let\n    c:f32[4,3,9] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x3x9xf32>, %arg1: tensor<2x3x9xf32>) -> (tensor<4x3x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<2x3x9xf32>, tensor<2x3x9xf32>) -> tensor<4x3x9xf32>\n    return %0 : tensor<4x3x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>) -> (tensor<6xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          10,
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,10] b:f32[10,10]. let\n    c:f32[1,10,10] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 10, 10)\n      sharding=None\n    ] a\n    d:f32[1,10,10] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 10, 10)\n      sharding=None\n    ] b\n    e:f32[2,10,10] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x10xf32>, %arg1: tensor<10x10xf32>) -> (tensor<2x10x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<10x10xf32>) -> tensor<1x10x10xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<10x10xf32>) -> tensor<1x10x10xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x10x10xf32>, tensor<1x10x10xf32>) -> tensor<2x10x10xf32>\n    return %2 : tensor<2x10x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (630,))",
    "input_info": [
      {
        "shape": [
          7,
          10,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,10,9]. let\n    b:f32[630] = reshape[dimensions=None new_sizes=(630,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x10x9xf32>) -> (tensor<630xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<7x10x9xf32>) -> tensor<630xf32>\n    return %0 : tensor<630xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (50,))",
    "input_info": [
      {
        "shape": [
          5,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,10]. let\n    b:f32[50] = reshape[dimensions=None new_sizes=(50,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x10xf32>) -> (tensor<50xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<5x10xf32>) -> tensor<50xf32>\n    return %0 : tensor<50xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6]. let\n    b:f32[5] = slice[limit_indices=(6,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>) -> (tensor<5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:6] : (tensor<6xf32>) -> tensor<5xf32>\n    return %0 : tensor<5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (42,))",
    "input_info": [
      {
        "shape": [
          6,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,7]. let\n    b:f32[42] = reshape[dimensions=None new_sizes=(42,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x7xf32>) -> (tensor<42xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<6x7xf32>) -> tensor<42xf32>\n    return %0 : tensor<42xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (72,))",
    "input_info": [
      {
        "shape": [
          2,
          9,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,9,4]. let\n    b:f32[72] = reshape[dimensions=None new_sizes=(72,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x9x4xf32>) -> (tensor<72xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<2x9x4xf32>) -> tensor<72xf32>\n    return %0 : tensor<72xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>) -> (tensor<2xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          8,
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,4] b:f32[8,4]. let\n    c:f32[16,4] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x4xf32>, %arg1: tensor<8x4xf32>) -> (tensor<16x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<8x4xf32>, tensor<8x4xf32>) -> tensor<16x4xf32>\n    return %0 : tensor<16x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          5,
          8,
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5,
          8,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,8,2] b:f32[5,8,2]. let\n    c:f32[1,5,8,2] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 5, 8, 2)\n      sharding=None\n    ] a\n    d:f32[1,5,8,2] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 5, 8, 2)\n      sharding=None\n    ] b\n    e:f32[2,5,8,2] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x8x2xf32>, %arg1: tensor<5x8x2xf32>) -> (tensor<2x5x8x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<5x8x2xf32>) -> tensor<1x5x8x2xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<5x8x2xf32>) -> tensor<1x5x8x2xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x5x8x2xf32>, tensor<1x5x8x2xf32>) -> tensor<2x5x8x2xf32>\n    return %2 : tensor<2x5x8x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          2,
          9,
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2,
          9,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,9,7] b:f32[2,9,7]. let\n    c:f32[1,2,9,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 2, 9, 7)\n      sharding=None\n    ] a\n    d:f32[1,2,9,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 2, 9, 7)\n      sharding=None\n    ] b\n    e:f32[2,2,9,7] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x9x7xf32>, %arg1: tensor<2x9x7xf32>) -> (tensor<2x2x9x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<2x9x7xf32>) -> tensor<1x2x9x7xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<2x9x7xf32>) -> tensor<1x2x9x7xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x2x9x7xf32>, tensor<1x2x9x7xf32>) -> tensor<2x2x9x7xf32>\n    return %2 : tensor<2x2x9x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8xf32>) -> (tensor<8xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          4,
          4,
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4,
          4,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,4,9] b:f32[4,4,9]. let\n    c:f32[8,4,9] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x4x9xf32>, %arg1: tensor<4x4x9xf32>) -> (tensor<8x4x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<4x4x9xf32>, tensor<4x4x9xf32>) -> tensor<8x4x9xf32>\n    return %0 : tensor<8x4x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          10,
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,4] b:f32[10,4]. let\n    c:f32[20,4] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x4xf32>, %arg1: tensor<10x4xf32>) -> (tensor<20x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<10x4xf32>, tensor<10x4xf32>) -> tensor<20x4xf32>\n    return %0 : tensor<20x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3] b:f32[3]. let\n    c:f32[1,3] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 3)\n      sharding=None\n    ] a\n    d:f32[1,3] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 3)\n      sharding=None\n    ] b\n    e:f32[2,3] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>, %arg1: tensor<3xf32>) -> (tensor<2x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<3xf32>) -> tensor<1x3xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<3xf32>) -> tensor<1x3xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x3xf32>, tensor<1x3xf32>) -> tensor<2x3xf32>\n    return %2 : tensor<2x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          7,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,6]. let\n    b:f32[6,7] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x6xf32>) -> (tensor<6x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<7x6xf32>) -> tensor<6x7xf32>\n    return %0 : tensor<6x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          10,
          10,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,10,10]. let\n    b:f32[10,10,10] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x10x10xf32>) -> (tensor<10x10x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<10x10x10xf32>) -> tensor<10x10x10xf32>\n    return %0 : tensor<10x10x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          8,
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,5] b:f32[8,5]. let\n    c:f32[16,5] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x5xf32>, %arg1: tensor<8x5xf32>) -> (tensor<16x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<8x5xf32>, tensor<8x5xf32>) -> tensor<16x5xf32>\n    return %0 : tensor<16x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          6,
          3,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,3,10]. let\n    b:f32[5,3,10] = slice[\n      limit_indices=(6, 3, 10)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x3x10xf32>) -> (tensor<5x3x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:6, 0:3, 0:10] : (tensor<6x3x10xf32>) -> tensor<5x3x10xf32>\n    return %0 : tensor<5x3x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          4,
          2,
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4,
          2,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,2,4] b:f32[4,2,4]. let\n    c:f32[1,4,2,4] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 4, 2, 4)\n      sharding=None\n    ] a\n    d:f32[1,4,2,4] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 4, 2, 4)\n      sharding=None\n    ] b\n    e:f32[2,4,2,4] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x2x4xf32>, %arg1: tensor<4x2x4xf32>) -> (tensor<2x4x2x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<4x2x4xf32>) -> tensor<1x4x2x4xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<4x2x4xf32>) -> tensor<1x4x2x4xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x4x2x4xf32>, tensor<1x4x2x4xf32>) -> tensor<2x4x2x4xf32>\n    return %2 : tensor<2x4x2x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          8,
          6,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,6,10]. let\n    b:f32[7,6,10] = slice[\n      limit_indices=(8, 6, 10)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x6x10xf32>) -> (tensor<7x6x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:8, 0:6, 0:10] : (tensor<8x6x10xf32>) -> tensor<7x6x10xf32>\n    return %0 : tensor<7x6x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          2,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,8]. let\n    b:f32[8,2] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x8xf32>) -> (tensor<8x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<2x8xf32>) -> tensor<8x2xf32>\n    return %0 : tensor<8x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3] b:f32[3]. let\n    c:f32[1,3] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 3)\n      sharding=None\n    ] a\n    d:f32[1,3] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 3)\n      sharding=None\n    ] b\n    e:f32[2,3] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>, %arg1: tensor<3xf32>) -> (tensor<2x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<3xf32>) -> tensor<1x3xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<3xf32>) -> tensor<1x3xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x3xf32>, tensor<1x3xf32>) -> tensor<2x3xf32>\n    return %2 : tensor<2x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          3,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,5]. let\n    b:f32[5,3] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x5xf32>) -> (tensor<5x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<3x5xf32>) -> tensor<5x3xf32>\n    return %0 : tensor<5x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          2,
          10,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,10,2]. let\n    b:f32[1,10,2] = slice[\n      limit_indices=(2, 10, 2)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x10x2xf32>) -> (tensor<1x10x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:2, 0:10, 0:2] : (tensor<2x10x2xf32>) -> tensor<1x10x2xf32>\n    return %0 : tensor<1x10x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          9,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,8]. let\n    b:f32[8,9] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x8xf32>) -> (tensor<8x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<9x8xf32>) -> tensor<8x9xf32>\n    return %0 : tensor<8x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          3,
          3,
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3,
          3,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,3,4] b:f32[3,3,4]. let\n    c:f32[1,3,3,4] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 3, 3, 4)\n      sharding=None\n    ] a\n    d:f32[1,3,3,4] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 3, 3, 4)\n      sharding=None\n    ] b\n    e:f32[2,3,3,4] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x3x4xf32>, %arg1: tensor<3x3x4xf32>) -> (tensor<2x3x3x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<3x3x4xf32>) -> tensor<1x3x3x4xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<3x3x4xf32>) -> tensor<1x3x3x4xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x3x3x4xf32>, tensor<1x3x3x4xf32>) -> tensor<2x3x3x4xf32>\n    return %2 : tensor<2x3x3x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          10,
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,7] b:f32[10,7]. let\n    c:f32[1,10,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 10, 7)\n      sharding=None\n    ] a\n    d:f32[1,10,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 10, 7)\n      sharding=None\n    ] b\n    e:f32[2,10,7] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x7xf32>, %arg1: tensor<10x7xf32>) -> (tensor<2x10x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<10x7xf32>) -> tensor<1x10x7xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<10x7xf32>) -> tensor<1x10x7xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x10x7xf32>, tensor<1x10x7xf32>) -> tensor<2x10x7xf32>\n    return %2 : tensor<2x10x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          8,
          9,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,9,6]. let\n    b:f32[7,9,6] = slice[\n      limit_indices=(8, 9, 6)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x9x6xf32>) -> (tensor<7x9x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:8, 0:9, 0:6] : (tensor<8x9x6xf32>) -> tensor<7x9x6xf32>\n    return %0 : tensor<7x9x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          9,
          7,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,7,4]. let\n    b:f32[8,7,4] = slice[\n      limit_indices=(9, 7, 4)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x7x4xf32>) -> (tensor<8x7x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:9, 0:7, 0:4] : (tensor<9x7x4xf32>) -> tensor<8x7x4xf32>\n    return %0 : tensor<8x7x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 2))",
    "input_info": [
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4]. let\n    b:f32[2,2] = reshape[dimensions=None new_sizes=(2, 2) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<2x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<4xf32>) -> tensor<2x2xf32>\n    return %0 : tensor<2x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          10,
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,5] b:f32[10,5]. let\n    c:f32[20,5] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x5xf32>, %arg1: tensor<10x5xf32>) -> (tensor<20x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<10x5xf32>, tensor<10x5xf32>) -> tensor<20x5xf32>\n    return %0 : tensor<20x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          6,
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,5] b:f32[6,5]. let\n    c:f32[1,6,5] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 6, 5)\n      sharding=None\n    ] a\n    d:f32[1,6,5] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 6, 5)\n      sharding=None\n    ] b\n    e:f32[2,6,5] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x5xf32>, %arg1: tensor<6x5xf32>) -> (tensor<2x6x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<6x5xf32>) -> tensor<1x6x5xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<6x5xf32>) -> tensor<1x6x5xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x6x5xf32>, tensor<1x6x5xf32>) -> tensor<2x6x5xf32>\n    return %2 : tensor<2x6x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          6,
          6,
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6,
          6,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,6,10] b:f32[6,6,10]. let\n    c:f32[1,6,6,10] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 6, 6, 10)\n      sharding=None\n    ] a\n    d:f32[1,6,6,10] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 6, 6, 10)\n      sharding=None\n    ] b\n    e:f32[2,6,6,10] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x6x10xf32>, %arg1: tensor<6x6x10xf32>) -> (tensor<2x6x6x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<6x6x10xf32>) -> tensor<1x6x6x10xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<6x6x10xf32>) -> tensor<1x6x6x10xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x6x6x10xf32>, tensor<1x6x6x10xf32>) -> tensor<2x6x6x10xf32>\n    return %2 : tensor<2x6x6x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          9,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,2]. let\n    b:f32[2,9] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x2xf32>) -> (tensor<2x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<9x2xf32>) -> tensor<2x9xf32>\n    return %0 : tensor<2x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>) -> (tensor<9xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5]. let\n    b:f32[4] = slice[limit_indices=(5,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:5] : (tensor<5xf32>) -> tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10] b:f32[10]. let\n    c:f32[20] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>, %arg1: tensor<10xf32>) -> (tensor<20xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<10xf32>, tensor<10xf32>) -> tensor<20xf32>\n    return %0 : tensor<20xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2,))",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>) -> (tensor<2xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2,))",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>) -> (tensor<2xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 5))",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10]. let\n    b:f32[2,5] = reshape[dimensions=None new_sizes=(2, 5) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<2x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<10xf32>) -> tensor<2x5xf32>\n    return %0 : tensor<2x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          2,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,9]. let\n    b:f32[9,2] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x9xf32>) -> (tensor<9x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<2x9xf32>) -> tensor<9x2xf32>\n    return %0 : tensor<9x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6]. let\n    b:f32[5] = slice[limit_indices=(6,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>) -> (tensor<5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:6] : (tensor<6xf32>) -> tensor<5xf32>\n    return %0 : tensor<5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4]. let\n    b:f32[3] = slice[limit_indices=(4,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:4] : (tensor<4xf32>) -> tensor<3xf32>\n    return %0 : tensor<3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5] b:f32[5]. let\n    c:f32[10] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>, %arg1: tensor<5xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<5xf32>, tensor<5xf32>) -> tensor<10xf32>\n    return %0 : tensor<10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 14))",
    "input_info": [
      {
        "shape": [
          4,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,7]. let\n    b:f32[2,14] = reshape[dimensions=None new_sizes=(2, 14) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x7xf32>) -> (tensor<2x14xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<4x7xf32>) -> tensor<2x14xf32>\n    return %0 : tensor<2x14xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9] b:f32[9]. let\n    c:f32[1,9] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 9)\n      sharding=None\n    ] a\n    d:f32[1,9] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 9)\n      sharding=None\n    ] b\n    e:f32[2,9] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>, %arg1: tensor<9xf32>) -> (tensor<2x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<9xf32>) -> tensor<1x9xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<9xf32>) -> tensor<1x9xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x9xf32>, tensor<1x9xf32>) -> tensor<2x9xf32>\n    return %2 : tensor<2x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9] b:f32[9]. let\n    c:f32[18] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>, %arg1: tensor<9xf32>) -> (tensor<18xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<9xf32>, tensor<9xf32>) -> tensor<18xf32>\n    return %0 : tensor<18xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>) -> (tensor<9xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8] b:f32[8]. let\n    c:f32[1,8] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 8)\n      sharding=None\n    ] a\n    d:f32[1,8] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 8)\n      sharding=None\n    ] b\n    e:f32[2,8] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8xf32>, %arg1: tensor<8xf32>) -> (tensor<2x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<8xf32>) -> tensor<1x8xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<8xf32>) -> tensor<1x8xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x8xf32>, tensor<1x8xf32>) -> tensor<2x8xf32>\n    return %2 : tensor<2x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          5,
          9,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,9,3]. let\n    b:f32[4,9,3] = slice[\n      limit_indices=(5, 9, 3)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x9x3xf32>) -> (tensor<4x9x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:5, 0:9, 0:3] : (tensor<5x9x3xf32>) -> tensor<4x9x3xf32>\n    return %0 : tensor<4x9x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (392,))",
    "input_info": [
      {
        "shape": [
          7,
          8,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,8,7]. let\n    b:f32[392] = reshape[dimensions=None new_sizes=(392,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x8x7xf32>) -> (tensor<392xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<7x8x7xf32>) -> tensor<392xf32>\n    return %0 : tensor<392xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>) -> (tensor<9xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          8,
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,9] b:f32[8,9]. let\n    c:f32[16,9] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x9xf32>, %arg1: tensor<8x9xf32>) -> (tensor<16x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<8x9xf32>, tensor<8x9xf32>) -> tensor<16x9xf32>\n    return %0 : tensor<16x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          8,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,4]. let\n    b:f32[4,8] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x4xf32>) -> (tensor<4x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<8x4xf32>) -> tensor<4x8xf32>\n    return %0 : tensor<4x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          2,
          8,
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2,
          8,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,8,2] b:f32[2,8,2]. let\n    c:f32[4,8,2] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x8x2xf32>, %arg1: tensor<2x8x2xf32>) -> (tensor<4x8x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<2x8x2xf32>, tensor<2x8x2xf32>) -> tensor<4x8x2xf32>\n    return %0 : tensor<4x8x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7] b:f32[7]. let\n    c:f32[1,7] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 7)\n      sharding=None\n    ] a\n    d:f32[1,7] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 7)\n      sharding=None\n    ] b\n    e:f32[2,7] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>, %arg1: tensor<7xf32>) -> (tensor<2x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<7xf32>) -> tensor<1x7xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<7xf32>) -> tensor<1x7xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x7xf32>, tensor<1x7xf32>) -> tensor<2x7xf32>\n    return %2 : tensor<2x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          4,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,10]. let\n    b:f32[3,10] = slice[limit_indices=(4, 10) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x10xf32>) -> (tensor<3x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:4, 0:10] : (tensor<4x10xf32>) -> tensor<3x10xf32>\n    return %0 : tensor<3x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2]. let\n    b:f32[1] = slice[limit_indices=(2,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>) -> (tensor<1xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:2] : (tensor<2xf32>) -> tensor<1xf32>\n    return %0 : tensor<1xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          2,
          6,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,6,5]. let\n    b:f32[1,6,5] = slice[\n      limit_indices=(2, 6, 5)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x6x5xf32>) -> (tensor<1x6x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:2, 0:6, 0:5] : (tensor<2x6x5xf32>) -> tensor<1x6x5xf32>\n    return %0 : tensor<1x6x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          10,
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,6] b:f32[10,6]. let\n    c:f32[20,6] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x6xf32>, %arg1: tensor<10x6xf32>) -> (tensor<20x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<10x6xf32>, tensor<10x6xf32>) -> tensor<20x6xf32>\n    return %0 : tensor<20x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          4,
          2,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,2,6]. let\n    b:f32[3,2,6] = slice[\n      limit_indices=(4, 2, 6)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x2x6xf32>) -> (tensor<3x2x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:4, 0:2, 0:6] : (tensor<4x2x6xf32>) -> tensor<3x2x6xf32>\n    return %0 : tensor<3x2x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          6,
          2,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,2,3]. let\n    b:f32[3,2,6] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x2x3xf32>) -> (tensor<3x2x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<6x2x3xf32>) -> tensor<3x2x6xf32>\n    return %0 : tensor<3x2x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          10,
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,9] b:f32[10,9]. let\n    c:f32[1,10,9] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 10, 9)\n      sharding=None\n    ] a\n    d:f32[1,10,9] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 10, 9)\n      sharding=None\n    ] b\n    e:f32[2,10,9] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x9xf32>, %arg1: tensor<10x9xf32>) -> (tensor<2x10x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<10x9xf32>) -> tensor<1x10x9xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<10x9xf32>) -> tensor<1x10x9xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x10x9xf32>, tensor<1x10x9xf32>) -> tensor<2x10x9xf32>\n    return %2 : tensor<2x10x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          8,
          6,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,6,2]. let\n    b:f32[7,6,2] = slice[\n      limit_indices=(8, 6, 2)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x6x2xf32>) -> (tensor<7x6x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:8, 0:6, 0:2] : (tensor<8x6x2xf32>) -> tensor<7x6x2xf32>\n    return %0 : tensor<7x6x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          7,
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,10] b:f32[7,10]. let\n    c:f32[1,7,10] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 7, 10)\n      sharding=None\n    ] a\n    d:f32[1,7,10] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 7, 10)\n      sharding=None\n    ] b\n    e:f32[2,7,10] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x10xf32>, %arg1: tensor<7x10xf32>) -> (tensor<2x7x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<7x10xf32>) -> tensor<1x7x10xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<7x10xf32>) -> tensor<1x7x10xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x7x10xf32>, tensor<1x7x10xf32>) -> tensor<2x7x10xf32>\n    return %2 : tensor<2x7x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (30,))",
    "input_info": [
      {
        "shape": [
          5,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,6]. let\n    b:f32[30] = reshape[dimensions=None new_sizes=(30,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x6xf32>) -> (tensor<30xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<5x6xf32>) -> tensor<30xf32>\n    return %0 : tensor<30xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (8,))",
    "input_info": [
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8xf32>) -> (tensor<8xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6]. let\n    b:f32[5] = slice[limit_indices=(6,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>) -> (tensor<5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:6] : (tensor<6xf32>) -> tensor<5xf32>\n    return %0 : tensor<5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2] b:f32[2]. let\n    c:f32[4] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>, %arg1: tensor<2xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<2xf32>, tensor<2xf32>) -> tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          7,
          4,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,4,6]. let\n    b:f32[6,4,6] = slice[\n      limit_indices=(7, 4, 6)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x4x6xf32>) -> (tensor<6x4x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:7, 0:4, 0:6] : (tensor<7x4x6xf32>) -> tensor<6x4x6xf32>\n    return %0 : tensor<6x4x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          10,
          7,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,7,7]. let\n    b:f32[7,7,10] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x7x7xf32>) -> (tensor<7x7x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<10x7x7xf32>) -> tensor<7x7x10xf32>\n    return %0 : tensor<7x7x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          10,
          8,
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10,
          8,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,8,10] b:f32[10,8,10]. let\n    c:f32[20,8,10] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x8x10xf32>, %arg1: tensor<10x8x10xf32>) -> (tensor<20x8x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<10x8x10xf32>, tensor<10x8x10xf32>) -> tensor<20x8x10xf32>\n    return %0 : tensor<20x8x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          6,
          8,
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6,
          8,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,8,2] b:f32[6,8,2]. let\n    c:f32[1,6,8,2] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 6, 8, 2)\n      sharding=None\n    ] a\n    d:f32[1,6,8,2] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 6, 8, 2)\n      sharding=None\n    ] b\n    e:f32[2,6,8,2] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x8x2xf32>, %arg1: tensor<6x8x2xf32>) -> (tensor<2x6x8x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<6x8x2xf32>) -> tensor<1x6x8x2xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<6x8x2xf32>) -> tensor<1x6x8x2xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x6x8x2xf32>, tensor<1x6x8x2xf32>) -> tensor<2x6x8x2xf32>\n    return %2 : tensor<2x6x8x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8]. let\n    b:f32[7] = slice[limit_indices=(8,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8xf32>) -> (tensor<7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:8] : (tensor<8xf32>) -> tensor<7xf32>\n    return %0 : tensor<7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          4,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,10]. let\n    b:f32[3,10] = slice[limit_indices=(4, 10) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x10xf32>) -> (tensor<3x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:4, 0:10] : (tensor<4x10xf32>) -> tensor<3x10xf32>\n    return %0 : tensor<3x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4] b:f32[4]. let\n    c:f32[8] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> (tensor<8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<4xf32>, tensor<4xf32>) -> tensor<8xf32>\n    return %0 : tensor<8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          9,
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,2] b:f32[9,2]. let\n    c:f32[18,2] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x2xf32>, %arg1: tensor<9x2xf32>) -> (tensor<18x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<9x2xf32>, tensor<9x2xf32>) -> tensor<18x2xf32>\n    return %0 : tensor<18x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (14,))",
    "input_info": [
      {
        "shape": [
          7,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,2]. let\n    b:f32[14] = reshape[dimensions=None new_sizes=(14,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x2xf32>) -> (tensor<14xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<7x2xf32>) -> tensor<14xf32>\n    return %0 : tensor<14xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6] b:f32[6]. let\n    c:f32[12] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>, %arg1: tensor<6xf32>) -> (tensor<12xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<6xf32>, tensor<6xf32>) -> tensor<12xf32>\n    return %0 : tensor<12xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7] b:f32[7]. let\n    c:f32[14] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>, %arg1: tensor<7xf32>) -> (tensor<14xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<7xf32>, tensor<7xf32>) -> tensor<14xf32>\n    return %0 : tensor<14xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          7,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,7]. let\n    b:f32[7,7] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x7xf32>) -> (tensor<7x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<7x7xf32>) -> tensor<7x7xf32>\n    return %0 : tensor<7x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          8,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,5]. let\n    b:f32[5,8] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x5xf32>) -> (tensor<5x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<8x5xf32>) -> tensor<5x8xf32>\n    return %0 : tensor<5x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4] b:f32[4]. let\n    c:f32[8] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> (tensor<8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<4xf32>, tensor<4xf32>) -> tensor<8xf32>\n    return %0 : tensor<8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          5,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,6]. let\n    b:f32[4,6] = slice[limit_indices=(5, 6) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x6xf32>) -> (tensor<4x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:5, 0:6] : (tensor<5x6xf32>) -> tensor<4x6xf32>\n    return %0 : tensor<4x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (6,))",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>) -> (tensor<6xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          2,
          2,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,2,3]. let\n    b:f32[3,2,2] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x2x3xf32>) -> (tensor<3x2x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<2x2x3xf32>) -> tensor<3x2x2xf32>\n    return %0 : tensor<3x2x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 60))",
    "input_info": [
      {
        "shape": [
          10,
          2,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,2,6]. let\n    b:f32[2,60] = reshape[dimensions=None new_sizes=(2, 60) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x2x6xf32>) -> (tensor<2x60xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<10x2x6xf32>) -> tensor<2x60xf32>\n    return %0 : tensor<2x60xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          8,
          4,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,4,10]. let\n    b:f32[7,4,10] = slice[\n      limit_indices=(8, 4, 10)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x4x10xf32>) -> (tensor<7x4x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:8, 0:4, 0:10] : (tensor<8x4x10xf32>) -> tensor<7x4x10xf32>\n    return %0 : tensor<7x4x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 320))",
    "input_info": [
      {
        "shape": [
          10,
          8,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,8,8]. let\n    b:f32[2,320] = reshape[dimensions=None new_sizes=(2, 320) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x8x8xf32>) -> (tensor<2x320xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<10x8x8xf32>) -> tensor<2x320xf32>\n    return %0 : tensor<2x320xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          5,
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,7] b:f32[5,7]. let\n    c:f32[1,5,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 5, 7)\n      sharding=None\n    ] a\n    d:f32[1,5,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 5, 7)\n      sharding=None\n    ] b\n    e:f32[2,5,7] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x7xf32>, %arg1: tensor<5x7xf32>) -> (tensor<2x5x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<5x7xf32>) -> tensor<1x5x7xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<5x7xf32>) -> tensor<1x5x7xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x5x7xf32>, tensor<1x5x7xf32>) -> tensor<2x5x7xf32>\n    return %2 : tensor<2x5x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          7,
          5,
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7,
          5,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,5,9] b:f32[7,5,9]. let\n    c:f32[14,5,9] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x5x9xf32>, %arg1: tensor<7x5x9xf32>) -> (tensor<14x5x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<7x5x9xf32>, tensor<7x5x9xf32>) -> tensor<14x5x9xf32>\n    return %0 : tensor<14x5x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8xf32>) -> (tensor<8xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6]. let\n    b:f32[5] = slice[limit_indices=(6,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>) -> (tensor<5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:6] : (tensor<6xf32>) -> tensor<5xf32>\n    return %0 : tensor<5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          6,
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,7] b:f32[6,7]. let\n    c:f32[1,6,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 6, 7)\n      sharding=None\n    ] a\n    d:f32[1,6,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 6, 7)\n      sharding=None\n    ] b\n    e:f32[2,6,7] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x7xf32>, %arg1: tensor<6x7xf32>) -> (tensor<2x6x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<6x7xf32>) -> tensor<1x6x7xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<6x7xf32>) -> tensor<1x6x7xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x6x7xf32>, tensor<1x6x7xf32>) -> tensor<2x6x7xf32>\n    return %2 : tensor<2x6x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (8,))",
    "input_info": [
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8xf32>) -> (tensor<8xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          5,
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,3] b:f32[5,3]. let\n    c:f32[1,5,3] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 5, 3)\n      sharding=None\n    ] a\n    d:f32[1,5,3] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 5, 3)\n      sharding=None\n    ] b\n    e:f32[2,5,3] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x3xf32>, %arg1: tensor<5x3xf32>) -> (tensor<2x5x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<5x3xf32>) -> tensor<1x5x3xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<5x3xf32>) -> tensor<1x5x3xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x5x3xf32>, tensor<1x5x3xf32>) -> tensor<2x5x3xf32>\n    return %2 : tensor<2x5x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          8,
          3,
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          3,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,3,6] b:f32[8,3,6]. let\n    c:f32[16,3,6] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x3x6xf32>, %arg1: tensor<8x3x6xf32>) -> (tensor<16x3x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<8x3x6xf32>, tensor<8x3x6xf32>) -> tensor<16x3x6xf32>\n    return %0 : tensor<16x3x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          3,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,6]. let\n    b:f32[2,6] = slice[limit_indices=(3, 6) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x6xf32>) -> (tensor<2x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:3, 0:6] : (tensor<3x6xf32>) -> tensor<2x6xf32>\n    return %0 : tensor<2x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (72,))",
    "input_info": [
      {
        "shape": [
          3,
          3,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,3,8]. let\n    b:f32[72] = reshape[dimensions=None new_sizes=(72,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x3x8xf32>) -> (tensor<72xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<3x3x8xf32>) -> tensor<72xf32>\n    return %0 : tensor<72xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          3,
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,3] b:f32[3,3]. let\n    c:f32[1,3,3] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 3, 3)\n      sharding=None\n    ] a\n    d:f32[1,3,3] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 3, 3)\n      sharding=None\n    ] b\n    e:f32[2,3,3] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x3xf32>, %arg1: tensor<3x3xf32>) -> (tensor<2x3x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<3x3xf32>) -> tensor<1x3x3xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<3x3xf32>) -> tensor<1x3x3xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x3x3xf32>, tensor<1x3x3xf32>) -> tensor<2x3x3xf32>\n    return %2 : tensor<2x3x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6] b:f32[6]. let\n    c:f32[12] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>, %arg1: tensor<6xf32>) -> (tensor<12xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<6xf32>, tensor<6xf32>) -> tensor<12xf32>\n    return %0 : tensor<12xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          9,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,5]. let\n    b:f32[5,9] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x5xf32>) -> (tensor<5x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<9x5xf32>) -> tensor<5x9xf32>\n    return %0 : tensor<5x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          8,
          10,
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          10,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,10,8] b:f32[8,10,8]. let\n    c:f32[16,10,8] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x10x8xf32>, %arg1: tensor<8x10x8xf32>) -> (tensor<16x10x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<8x10x8xf32>, tensor<8x10x8xf32>) -> tensor<16x10x8xf32>\n    return %0 : tensor<16x10x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (3, 21))",
    "input_info": [
      {
        "shape": [
          7,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,9]. let\n    b:f32[3,21] = reshape[dimensions=None new_sizes=(3, 21) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x9xf32>) -> (tensor<3x21xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<7x9xf32>) -> tensor<3x21xf32>\n    return %0 : tensor<3x21xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          10,
          4,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,4,3]. let\n    b:f32[3,4,10] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x4x3xf32>) -> (tensor<3x4x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<10x4x3xf32>) -> tensor<3x4x10xf32>\n    return %0 : tensor<3x4x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 75))",
    "input_info": [
      {
        "shape": [
          3,
          10,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,10,5]. let\n    b:f32[2,75] = reshape[dimensions=None new_sizes=(2, 75) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x10x5xf32>) -> (tensor<2x75xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<3x10x5xf32>) -> tensor<2x75xf32>\n    return %0 : tensor<2x75xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          2,
          6,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,6,9]. let\n    b:f32[1,6,9] = slice[\n      limit_indices=(2, 6, 9)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x6x9xf32>) -> (tensor<1x6x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:2, 0:6, 0:9] : (tensor<2x6x9xf32>) -> tensor<1x6x9xf32>\n    return %0 : tensor<1x6x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 3))",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6]. let\n    b:f32[2,3] = reshape[dimensions=None new_sizes=(2, 3) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>) -> (tensor<2x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<6xf32>) -> tensor<2x3xf32>\n    return %0 : tensor<2x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          8,
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,4] b:f32[8,4]. let\n    c:f32[1,8,4] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 8, 4)\n      sharding=None\n    ] a\n    d:f32[1,8,4] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 8, 4)\n      sharding=None\n    ] b\n    e:f32[2,8,4] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x4xf32>, %arg1: tensor<8x4xf32>) -> (tensor<2x8x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<8x4xf32>) -> tensor<1x8x4xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<8x4xf32>) -> tensor<1x8x4xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x8x4xf32>, tensor<1x8x4xf32>) -> tensor<2x8x4xf32>\n    return %2 : tensor<2x8x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5]. let\n    b:f32[4] = slice[limit_indices=(5,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:5] : (tensor<5xf32>) -> tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (8,))",
    "input_info": [
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8xf32>) -> (tensor<8xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6]. let\n    b:f32[5] = slice[limit_indices=(6,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>) -> (tensor<5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:6] : (tensor<6xf32>) -> tensor<5xf32>\n    return %0 : tensor<5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (18,))",
    "input_info": [
      {
        "shape": [
          3,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,6]. let\n    b:f32[18] = reshape[dimensions=None new_sizes=(18,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x6xf32>) -> (tensor<18xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<3x6xf32>) -> tensor<18xf32>\n    return %0 : tensor<18xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          7,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,3]. let\n    b:f32[3,7] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x3xf32>) -> (tensor<3x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<7x3xf32>) -> tensor<3x7xf32>\n    return %0 : tensor<3x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          6,
          8,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,8,2]. let\n    b:f32[5,8,2] = slice[\n      limit_indices=(6, 8, 2)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x8x2xf32>) -> (tensor<5x8x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:6, 0:8, 0:2] : (tensor<6x8x2xf32>) -> tensor<5x8x2xf32>\n    return %0 : tensor<5x8x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (120,))",
    "input_info": [
      {
        "shape": [
          3,
          5,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,5,8]. let\n    b:f32[120] = reshape[dimensions=None new_sizes=(120,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x5x8xf32>) -> (tensor<120xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<3x5x8xf32>) -> tensor<120xf32>\n    return %0 : tensor<120xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          3,
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,4] b:f32[3,4]. let\n    c:f32[6,4] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x4xf32>, %arg1: tensor<3x4xf32>) -> (tensor<6x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<3x4xf32>, tensor<3x4xf32>) -> tensor<6x4xf32>\n    return %0 : tensor<6x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (216,))",
    "input_info": [
      {
        "shape": [
          6,
          6,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,6,6]. let\n    b:f32[216] = reshape[dimensions=None new_sizes=(216,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x6x6xf32>) -> (tensor<216xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<6x6x6xf32>) -> tensor<216xf32>\n    return %0 : tensor<216xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          4,
          9,
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4,
          9,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,9,8] b:f32[4,9,8]. let\n    c:f32[8,9,8] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x9x8xf32>, %arg1: tensor<4x9x8xf32>) -> (tensor<8x9x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<4x9x8xf32>, tensor<4x9x8xf32>) -> tensor<8x9x8xf32>\n    return %0 : tensor<8x9x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          9,
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,2] b:f32[9,2]. let\n    c:f32[18,2] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x2xf32>, %arg1: tensor<9x2xf32>) -> (tensor<18x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<9x2xf32>, tensor<9x2xf32>) -> tensor<18x2xf32>\n    return %0 : tensor<18x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          6,
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,9] b:f32[6,9]. let\n    c:f32[12,9] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x9xf32>, %arg1: tensor<6x9xf32>) -> (tensor<12x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<6x9xf32>, tensor<6x9xf32>) -> tensor<12x9xf32>\n    return %0 : tensor<12x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (3, 135))",
    "input_info": [
      {
        "shape": [
          9,
          9,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,9,5]. let\n    b:f32[3,135] = reshape[dimensions=None new_sizes=(3, 135) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x9x5xf32>) -> (tensor<3x135xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<9x9x5xf32>) -> tensor<3x135xf32>\n    return %0 : tensor<3x135xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 12))",
    "input_info": [
      {
        "shape": [
          2,
          2,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,2,6]. let\n    b:f32[2,12] = reshape[dimensions=None new_sizes=(2, 12) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x2x6xf32>) -> (tensor<2x12xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<2x2x6xf32>) -> tensor<2x12xf32>\n    return %0 : tensor<2x12xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5]. let\n    b:f32[4] = slice[limit_indices=(5,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:5] : (tensor<5xf32>) -> tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          2,
          3,
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2,
          3,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,3,7] b:f32[2,3,7]. let\n    c:f32[1,2,3,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 2, 3, 7)\n      sharding=None\n    ] a\n    d:f32[1,2,3,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 2, 3, 7)\n      sharding=None\n    ] b\n    e:f32[2,2,3,7] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x3x7xf32>, %arg1: tensor<2x3x7xf32>) -> (tensor<2x2x3x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<2x3x7xf32>) -> tensor<1x2x3x7xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<2x3x7xf32>) -> tensor<1x2x3x7xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x2x3x7xf32>, tensor<1x2x3x7xf32>) -> tensor<2x2x3x7xf32>\n    return %2 : tensor<2x2x3x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          4,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,8]. let\n    b:f32[3,8] = slice[limit_indices=(4, 8) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x8xf32>) -> (tensor<3x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:4, 0:8] : (tensor<4x8xf32>) -> tensor<3x8xf32>\n    return %0 : tensor<3x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          8,
          6,
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          6,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,6,10] b:f32[8,6,10]. let\n    c:f32[1,8,6,10] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 8, 6, 10)\n      sharding=None\n    ] a\n    d:f32[1,8,6,10] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 8, 6, 10)\n      sharding=None\n    ] b\n    e:f32[2,8,6,10] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x6x10xf32>, %arg1: tensor<8x6x10xf32>) -> (tensor<2x8x6x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<8x6x10xf32>) -> tensor<1x8x6x10xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<8x6x10xf32>) -> tensor<1x8x6x10xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x8x6x10xf32>, tensor<1x8x6x10xf32>) -> tensor<2x8x6x10xf32>\n    return %2 : tensor<2x8x6x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          3,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,7]. let\n    b:f32[2,7] = slice[limit_indices=(3, 7) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x7xf32>) -> (tensor<2x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:3, 0:7] : (tensor<3x7xf32>) -> tensor<2x7xf32>\n    return %0 : tensor<2x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          3,
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,10] b:f32[3,10]. let\n    c:f32[6,10] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x10xf32>, %arg1: tensor<3x10xf32>) -> (tensor<6x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<3x10xf32>, tensor<3x10xf32>) -> tensor<6x10xf32>\n    return %0 : tensor<6x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>) -> (tensor<6xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          9,
          2,
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9,
          2,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,2,7] b:f32[9,2,7]. let\n    c:f32[18,2,7] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x2x7xf32>, %arg1: tensor<9x2x7xf32>) -> (tensor<18x2x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<9x2x7xf32>, tensor<9x2x7xf32>) -> tensor<18x2x7xf32>\n    return %0 : tensor<18x2x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (90,))",
    "input_info": [
      {
        "shape": [
          2,
          5,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,5,9]. let\n    b:f32[90] = reshape[dimensions=None new_sizes=(90,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x5x9xf32>) -> (tensor<90xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<2x5x9xf32>) -> tensor<90xf32>\n    return %0 : tensor<90xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          10,
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,4] b:f32[10,4]. let\n    c:f32[20,4] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x4xf32>, %arg1: tensor<10x4xf32>) -> (tensor<20x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<10x4xf32>, tensor<10x4xf32>) -> tensor<20x4xf32>\n    return %0 : tensor<20x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          3,
          5,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,5,9]. let\n    b:f32[2,5,9] = slice[\n      limit_indices=(3, 5, 9)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x5x9xf32>) -> (tensor<2x5x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:3, 0:5, 0:9] : (tensor<3x5x9xf32>) -> tensor<2x5x9xf32>\n    return %0 : tensor<2x5x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6] b:f32[6]. let\n    c:f32[1,6] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 6)\n      sharding=None\n    ] a\n    d:f32[1,6] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 6)\n      sharding=None\n    ] b\n    e:f32[2,6] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>, %arg1: tensor<6xf32>) -> (tensor<2x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<6xf32>) -> tensor<1x6xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<6xf32>) -> tensor<1x6xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x6xf32>, tensor<1x6xf32>) -> tensor<2x6xf32>\n    return %2 : tensor<2x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10] b:f32[10]. let\n    c:f32[20] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>, %arg1: tensor<10xf32>) -> (tensor<20xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<10xf32>, tensor<10xf32>) -> tensor<20xf32>\n    return %0 : tensor<20xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          4,
          8,
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4,
          8,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,8,2] b:f32[4,8,2]. let\n    c:f32[1,4,8,2] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 4, 8, 2)\n      sharding=None\n    ] a\n    d:f32[1,4,8,2] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 4, 8, 2)\n      sharding=None\n    ] b\n    e:f32[2,4,8,2] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x8x2xf32>, %arg1: tensor<4x8x2xf32>) -> (tensor<2x4x8x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<4x8x2xf32>) -> tensor<1x4x8x2xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<4x8x2xf32>) -> tensor<1x4x8x2xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x4x8x2xf32>, tensor<1x4x8x2xf32>) -> tensor<2x4x8x2xf32>\n    return %2 : tensor<2x4x8x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          5,
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,9] b:f32[5,9]. let\n    c:f32[10,9] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x9xf32>, %arg1: tensor<5x9xf32>) -> (tensor<10x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<5x9xf32>, tensor<5x9xf32>) -> tensor<10x9xf32>\n    return %0 : tensor<10x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          9,
          5,
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9,
          5,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,5,8] b:f32[9,5,8]. let\n    c:f32[18,5,8] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x5x8xf32>, %arg1: tensor<9x5x8xf32>) -> (tensor<18x5x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<9x5x8xf32>, tensor<9x5x8xf32>) -> tensor<18x5x8xf32>\n    return %0 : tensor<18x5x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2] b:f32[2]. let\n    c:f32[1,2] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 2)\n      sharding=None\n    ] a\n    d:f32[1,2] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 2)\n      sharding=None\n    ] b\n    e:f32[2,2] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>, %arg1: tensor<2xf32>) -> (tensor<2x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<2xf32>) -> tensor<1x2xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<2xf32>) -> tensor<1x2xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x2xf32>, tensor<1x2xf32>) -> tensor<2x2xf32>\n    return %2 : tensor<2x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2] b:f32[2]. let\n    c:f32[4] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>, %arg1: tensor<2xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<2xf32>, tensor<2xf32>) -> tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          8,
          2,
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          2,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,2,6] b:f32[8,2,6]. let\n    c:f32[16,2,6] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x2x6xf32>, %arg1: tensor<8x2x6xf32>) -> (tensor<16x2x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<8x2x6xf32>, tensor<8x2x6xf32>) -> tensor<16x2x6xf32>\n    return %0 : tensor<16x2x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9]. let\n    b:f32[8] = slice[limit_indices=(9,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>) -> (tensor<8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:9] : (tensor<9xf32>) -> tensor<8xf32>\n    return %0 : tensor<8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 125))",
    "input_info": [
      {
        "shape": [
          5,
          5,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,5,10]. let\n    b:f32[2,125] = reshape[dimensions=None new_sizes=(2, 125) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x5x10xf32>) -> (tensor<2x125xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<5x5x10xf32>) -> tensor<2x125xf32>\n    return %0 : tensor<2x125xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6]. let\n    b:f32[5] = slice[limit_indices=(6,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>) -> (tensor<5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:6] : (tensor<6xf32>) -> tensor<5xf32>\n    return %0 : tensor<5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          9,
          7,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,7,5]. let\n    b:f32[5,7,9] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x7x5xf32>) -> (tensor<5x7x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<9x7x5xf32>) -> tensor<5x7x9xf32>\n    return %0 : tensor<5x7x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          2,
          3,
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2,
          3,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,3,7] b:f32[2,3,7]. let\n    c:f32[1,2,3,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 2, 3, 7)\n      sharding=None\n    ] a\n    d:f32[1,2,3,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 2, 3, 7)\n      sharding=None\n    ] b\n    e:f32[2,2,3,7] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x3x7xf32>, %arg1: tensor<2x3x7xf32>) -> (tensor<2x2x3x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<2x3x7xf32>) -> tensor<1x2x3x7xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<2x3x7xf32>) -> tensor<1x2x3x7xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x2x3x7xf32>, tensor<1x2x3x7xf32>) -> tensor<2x2x3x7xf32>\n    return %2 : tensor<2x2x3x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          8,
          3,
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          3,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,3,4] b:f32[8,3,4]. let\n    c:f32[16,3,4] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x3x4xf32>, %arg1: tensor<8x3x4xf32>) -> (tensor<16x3x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<8x3x4xf32>, tensor<8x3x4xf32>) -> tensor<16x3x4xf32>\n    return %0 : tensor<16x3x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          8,
          9,
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          9,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,9,8] b:f32[8,9,8]. let\n    c:f32[1,8,9,8] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 8, 9, 8)\n      sharding=None\n    ] a\n    d:f32[1,8,9,8] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 8, 9, 8)\n      sharding=None\n    ] b\n    e:f32[2,8,9,8] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x9x8xf32>, %arg1: tensor<8x9x8xf32>) -> (tensor<2x8x9x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<8x9x8xf32>) -> tensor<1x8x9x8xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<8x9x8xf32>) -> tensor<1x8x9x8xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x8x9x8xf32>, tensor<1x8x9x8xf32>) -> tensor<2x8x9x8xf32>\n    return %2 : tensor<2x8x9x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9]. let\n    b:f32[8] = slice[limit_indices=(9,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>) -> (tensor<8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:9] : (tensor<9xf32>) -> tensor<8xf32>\n    return %0 : tensor<8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          8,
          4,
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          4,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,4,4] b:f32[8,4,4]. let\n    c:f32[16,4,4] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x4x4xf32>, %arg1: tensor<8x4x4xf32>) -> (tensor<16x4x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<8x4x4xf32>, tensor<8x4x4xf32>) -> tensor<16x4x4xf32>\n    return %0 : tensor<16x4x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          6,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,4]. let\n    b:f32[4,6] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x4xf32>) -> (tensor<4x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<6x4xf32>) -> tensor<4x6xf32>\n    return %0 : tensor<4x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          9,
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,7] b:f32[9,7]. let\n    c:f32[1,9,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 9, 7)\n      sharding=None\n    ] a\n    d:f32[1,9,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 9, 7)\n      sharding=None\n    ] b\n    e:f32[2,9,7] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x7xf32>, %arg1: tensor<9x7xf32>) -> (tensor<2x9x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<9x7xf32>) -> tensor<1x9x7xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<9x7xf32>) -> tensor<1x9x7xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x9x7xf32>, tensor<1x9x7xf32>) -> tensor<2x9x7xf32>\n    return %2 : tensor<2x9x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>) -> (tensor<5xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 60))",
    "input_info": [
      {
        "shape": [
          10,
          4,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,4,3]. let\n    b:f32[2,60] = reshape[dimensions=None new_sizes=(2, 60) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x4x3xf32>) -> (tensor<2x60xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<10x4x3xf32>) -> tensor<2x60xf32>\n    return %0 : tensor<2x60xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          5,
          5,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,5,4]. let\n    b:f32[4,5,4] = slice[\n      limit_indices=(5, 5, 4)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x5x4xf32>) -> (tensor<4x5x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:5, 0:5, 0:4] : (tensor<5x5x4xf32>) -> tensor<4x5x4xf32>\n    return %0 : tensor<4x5x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7] b:f32[7]. let\n    c:f32[1,7] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 7)\n      sharding=None\n    ] a\n    d:f32[1,7] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 7)\n      sharding=None\n    ] b\n    e:f32[2,7] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>, %arg1: tensor<7xf32>) -> (tensor<2x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<7xf32>) -> tensor<1x7xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<7xf32>) -> tensor<1x7xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x7xf32>, tensor<1x7xf32>) -> tensor<2x7xf32>\n    return %2 : tensor<2x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          9,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,5]. let\n    b:f32[8,5] = slice[limit_indices=(9, 5) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x5xf32>) -> (tensor<8x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:9, 0:5] : (tensor<9x5xf32>) -> tensor<8x5xf32>\n    return %0 : tensor<8x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          8,
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,9] b:f32[8,9]. let\n    c:f32[16,9] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x9xf32>, %arg1: tensor<8x9xf32>) -> (tensor<16x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<8x9xf32>, tensor<8x9xf32>) -> tensor<16x9xf32>\n    return %0 : tensor<16x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10] b:f32[10]. let\n    c:f32[1,10] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 10)\n      sharding=None\n    ] a\n    d:f32[1,10] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 10)\n      sharding=None\n    ] b\n    e:f32[2,10] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>, %arg1: tensor<10xf32>) -> (tensor<2x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<10xf32>) -> tensor<1x10xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<10xf32>) -> tensor<1x10xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<2x10xf32>\n    return %2 : tensor<2x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          10,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,5]. let\n    b:f32[9,5] = slice[limit_indices=(10, 5) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x5xf32>) -> (tensor<9x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:10, 0:5] : (tensor<10x5xf32>) -> tensor<9x5xf32>\n    return %0 : tensor<9x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          8,
          8,
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          8,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,8,7] b:f32[8,8,7]. let\n    c:f32[1,8,8,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 8, 8, 7)\n      sharding=None\n    ] a\n    d:f32[1,8,8,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 8, 8, 7)\n      sharding=None\n    ] b\n    e:f32[2,8,8,7] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x8x7xf32>, %arg1: tensor<8x8x7xf32>) -> (tensor<2x8x8x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<8x8x7xf32>) -> tensor<1x8x8x7xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<8x8x7xf32>) -> tensor<1x8x8x7xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x8x8x7xf32>, tensor<1x8x8x7xf32>) -> tensor<2x8x8x7xf32>\n    return %2 : tensor<2x8x8x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          2,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,5]. let\n    b:f32[5,2] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x5xf32>) -> (tensor<5x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<2x5xf32>) -> tensor<5x2xf32>\n    return %0 : tensor<5x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8] b:f32[8]. let\n    c:f32[16] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8xf32>, %arg1: tensor<8xf32>) -> (tensor<16xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<8xf32>, tensor<8xf32>) -> tensor<16xf32>\n    return %0 : tensor<16xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          9,
          6,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,6,8]. let\n    b:f32[8,6,9] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x6x8xf32>) -> (tensor<8x6x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<9x6x8xf32>) -> tensor<8x6x9xf32>\n    return %0 : tensor<8x6x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8] b:f32[8]. let\n    c:f32[16] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8xf32>, %arg1: tensor<8xf32>) -> (tensor<16xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<8xf32>, tensor<8xf32>) -> tensor<16xf32>\n    return %0 : tensor<16xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5] b:f32[5]. let\n    c:f32[10] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>, %arg1: tensor<5xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<5xf32>, tensor<5xf32>) -> tensor<10xf32>\n    return %0 : tensor<10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>) -> (tensor<2xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          4,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,7]. let\n    b:f32[3,7] = slice[limit_indices=(4, 7) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x7xf32>) -> (tensor<3x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:4, 0:7] : (tensor<4x7xf32>) -> tensor<3x7xf32>\n    return %0 : tensor<3x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          10,
          8,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,8,8]. let\n    b:f32[8,8,10] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x8x8xf32>) -> (tensor<8x8x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<10x8x8xf32>) -> tensor<8x8x10xf32>\n    return %0 : tensor<8x8x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 35))",
    "input_info": [
      {
        "shape": [
          7,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,10]. let\n    b:f32[2,35] = reshape[dimensions=None new_sizes=(2, 35) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x10xf32>) -> (tensor<2x35xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<7x10xf32>) -> tensor<2x35xf32>\n    return %0 : tensor<2x35xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          8,
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,4] b:f32[8,4]. let\n    c:f32[1,8,4] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 8, 4)\n      sharding=None\n    ] a\n    d:f32[1,8,4] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 8, 4)\n      sharding=None\n    ] b\n    e:f32[2,8,4] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x4xf32>, %arg1: tensor<8x4xf32>) -> (tensor<2x8x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<8x4xf32>) -> tensor<1x8x4xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<8x4xf32>) -> tensor<1x8x4xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x8x4xf32>, tensor<1x8x4xf32>) -> tensor<2x8x4xf32>\n    return %2 : tensor<2x8x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2] b:f32[2]. let\n    c:f32[4] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>, %arg1: tensor<2xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<2xf32>, tensor<2xf32>) -> tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8] b:f32[8]. let\n    c:f32[16] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8xf32>, %arg1: tensor<8xf32>) -> (tensor<16xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<8xf32>, tensor<8xf32>) -> tensor<16xf32>\n    return %0 : tensor<16xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (3, 3))",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9]. let\n    b:f32[3,3] = reshape[dimensions=None new_sizes=(3, 3) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>) -> (tensor<3x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<9xf32>) -> tensor<3x3xf32>\n    return %0 : tensor<3x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>) -> (tensor<2xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 21))",
    "input_info": [
      {
        "shape": [
          7,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,6]. let\n    b:f32[2,21] = reshape[dimensions=None new_sizes=(2, 21) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x6xf32>) -> (tensor<2x21xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<7x6xf32>) -> tensor<2x21xf32>\n    return %0 : tensor<2x21xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          10,
          3,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,3,3]. let\n    b:f32[9,3,3] = slice[\n      limit_indices=(10, 3, 3)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x3x3xf32>) -> (tensor<9x3x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:10, 0:3, 0:3] : (tensor<10x3x3xf32>) -> tensor<9x3x3xf32>\n    return %0 : tensor<9x3x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          7,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,7]. let\n    b:f32[6,7] = slice[limit_indices=(7, 7) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x7xf32>) -> (tensor<6x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:7, 0:7] : (tensor<7x7xf32>) -> tensor<6x7xf32>\n    return %0 : tensor<6x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          4,
          10,
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4,
          10,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,10,4] b:f32[4,10,4]. let\n    c:f32[1,4,10,4] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 4, 10, 4)\n      sharding=None\n    ] a\n    d:f32[1,4,10,4] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 4, 10, 4)\n      sharding=None\n    ] b\n    e:f32[2,4,10,4] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x10x4xf32>, %arg1: tensor<4x10x4xf32>) -> (tensor<2x4x10x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<4x10x4xf32>) -> tensor<1x4x10x4xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<4x10x4xf32>) -> tensor<1x4x10x4xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x4x10x4xf32>, tensor<1x4x10x4xf32>) -> tensor<2x4x10x4xf32>\n    return %2 : tensor<2x4x10x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          8,
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,7] b:f32[8,7]. let\n    c:f32[1,8,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 8, 7)\n      sharding=None\n    ] a\n    d:f32[1,8,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 8, 7)\n      sharding=None\n    ] b\n    e:f32[2,8,7] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x7xf32>, %arg1: tensor<8x7xf32>) -> (tensor<2x8x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<8x7xf32>) -> tensor<1x8x7xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<8x7xf32>) -> tensor<1x8x7xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x8x7xf32>, tensor<1x8x7xf32>) -> tensor<2x8x7xf32>\n    return %2 : tensor<2x8x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9]. let\n    b:f32[8] = slice[limit_indices=(9,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>) -> (tensor<8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:9] : (tensor<9xf32>) -> tensor<8xf32>\n    return %0 : tensor<8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          3,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,10]. let\n    b:f32[2,10] = slice[limit_indices=(3, 10) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x10xf32>) -> (tensor<2x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:3, 0:10] : (tensor<3x10xf32>) -> tensor<2x10xf32>\n    return %0 : tensor<2x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          4,
          8,
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4,
          8,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,8,6] b:f32[4,8,6]. let\n    c:f32[8,8,6] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x8x6xf32>, %arg1: tensor<4x8x6xf32>) -> (tensor<8x8x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<4x8x6xf32>, tensor<4x8x6xf32>) -> tensor<8x8x6xf32>\n    return %0 : tensor<8x8x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          4,
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,10] b:f32[4,10]. let\n    c:f32[8,10] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x10xf32>, %arg1: tensor<4x10xf32>) -> (tensor<8x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<4x10xf32>, tensor<4x10xf32>) -> tensor<8x10xf32>\n    return %0 : tensor<8x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10] b:f32[10]. let\n    c:f32[20] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>, %arg1: tensor<10xf32>) -> (tensor<20xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<10xf32>, tensor<10xf32>) -> tensor<20xf32>\n    return %0 : tensor<20xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          8,
          8,
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          8,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,8,7] b:f32[8,8,7]. let\n    c:f32[1,8,8,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 8, 8, 7)\n      sharding=None\n    ] a\n    d:f32[1,8,8,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 8, 8, 7)\n      sharding=None\n    ] b\n    e:f32[2,8,8,7] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x8x7xf32>, %arg1: tensor<8x8x7xf32>) -> (tensor<2x8x8x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<8x8x7xf32>) -> tensor<1x8x8x7xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<8x8x7xf32>) -> tensor<1x8x8x7xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x8x8x7xf32>, tensor<1x8x8x7xf32>) -> tensor<2x8x8x7xf32>\n    return %2 : tensor<2x8x8x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 4))",
    "input_info": [
      {
        "shape": [
          2,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,4]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x4xf32>) -> (tensor<2x4xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<2x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (20,))",
    "input_info": [
      {
        "shape": [
          10,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,2]. let\n    b:f32[20] = reshape[dimensions=None new_sizes=(20,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x2xf32>) -> (tensor<20xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<10x2xf32>) -> tensor<20xf32>\n    return %0 : tensor<20xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          6,
          9,
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6,
          9,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,9,6] b:f32[6,9,6]. let\n    c:f32[1,6,9,6] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 6, 9, 6)\n      sharding=None\n    ] a\n    d:f32[1,6,9,6] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 6, 9, 6)\n      sharding=None\n    ] b\n    e:f32[2,6,9,6] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x9x6xf32>, %arg1: tensor<6x9x6xf32>) -> (tensor<2x6x9x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<6x9x6xf32>) -> tensor<1x6x9x6xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<6x9x6xf32>) -> tensor<1x6x9x6xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x6x9x6xf32>, tensor<1x6x9x6xf32>) -> tensor<2x6x9x6xf32>\n    return %2 : tensor<2x6x9x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10] b:f32[10]. let\n    c:f32[20] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>, %arg1: tensor<10xf32>) -> (tensor<20xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<10xf32>, tensor<10xf32>) -> tensor<20xf32>\n    return %0 : tensor<20xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          2,
          3,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,3,7]. let\n    b:f32[1,3,7] = slice[\n      limit_indices=(2, 3, 7)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x3x7xf32>) -> (tensor<1x3x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:2, 0:3, 0:7] : (tensor<2x3x7xf32>) -> tensor<1x3x7xf32>\n    return %0 : tensor<1x3x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          7,
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,9] b:f32[7,9]. let\n    c:f32[14,9] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x9xf32>, %arg1: tensor<7x9xf32>) -> (tensor<14x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<7x9xf32>, tensor<7x9xf32>) -> tensor<14x9xf32>\n    return %0 : tensor<14x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>) -> (tensor<7xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          4,
          10,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,10,7]. let\n    b:f32[3,10,7] = slice[\n      limit_indices=(4, 10, 7)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x10x7xf32>) -> (tensor<3x10x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:4, 0:10, 0:7] : (tensor<4x10x7xf32>) -> tensor<3x10x7xf32>\n    return %0 : tensor<3x10x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2] b:f32[2]. let\n    c:f32[4] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>, %arg1: tensor<2xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<2xf32>, tensor<2xf32>) -> tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          7,
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,6] b:f32[7,6]. let\n    c:f32[1,7,6] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 7, 6)\n      sharding=None\n    ] a\n    d:f32[1,7,6] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 7, 6)\n      sharding=None\n    ] b\n    e:f32[2,7,6] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x6xf32>, %arg1: tensor<7x6xf32>) -> (tensor<2x7x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<7x6xf32>) -> tensor<1x7x6xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<7x6xf32>) -> tensor<1x7x6xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x7x6xf32>, tensor<1x7x6xf32>) -> tensor<2x7x6xf32>\n    return %2 : tensor<2x7x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4] b:f32[4]. let\n    c:f32[1,4] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 4)\n      sharding=None\n    ] a\n    d:f32[1,4] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 4)\n      sharding=None\n    ] b\n    e:f32[2,4] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> (tensor<2x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<4xf32>) -> tensor<1x4xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<4xf32>) -> tensor<1x4xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x4xf32>, tensor<1x4xf32>) -> tensor<2x4xf32>\n    return %2 : tensor<2x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          8,
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,4] b:f32[8,4]. let\n    c:f32[16,4] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x4xf32>, %arg1: tensor<8x4xf32>) -> (tensor<16x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<8x4xf32>, tensor<8x4xf32>) -> tensor<16x4xf32>\n    return %0 : tensor<16x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          3,
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,6] b:f32[3,6]. let\n    c:f32[1,3,6] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 3, 6)\n      sharding=None\n    ] a\n    d:f32[1,3,6] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 3, 6)\n      sharding=None\n    ] b\n    e:f32[2,3,6] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x6xf32>, %arg1: tensor<3x6xf32>) -> (tensor<2x3x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<3x6xf32>) -> tensor<1x3x6xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<3x6xf32>) -> tensor<1x3x6xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x3x6xf32>, tensor<1x3x6xf32>) -> tensor<2x3x6xf32>\n    return %2 : tensor<2x3x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8] b:f32[8]. let\n    c:f32[1,8] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 8)\n      sharding=None\n    ] a\n    d:f32[1,8] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 8)\n      sharding=None\n    ] b\n    e:f32[2,8] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8xf32>, %arg1: tensor<8xf32>) -> (tensor<2x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<8xf32>) -> tensor<1x8xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<8xf32>) -> tensor<1x8xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x8xf32>, tensor<1x8xf32>) -> tensor<2x8xf32>\n    return %2 : tensor<2x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (10,))",
    "input_info": [
      {
        "shape": [
          5,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,2]. let\n    b:f32[10] = reshape[dimensions=None new_sizes=(10,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x2xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<5x2xf32>) -> tensor<10xf32>\n    return %0 : tensor<10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          8,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,7]. let\n    b:f32[7,7] = slice[limit_indices=(8, 7) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x7xf32>) -> (tensor<7x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:8, 0:7] : (tensor<8x7xf32>) -> tensor<7x7xf32>\n    return %0 : tensor<7x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          2,
          10,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,10,3]. let\n    b:f32[1,10,3] = slice[\n      limit_indices=(2, 10, 3)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x10x3xf32>) -> (tensor<1x10x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:2, 0:10, 0:3] : (tensor<2x10x3xf32>) -> tensor<1x10x3xf32>\n    return %0 : tensor<1x10x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          5,
          4,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,4,9]. let\n    b:f32[9,4,5] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x4x9xf32>) -> (tensor<9x4x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<5x4x9xf32>) -> tensor<9x4x5xf32>\n    return %0 : tensor<9x4x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          3,
          5,
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3,
          5,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,5,7] b:f32[3,5,7]. let\n    c:f32[1,3,5,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 3, 5, 7)\n      sharding=None\n    ] a\n    d:f32[1,3,5,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 3, 5, 7)\n      sharding=None\n    ] b\n    e:f32[2,3,5,7] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x5x7xf32>, %arg1: tensor<3x5x7xf32>) -> (tensor<2x3x5x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<3x5x7xf32>) -> tensor<1x3x5x7xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<3x5x7xf32>) -> tensor<1x3x5x7xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x3x5x7xf32>, tensor<1x3x5x7xf32>) -> tensor<2x3x5x7xf32>\n    return %2 : tensor<2x3x5x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          9,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,6]. let\n    b:f32[6,9] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x6xf32>) -> (tensor<6x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<9x6xf32>) -> tensor<6x9xf32>\n    return %0 : tensor<6x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 150))",
    "input_info": [
      {
        "shape": [
          6,
          5,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,5,10]. let\n    b:f32[2,150] = reshape[dimensions=None new_sizes=(2, 150) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x5x10xf32>) -> (tensor<2x150xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<6x5x10xf32>) -> tensor<2x150xf32>\n    return %0 : tensor<2x150xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          8,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,4]. let\n    b:f32[4,8] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x4xf32>) -> (tensor<4x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<8x4xf32>) -> tensor<4x8xf32>\n    return %0 : tensor<4x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (240,))",
    "input_info": [
      {
        "shape": [
          3,
          10,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,10,8]. let\n    b:f32[240] = reshape[dimensions=None new_sizes=(240,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x10x8xf32>) -> (tensor<240xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<3x10x8xf32>) -> tensor<240xf32>\n    return %0 : tensor<240xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8] b:f32[8]. let\n    c:f32[16] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8xf32>, %arg1: tensor<8xf32>) -> (tensor<16xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<8xf32>, tensor<8xf32>) -> tensor<16xf32>\n    return %0 : tensor<16xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          5,
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,10] b:f32[5,10]. let\n    c:f32[1,5,10] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 5, 10)\n      sharding=None\n    ] a\n    d:f32[1,5,10] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 5, 10)\n      sharding=None\n    ] b\n    e:f32[2,5,10] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x10xf32>, %arg1: tensor<5x10xf32>) -> (tensor<2x5x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<5x10xf32>) -> tensor<1x5x10xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<5x10xf32>) -> tensor<1x5x10xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x5x10xf32>, tensor<1x5x10xf32>) -> tensor<2x5x10xf32>\n    return %2 : tensor<2x5x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          3,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,2]. let\n    b:f32[2,2] = slice[limit_indices=(3, 2) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x2xf32>) -> (tensor<2x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:3, 0:2] : (tensor<3x2xf32>) -> tensor<2x2xf32>\n    return %0 : tensor<2x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          7,
          9,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,9,8]. let\n    b:f32[6,9,8] = slice[\n      limit_indices=(7, 9, 8)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x9x8xf32>) -> (tensor<6x9x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:7, 0:9, 0:8] : (tensor<7x9x8xf32>) -> tensor<6x9x8xf32>\n    return %0 : tensor<6x9x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          6,
          6,
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6,
          6,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,6,7] b:f32[6,6,7]. let\n    c:f32[12,6,7] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x6x7xf32>, %arg1: tensor<6x6x7xf32>) -> (tensor<12x6x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<6x6x7xf32>, tensor<6x6x7xf32>) -> tensor<12x6x7xf32>\n    return %0 : tensor<12x6x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          5,
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,7] b:f32[5,7]. let\n    c:f32[1,5,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 5, 7)\n      sharding=None\n    ] a\n    d:f32[1,5,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 5, 7)\n      sharding=None\n    ] b\n    e:f32[2,5,7] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x7xf32>, %arg1: tensor<5x7xf32>) -> (tensor<2x5x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<5x7xf32>) -> tensor<1x5x7xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<5x7xf32>) -> tensor<1x5x7xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x5x7xf32>, tensor<1x5x7xf32>) -> tensor<2x5x7xf32>\n    return %2 : tensor<2x5x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          5,
          10,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,10,6]. let\n    b:f32[6,10,5] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x10x6xf32>) -> (tensor<6x10x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<5x10x6xf32>) -> tensor<6x10x5xf32>\n    return %0 : tensor<6x10x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          2,
          10,
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2,
          10,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,10,6] b:f32[2,10,6]. let\n    c:f32[4,10,6] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x10x6xf32>, %arg1: tensor<2x10x6xf32>) -> (tensor<4x10x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<2x10x6xf32>, tensor<2x10x6xf32>) -> tensor<4x10x6xf32>\n    return %0 : tensor<4x10x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3]. let\n    b:f32[2] = slice[limit_indices=(3,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>) -> (tensor<2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:3] : (tensor<3xf32>) -> tensor<2xf32>\n    return %0 : tensor<2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5] b:f32[5]. let\n    c:f32[1,5] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 5)\n      sharding=None\n    ] a\n    d:f32[1,5] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 5)\n      sharding=None\n    ] b\n    e:f32[2,5] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>, %arg1: tensor<5xf32>) -> (tensor<2x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<5xf32>) -> tensor<1x5xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<5xf32>) -> tensor<1x5xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x5xf32>, tensor<1x5xf32>) -> tensor<2x5xf32>\n    return %2 : tensor<2x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          5,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,7]. let\n    b:f32[7,5] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x7xf32>) -> (tensor<7x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<5x7xf32>) -> tensor<7x5xf32>\n    return %0 : tensor<7x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          10,
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,8] b:f32[10,8]. let\n    c:f32[20,8] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x8xf32>, %arg1: tensor<10x8xf32>) -> (tensor<20x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<10x8xf32>, tensor<10x8xf32>) -> tensor<20x8xf32>\n    return %0 : tensor<20x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4]. let\n    b:f32[3] = slice[limit_indices=(4,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:4] : (tensor<4xf32>) -> tensor<3xf32>\n    return %0 : tensor<3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          7,
          5,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,5,6]. let\n    b:f32[6,5,7] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x5x6xf32>) -> (tensor<6x5x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<7x5x6xf32>) -> tensor<6x5x7xf32>\n    return %0 : tensor<6x5x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          9,
          10,
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9,
          10,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,10,2] b:f32[9,10,2]. let\n    c:f32[1,9,10,2] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 9, 10, 2)\n      sharding=None\n    ] a\n    d:f32[1,9,10,2] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 9, 10, 2)\n      sharding=None\n    ] b\n    e:f32[2,9,10,2] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x10x2xf32>, %arg1: tensor<9x10x2xf32>) -> (tensor<2x9x10x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<9x10x2xf32>) -> tensor<1x9x10x2xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<9x10x2xf32>) -> tensor<1x9x10x2xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x9x10x2xf32>, tensor<1x9x10x2xf32>) -> tensor<2x9x10x2xf32>\n    return %2 : tensor<2x9x10x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2] b:f32[2]. let\n    c:f32[4] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>, %arg1: tensor<2xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<2xf32>, tensor<2xf32>) -> tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3] b:f32[3]. let\n    c:f32[6] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>, %arg1: tensor<3xf32>) -> (tensor<6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<3xf32>, tensor<3xf32>) -> tensor<6xf32>\n    return %0 : tensor<6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          9,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,6]. let\n    b:f32[6,9] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x6xf32>) -> (tensor<6x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<9x6xf32>) -> tensor<6x9xf32>\n    return %0 : tensor<6x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          6,
          8,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,8,8]. let\n    b:f32[8,8,6] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x8x8xf32>) -> (tensor<8x8x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<6x8x8xf32>) -> tensor<8x8x6xf32>\n    return %0 : tensor<8x8x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          7,
          2,
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7,
          2,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,2,9] b:f32[7,2,9]. let\n    c:f32[1,7,2,9] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 7, 2, 9)\n      sharding=None\n    ] a\n    d:f32[1,7,2,9] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 7, 2, 9)\n      sharding=None\n    ] b\n    e:f32[2,7,2,9] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x2x9xf32>, %arg1: tensor<7x2x9xf32>) -> (tensor<2x7x2x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<7x2x9xf32>) -> tensor<1x7x2x9xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<7x2x9xf32>) -> tensor<1x7x2x9xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x7x2x9xf32>, tensor<1x7x2x9xf32>) -> tensor<2x7x2x9xf32>\n    return %2 : tensor<2x7x2x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>) -> (tensor<7xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          7,
          9,
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7,
          9,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,9,3] b:f32[7,9,3]. let\n    c:f32[1,7,9,3] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 7, 9, 3)\n      sharding=None\n    ] a\n    d:f32[1,7,9,3] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 7, 9, 3)\n      sharding=None\n    ] b\n    e:f32[2,7,9,3] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x9x3xf32>, %arg1: tensor<7x9x3xf32>) -> (tensor<2x7x9x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<7x9x3xf32>) -> tensor<1x7x9x3xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<7x9x3xf32>) -> tensor<1x7x9x3xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x7x9x3xf32>, tensor<1x7x9x3xf32>) -> tensor<2x7x9x3xf32>\n    return %2 : tensor<2x7x9x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8] b:f32[8]. let\n    c:f32[1,8] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 8)\n      sharding=None\n    ] a\n    d:f32[1,8] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 8)\n      sharding=None\n    ] b\n    e:f32[2,8] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8xf32>, %arg1: tensor<8xf32>) -> (tensor<2x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<8xf32>) -> tensor<1x8xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<8xf32>) -> tensor<1x8xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x8xf32>, tensor<1x8xf32>) -> tensor<2x8xf32>\n    return %2 : tensor<2x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8xf32>) -> (tensor<8xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 162))",
    "input_info": [
      {
        "shape": [
          9,
          9,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,9,4]. let\n    b:f32[2,162] = reshape[dimensions=None new_sizes=(2, 162) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x9x4xf32>) -> (tensor<2x162xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<9x9x4xf32>) -> tensor<2x162xf32>\n    return %0 : tensor<2x162xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          6,
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,2] b:f32[6,2]. let\n    c:f32[1,6,2] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 6, 2)\n      sharding=None\n    ] a\n    d:f32[1,6,2] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 6, 2)\n      sharding=None\n    ] b\n    e:f32[2,6,2] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x2xf32>, %arg1: tensor<6x2xf32>) -> (tensor<2x6x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<6x2xf32>) -> tensor<1x6x2xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<6x2xf32>) -> tensor<1x6x2xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x6x2xf32>, tensor<1x6x2xf32>) -> tensor<2x6x2xf32>\n    return %2 : tensor<2x6x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          5,
          2,
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5,
          2,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,2,9] b:f32[5,2,9]. let\n    c:f32[10,2,9] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x2x9xf32>, %arg1: tensor<5x2x9xf32>) -> (tensor<10x2x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<5x2x9xf32>, tensor<5x2x9xf32>) -> tensor<10x2x9xf32>\n    return %0 : tensor<10x2x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          8,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,3]. let\n    b:f32[7,3] = slice[limit_indices=(8, 3) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x3xf32>) -> (tensor<7x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:8, 0:3] : (tensor<8x3xf32>) -> tensor<7x3xf32>\n    return %0 : tensor<7x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          6,
          9,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,9,7]. let\n    b:f32[7,9,6] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x9x7xf32>) -> (tensor<7x9x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<6x9x7xf32>) -> tensor<7x9x6xf32>\n    return %0 : tensor<7x9x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          6,
          4,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,4,7]. let\n    b:f32[5,4,7] = slice[\n      limit_indices=(6, 4, 7)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x4x7xf32>) -> (tensor<5x4x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:6, 0:4, 0:7] : (tensor<6x4x7xf32>) -> tensor<5x4x7xf32>\n    return %0 : tensor<5x4x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6]. let\n    b:f32[5] = slice[limit_indices=(6,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>) -> (tensor<5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:6] : (tensor<6xf32>) -> tensor<5xf32>\n    return %0 : tensor<5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 100))",
    "input_info": [
      {
        "shape": [
          4,
          10,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,10,5]. let\n    b:f32[2,100] = reshape[dimensions=None new_sizes=(2, 100) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x10x5xf32>) -> (tensor<2x100xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<4x10x5xf32>) -> tensor<2x100xf32>\n    return %0 : tensor<2x100xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          2,
          8,
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2,
          8,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,8,10] b:f32[2,8,10]. let\n    c:f32[1,2,8,10] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 2, 8, 10)\n      sharding=None\n    ] a\n    d:f32[1,2,8,10] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 2, 8, 10)\n      sharding=None\n    ] b\n    e:f32[2,2,8,10] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x8x10xf32>, %arg1: tensor<2x8x10xf32>) -> (tensor<2x2x8x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<2x8x10xf32>) -> tensor<1x2x8x10xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<2x8x10xf32>) -> tensor<1x2x8x10xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x2x8x10xf32>, tensor<1x2x8x10xf32>) -> tensor<2x2x8x10xf32>\n    return %2 : tensor<2x2x8x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          2,
          6,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,6,10]. let\n    b:f32[1,6,10] = slice[\n      limit_indices=(2, 6, 10)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x6x10xf32>) -> (tensor<1x6x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:2, 0:6, 0:10] : (tensor<2x6x10xf32>) -> tensor<1x6x10xf32>\n    return %0 : tensor<1x6x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9] b:f32[9]. let\n    c:f32[18] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>, %arg1: tensor<9xf32>) -> (tensor<18xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<9xf32>, tensor<9xf32>) -> tensor<18xf32>\n    return %0 : tensor<18xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2]. let\n    b:f32[1] = slice[limit_indices=(2,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>) -> (tensor<1xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:2] : (tensor<2xf32>) -> tensor<1xf32>\n    return %0 : tensor<1xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:]",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9]. let\n    b:f32[8] = slice[limit_indices=(9,) start_indices=(1,) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>) -> (tensor<8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:9] : (tensor<9xf32>) -> tensor<8xf32>\n    return %0 : tensor<8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>) -> (tensor<2xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>) -> (tensor<7xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          2,
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,5] b:f32[2,5]. let\n    c:f32[4,5] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x5xf32>, %arg1: tensor<2x5xf32>) -> (tensor<4x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<2x5xf32>, tensor<2x5xf32>) -> tensor<4x5xf32>\n    return %0 : tensor<4x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8xf32>) -> (tensor<8xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (18,))",
    "input_info": [
      {
        "shape": [
          6,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,3]. let\n    b:f32[18] = reshape[dimensions=None new_sizes=(18,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x3xf32>) -> (tensor<18xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<6x3xf32>) -> tensor<18xf32>\n    return %0 : tensor<18xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          10,
          4,
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10,
          4,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,4,7] b:f32[10,4,7]. let\n    c:f32[1,10,4,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 10, 4, 7)\n      sharding=None\n    ] a\n    d:f32[1,10,4,7] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 10, 4, 7)\n      sharding=None\n    ] b\n    e:f32[2,10,4,7] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x4x7xf32>, %arg1: tensor<10x4x7xf32>) -> (tensor<2x10x4x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<10x4x7xf32>) -> tensor<1x10x4x7xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<10x4x7xf32>) -> tensor<1x10x4x7xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x10x4x7xf32>, tensor<1x10x4x7xf32>) -> tensor<2x10x4x7xf32>\n    return %2 : tensor<2x10x4x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          10,
          4,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,4,5]. let\n    b:f32[9,4,5] = slice[\n      limit_indices=(10, 4, 5)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x4x5xf32>) -> (tensor<9x4x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:10, 0:4, 0:5] : (tensor<10x4x5xf32>) -> tensor<9x4x5xf32>\n    return %0 : tensor<9x4x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          7,
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,10] b:f32[7,10]. let\n    c:f32[14,10] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x10xf32>, %arg1: tensor<7x10xf32>) -> (tensor<14x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<7x10xf32>, tensor<7x10xf32>) -> tensor<14x10xf32>\n    return %0 : tensor<14x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9] b:f32[9]. let\n    c:f32[18] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>, %arg1: tensor<9xf32>) -> (tensor<18xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<9xf32>, tensor<9xf32>) -> tensor<18xf32>\n    return %0 : tensor<18xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (10,))",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          4,
          2,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,2,4]. let\n    b:f32[3,2,4] = slice[\n      limit_indices=(4, 2, 4)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x2x4xf32>) -> (tensor<3x2x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:4, 0:2, 0:4] : (tensor<4x2x4xf32>) -> tensor<3x2x4xf32>\n    return %0 : tensor<3x2x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          7,
          10,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,10,10]. let\n    b:f32[10,10,7] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x10x10xf32>) -> (tensor<10x10x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<7x10x10xf32>) -> tensor<10x10x7xf32>\n    return %0 : tensor<10x10x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          8,
          3,
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          3,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,3,3] b:f32[8,3,3]. let\n    c:f32[16,3,3] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x3x3xf32>, %arg1: tensor<8x3x3xf32>) -> (tensor<16x3x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<8x3x3xf32>, tensor<8x3x3xf32>) -> tensor<16x3x3xf32>\n    return %0 : tensor<16x3x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          8,
          5,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,5,8]. let\n    b:f32[7,5,8] = slice[\n      limit_indices=(8, 5, 8)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x5x8xf32>) -> (tensor<7x5x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:8, 0:5, 0:8] : (tensor<8x5x8xf32>) -> tensor<7x5x8xf32>\n    return %0 : tensor<7x5x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          5,
          3,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,3,10]. let\n    b:f32[10,3,5] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x3x10xf32>) -> (tensor<10x3x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<5x3x10xf32>) -> tensor<10x3x5xf32>\n    return %0 : tensor<10x3x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          8,
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,4] b:f32[8,4]. let\n    c:f32[16,4] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x4xf32>, %arg1: tensor<8x4xf32>) -> (tensor<16x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<8x4xf32>, tensor<8x4xf32>) -> tensor<16x4xf32>\n    return %0 : tensor<16x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          4,
          2,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,2,9]. let\n    b:f32[9,2,4] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x2x9xf32>) -> (tensor<9x2x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<4x2x9xf32>) -> tensor<9x2x4xf32>\n    return %0 : tensor<9x2x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          4,
          8,
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          4,
          8,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,8,8] b:f32[4,8,8]. let\n    c:f32[1,4,8,8] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 4, 8, 8)\n      sharding=None\n    ] a\n    d:f32[1,4,8,8] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 4, 8, 8)\n      sharding=None\n    ] b\n    e:f32[2,4,8,8] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x8x8xf32>, %arg1: tensor<4x8x8xf32>) -> (tensor<2x4x8x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<4x8x8xf32>) -> tensor<1x4x8x8xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<4x8x8xf32>) -> tensor<1x4x8x8xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x4x8x8xf32>, tensor<1x4x8x8xf32>) -> tensor<2x4x8x8xf32>\n    return %2 : tensor<2x4x8x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          5,
          5,
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5,
          5,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,5,10] b:f32[5,5,10]. let\n    c:f32[10,5,10] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x5x10xf32>, %arg1: tensor<5x5x10xf32>) -> (tensor<10x5x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<5x5x10xf32>, tensor<5x5x10xf32>) -> tensor<10x5x10xf32>\n    return %0 : tensor<10x5x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          5,
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,2] b:f32[5,2]. let\n    c:f32[10,2] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x2xf32>, %arg1: tensor<5x2xf32>) -> (tensor<10x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<5x2xf32>, tensor<5x2xf32>) -> tensor<10x2xf32>\n    return %0 : tensor<10x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          3,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,7]. let\n    b:f32[7,3] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x7xf32>) -> (tensor<7x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<3x7xf32>) -> tensor<7x3xf32>\n    return %0 : tensor<7x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          2,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,2]. let\n    b:f32[2,2] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x2xf32>) -> (tensor<2x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<2x2xf32>) -> tensor<2x2xf32>\n    return %0 : tensor<2x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          8,
          4,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,4,10]. let\n    b:f32[10,4,8] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x4x10xf32>) -> (tensor<10x4x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<8x4x10xf32>) -> tensor<10x4x8xf32>\n    return %0 : tensor<10x4x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          6,
          5,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,5,8]. let\n    b:f32[8,5,6] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x5x8xf32>) -> (tensor<8x5x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<6x5x8xf32>) -> tensor<8x5x6xf32>\n    return %0 : tensor<8x5x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          4,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,2]. let\n    b:f32[3,2] = slice[limit_indices=(4, 2) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x2xf32>) -> (tensor<3x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:4, 0:2] : (tensor<4x2xf32>) -> tensor<3x2xf32>\n    return %0 : tensor<3x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9] b:f32[9]. let\n    c:f32[1,9] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 9)\n      sharding=None\n    ] a\n    d:f32[1,9] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 9)\n      sharding=None\n    ] b\n    e:f32[2,9] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>, %arg1: tensor<9xf32>) -> (tensor<2x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<9xf32>) -> tensor<1x9xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<9xf32>) -> tensor<1x9xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x9xf32>, tensor<1x9xf32>) -> tensor<2x9xf32>\n    return %2 : tensor<2x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 16))",
    "input_info": [
      {
        "shape": [
          4,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,8]. let\n    b:f32[2,16] = reshape[dimensions=None new_sizes=(2, 16) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x8xf32>) -> (tensor<2x16xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<4x8xf32>) -> tensor<2x16xf32>\n    return %0 : tensor<2x16xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          8,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,3]. let\n    b:f32[7,3] = slice[limit_indices=(8, 3) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x3xf32>) -> (tensor<7x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:8, 0:3] : (tensor<8x3xf32>) -> tensor<7x3xf32>\n    return %0 : tensor<7x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          2,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,10]. let\n    b:f32[1,10] = slice[limit_indices=(2, 10) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x10xf32>) -> (tensor<1x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:2, 0:10] : (tensor<2x10xf32>) -> tensor<1x10xf32>\n    return %0 : tensor<1x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          6,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,7]. let\n    b:f32[5,7] = slice[limit_indices=(6, 7) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x7xf32>) -> (tensor<5x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:6, 0:7] : (tensor<6x7xf32>) -> tensor<5x7xf32>\n    return %0 : tensor<5x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          6,
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,8] b:f32[6,8]. let\n    c:f32[1,6,8] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 6, 8)\n      sharding=None\n    ] a\n    d:f32[1,6,8] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 6, 8)\n      sharding=None\n    ] b\n    e:f32[2,6,8] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x8xf32>, %arg1: tensor<6x8xf32>) -> (tensor<2x6x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<6x8xf32>) -> tensor<1x6x8xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<6x8xf32>) -> tensor<1x6x8xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x6x8xf32>, tensor<1x6x8xf32>) -> tensor<2x6x8xf32>\n    return %2 : tensor<2x6x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          6,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,4]. let\n    b:f32[4,6] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x4xf32>) -> (tensor<4x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<6x4xf32>) -> tensor<4x6xf32>\n    return %0 : tensor<4x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5] b:f32[5]. let\n    c:f32[10] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>, %arg1: tensor<5xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<5xf32>, tensor<5xf32>) -> tensor<10xf32>\n    return %0 : tensor<10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3] b:f32[3]. let\n    c:f32[6] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>, %arg1: tensor<3xf32>) -> (tensor<6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<3xf32>, tensor<3xf32>) -> tensor<6xf32>\n    return %0 : tensor<6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          5,
          5,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,5,7]. let\n    b:f32[4,5,7] = slice[\n      limit_indices=(5, 5, 7)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x5x7xf32>) -> (tensor<4x5x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:5, 0:5, 0:7] : (tensor<5x5x7xf32>) -> tensor<4x5x7xf32>\n    return %0 : tensor<4x5x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 60))",
    "input_info": [
      {
        "shape": [
          6,
          2,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,2,10]. let\n    b:f32[2,60] = reshape[dimensions=None new_sizes=(2, 60) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x2x10xf32>) -> (tensor<2x60xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<6x2x10xf32>) -> tensor<2x60xf32>\n    return %0 : tensor<2x60xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          6,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,10]. let\n    b:f32[5,10] = slice[limit_indices=(6, 10) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x10xf32>) -> (tensor<5x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:6, 0:10] : (tensor<6x10xf32>) -> tensor<5x10xf32>\n    return %0 : tensor<5x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3] b:f32[3]. let\n    c:f32[1,3] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 3)\n      sharding=None\n    ] a\n    d:f32[1,3] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 3)\n      sharding=None\n    ] b\n    e:f32[2,3] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>, %arg1: tensor<3xf32>) -> (tensor<2x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<3xf32>) -> tensor<1x3xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<3xf32>) -> tensor<1x3xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x3xf32>, tensor<1x3xf32>) -> tensor<2x3xf32>\n    return %2 : tensor<2x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          6,
          4,
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          6,
          4,
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,4,7] b:f32[6,4,7]. let\n    c:f32[12,4,7] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x4x7xf32>, %arg1: tensor<6x4x7xf32>) -> (tensor<12x4x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<6x4x7xf32>, tensor<6x4x7xf32>) -> tensor<12x4x7xf32>\n    return %0 : tensor<12x4x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          5,
          4,
          2
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5,
          4,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,4,2] b:f32[5,4,2]. let\n    c:f32[10,4,2] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x4x2xf32>, %arg1: tensor<5x4x2xf32>) -> (tensor<10x4x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<5x4x2xf32>, tensor<5x4x2xf32>) -> tensor<10x4x2xf32>\n    return %0 : tensor<10x4x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3] b:f32[3]. let\n    c:f32[1,3] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 3)\n      sharding=None\n    ] a\n    d:f32[1,3] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 3)\n      sharding=None\n    ] b\n    e:f32[2,3] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>, %arg1: tensor<3xf32>) -> (tensor<2x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<3xf32>) -> tensor<1x3xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<3xf32>) -> tensor<1x3xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x3xf32>, tensor<1x3xf32>) -> tensor<2x3xf32>\n    return %2 : tensor<2x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7] b:f32[7]. let\n    c:f32[1,7] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 7)\n      sharding=None\n    ] a\n    d:f32[1,7] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 7)\n      sharding=None\n    ] b\n    e:f32[2,7] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>, %arg1: tensor<7xf32>) -> (tensor<2x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<7xf32>) -> tensor<1x7xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<7xf32>) -> tensor<1x7xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x7xf32>, tensor<1x7xf32>) -> tensor<2x7xf32>\n    return %2 : tensor<2x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          9,
          4,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,4,4]. let\n    b:f32[8,4,4] = slice[\n      limit_indices=(9, 4, 4)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x4x4xf32>) -> (tensor<8x4x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:9, 0:4, 0:4] : (tensor<9x4x4xf32>) -> tensor<8x4x4xf32>\n    return %0 : tensor<8x4x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          2,
          9,
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          2,
          9,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,9,3] b:f32[2,9,3]. let\n    c:f32[1,2,9,3] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 2, 9, 3)\n      sharding=None\n    ] a\n    d:f32[1,2,9,3] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 2, 9, 3)\n      sharding=None\n    ] b\n    e:f32[2,2,9,3] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x9x3xf32>, %arg1: tensor<2x9x3xf32>) -> (tensor<2x2x9x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<2x9x3xf32>) -> tensor<1x2x9x3xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<2x9x3xf32>) -> tensor<1x2x9x3xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x2x9x3xf32>, tensor<1x2x9x3xf32>) -> tensor<2x2x9x3xf32>\n    return %2 : tensor<2x2x9x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          8,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,5]. let\n    b:f32[5,8] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x5xf32>) -> (tensor<5x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<8x5xf32>) -> tensor<5x8xf32>\n    return %0 : tensor<5x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>) -> (tensor<2xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          3,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[3,10]. let\n    b:f32[10,3] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x10xf32>) -> (tensor<10x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<3x10xf32>) -> tensor<10x3xf32>\n    return %0 : tensor<10x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          8,
          8,
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          8,
          8,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,8,6] b:f32[8,8,6]. let\n    c:f32[1,8,8,6] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 8, 8, 6)\n      sharding=None\n    ] a\n    d:f32[1,8,8,6] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2, 3)\n      shape=(1, 8, 8, 6)\n      sharding=None\n    ] b\n    e:f32[2,8,8,6] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x8x6xf32>, %arg1: tensor<8x8x6xf32>) -> (tensor<2x8x8x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2, 3] : (tensor<8x8x6xf32>) -> tensor<1x8x8x6xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2, 3] : (tensor<8x8x6xf32>) -> tensor<1x8x8x6xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x8x8x6xf32>, tensor<1x8x8x6xf32>) -> tensor<2x8x8x6xf32>\n    return %2 : tensor<2x8x8x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          10,
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,9] b:f32[10,9]. let\n    c:f32[20,9] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x9xf32>, %arg1: tensor<10x9xf32>) -> (tensor<20x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<10x9xf32>, tensor<10x9xf32>) -> tensor<20x9xf32>\n    return %0 : tensor<20x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          9,
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9,
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,10] b:f32[9,10]. let\n    c:f32[1,9,10] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 9, 10)\n      sharding=None\n    ] a\n    d:f32[1,9,10] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 9, 10)\n      sharding=None\n    ] b\n    e:f32[2,9,10] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x10xf32>, %arg1: tensor<9x10xf32>) -> (tensor<2x9x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<9x10xf32>) -> tensor<1x9x10xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<9x10xf32>) -> tensor<1x9x10xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x9x10xf32>, tensor<1x9x10xf32>) -> tensor<2x9x10xf32>\n    return %2 : tensor<2x9x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          9,
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          9,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9,6] b:f32[9,6]. let\n    c:f32[1,9,6] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 9, 6)\n      sharding=None\n    ] a\n    d:f32[1,9,6] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 9, 6)\n      sharding=None\n    ] b\n    e:f32[2,9,6] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x6xf32>, %arg1: tensor<9x6xf32>) -> (tensor<2x9x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<9x6xf32>) -> tensor<1x9x6xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<9x6xf32>) -> tensor<1x9x6xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x9x6xf32>, tensor<1x9x6xf32>) -> tensor<2x9x6xf32>\n    return %2 : tensor<2x9x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7] b:f32[7]. let\n    c:f32[1,7] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 7)\n      sharding=None\n    ] a\n    d:f32[1,7] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 7)\n      sharding=None\n    ] b\n    e:f32[2,7] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>, %arg1: tensor<7xf32>) -> (tensor<2x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<7xf32>) -> tensor<1x7xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<7xf32>) -> tensor<1x7xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x7xf32>, tensor<1x7xf32>) -> tensor<2x7xf32>\n    return %2 : tensor<2x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          7,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,4]. let\n    b:f32[4,7] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x4xf32>) -> (tensor<4x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<7x4xf32>) -> tensor<4x7xf32>\n    return %0 : tensor<4x7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (4,))",
    "input_info": [
      {
        "shape": [
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          7,
          8
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,8] b:f32[7,8]. let\n    c:f32[1,7,8] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 7, 8)\n      sharding=None\n    ] a\n    d:f32[1,7,8] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 7, 8)\n      sharding=None\n    ] b\n    e:f32[2,7,8] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x8xf32>, %arg1: tensor<7x8xf32>) -> (tensor<2x7x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<7x8xf32>) -> tensor<1x7x8xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<7x8xf32>) -> tensor<1x7x8xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x7x8xf32>, tensor<1x7x8xf32>) -> tensor<2x7x8xf32>\n    return %2 : tensor<2x7x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          10,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,6]. let\n    b:f32[9,6] = slice[limit_indices=(10, 6) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x6xf32>) -> (tensor<9x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:10, 0:6] : (tensor<10x6xf32>) -> tensor<9x6xf32>\n    return %0 : tensor<9x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          7
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>) -> (tensor<7xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<7xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          4,
          2,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,2,6]. let\n    b:f32[6,2,4] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x2x6xf32>) -> (tensor<6x2x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<4x2x6xf32>) -> tensor<6x2x4xf32>\n    return %0 : tensor<6x2x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x.T",
    "input_info": [
      {
        "shape": [
          8,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,2]. let\n    b:f32[2,8] = transpose[permutation=(1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x2xf32>) -> (tensor<2x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [1, 0] : (tensor<8x2xf32>) -> tensor<2x8xf32>\n    return %0 : tensor<2x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (56,))",
    "input_info": [
      {
        "shape": [
          7,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,8]. let\n    b:f32[56] = reshape[dimensions=None new_sizes=(56,) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x8xf32>) -> (tensor<56xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<7x8xf32>) -> tensor<56xf32>\n    return %0 : tensor<56xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          4,
          8,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,8,2]. let\n    b:f32[2,8,4] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x8x2xf32>) -> (tensor<2x8x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<4x8x2xf32>) -> tensor<2x8x4xf32>\n    return %0 : tensor<2x8x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          6,
          5,
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[6,5,2]. let\n    b:f32[5,5,2] = slice[\n      limit_indices=(6, 5, 2)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x5x2xf32>) -> (tensor<5x5x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:6, 0:5, 0:2] : (tensor<6x5x2xf32>) -> tensor<5x5x2xf32>\n    return %0 : tensor<5x5x2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (2, 224))",
    "input_info": [
      {
        "shape": [
          8,
          7,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,7,8]. let\n    b:f32[2,224] = reshape[dimensions=None new_sizes=(2, 224) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x7x8xf32>) -> (tensor<2x224xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<8x7x8xf32>) -> tensor<2x224xf32>\n    return %0 : tensor<2x224xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10] b:f32[10]. let\n    c:f32[1,10] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 10)\n      sharding=None\n    ] a\n    d:f32[1,10] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 10)\n      sharding=None\n    ] b\n    e:f32[2,10] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>, %arg1: tensor<10xf32>) -> (tensor<2x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1] : (tensor<10xf32>) -> tensor<1x10xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1] : (tensor<10xf32>) -> tensor<1x10xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x10xf32>, tensor<1x10xf32>) -> tensor<2x10xf32>\n    return %2 : tensor<2x10xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          4,
          4,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[4,4,8]. let\n    b:f32[8,4,4] = transpose[permutation=(2, 1, 0)] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x4x8xf32>) -> (tensor<8x4x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.transpose %arg0, dims = [2, 1, 0] : (tensor<4x4x8xf32>) -> tensor<8x4x4xf32>\n    return %0 : tensor<8x4x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          5,
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,9] b:f32[5,9]. let\n    c:f32[1,5,9] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 5, 9)\n      sharding=None\n    ] a\n    d:f32[1,5,9] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 5, 9)\n      sharding=None\n    ] b\n    e:f32[2,5,9] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x9xf32>, %arg1: tensor<5x9xf32>) -> (tensor<2x5x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<5x9xf32>) -> tensor<1x5x9xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<5x9xf32>) -> tensor<1x5x9xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x5x9xf32>, tensor<1x5x9xf32>) -> tensor<2x5x9xf32>\n    return %2 : tensor<2x5x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          5,
          2,
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5,
          2,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,2,4] b:f32[5,2,4]. let\n    c:f32[10,2,4] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x2x4xf32>, %arg1: tensor<5x2x4xf32>) -> (tensor<10x2x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<5x2x4xf32>, tensor<5x2x4xf32>) -> tensor<10x2x4xf32>\n    return %0 : tensor<10x2x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          2,
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2,8]. let\n    b:f32[1,8] = slice[limit_indices=(2, 8) start_indices=(1, 0) strides=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x8xf32>) -> (tensor<1x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:2, 0:8] : (tensor<2x8xf32>) -> tensor<1x8xf32>\n    return %0 : tensor<1x8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x: jnp.reshape(x, (3, 3))",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9]. let\n    b:f32[3,3] = reshape[dimensions=None new_sizes=(3, 3) sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>) -> (tensor<3x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.reshape %arg0 : (tensor<9xf32>) -> tensor<3x3xf32>\n    return %0 : tensor<3x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "reshape",
      "description": "Array operation: reshape"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          5,
          4
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5,
          4
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,4] b:f32[5,4]. let\n    c:f32[1,5,4] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 5, 4)\n      sharding=None\n    ] a\n    d:f32[1,5,4] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 5, 4)\n      sharding=None\n    ] b\n    e:f32[2,5,4] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x4xf32>, %arg1: tensor<5x4xf32>) -> (tensor<2x5x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<5x4xf32>) -> tensor<1x5x4xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<5x4xf32>) -> tensor<1x5x4xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x5x4xf32>, tensor<1x5x4xf32>) -> tensor<2x5x4xf32>\n    return %2 : tensor<2x5x4xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          10,
          6
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          10,
          6
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[10,6] b:f32[10,6]. let\n    c:f32[20,6] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x6xf32>, %arg1: tensor<10x6xf32>) -> (tensor<20x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<10x6xf32>, tensor<10x6xf32>) -> tensor<20x6xf32>\n    return %0 : tensor<20x6xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          8
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8xf32>) -> (tensor<8xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<8xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: x[1:, :]",
    "input_info": [
      {
        "shape": [
          8,
          10,
          5
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[8,10,5]. let\n    b:f32[7,10,5] = slice[\n      limit_indices=(8, 10, 5)\n      start_indices=(1, 0, 0)\n      strides=None\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x10x5xf32>) -> (tensor<7x10x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.slice %arg0 [1:8, 0:10, 0:5] : (tensor<8x10x5xf32>) -> tensor<7x10x5xf32>\n    return %0 : tensor<7x10x5xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "slice",
      "description": "Array operation: slice"
    }
  },
  {
    "python_code": "lambda x, y: jnp.stack([x, y])",
    "input_info": [
      {
        "shape": [
          5,
          3
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          5,
          3
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[5,3] b:f32[5,3]. let\n    c:f32[1,5,3] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 5, 3)\n      sharding=None\n    ] a\n    d:f32[1,5,3] = broadcast_in_dim[\n      broadcast_dimensions=(1, 2)\n      shape=(1, 5, 3)\n      sharding=None\n    ] b\n    e:f32[2,5,3] = concatenate[dimension=0] c d\n  in (e,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x3xf32>, %arg1: tensor<5x3xf32>) -> (tensor<2x5x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [1, 2] : (tensor<5x3xf32>) -> tensor<1x5x3xf32>\n    %1 = stablehlo.broadcast_in_dim %arg1, dims = [1, 2] : (tensor<5x3xf32>) -> tensor<1x5x3xf32>\n    %2 = stablehlo.concatenate %0, %1, dim = 0 : (tensor<1x5x3xf32>, tensor<1x5x3xf32>) -> tensor<2x5x3xf32>\n    return %2 : tensor<2x5x3xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "stack",
      "description": "Array operation: stack"
    }
  },
  {
    "python_code": "lambda x, y: jnp.concatenate([x, y], axis=0)",
    "input_info": [
      {
        "shape": [
          7,
          9
        ],
        "dtype": "float32"
      },
      {
        "shape": [
          7,
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[7,9] b:f32[7,9]. let\n    c:f32[14,9] = concatenate[dimension=0] a b\n  in (c,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x9xf32>, %arg1: tensor<7x9xf32>) -> (tensor<14x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.concatenate %arg0, %arg1, dim = 0 : (tensor<7x9xf32>, tensor<7x9xf32>) -> tensor<14x9xf32>\n    return %0 : tensor<14x9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "concatenate",
      "description": "Array operation: concatenate"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          2
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[2]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>) -> (tensor<2xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<2xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  },
  {
    "python_code": "lambda x: jnp.transpose(x)",
    "input_info": [
      {
        "shape": [
          9
        ],
        "dtype": "float32"
      }
    ],
    "jaxpr": "{ lambda ; a:f32[9]. let  in (a,) }",
    "stablehlo": "module @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>) -> (tensor<9xf32> {jax.result_info = \"result\"}) {\n    return %arg0 : tensor<9xf32>\n  }\n}\n",
    "metadata": {
      "category": "array",
      "operation": "transpose",
      "description": "Array operation: transpose"
    }
  }
]