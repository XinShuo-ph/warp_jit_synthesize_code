{"id": 0, "kernel_name": "scalar_arr_qahf", "python": "import jax\nimport jax.numpy as jnp\n\ndef scalar_arr_qahf(alpha, x, y):\n    return alpha * x + y\n\nEXAMPLE_ARGS = (\n    jnp.array(2.0, dtype=jnp.float32),\n    jnp.arange(8, dtype=jnp.float32),\n    (jnp.arange(8, dtype=jnp.float32) * 10.0),\n)", "type": "generate_scalar_array_op", "cpp": "# JAX compiler IR (backend=cpu, dialect=stablehlo)\n## Forward\nmodule @jit_scalar_arr_qahf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  sdy.mesh @empty_mesh = <[]>\n  func.func public @main(%arg0: tensor<f32> {sdy.sharding = #sdy.sharding<@empty_mesh, []>}, %arg1: tensor<8xf32> {sdy.sharding = #sdy.sharding<@empty_mesh, [{}]>}, %arg2: tensor<8xf32> {sdy.sharding = #sdy.sharding<@empty_mesh, [{}]>}) -> (tensor<8xf32> {jax.result_info = \"result\", sdy.sharding = #sdy.sharding<@empty_mesh, [{}]>}) {\n    %0 = stablehlo.broadcast_in_dim %arg0, dims = [] : (tensor<f32>) -> tensor<8xf32>\n    %1 = stablehlo.multiply %0, %arg1 : tensor<8xf32>\n    %2 = stablehlo.add %1, %arg2 : tensor<8xf32>\n    return %2 : tensor<8xf32>\n  }\n}\n\n\n## Backward (jax.grad of scalar loss)\nmodule @jit_loss_fn attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  sdy.mesh @empty_mesh = <[]>\n  func.func public @main(%arg0: tensor<8xf32> {sdy.sharding = #sdy.sharding<@empty_mesh, [{}]>}) -> (tensor<f32> {jax.result_info = \"result\", sdy.sharding = #sdy.sharding<@empty_mesh, []>}) {\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<8xf32>\n    %1 = stablehlo.multiply %0, %arg0 : tensor<8xf32>\n    %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %2 = stablehlo.reduce(%1 init: %cst_0) applies stablehlo.add across dimensions = [0] : (tensor<8xf32>, tensor<f32>) -> tensor<f32>\n    return %2 : tensor<f32>\n  }\n}\n"}
{"id": 1, "kernel_name": "elementwise_bsdm", "python": "import jax\nimport jax.numpy as jnp\n\ndef elementwise_bsdm(a, b):\n    return a * b\n\nEXAMPLE_ARGS = (\n    jnp.arange(8, dtype=jnp.float32),\n    jnp.arange(8, dtype=jnp.float32),\n)", "type": "generate_simple_elementwise", "cpp": "# JAX compiler IR (backend=cpu, dialect=stablehlo)\n## Forward\nmodule @jit_elementwise_bsdm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  sdy.mesh @empty_mesh = <[]>\n  func.func public @main(%arg0: tensor<8xf32> {sdy.sharding = #sdy.sharding<@empty_mesh, [{}]>}, %arg1: tensor<8xf32> {sdy.sharding = #sdy.sharding<@empty_mesh, [{}]>}) -> (tensor<8xf32> {jax.result_info = \"result\", sdy.sharding = #sdy.sharding<@empty_mesh, [{}]>}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<8xf32>\n    return %0 : tensor<8xf32>\n  }\n}\n\n\n## Backward (jax.grad of scalar loss)\nmodule @jit_loss_fn attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  sdy.mesh @empty_mesh = <[]>\n  func.func public @main(%arg0: tensor<8xf32> {sdy.sharding = #sdy.sharding<@empty_mesh, [{}]>}) -> (tensor<8xf32> {jax.result_info = \"result\", sdy.sharding = #sdy.sharding<@empty_mesh, [{}]>}) {\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<8xf32>\n    %1 = stablehlo.multiply %0, %arg0 : tensor<8xf32>\n    return %1 : tensor<8xf32>\n  }\n}\n"}
{"id": 2, "kernel_name": "vec_kowe", "python": "import jax\nimport jax.numpy as jnp\n\ndef vec_kowe(a, b):\n    # a, b: (N, 3)\n    return jnp.sum(a * b, axis=-1)\n\nEXAMPLE_ARGS = (\n    jnp.arange(24, dtype=jnp.float32).reshape(8, 3),\n    (jnp.arange(24, dtype=jnp.float32).reshape(8, 3) * 0.5),\n)", "type": "generate_vector_kernel", "cpp": "# JAX compiler IR (backend=cpu, dialect=stablehlo)\n## Forward\nmodule @jit_vec_kowe attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  sdy.mesh @empty_mesh = <[]>\n  func.func public @main(%arg0: tensor<8x3xf32> {sdy.sharding = #sdy.sharding<@empty_mesh, [{}, {}]>}, %arg1: tensor<8x3xf32> {sdy.sharding = #sdy.sharding<@empty_mesh, [{}, {}]>}) -> (tensor<8xf32> {jax.result_info = \"result\", sdy.sharding = #sdy.sharding<@empty_mesh, [{}]>}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<8x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<8x3xf32>, tensor<f32>) -> tensor<8xf32>\n    return %1 : tensor<8xf32>\n  }\n}\n\n\n## Backward (jax.grad of scalar loss)\nmodule @jit_loss_fn attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  sdy.mesh @empty_mesh = <[]>\n  func.func public @main(%arg0: tensor<8x3xf32> {sdy.sharding = #sdy.sharding<@empty_mesh, [{}, {}]>}) -> (tensor<8x3xf32> {jax.result_info = \"result\", sdy.sharding = #sdy.sharding<@empty_mesh, [{}, {}]>}) {\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<8xf32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [0] : (tensor<8xf32>) -> tensor<8x3xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<8x3xf32>\n    return %2 : tensor<8x3xf32>\n  }\n}\n"}
{"id": 3, "kernel_name": "loop_hmci", "python": "import jax\nimport jax.numpy as jnp\nfrom jax import lax\n\ndef loop_hmci(a, n):\n    def body(i, total):\n        return total + a\n    total0 = jnp.zeros_like(a)\n    return lax.fori_loop(0, n, body, total0)\n\nEXAMPLE_ARGS = (\n    jnp.arange(8, dtype=jnp.float32),\n    5,\n)", "type": "generate_loop_kernel", "cpp": "# JAX compiler IR (backend=cpu, dialect=stablehlo)\n## Forward\nmodule @jit_loop_hmci attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  sdy.mesh @empty_mesh = <[]>\n  func.func public @main(%arg0: tensor<8xf32> {sdy.sharding = #sdy.sharding<@empty_mesh, [{}]>}) -> (tensor<8xf32> {jax.result_info = \"result\", sdy.sharding = #sdy.sharding<@empty_mesh, [{}]>}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<8xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:3 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %0) : tensor<8xf32>, tensor<i32>, tensor<8xf32>\n    cond {\n      %c_2 = stablehlo.constant dense<5> : tensor<i32>\n      %2 = stablehlo.compare  LT, %iterArg_0, %c_2,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %2 = func.call @closed_call(%iterArg, %iterArg_1) : (tensor<8xf32>, tensor<8xf32>) -> tensor<8xf32>\n      %c_2 = stablehlo.constant dense<1> : tensor<i32>\n      %3 = stablehlo.add %iterArg_0, %c_2 : tensor<i32>\n      stablehlo.return %iterArg, %3, %2 : tensor<8xf32>, tensor<i32>, tensor<8xf32>\n    }\n    return %1#2 : tensor<8xf32>\n  }\n  func.func private @closed_call(%arg0: tensor<8xf32>, %arg1: tensor<8xf32>) -> tensor<8xf32> {\n    %0 = stablehlo.add %arg1, %arg0 : tensor<8xf32>\n    return %0 : tensor<8xf32>\n  }\n}\n\n\n## Backward (jax.grad of scalar loss)\nmodule @jit_loss_fn attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  sdy.mesh @empty_mesh = <[]>\n  func.func public @main() -> (tensor<8xf32> {jax.result_info = \"result\", sdy.sharding = #sdy.sharding<@empty_mesh, [{}]>}) {\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<8xf32>\n    %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<8xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %2:3 = stablehlo.while(%iterArg = %c, %iterArg_1 = %1, %iterArg_2 = %0) : tensor<i32>, tensor<8xf32>, tensor<8xf32>\n    cond {\n      %c_3 = stablehlo.constant dense<5> : tensor<i32>\n      %3 = stablehlo.compare  LT, %iterArg, %c_3,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %3 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<5> : tensor<i32>\n      %3 = stablehlo.subtract %c_3, %iterArg : tensor<i32>\n      %c_4 = stablehlo.constant dense<1> : tensor<i32>\n      %4 = stablehlo.subtract %3, %c_4 : tensor<i32>\n      %5:2 = func.call @closed_call(%iterArg_1, %iterArg_2) : (tensor<8xf32>, tensor<8xf32>) -> (tensor<8xf32>, tensor<8xf32>)\n      %c_5 = stablehlo.constant dense<1> : tensor<i32>\n      %6 = stablehlo.add %iterArg, %c_5 : tensor<i32>\n      stablehlo.return %6, %5#0, %5#1 : tensor<i32>, tensor<8xf32>, tensor<8xf32>\n    }\n    return %2#1 : tensor<8xf32>\n  }\n  func.func private @closed_call(%arg0: tensor<8xf32>, %arg1: tensor<8xf32>) -> (tensor<8xf32>, tensor<8xf32>) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<8xf32>\n    return %0, %arg1 : tensor<8xf32>, tensor<8xf32>\n  }\n}\n"}
{"id": 4, "kernel_name": "scalar_arr_xkpw", "python": "import jax\nimport jax.numpy as jnp\n\ndef scalar_arr_xkpw(alpha, x, y):\n    return alpha + x * y\n\nEXAMPLE_ARGS = (\n    jnp.array(2.0, dtype=jnp.float32),\n    jnp.arange(8, dtype=jnp.float32),\n    (jnp.arange(8, dtype=jnp.float32) * 10.0),\n)", "type": "generate_scalar_array_op", "cpp": "# JAX compiler IR (backend=cpu, dialect=stablehlo)\n## Forward\nmodule @jit_scalar_arr_xkpw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  sdy.mesh @empty_mesh = <[]>\n  func.func public @main(%arg0: tensor<f32> {sdy.sharding = #sdy.sharding<@empty_mesh, []>}, %arg1: tensor<8xf32> {sdy.sharding = #sdy.sharding<@empty_mesh, [{}]>}, %arg2: tensor<8xf32> {sdy.sharding = #sdy.sharding<@empty_mesh, [{}]>}) -> (tensor<8xf32> {jax.result_info = \"result\", sdy.sharding = #sdy.sharding<@empty_mesh, [{}]>}) {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<8xf32>\n    %1 = stablehlo.broadcast_in_dim %arg0, dims = [] : (tensor<f32>) -> tensor<8xf32>\n    %2 = stablehlo.add %1, %0 : tensor<8xf32>\n    return %2 : tensor<8xf32>\n  }\n}\n\n\n## Backward (jax.grad of scalar loss)\nmodule @jit_loss_fn attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  sdy.mesh @empty_mesh = <[]>\n  func.func public @main() -> (tensor<f32> {jax.result_info = \"result\", sdy.sharding = #sdy.sharding<@empty_mesh, []>}) {\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<8xf32>\n    %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst_0) applies stablehlo.add across dimensions = [0] : (tensor<8xf32>, tensor<f32>) -> tensor<f32>\n    return %1 : tensor<f32>\n  }\n}\n"}
