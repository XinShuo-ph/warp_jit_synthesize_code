[
  {
    "id": 0,
    "function_name": "elementwise_dlus",
    "python": "def elementwise_dlus(a, b):\n    return a * b",
    "jaxpr": "{ lambda ; a:f32[256] b:f32[256]. let c:f32[256] = mul a b in (c,) }\n\n=== BACKWARD (GRADIENT) ===\n{ lambda ; a:f32[256] b:f32[256]. let\n    c:f32[256] = mul a b\n    _:f32[] = reduce_sum[axes=(0,) out_sharding=None] c\n    d:f32[256] = broadcast_in_dim[\n      broadcast_dimensions=()\n      shape=(256,)\n      sharding=None\n    ] 1.0:f32[]\n    e:f32[256] = mul d b\n  in (e,) }",
    "hlo": "module @jit_elementwise_dlus attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<256xf32>, %arg1: tensor<256xf32>) -> (tensor<256xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<256xf32>\n    return %0 : tensor<256xf32>\n  }\n}\n\n\n=== BACKWARD (GRADIENT) HLO ===\nmodule @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<256xf32>) -> (tensor<256xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<256xf32>\n    %1 = stablehlo.multiply %0, %arg0 : tensor<256xf32>\n    return %1 : tensor<256xf32>\n  }\n}\n",
    "type": "generate_simple_elementwise"
  },
  {
    "id": 1,
    "function_name": "matmul_pfzy",
    "python": "def matmul_pfzy(a, b):\n    return jnp.matmul(a, b)",
    "jaxpr": "{ lambda ; a:f32[64,128] b:f32[128,32]. let\n    c:f32[64,32] = dot_general[\n      dimension_numbers=(([1], [0]), ([], []))\n      preferred_element_type=float32\n    ] a b\n  in (c,) }\n\n=== BACKWARD (GRADIENT) ===\n{ lambda ; a:f32[64,128] b:f32[128,32]. let\n    c:f32[64,32] = dot_general[\n      dimension_numbers=(([1], [0]), ([], []))\n      preferred_element_type=float32\n    ] a b\n    _:f32[] = reduce_sum[axes=(0, 1) out_sharding=None] c\n    d:f32[64,32] = broadcast_in_dim[\n      broadcast_dimensio...",
    "hlo": "module @jit_matmul_pfzy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<64x128xf32>, %arg1: tensor<128x32xf32>) -> (tensor<64x32xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.dot_general %arg0, %arg1, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<64x128xf32>, tensor<128x32xf32>) -> tensor<64x32xf32>\n    return %0 : tensor<64x32xf32>\n  }\n}\n\n\n=== BACKWARD (GRADIENT) HLO ===\nmodule @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128x32xf32>) -> (tensor<64x128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tenso...",
    "type": "generate_matmul_function"
  },
  {
    "id": 2,
    "function_name": "reduce_dqes",
    "python": "def reduce_dqes(a):\n    return jnp.mean(a)",
    "jaxpr": "{ lambda ; a:f32[128]. let\n    b:f32[] = reduce_sum[axes=(0,) out_sharding=None] a\n    c:f32[] = div b 128.0:f32[]\n  in (c,) }\n\n=== BACKWARD (GRADIENT) ===\n{ lambda ; a:f32[128]. let\n    b:f32[] = reduce_sum[axes=(0,) out_sharding=None] a\n    c:f32[] = div b 128.0:f32[]\n    _:f32[] = reduce_sum[axes=() out_sharding=None] c\n    d:f32[] = div 1.0:f32[] 128.0:f32[]\n    e:f32[128] = broadcast_in_dim[\n      broadcast_dimensions=()\n      shape=(128,)\n      sharding=None\n    ] d\n  in (e,) }",
    "hlo": "module @jit_reduce_dqes attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<128xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<128xf32>, tensor<f32>) -> tensor<f32>\n    %cst_0 = stablehlo.constant dense<1.280000e+02> : tensor<f32>\n    %1 = stablehlo.divide %0, %cst_0 : tensor<f32>\n    return %1 : tensor<f32>\n  }\n}\n\n\n=== BACKWARD (GRADIENT) HLO ===\nmodule @jit__lambda attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main() -> (tensor<128xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.000000e...",
    "type": "generate_reduction_function"
  }
]