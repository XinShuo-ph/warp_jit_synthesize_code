[
  {
    "id": 0,
    "kernel_name": "matmul_qahf",
    "python": "def matmul_qahf(a, b):\n    \"\"\"Matrix multiplication.\"\"\"\n    return jnp.matmul(a, b)",
    "type": "generate_matmul_kernel",
    "hlo": "module @jit_matmul_qahf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<32x64xf32>, %arg1: tensor<64x32xf32>) -> (tensor<32x32xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.dot_general %arg0, %arg1, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<32x64xf32>, tensor<64x32xf32>) -> tensor<32x32xf32>\n    return %0 : tensor<32x32xf32>\n  }\n}\n\n\n// === BACKWARD (GRADIENT) HLO ===\nmodule @jit_scalar_loss attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<64x32xf32>) -> (tensor<32x64xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<32x32xf32>\n    %1 = stablehlo.dot_general %0, %arg0, contracting_dims = [1] x [1], precision = [DEFAULT, DEFAULT] : (tensor<32x32xf32>, tensor<64x32xf32>) -> tensor<32x64xf32>\n    return %1 : tensor<32x64xf32>\n  }\n}\n",
    "jaxpr": "{ lambda ; a:f32[32,64] b:f32[64,32]. let\n    c:f32[32,32] = dot_general[\n      dimension_numbers=(([1], [0]), ([], []))\n      preferred_element_type=float32\n    ] a b\n  in (c,) }\n\n# === BACKWARD (GRADIENT) JAXPR ===\n{ lambda ; a:f32[32,64] b:f32[64,32]. let\n    c:f32[32,32] = dot_general[\n      dimension_numbers=(([1], [0]), ([], []))\n      preferred_element_type=float32\n    ] a b\n    _:f32[] = reduce_sum[axes=(0, 1) out_sharding=None] c\n    d:f32[32,32] = broadcast_in_dim[\n      broadcast_dimensions=()\n      shape=(32, 32)\n      sharding=None\n    ] 1.0:f32[]\n    e:f32[32,64] = dot_general[\n      dimension_numbers=(([1], [1]), ([], []))\n      preferred_element_type=float32\n    ] d b\n  in (e,) }",
    "optimized_hlo": "HloModule jit_matmul_qahf, is_scheduled=true, entry_computation_layout={(f32[32,64]{1,0}, f32[64,32]{1,0})->f32[32,32]{1,0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"<string>\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"matmul_qahf\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=269 end_line=269 column=8 end_column=81}\n2 {file_name_id=1 function_name_id=2 line=212 end_line=216 column=54 end_column=17}\n3 {file_name_id=1 function_name_id=3 line=63 end_line=63 column=16 end_column=50}\n4 {file_name_id=2 function_name_id=4 line=3 end_line=3 column=11 end_column=27}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n\n\nENTRY %main.1 (a.1: f32[32,64], b.1: f32[64,32]) -> f32[32,32] {\n  %a.1 = f32[32,64]{1,0} parameter(0), metadata={op_name=\"a\"}\n  %b.1 = f32[64,32]{1,0} parameter(1), metadata={op_name=\"b\"}\n  ROOT %dot_general.1 = f32[32,32]{1,0} dot(%a.1, %b.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}, metadata={op_name=\"jit(matmul_qahf)/dot_general\" stack_frame_id=4}\n}\n\n\n\n// === BACKWARD (GRADIENT) OPTIMIZED HLO ===\nHloModule jit_scalar_loss, is_scheduled=true, entry_computation_layout={(f32[64,32]{1,0})->f32[32,64]{1,0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"<string>\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir_from_source.<locals>.scalar_loss\"\n5 \"matmul_qahf\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=269 end_line=269 column=8 end_column=81}\n2 {file_name_id=1 function_name_id=2 line=212 end_line=216 column=54 end_column=17}\n3 {file_name_id=1 function_name_id=3 line=93 end_line=93 column=21 end_column=58}\n4 {file_name_id=1 function_name_id=4 line=89 end_line=89 column=23 end_column=38}\n5 {file_name_id=1 function_name_id=4 line=87 end_line=87 column=21 end_column=32}\n6 {file_name_id=2 function_name_id=5 line=3 end_line=3 column=11 end_column=27}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=4}\n6 {file_location_id=6 parent_frame_id=6}\n\n\n%wrapped_broadcast_computation (param_0: f32[]) -> f32[32,32] {\n  %param_0 = f32[] parameter(0)\n  ROOT %broadcast_in_dim.0 = f32[32,32]{1,0} broadcast(%param_0), dimensions={}, metadata={op_name=\"jit(scalar_loss)/transpose(jvp())/broadcast_in_dim\" stack_frame_id=4}\n}\n\nENTRY %main.1 (args_1_.1: f32[64,32]) -> f32[32,64] {\n  %args_1_.1 = f32[64,32]{1,0} parameter(0), metadata={op_name=\"args[1]\"}\n  %constant.1 = f32[] constant(1)\n  %wrapped_broadcast = f32[32,32]{1,0} fusion(%constant.1), kind=kLoop, calls=%wrapped_broadcast_computation, metadata={op_name=\"jit(scalar_loss)/transpose(jvp())/broadcast_in_dim\" stack_frame_id=4}\n  ROOT %dot_general.1 = f32[32,64]{1,0} dot(%wrapped_broadcast, %args_1_.1), lhs_contracting_dims={1}, rhs_contracting_dims={1}, metadata={op_name=\"jit(scalar_loss)/transpose(jvp())/dot_general\" stack_frame_id=6}\n}\n\n"
  },
  {
    "id": 1,
    "kernel_name": "elementwise_bsdm",
    "python": "def elementwise_bsdm(a, b):\n    \"\"\"Elementwise operation on two arrays.\"\"\"\n    return a * b",
    "type": "generate_simple_elementwise",
    "hlo": "module @jit_elementwise_bsdm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<100xf32>, %arg1: tensor<100xf32>) -> (tensor<100xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<100xf32>\n    return %0 : tensor<100xf32>\n  }\n}\n\n\n// === BACKWARD (GRADIENT) HLO ===\nmodule @jit_scalar_loss attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<100xf32>) -> (tensor<100xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<100xf32>\n    %1 = stablehlo.multiply %0, %arg0 : tensor<100xf32>\n    return %1 : tensor<100xf32>\n  }\n}\n",
    "jaxpr": "{ lambda ; a:f32[100] b:f32[100]. let c:f32[100] = mul a b in (c,) }\n\n# === BACKWARD (GRADIENT) JAXPR ===\n{ lambda ; a:f32[100] b:f32[100]. let\n    c:f32[100] = mul a b\n    _:f32[] = reduce_sum[axes=(0,) out_sharding=None] c\n    d:f32[100] = broadcast_in_dim[\n      broadcast_dimensions=()\n      shape=(100,)\n      sharding=None\n    ] 1.0:f32[]\n    e:f32[100] = mul d b\n  in (e,) }",
    "optimized_hlo": "HloModule jit_elementwise_bsdm, is_scheduled=true, entry_computation_layout={(f32[100]{0}, f32[100]{0})->f32[100]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"<string>\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"elementwise_bsdm\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=269 end_line=269 column=8 end_column=81}\n2 {file_name_id=1 function_name_id=2 line=212 end_line=216 column=54 end_column=17}\n3 {file_name_id=1 function_name_id=3 line=63 end_line=63 column=16 end_column=50}\n4 {file_name_id=2 function_name_id=4 line=3 end_line=3 column=11 end_column=16}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n\n\n%wrapped_multiply_computation (param_0: f32[100], param_1: f32[100]) -> f32[100] {\n  %param_0 = f32[100]{0} parameter(0)\n  %param_1 = f32[100]{0} parameter(1)\n  ROOT %mul.0 = f32[100]{0} multiply(%param_0, %param_1), metadata={op_name=\"jit(elementwise_bsdm)/mul\" stack_frame_id=4}\n}\n\nENTRY %main.1 (a.1: f32[100], b.1: f32[100]) -> f32[100] {\n  %a.1 = f32[100]{0} parameter(0), metadata={op_name=\"a\"}\n  %b.1 = f32[100]{0} parameter(1), metadata={op_name=\"b\"}\n  ROOT %wrapped_multiply = f32[100]{0} fusion(%a.1, %b.1), kind=kLoop, calls=%wrapped_multiply_computation, metadata={op_name=\"jit(elementwise_bsdm)/mul\" stack_frame_id=4}\n}\n\n\n\n// === BACKWARD (GRADIENT) OPTIMIZED HLO ===\nHloModule jit_scalar_loss, is_scheduled=true, entry_computation_layout={(f32[100]{0})->f32[100]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nENTRY %main.1 (args_1_.1: f32[100]) -> f32[100] {\n  %args_1_.1 = f32[100]{0} parameter(0), metadata={op_name=\"args[1]\"}\n  ROOT %copy = f32[100]{0} copy(%args_1_.1)\n}\n\n"
  },
  {
    "id": 2,
    "kernel_name": "vec_kowe",
    "python": "def vec_kowe(a, b):\n    \"\"\"Dot product operation.\"\"\"\n    return jnp.sum(a * b, axis=-1)",
    "type": "generate_vector_kernel",
    "hlo": "module @jit_vec_kowe attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<100x3xf32>, %arg1: tensor<100x3xf32>) -> (tensor<100xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<100x3xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<100x3xf32>, tensor<f32>) -> tensor<100xf32>\n    return %1 : tensor<100xf32>\n  }\n}\n\n\n// === BACKWARD (GRADIENT) HLO ===\nmodule @jit_scalar_loss attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<100x3xf32>) -> (tensor<100x3xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<100xf32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [0] : (tensor<100xf32>) -> tensor<100x3xf32>\n    %2 = stablehlo.multiply %1, %arg0 : tensor<100x3xf32>\n    return %2 : tensor<100x3xf32>\n  }\n}\n",
    "jaxpr": "{ lambda ; a:f32[100,3] b:f32[100,3]. let\n    c:f32[100,3] = mul a b\n    d:f32[100] = reduce_sum[axes=(1,) out_sharding=None] c\n  in (d,) }\n\n# === BACKWARD (GRADIENT) JAXPR ===\n{ lambda ; a:f32[100,3] b:f32[100,3]. let\n    c:f32[100,3] = mul a b\n    d:f32[100] = reduce_sum[axes=(1,) out_sharding=None] c\n    _:f32[] = reduce_sum[axes=(0,) out_sharding=None] d\n    e:f32[100] = broadcast_in_dim[\n      broadcast_dimensions=()\n      shape=(100,)\n      sharding=None\n    ] 1.0:f32[]\n    f:f32[100,3] = broadcast_in_dim[\n      broadcast_dimensions=(np.int64(0),)\n      shape=(100, 3)\n      sharding=None\n    ] e\n    g:f32[100,3] = mul f b\n  in (g,) }",
    "optimized_hlo": "HloModule jit_vec_kowe, is_scheduled=true, entry_computation_layout={(f32[100,3]{1,0}, f32[100,3]{1,0})->f32[100]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"<string>\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"vec_kowe\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=269 end_line=269 column=8 end_column=81}\n2 {file_name_id=1 function_name_id=2 line=212 end_line=216 column=54 end_column=17}\n3 {file_name_id=1 function_name_id=3 line=63 end_line=63 column=16 end_column=50}\n4 {file_name_id=2 function_name_id=4 line=3 end_line=3 column=11 end_column=34}\n5 {file_name_id=2 function_name_id=4 line=3 end_line=3 column=19 end_column=24}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=4}\n\n\n%region_0.1 (reduce_sum.3: f32[], reduce_sum.4: f32[]) -> f32[] {\n  %reduce_sum.3 = f32[] parameter(0), metadata={op_name=\"reduce_sum\"}\n  %reduce_sum.4 = f32[] parameter(1), metadata={op_name=\"reduce_sum\"}\n  ROOT %reduce_sum.5 = f32[] add(%reduce_sum.3, %reduce_sum.4), metadata={op_name=\"jit(vec_kowe)/reduce_sum\" stack_frame_id=4}\n}\n\n%fused_computation (param_0.2: f32[100,3], param_1.2: f32[100,3]) -> f32[100] {\n  %param_0.2 = f32[100,3]{1,0} parameter(0)\n  %param_1.2 = f32[100,3]{1,0} parameter(1)\n  %mul.0 = f32[100,3]{1,0} multiply(%param_0.2, %param_1.2), metadata={op_name=\"jit(vec_kowe)/mul\" stack_frame_id=5}\n  %constant.0 = f32[] constant(0)\n  ROOT %reduce_sum.0 = f32[100]{0} reduce(%mul.0, %constant.0), dimensions={1}, to_apply=%region_0.1, metadata={op_name=\"jit(vec_kowe)/reduce_sum\" stack_frame_id=4}\n}\n\nENTRY %main.2 (a.1: f32[100,3], b.1: f32[100,3]) -> f32[100] {\n  %a.1 = f32[100,3]{1,0} parameter(0), metadata={op_name=\"a\"}\n  %b.1 = f32[100,3]{1,0} parameter(1), metadata={op_name=\"b\"}\n  ROOT %multiply_reduce_fusion = f32[100]{0} fusion(%a.1, %b.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(vec_kowe)/reduce_sum\" stack_frame_id=4}\n}\n\n\n\n// === BACKWARD (GRADIENT) OPTIMIZED HLO ===\nHloModule jit_scalar_loss, is_scheduled=true, entry_computation_layout={(f32[100,3]{1,0})->f32[100,3]{1,0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nENTRY %main.1 (args_1_.1: f32[100,3]) -> f32[100,3] {\n  %args_1_.1 = f32[100,3]{1,0} parameter(0), metadata={op_name=\"args[1]\"}\n  ROOT %copy = f32[100,3]{1,0} copy(%args_1_.1)\n}\n\n"
  },
  {
    "id": 3,
    "kernel_name": "loop_hmci",
    "python": "def loop_hmci(a):\n    \"\"\"Loop operation using jax.lax.fori_loop.\"\"\"\n    def body_fn(i, total):\n        return total + a\n    return jax.lax.fori_loop(0, 10, body_fn, jnp.zeros_like(a))",
    "type": "generate_loop_kernel",
    "hlo": "module @jit_loop_hmci attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<100xf32>) -> (tensor<100xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<100xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:3 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %0) : tensor<100xf32>, tensor<i32>, tensor<100xf32>\n    cond {\n      %c_2 = stablehlo.constant dense<10> : tensor<i32>\n      %2 = stablehlo.compare  LT, %iterArg_0, %c_2,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %2 = func.call @closed_call(%iterArg, %iterArg_1) : (tensor<100xf32>, tensor<100xf32>) -> tensor<100xf32>\n      %c_2 = stablehlo.constant dense<1> : tensor<i32>\n      %3 = stablehlo.add %iterArg_0, %c_2 : tensor<i32>\n      stablehlo.return %iterArg, %3, %2 : tensor<100xf32>, tensor<i32>, tensor<100xf32>\n    }\n    return %1#2 : tensor<100xf32>\n  }\n  func.func private @closed_call(%arg0: tensor<100xf32>, %arg1: tensor<100xf32>) -> tensor<100xf32> {\n    %0 = stablehlo.add %arg1, %arg0 : tensor<100xf32>\n    return %0 : tensor<100xf32>\n  }\n}\n\n\n// === BACKWARD (GRADIENT) HLO ===\nmodule @jit_scalar_loss attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main() -> (tensor<100xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<100xf32>\n    %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<100xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %2:3 = stablehlo.while(%iterArg = %c, %iterArg_1 = %1, %iterArg_2 = %0) : tensor<i32>, tensor<100xf32>, tensor<100xf32>\n    cond {\n      %c_3 = stablehlo.constant dense<10> : tensor<i32>\n      %3 = stablehlo.compare  LT, %iterArg, %c_3,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %3 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<10> : tensor<i32>\n      %3 = stablehlo.subtract %c_3, %iterArg : tensor<i32>\n      %c_4 = stablehlo.constant dense<1> : tensor<i32>\n      %4 = stablehlo.subtract %3, %c_4 : tensor<i32>\n      %5:2 = func.call @closed_call(%iterArg_1, %iterArg_2) : (tensor<100xf32>, tensor<100xf32>) -> (tensor<100xf32>, tensor<100xf32>)\n      %c_5 = stablehlo.constant dense<1> : tensor<i32>\n      %6 = stablehlo.add %iterArg, %c_5 : tensor<i32>\n      stablehlo.return %6, %5#0, %5#1 : tensor<i32>, tensor<100xf32>, tensor<100xf32>\n    }\n    return %2#1 : tensor<100xf32>\n  }\n  func.func private @closed_call(%arg0: tensor<100xf32>, %arg1: tensor<100xf32>) -> (tensor<100xf32>, tensor<100xf32>) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<100xf32>\n    return %0, %arg1 : tensor<100xf32>, tensor<100xf32>\n  }\n}\n",
    "jaxpr": "{ lambda ; a:f32[100]. let\n    b:f32[100] = broadcast_in_dim[\n      broadcast_dimensions=()\n      shape=(100,)\n      sharding=None\n    ] 0.0:f32[]\n    _:i32[] c:f32[100] = scan[\n      _split_transpose=False\n      jaxpr={ lambda ; d:f32[100] e:i32[] f:f32[100]. let\n          g:i32[] = add e 1:i32[]\n          h:f32[100] = add f d\n        in (g, h) }\n      length=10\n      linear=(False, False, False)\n      num_carry=2\n      num_consts=1\n      reverse=False\n      unroll=1\n    ] a 0:i32[] b\n  in (c,) }\n\n# === BACKWARD (GRADIENT) JAXPR ===\n{ lambda ; a:f32[100]. let\n    b:f32[100] = broadcast_in_dim[\n      broadcast_dimensions=()\n      shape=(100,)\n      sharding=None\n    ] 0.0:f32[]\n    _:i32[] c:f32[100] = scan[\n      _split_transpose=False\n      jaxpr={ lambda ; d:f32[100] e:i32[] f:f32[100]. let\n          g:i32[] = add e 1:i32[]\n          h:f32[100] = add f d\n        in (g, h) }\n      length=10\n      linear=(False, False, False)\n      num_carry=2\n      num_consts=1\n      reverse=False\n      unroll=1\n    ] a 0:i32[] b\n    _:f32[] = reduce_sum[axes=(0,) out_sharding=None] c\n    _:f32[100] = broadcast_in_dim[\n      broadcast_dimensions=()\n      shape=(100,)\n      sharding=None\n    ] 0.0:f32[]\n    i:f32[100] = broadcast_in_dim[\n      broadcast_dimensions=()\n      shape=(100,)\n      sharding=None\n    ] 1.0:f32[]\n    j:f32[100] = broadcast_in_dim[\n      broadcast_dimensions=()\n      shape=(100,)\n      sharding=None\n    ] 0.0:f32[]\n    k:f32[100] _:f32[100] = scan[\n      _split_transpose=False\n      jaxpr={ lambda ; l:f32[100] m:f32[100]. let\n          n:f32[100] = add_any l m\n        in (n, m) }\n      length=10\n      linear=(True, True)\n      num_carry=2\n      num_consts=0\n      reverse=True\n      unroll=1\n    ] j i\n  in (k,) }",
    "optimized_hlo": "HloModule jit_loop_hmci, is_scheduled=true, entry_computation_layout={(f32[100]{0})->f32[100]{0}}, allow_spmd_sharding_propagation_to_parameters={true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"<string>\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"loop_hmci\"\n5 \"loop_hmci.<locals>.body_fn\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=269 end_line=269 column=8 end_column=81}\n2 {file_name_id=1 function_name_id=2 line=212 end_line=216 column=54 end_column=17}\n3 {file_name_id=1 function_name_id=3 line=63 end_line=63 column=16 end_column=50}\n4 {file_name_id=2 function_name_id=4 line=5 end_line=5 column=11 end_column=63}\n5 {file_name_id=2 function_name_id=4 line=5 end_line=5 column=45 end_column=62}\n6 {file_name_id=2 function_name_id=5 line=4 end_line=4 column=15 end_column=24}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=4}\n6 {file_location_id=6 parent_frame_id=5}\n\n\n%wrapped_add_computation (param_0.1: s32[], param_1: s32[]) -> s32[] {\n  %param_0.1 = s32[] parameter(0)\n  %param_1 = s32[] parameter(1)\n  ROOT %add.2 = s32[] add(%param_0.1, %param_1), metadata={op_name=\"jit(loop_hmci)/while/body/add\" stack_frame_id=4}\n}\n\n%wrapped_add_computation.1 (param_0.2: f32[100], param_1.1: f32[100]) -> f32[100] {\n  %param_0.2 = f32[100]{0} parameter(0)\n  %param_1.1 = f32[100]{0} parameter(1)\n  ROOT %add.4 = f32[100]{0} add(%param_0.2, %param_1.1), metadata={op_name=\"jit(loop_hmci)/while/body/closed_call/add\" stack_frame_id=6}\n}\n\n%region_0.2 (arg_tuple.1: (s32[], f32[100], f32[100])) -> (s32[], f32[100], f32[100]) {\n  %arg_tuple.1 = (s32[], f32[100]{0}, f32[100]{0}) parameter(0)\n  %constant.3 = s32[] constant(1)\n  %get-tuple-element.14 = f32[100]{0} get-tuple-element(%arg_tuple.1), index=2\n  %get-tuple-element.7 = f32[100]{0} get-tuple-element(%arg_tuple.1), index=1\n  %get-tuple-element.6 = s32[] get-tuple-element(%arg_tuple.1), index=0\n  %wrapped_add.1 = f32[100]{0} fusion(%get-tuple-element.7, %get-tuple-element.14), kind=kLoop, calls=%wrapped_add_computation.1, metadata={op_name=\"jit(loop_hmci)/while/body/closed_call/add\" stack_frame_id=6}\n  %wrapped_add = s32[] fusion(%get-tuple-element.6, %constant.3), kind=kLoop, calls=%wrapped_add_computation, metadata={op_name=\"jit(loop_hmci)/while/body/add\" stack_frame_id=4}\n  ROOT %tuple.3 = (s32[], f32[100]{0}, f32[100]{0}) tuple(%wrapped_add, %wrapped_add.1, %get-tuple-element.14)\n}\n\n%wrapped_compare_computation (param_0.3: s32[], param_1.2: s32[]) -> pred[] {\n  %param_0.3 = s32[] parameter(0)\n  %param_1.2 = s32[] parameter(1)\n  ROOT %lt.0 = pred[] compare(%param_0.3, %param_1.2), direction=LT, metadata={op_name=\"jit(loop_hmci)/while/cond/lt\" stack_frame_id=4}\n}\n\n%region_1.3 (arg_tuple.3: (s32[], f32[100], f32[100])) -> pred[] {\n  %arg_tuple.3 = (s32[], f32[100]{0}, f32[100]{0}) parameter(0)\n  %constant.5 = s32[] constant(10)\n  %get-tuple-element.9 = s32[] get-tuple-element(%arg_tuple.3), index=0\n  ROOT %wrapped_compare = pred[] fusion(%get-tuple-element.9, %constant.5), kind=kLoop, calls=%wrapped_compare_computation, metadata={op_name=\"jit(loop_hmci)/while/cond/lt\" stack_frame_id=4}\n}\n\n%wrapped_broadcast_computation (param_0: f32[]) -> f32[100] {\n  %param_0 = f32[] parameter(0)\n  ROOT %broadcast_in_dim.0 = f32[100]{0} broadcast(%param_0), dimensions={}, metadata={op_name=\"jit(loop_hmci)/broadcast_in_dim\" stack_frame_id=5}\n}\n\nENTRY %main.4 (a.1: f32[100]) -> f32[100] {\n  %a.1 = f32[100]{0} parameter(0), metadata={op_name=\"a\"}\n  %constant.6 = s32[] constant(0)\n  %constant.7 = f32[] constant(0)\n  %copy.6 = s32[] copy(%constant.6)\n  %wrapped_broadcast = f32[100]{0} fusion(%constant.7), kind=kLoop, calls=%wrapped_broadcast_computation, metadata={op_name=\"jit(loop_hmci)/broadcast_in_dim\" stack_frame_id=5}\n  %tuple = (s32[], f32[100]{0}, f32[100]{0}) tuple(%copy.6, %wrapped_broadcast, %a.1)\n  %while.5 = (s32[], f32[100]{0}, f32[100]{0}) while(%tuple), condition=%region_1.3, body=%region_0.2, metadata={op_name=\"jit(loop_hmci)/while\" stack_frame_id=4}, backend_config={\"known_trip_count\":{\"n\":\"10\"},\"known_init_step\":{\"init\":\"0\",\"step\":\"1\"},\"known_induction_variable\":{\"tuple_index\":\"0\"},\"dynamic_variable_tuple_indices\":[]}\n  ROOT %while.7 = f32[100]{0} get-tuple-element(%while.5), index=1, metadata={op_name=\"jit(loop_hmci)/while\" stack_frame_id=4}\n}\n\n\n\n// === BACKWARD (GRADIENT) OPTIMIZED HLO ===\nHloModule jit_scalar_loss, is_scheduled=true, entry_computation_layout={()->f32[100]{0}}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"<string>\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir_from_source.<locals>.scalar_loss\"\n5 \"loop_hmci\"\n6 \"loop_hmci.<locals>.body_fn\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=269 end_line=269 column=8 end_column=81}\n2 {file_name_id=1 function_name_id=2 line=212 end_line=216 column=54 end_column=17}\n3 {file_name_id=1 function_name_id=3 line=93 end_line=93 column=21 end_column=58}\n4 {file_name_id=1 function_name_id=4 line=87 end_line=87 column=21 end_column=32}\n5 {file_name_id=2 function_name_id=5 line=5 end_line=5 column=11 end_column=63}\n6 {file_name_id=1 function_name_id=4 line=89 end_line=89 column=23 end_column=38}\n7 {file_name_id=2 function_name_id=6 line=4 end_line=4 column=15 end_column=24}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=4}\n7 {file_location_id=7 parent_frame_id=6}\n\n\n%wrapped_add_computation (param_0.2: s32[], param_1: s32[]) -> s32[] {\n  %param_0.2 = s32[] parameter(0)\n  %param_1 = s32[] parameter(1)\n  ROOT %add.0 = s32[] add(%param_0.2, %param_1), metadata={op_name=\"jit(scalar_loss)/transpose(jvp())/while/body/add\" stack_frame_id=5}\n}\n\n%wrapped_add_computation.1 (param_0.3: f32[100], param_1.1: f32[100]) -> f32[100] {\n  %param_0.3 = f32[100]{0} parameter(0)\n  %param_1.1 = f32[100]{0} parameter(1)\n  ROOT %add_any.2 = f32[100]{0} add(%param_0.3, %param_1.1), metadata={op_name=\"jit(scalar_loss)/transpose(jvp())/while/body/closed_call/add_any\" stack_frame_id=7}\n}\n\n%region_0.2 (arg_tuple.1: (s32[], f32[100], f32[100])) -> (s32[], f32[100], f32[100]) {\n  %arg_tuple.1 = (s32[], f32[100]{0}, f32[100]{0}) parameter(0)\n  %constant.4 = s32[] constant(1)\n  %get-tuple-element.14 = f32[100]{0} get-tuple-element(%arg_tuple.1), index=2, metadata={op_name=\"jit(scalar_loss)/transpose(jvp())/while/body/closed_call\" stack_frame_id=5}\n  %get-tuple-element.7 = f32[100]{0} get-tuple-element(%arg_tuple.1), index=1\n  %get-tuple-element.6 = s32[] get-tuple-element(%arg_tuple.1), index=0\n  %wrapped_add.1 = f32[100]{0} fusion(%get-tuple-element.7, %get-tuple-element.14), kind=kLoop, calls=%wrapped_add_computation.1, metadata={op_name=\"jit(scalar_loss)/transpose(jvp())/while/body/closed_call/add_any\" stack_frame_id=7}\n  %wrapped_add = s32[] fusion(%get-tuple-element.6, %constant.4), kind=kLoop, calls=%wrapped_add_computation, metadata={op_name=\"jit(scalar_loss)/transpose(jvp())/while/body/add\" stack_frame_id=5}\n  ROOT %tuple.5 = (s32[], f32[100]{0}, f32[100]{0}) tuple(%wrapped_add, %wrapped_add.1, %get-tuple-element.14)\n}\n\n%wrapped_compare_computation (param_0.4: s32[], param_1.2: s32[]) -> pred[] {\n  %param_0.4 = s32[] parameter(0)\n  %param_1.2 = s32[] parameter(1)\n  ROOT %lt.0 = pred[] compare(%param_0.4, %param_1.2), direction=LT, metadata={op_name=\"jit(scalar_loss)/transpose(jvp())/while/cond/lt\" stack_frame_id=5}\n}\n\n%region_1.3 (arg_tuple.3: (s32[], f32[100], f32[100])) -> pred[] {\n  %arg_tuple.3 = (s32[], f32[100]{0}, f32[100]{0}) parameter(0)\n  %constant.6 = s32[] constant(10)\n  %get-tuple-element.9 = s32[] get-tuple-element(%arg_tuple.3), index=0\n  ROOT %wrapped_compare = pred[] fusion(%get-tuple-element.9, %constant.6), kind=kLoop, calls=%wrapped_compare_computation, metadata={op_name=\"jit(scalar_loss)/transpose(jvp())/while/cond/lt\" stack_frame_id=5}\n}\n\n%wrapped_broadcast_computation (param_0: f32[]) -> f32[100] {\n  %param_0 = f32[] parameter(0)\n  ROOT %broadcast_in_dim.0 = f32[100]{0} broadcast(%param_0), dimensions={}, metadata={op_name=\"jit(scalar_loss)/transpose(jvp())/broadcast_in_dim\" stack_frame_id=5}\n}\n\n%wrapped_broadcast_computation.1 (param_0.1: f32[]) -> f32[100] {\n  %param_0.1 = f32[] parameter(0)\n  ROOT %broadcast_in_dim.1 = f32[100]{0} broadcast(%param_0.1), dimensions={}, metadata={op_name=\"jit(scalar_loss)/transpose(jvp())/broadcast_in_dim\" stack_frame_id=6}\n}\n\nENTRY %main.4 () -> f32[100] {\n  %constant.7 = s32[] constant(0)\n  %constant.8 = f32[] constant(1)\n  %constant.9 = f32[] constant(0)\n  %copy.6 = s32[] copy(%constant.7)\n  %wrapped_broadcast.1 = f32[100]{0} fusion(%constant.8), kind=kLoop, calls=%wrapped_broadcast_computation.1, metadata={op_name=\"jit(scalar_loss)/transpose(jvp())/broadcast_in_dim\" stack_frame_id=6}\n  %wrapped_broadcast = f32[100]{0} fusion(%constant.9), kind=kLoop, calls=%wrapped_broadcast_computation, metadata={op_name=\"jit(scalar_loss)/transpose(jvp())/broadcast_in_dim\" stack_frame_id=5}\n  %tuple.2 = (s32[], f32[100]{0}, f32[100]{0}) tuple(%copy.6, %wrapped_broadcast, %wrapped_broadcast.1)\n  %while.6 = (s32[], f32[100]{0}, f32[100]{0}) while(%tuple.2), condition=%region_1.3, body=%region_0.2, metadata={op_name=\"jit(scalar_loss)/transpose(jvp())/while\" stack_frame_id=5}, backend_config={\"known_trip_count\":{\"n\":\"10\"},\"known_init_step\":{\"init\":\"0\",\"step\":\"1\"},\"known_induction_variable\":{\"tuple_index\":\"0\"},\"dynamic_variable_tuple_indices\":[]}\n  ROOT %while.8 = f32[100]{0} get-tuple-element(%while.6), index=1, metadata={op_name=\"jit(scalar_loss)/transpose(jvp())/while\" stack_frame_id=5}\n}\n\n"
  },
  {
    "id": 4,
    "kernel_name": "scalar_arr_xkpw",
    "python": "def scalar_arr_xkpw(alpha, x, y):\n    \"\"\"Scalar-array operation: alpha op1 x op2 y.\"\"\"\n    return alpha + x * y",
    "type": "generate_scalar_array_op",
    "hlo": "module @jit_scalar_arr_xkpw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<100xf32>, %arg2: tensor<100xf32>) -> (tensor<100xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<100xf32>\n    %1 = stablehlo.convert %arg0 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<100xf32>\n    %3 = stablehlo.add %2, %0 : tensor<100xf32>\n    return %3 : tensor<100xf32>\n  }\n}\n\n\n// === BACKWARD (GRADIENT) HLO ===\nmodule @jit_scalar_loss attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main() -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<100xf32>\n    %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst_0) applies stablehlo.add across dimensions = [0] : (tensor<100xf32>, tensor<f32>) -> tensor<f32>\n    %2 = stablehlo.convert %1 : tensor<f32>\n    return %2 : tensor<f32>\n  }\n}\n",
    "jaxpr": "{ lambda ; a:f32[] b:f32[100] c:f32[100]. let\n    d:f32[100] = mul b c\n    e:f32[] = convert_element_type[new_dtype=float32 weak_type=False] a\n    f:f32[100] = add e d\n  in (f,) }\n\n# === BACKWARD (GRADIENT) JAXPR ===\n{ lambda ; a:f32[] b:f32[100] c:f32[100]. let\n    d:f32[100] = mul b c\n    e:f32[] = convert_element_type[new_dtype=float32 weak_type=False] a\n    f:f32[100] = add e d\n    _:f32[] = reduce_sum[axes=(0,) out_sharding=None] f\n    g:f32[100] = broadcast_in_dim[\n      broadcast_dimensions=()\n      shape=(100,)\n      sharding=None\n    ] 1.0:f32[]\n    h:f32[] = reduce_sum[axes=(np.int64(0),) out_sharding=None] g\n    i:f32[] = convert_element_type[\n      new_dtype=float32\n      sharding=NamedSharding(mesh=AbstractMesh((), axis_types=()), spec=PartitionSpec())\n      weak_type=True\n    ] h\n  in (i,) }",
    "optimized_hlo": "HloModule jit_scalar_arr_xkpw, is_scheduled=true, entry_computation_layout={(f32[], f32[100]{0}, f32[100]{0})->f32[100]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"<string>\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"scalar_arr_xkpw\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=269 end_line=269 column=8 end_column=81}\n2 {file_name_id=1 function_name_id=2 line=212 end_line=216 column=54 end_column=17}\n3 {file_name_id=1 function_name_id=3 line=63 end_line=63 column=16 end_column=50}\n4 {file_name_id=2 function_name_id=4 line=3 end_line=3 column=19 end_column=24}\n5 {file_name_id=2 function_name_id=4 line=3 end_line=3 column=11 end_column=24}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=4}\n\n\n%fused_computation (param_0.1: f32[100], param_1.2: f32[100], param_2.1: f32[]) -> f32[100] {\n  %param_2.1 = f32[] parameter(2)\n  %add.1 = f32[100]{0} broadcast(%param_2.1), dimensions={}, metadata={op_name=\"jit(scalar_arr_xkpw)/add\" stack_frame_id=5}\n  %param_0.1 = f32[100]{0} parameter(0)\n  %param_1.2 = f32[100]{0} parameter(1)\n  %mul.0 = f32[100]{0} multiply(%param_0.1, %param_1.2), metadata={op_name=\"jit(scalar_arr_xkpw)/mul\" stack_frame_id=4}\n  ROOT %add.0 = f32[100]{0} add(%add.1, %mul.0), metadata={op_name=\"jit(scalar_arr_xkpw)/add\" stack_frame_id=5}\n}\n\nENTRY %main.1 (alpha.1: f32[], x.1: f32[100], y.1: f32[100]) -> f32[100] {\n  %alpha.1 = f32[] parameter(0), metadata={op_name=\"alpha\"}\n  %x.1 = f32[100]{0} parameter(1), metadata={op_name=\"x\"}\n  %y.1 = f32[100]{0} parameter(2), metadata={op_name=\"y\"}\n  ROOT %multiply_add_fusion = f32[100]{0} fusion(%x.1, %y.1, %alpha.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(scalar_arr_xkpw)/add\" stack_frame_id=5}\n}\n\n\n\n// === BACKWARD (GRADIENT) OPTIMIZED HLO ===\nHloModule jit_scalar_loss, is_scheduled=true, entry_computation_layout={()->f32[]}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jit/code/synthesis/pipeline.py\"\n2 \"<string>\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir_from_source.<locals>.scalar_loss\"\n5 \"scalar_arr_xkpw\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=269 end_line=269 column=8 end_column=81}\n2 {file_name_id=1 function_name_id=2 line=212 end_line=216 column=54 end_column=17}\n3 {file_name_id=1 function_name_id=3 line=93 end_line=93 column=21 end_column=58}\n4 {file_name_id=1 function_name_id=4 line=87 end_line=87 column=21 end_column=32}\n5 {file_name_id=2 function_name_id=5 line=3 end_line=3 column=11 end_column=24}\n6 {file_name_id=1 function_name_id=4 line=89 end_line=89 column=23 end_column=38}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=4}\n\n\nENTRY %main.2 () -> f32[] {\n  %constant = f32[] constant(100), metadata={op_name=\"jit(scalar_loss)/transpose(jvp())/reduce_sum\" stack_frame_id=5}\n  ROOT %copy = f32[] copy(%constant)\n}\n\n"
  }
]
