{"id": 0, "kernel_name": "unary_dwtg", "python": "def unary_dwtg(a):\n    return jnp.cos(a)\n\n\nEXAMPLE_ARGS = (jnp.linspace(-1.0, 1.0, 128, dtype=jnp.float32),)\nGRAD_ARGNUMS = (0,)", "type": "generate_unary_kernel", "cpu_ir": "module @jit_unary_dwtg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  sdy.mesh @empty_mesh = <[]>\n  func.func public @main(%arg0: tensor<128xf32> {sdy.sharding = #sdy.sharding<@empty_mesh, [{}]>}) -> (tensor<128xf32> {jax.result_info = \"result\", sdy.sharding = #sdy.sharding<@empty_mesh, [{}]>}) {\n    %0 = stablehlo.cosine %arg0 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cpu_backward_ir": "module @jit_loss attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  sdy.mesh @empty_mesh = <[]>\n  func.func public @main(%arg0: tensor<128xf32> {sdy.sharding = #sdy.sharding<@empty_mesh, [{}]>}) -> (tensor<128xf32> {jax.result_info = \"result[0]\", sdy.sharding = #sdy.sharding<@empty_mesh, [{}]>}) {\n    %0 = stablehlo.sine %arg0 : tensor<128xf32>\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %2 = stablehlo.negate %1 : tensor<128xf32>\n    %3 = stablehlo.multiply %2, %0 : tensor<128xf32>\n    return %3 : tensor<128xf32>\n  }\n}\n"}
{"id": 1, "kernel_name": "elementwise_yybc", "python": "def elementwise_yybc(a, b):\n    return a + b\n\n\nEXAMPLE_ARGS = (\n    jnp.arange(128, dtype=jnp.float32),\n    jnp.arange(128, dtype=jnp.float32) * 2.0,\n)\n\nGRAD_ARGNUMS = (0, 1)", "type": "generate_simple_elementwise", "cpu_ir": "module @jit_elementwise_yybc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  sdy.mesh @empty_mesh = <[]>\n  func.func public @main(%arg0: tensor<128xf32> {sdy.sharding = #sdy.sharding<@empty_mesh, [{}]>}, %arg1: tensor<128xf32> {sdy.sharding = #sdy.sharding<@empty_mesh, [{}]>}) -> (tensor<128xf32> {jax.result_info = \"result\", sdy.sharding = #sdy.sharding<@empty_mesh, [{}]>}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cpu_backward_ir": "module @jit_loss attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  sdy.mesh @empty_mesh = <[]>\n  func.func public @main() -> (tensor<128xf32> {jax.result_info = \"result[0]\", sdy.sharding = #sdy.sharding<@empty_mesh, [{}]>}, tensor<128xf32> {jax.result_info = \"result[1]\", sdy.sharding = #sdy.sharding<@empty_mesh, [{}]>}) {\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    return %0, %0 : tensor<128xf32>, tensor<128xf32>\n  }\n}\n"}
{"id": 2, "kernel_name": "branch_gojp", "python": "def branch_gojp(a):\n    return jnp.where(a > 0.25, a + 0.5, a * 2.6)\n\n\nEXAMPLE_ARGS = (jnp.linspace(-2.0, 2.0, 128, dtype=jnp.float32),)\nGRAD_ARGNUMS = (0,)", "type": "generate_branch_kernel", "cpu_ir": "module @jit_branch_gojp attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  sdy.mesh @empty_mesh = <[]>\n  func.func public @main(%arg0: tensor<128xf32> {sdy.sharding = #sdy.sharding<@empty_mesh, [{}]>}) -> (tensor<128xf32> {jax.result_info = \"result\", sdy.sharding = #sdy.sharding<@empty_mesh, [{}]>}) {\n    %cst = stablehlo.constant dense<2.500000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<128xf32>\n    %cst_1 = stablehlo.constant dense<2.600000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<128xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<128xi1>, tensor<128xf32>, tensor<128xf32>) -> tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>, %arg2: tensor<128xf32>) -> tensor<128xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<128xi1>, tensor<128xf32>\n    return %0 : tensor<128xf32>\n  }\n}\n", "cpu_backward_ir": "module @jit_loss attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  sdy.mesh @empty_mesh = <[]>\n  func.func public @main(%arg0: tensor<128xf32> {sdy.sharding = #sdy.sharding<@empty_mesh, [{}]>}) -> (tensor<128xf32> {jax.result_info = \"result[0]\", sdy.sharding = #sdy.sharding<@empty_mesh, [{}]>}) {\n    %cst = stablehlo.constant dense<2.500000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<128xf32>, tensor<128xf32>) -> tensor<128xi1>\n    %cst_0 = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %3:2 = call @_where(%1, %2) : (tensor<128xi1>, tensor<128xf32>) -> (tensor<128xf32>, tensor<128xf32>)\n    %cst_1 = stablehlo.constant dense<2.600000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %5 = stablehlo.multiply %3#1, %4 : tensor<128xf32>\n    %6 = stablehlo.add %3#0, %5 : tensor<128xf32>\n    return %6 : tensor<128xf32>\n  }\n  func.func private @_where(%arg0: tensor<128xi1>, %arg1: tensor<128xf32>) -> (tensor<128xf32>, tensor<128xf32>) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<128xf32>\n    %1 = stablehlo.select %arg0, %0, %arg1 : tensor<128xi1>, tensor<128xf32>\n    %2 = stablehlo.select %arg0, %arg1, %0 : tensor<128xi1>, tensor<128xf32>\n    return %2, %1 : tensor<128xf32>, tensor<128xf32>\n  }\n}\n"}
