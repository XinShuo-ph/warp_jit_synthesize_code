{
  "id": 60,
  "kernel_name": "kernel_60",
  "python_code": "\n@wp.kernel\ndef kernel_60(in1: wp.array(dtype=int), in2: wp.array(dtype=int), out: wp.array(dtype=int)):\n    tid = wp.tid()\n    x = in1[tid]\n    y = in2[tid]\n    out[tid] = x + y\n",
  "ir_code": "\n\nextern \"C\" __global__ void kernel_60_eaf30377_cuda_kernel_forward(\n    wp::launch_bounds_t dim,\n    wp::array_t<wp::int32> var_in1,\n    wp::array_t<wp::int32> var_in2,\n    wp::array_t<wp::int32> var_out)\n{\n    wp::tile_shared_storage_t tile_mem;\n\n    for (size_t _idx = static_cast<size_t>(blockDim.x) * static_cast<size_t>(blockIdx.x) + static_cast<size_t>(threadIdx.x);\n         _idx < dim.size;\n         _idx += static_cast<size_t>(blockDim.x) * static_cast<size_t>(gridDim.x))\n    {\n            // reset shared memory allocator\n        wp::tile_shared_storage_t::init();\n\n        //---------\n        // primal vars\n        wp::int32 var_0;\n        wp::int32* var_1;\n        wp::int32 var_2;\n        wp::int32 var_3;\n        wp::int32* var_4;\n        wp::int32 var_5;\n        wp::int32 var_6;\n        wp::int32 var_7;\n        //---------\n        // forward\n        // def kernel_60(in1: wp.array(dtype=int), in2: wp.array(dtype=int), out: wp.array(dtype=int)):       <L 5>\n        // tid = wp.tid()                                                                         <L 6>\n        var_0 = builtin_tid1d();\n        // x = in1[tid]                                                                           <L 7>\n        var_1 = wp::address(var_in1, var_0);\n        var_3 = wp::load(var_1);\n        var_2 = wp::copy(var_3);\n        // y = in2[tid]                                                                           <L 8>\n        var_4 = wp::address(var_in2, var_0);\n        var_6 = wp::load(var_4);\n        var_5 = wp::copy(var_6);\n        // out[tid] = x + y                                                                       <L 9>\n        var_7 = wp::add(var_2, var_5);\n        wp::array_store(var_out, var_0, var_7);\n    }\n}\n\n"
}