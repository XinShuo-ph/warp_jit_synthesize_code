{"id": 0, "function_name": "unary_dlus", "python": "def unary_dlus(a):\n    \"\"\"Unary jnp.log operation.\"\"\"\n    return jnp.log(a)", "type": "generate_unary_kernel", "jaxpr": "{ lambda ; a:f32[4]. let b:f32[4] = log a in (b,) }\n\n# Gradient extraction failed: Gradient only defined for scalar-output functions. Output had shape: (4,).", "hlo": "module @jit_unary_dlus attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.log %arg0 : tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n"}
{"id": 1, "function_name": "compound_pfzy", "python": "def compound_pfzy(a, b, scale):\n    \"\"\"Compound operation with multiple steps.\"\"\"\n    result = (a + b) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "jaxpr": "{ lambda ; a:f32[4] b:f32[4] c:f32[]. let\n    d:f32[4] = add a b\n    e:f32[] = convert_element_type[new_dtype=float32 weak_type=False] c\n    f:f32[4] = mul d e\n    g:f32[4] = floor f\n    h:f32[4] = sub f g\n    i:f32[4] = abs h\n  in (i,) }\n\n# Gradient extraction failed: Gradient only defined for scalar-output functions. Output had shape: (4,).", "hlo": "module @jit_compound_pfzy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>, %arg2: tensor<f32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<4xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<4xf32>\n    %4 = stablehlo.floor %3 : tensor<4xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<4xf32>\n    %6 = stablehlo.abs %5 : tensor<4xf32>\n    return %6 : tensor<4xf32>\n  }\n}\n"}
{"id": 2, "function_name": "unary_dqes", "python": "def unary_dqes(a):\n    \"\"\"Unary jnp.exp operation.\"\"\"\n    return jnp.exp(a)", "type": "generate_unary_kernel", "jaxpr": "{ lambda ; a:f32[4]. let b:f32[4] = exp a in (b,) }\n\n# Gradient extraction failed: Gradient only defined for scalar-output functions. Output had shape: (4,).", "hlo": "module @jit_unary_dqes attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.exponential %arg0 : tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n"}
{"id": 3, "function_name": "multi_zlst", "python": "def multi_zlst(a, b):\n    \"\"\"Multi-statement operation.\"\"\"\n    temp1 = a - b\n    temp2 = jnp.exp(temp1)\n    result = temp2 + a\n    return result", "type": "generate_multi_statement_kernel", "jaxpr": "{ lambda ; a:f32[4] b:f32[4]. let\n    c:f32[4] = sub a b\n    d:f32[4] = exp c\n    e:f32[4] = add d a\n  in (e,) }\n\n# Gradient extraction failed: Gradient only defined for scalar-output functions. Output had shape: (4,).", "hlo": "module @jit_multi_zlst attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<4xf32>\n    %1 = stablehlo.exponential %0 : tensor<4xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<4xf32>\n    return %2 : tensor<4xf32>\n  }\n}\n"}
{"id": 4, "function_name": "elementwise_ywtf", "python": "def elementwise_ywtf(a, b):\n    \"\"\"Elementwise * operation.\"\"\"\n    return a * b", "type": "generate_simple_elementwise", "jaxpr": "{ lambda ; a:f32[4] b:f32[4]. let c:f32[4] = mul a b in (c,) }\n\n# Gradient extraction failed: Gradient only defined for scalar-output functions. Output had shape: (4,).", "hlo": "module @jit_elementwise_ywtf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n"}
{"id": 5, "function_name": "compound_wjuy", "python": "def compound_wjuy(a, b, scale):\n    \"\"\"Compound operation with multiple steps.\"\"\"\n    result = (a + b) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "jaxpr": "{ lambda ; a:f32[4] b:f32[4] c:f32[]. let\n    d:f32[4] = add a b\n    e:f32[] = convert_element_type[new_dtype=float32 weak_type=False] c\n    f:f32[4] = mul d e\n    g:f32[4] = floor f\n    h:f32[4] = sub f g\n    i:f32[4] = abs h\n  in (i,) }\n\n# Gradient extraction failed: Gradient only defined for scalar-output functions. Output had shape: (4,).", "hlo": "module @jit_compound_wjuy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>, %arg2: tensor<f32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<4xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<4xf32>\n    %4 = stablehlo.floor %3 : tensor<4xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<4xf32>\n    %6 = stablehlo.abs %5 : tensor<4xf32>\n    return %6 : tensor<4xf32>\n  }\n}\n"}
{"id": 6, "function_name": "multi_sxay", "python": "def multi_sxay(a, b):\n    \"\"\"Multi-statement operation.\"\"\"\n    temp1 = a / b\n    temp2 = jnp.sin(temp1)\n    result = temp2 + a\n    return result", "type": "generate_multi_statement_kernel", "jaxpr": "{ lambda ; a:f32[4] b:f32[4]. let\n    c:f32[4] = div a b\n    d:f32[4] = sin c\n    e:f32[4] = add d a\n  in (e,) }\n\n# Gradient extraction failed: Gradient only defined for scalar-output functions. Output had shape: (4,).", "hlo": "module @jit_multi_sxay attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.divide %arg0, %arg1 : tensor<4xf32>\n    %1 = stablehlo.sine %0 : tensor<4xf32>\n    %2 = stablehlo.add %1, %arg0 : tensor<4xf32>\n    return %2 : tensor<4xf32>\n  }\n}\n"}
{"id": 7, "function_name": "branch_gpzr", "python": "def branch_gpzr(a):\n    \"\"\"Branching operation based on threshold.\"\"\"\n    return jnp.where(a > -0.03, a * 1.6, a - 1.9)", "type": "generate_branch_kernel", "jaxpr": "{ lambda ; a:f32[4]. let\n    b:bool[4] = gt a -0.029999999329447746:f32[]\n    c:f32[4] = mul a 1.600000023841858:f32[]\n    d:f32[4] = sub a 1.899999976158142:f32[]\n    e:f32[4] = jit[\n      name=_where\n      jaxpr={ lambda ; b:bool[4] c:f32[4] d:f32[4]. let\n          e:f32[4] = select_n b d c\n        in (e,) }\n    ] b c d\n  in (e,) }\n\n# Gradient extraction failed: Gradient only defined for scalar-output functions. Output had shape: (4,).", "hlo": "module @jit_branch_gpzr attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-3.000000e-02> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xi1>\n    %cst_0 = stablehlo.constant dense<1.600000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %3 = stablehlo.multiply %arg0, %2 : tensor<4xf32>\n    %cst_1 = stablehlo.constant dense<1.900000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<4xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<4xi1>, tensor<4xf32>, tensor<4xf32>) -> tensor<4xf32>\n    return %6 : tensor<4xf32>\n  }\n  func.func private @_where(%arg0: tensor<4xi1>, %arg1: tensor<4xf32>, %arg2: tensor<4xf32>) -> tensor<4xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<4xi1>, tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n"}
{"id": 8, "function_name": "unary_dvwk", "python": "def unary_dvwk(a):\n    \"\"\"Unary jnp.sin operation.\"\"\"\n    return jnp.sin(a)", "type": "generate_unary_kernel", "jaxpr": "{ lambda ; a:f32[4]. let b:f32[4] = sin a in (b,) }\n\n# Gradient extraction failed: Gradient only defined for scalar-output functions. Output had shape: (4,).", "hlo": "module @jit_unary_dvwk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n"}
{"id": 9, "function_name": "loop_hloc", "python": "def loop_hloc(a):\n    \"\"\"Loop operation using jax.lax.scan.\"\"\"\n    def body_fun(carry, _):\n        return carry / a, None\n    result, _ = jax.lax.scan(body_fun, jnp.zeros_like(a), jnp.arange(7))\n    return result", "type": "generate_loop_kernel", "jaxpr": "{ lambda ; a:f32[4]. let\n    b:f32[4] = broadcast_in_dim[\n      broadcast_dimensions=()\n      shape=(4,)\n      sharding=None\n    ] 0.0:f32[]\n    c:i32[7] = iota[dimension=0 dtype=int32 shape=(7,) sharding=None] \n    d:f32[4] = scan[\n      _split_transpose=False\n      jaxpr={ lambda ; e:f32[4] f:f32[4] g:i32[]. let\n          h:f32[4] = div f e\n        in (h,) }\n      length=7\n      linear=(False, False, False)\n      num_carry=1\n      num_consts=1\n      reverse=False\n      unroll=1\n    ] a b c\n  in (d,) }\n\n# Gradient extraction failed: Gradient only defined for scalar-output functions. Output had shape: (4,).", "hlo": "module @jit_loop_hloc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:3 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %0) : tensor<4xf32>, tensor<i32>, tensor<4xf32>\n    cond {\n      %c_2 = stablehlo.constant dense<7> : tensor<i32>\n      %2 = stablehlo.compare  LT, %iterArg_0, %c_2,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %2 = func.call @closed_call(%iterArg, %iterArg_1) : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xf32>\n      %c_2 = stablehlo.constant dense<1> : tensor<i32>\n      %3 = stablehlo.add %iterArg_0, %c_2 : tensor<i32>\n      stablehlo.return %iterArg, %3, %2 : tensor<4xf32>, tensor<i32>, tensor<4xf32>\n    }\n    return %1#2 : tensor<4xf32>\n  }\n  func.func private @closed_call(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> tensor<4xf32> {\n    %0 = stablehlo.divide %arg1, %arg0 : tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n"}
{"id": 10, "function_name": "vec_ykvg", "python": "def vec_ykvg(a):\n    \"\"\"Vector norm operation.\"\"\"\n    return jnp.linalg.norm(a, axis=-1)", "type": "generate_vector_kernel", "jaxpr": "{ lambda ; a:f32[4]. let\n    b:f32[] = jit[\n      name=norm\n      jaxpr={ lambda ; a:f32[4]. let\n          c:f32[4] = mul a a\n          d:f32[] = reduce_sum[axes=(0,) out_sharding=None] c\n          b:f32[] = sqrt d\n        in (b,) }\n    ] a\n  in (b,) }\n\n# Backward (Gradient) JAXPR:\n{ lambda ; a:f32[4]. let\n    _:f32[] b:f32[] = jit[\n      name=norm\n      jaxpr={ lambda ; a:f32[4]. let\n          c:f32[4] = mul a a\n          d:f32[] = reduce_sum[axes=(0,) out_sharding=None] c\n          _:f32[] = sqrt d\n          b:f32[] = div 0.5:f32[] _\n        in (_, b) }\n    ] a\n    e:f32[4] = jit[\n      name=norm\n      jaxpr={ lambda ; a:f32[4] b:f32[] f:f32[]. let\n          g:f32[] = mul f b\n          h:f32[4] = broadcast_in_dim[\n            broadcast_dimensions=()\n            shape=(4,)\n            sharding=None\n          ] g\n          i:f32[4] = mul a h\n          j:f32[4] = mul h a\n          e:f32[4] = add_any i j\n        in (e,) }\n    ] a b 1.0:f32[]\n  in (e,) }", "hlo": "module @jit_vec_ykvg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<4xf32>) -> tensor<f32>\n    return %0 : tensor<f32>\n  }\n  func.func private @norm(%arg0: tensor<4xf32>) -> tensor<f32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<4xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<4xf32>, tensor<f32>) -> tensor<f32>\n    %2 = stablehlo.sqrt %1 : tensor<f32>\n    return %2 : tensor<f32>\n  }\n}\n\n\n# Backward (Gradient) HLO:\nmodule @jit_vec_ykvg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = call @norm(%arg0) : (tensor<4xf32>) -> tensor<f32>\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %1 = call @norm_0(%arg0, %0, %cst) : (tensor<4xf32>, tensor<f32>, tensor<f32>) -> tensor<4xf32>\n    return %1 : tensor<4xf32>\n  }\n  func.func private @norm(%arg0: tensor<4xf32>) -> tensor<f32> {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<4xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<4xf32>, tensor<f32>) -> tensor<f32>\n    %2 = stablehlo.sqrt %1 : tensor<f32>\n    %cst_0 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %3 = stablehlo.divide %cst_0, %2 : tensor<f32>\n    return %3 : tensor<f32>\n  }\n  func.func private @norm_0(%arg0: tensor<4xf32>, %arg1: tensor<f32>, %arg2: tensor<f32>) -> tensor<4xf32> {\n    %0 = stablehlo.multiply %arg2, %arg1 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %2 = stablehlo.multiply %arg0, %1 : tensor<4xf32>\n    %3 = stablehlo.multiply %1, %arg0 : tensor<4xf32>\n    %4 = stablehlo.add %2, %3 : tensor<4xf32>\n    return %4 : tensor<4xf32>\n  }\n}\n"}
{"id": 11, "function_name": "branch_vfym", "python": "def branch_vfym(a):\n    \"\"\"Branching operation based on threshold.\"\"\"\n    return jnp.where(a > -0.2, a - 2.2, a - 1.9)", "type": "generate_branch_kernel", "jaxpr": "{ lambda ; a:f32[4]. let\n    b:bool[4] = gt a -0.20000000298023224:f32[]\n    c:f32[4] = sub a 2.200000047683716:f32[]\n    d:f32[4] = sub a 1.899999976158142:f32[]\n    e:f32[4] = jit[\n      name=_where\n      jaxpr={ lambda ; b:bool[4] c:f32[4] d:f32[4]. let\n          e:f32[4] = select_n b d c\n        in (e,) }\n    ] b c d\n  in (e,) }\n\n# Gradient extraction failed: Gradient only defined for scalar-output functions. Output had shape: (4,).", "hlo": "module @jit_branch_vfym attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-2.000000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xi1>\n    %cst_0 = stablehlo.constant dense<2.200000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<4xf32>\n    %cst_1 = stablehlo.constant dense<1.900000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %5 = stablehlo.subtract %arg0, %4 : tensor<4xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<4xi1>, tensor<4xf32>, tensor<4xf32>) -> tensor<4xf32>\n    return %6 : tensor<4xf32>\n  }\n  func.func private @_where(%arg0: tensor<4xi1>, %arg1: tensor<4xf32>, %arg2: tensor<4xf32>) -> tensor<4xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<4xi1>, tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n"}
{"id": 12, "function_name": "multi_mrqs", "python": "def multi_mrqs(a, b):\n    \"\"\"Multi-statement operation.\"\"\"\n    temp1 = a / b\n    temp2 = jnp.sin(temp1)\n    result = temp2 - a\n    return result", "type": "generate_multi_statement_kernel", "jaxpr": "{ lambda ; a:f32[4] b:f32[4]. let\n    c:f32[4] = div a b\n    d:f32[4] = sin c\n    e:f32[4] = sub d a\n  in (e,) }\n\n# Gradient extraction failed: Gradient only defined for scalar-output functions. Output had shape: (4,).", "hlo": "module @jit_multi_mrqs attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.divide %arg0, %arg1 : tensor<4xf32>\n    %1 = stablehlo.sine %0 : tensor<4xf32>\n    %2 = stablehlo.subtract %1, %arg0 : tensor<4xf32>\n    return %2 : tensor<4xf32>\n  }\n}\n"}
{"id": 13, "function_name": "elementwise_awsa", "python": "def elementwise_awsa(a, b):\n    \"\"\"Elementwise * operation.\"\"\"\n    return a * b", "type": "generate_simple_elementwise", "jaxpr": "{ lambda ; a:f32[4] b:f32[4]. let c:f32[4] = mul a b in (c,) }\n\n# Gradient extraction failed: Gradient only defined for scalar-output functions. Output had shape: (4,).", "hlo": "module @jit_elementwise_awsa attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n"}
{"id": 14, "function_name": "branch_gcrg", "python": "def branch_gcrg(a):\n    \"\"\"Branching operation based on threshold.\"\"\"\n    return jnp.where(a > 0.25, a - 2.0, a * 1.8)", "type": "generate_branch_kernel", "jaxpr": "{ lambda ; a:f32[4]. let\n    b:bool[4] = gt a 0.25:f32[]\n    c:f32[4] = sub a 2.0:f32[]\n    d:f32[4] = mul a 1.7999999523162842:f32[]\n    e:f32[4] = jit[\n      name=_where\n      jaxpr={ lambda ; b:bool[4] c:f32[4] d:f32[4]. let\n          e:f32[4] = select_n b d c\n        in (e,) }\n    ] b c d\n  in (e,) }\n\n# Gradient extraction failed: Gradient only defined for scalar-output functions. Output had shape: (4,).", "hlo": "module @jit_branch_gcrg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<2.500000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xi1>\n    %cst_0 = stablehlo.constant dense<2.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<4xf32>\n    %cst_1 = stablehlo.constant dense<1.800000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<4xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<4xi1>, tensor<4xf32>, tensor<4xf32>) -> tensor<4xf32>\n    return %6 : tensor<4xf32>\n  }\n  func.func private @_where(%arg0: tensor<4xi1>, %arg1: tensor<4xf32>, %arg2: tensor<4xf32>) -> tensor<4xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<4xi1>, tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n"}
{"id": 15, "function_name": "loop_xfcc", "python": "def loop_xfcc(a):\n    \"\"\"Loop operation using jax.lax.scan.\"\"\"\n    def body_fun(carry, _):\n        return carry * a, None\n    result, _ = jax.lax.scan(body_fun, jnp.zeros_like(a), jnp.arange(7))\n    return result", "type": "generate_loop_kernel", "jaxpr": "{ lambda ; a:f32[4]. let\n    b:f32[4] = broadcast_in_dim[\n      broadcast_dimensions=()\n      shape=(4,)\n      sharding=None\n    ] 0.0:f32[]\n    c:i32[7] = iota[dimension=0 dtype=int32 shape=(7,) sharding=None] \n    d:f32[4] = scan[\n      _split_transpose=False\n      jaxpr={ lambda ; e:f32[4] f:f32[4] g:i32[]. let\n          h:f32[4] = mul f e\n        in (h,) }\n      length=7\n      linear=(False, False, False)\n      num_carry=1\n      num_consts=1\n      reverse=False\n      unroll=1\n    ] a b c\n  in (d,) }\n\n# Gradient extraction failed: Gradient only defined for scalar-output functions. Output had shape: (4,).", "hlo": "module @jit_loop_xfcc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:3 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %0) : tensor<4xf32>, tensor<i32>, tensor<4xf32>\n    cond {\n      %c_2 = stablehlo.constant dense<7> : tensor<i32>\n      %2 = stablehlo.compare  LT, %iterArg_0, %c_2,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %2 = func.call @closed_call(%iterArg, %iterArg_1) : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xf32>\n      %c_2 = stablehlo.constant dense<1> : tensor<i32>\n      %3 = stablehlo.add %iterArg_0, %c_2 : tensor<i32>\n      stablehlo.return %iterArg, %3, %2 : tensor<4xf32>, tensor<i32>, tensor<4xf32>\n    }\n    return %1#2 : tensor<4xf32>\n  }\n  func.func private @closed_call(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> tensor<4xf32> {\n    %0 = stablehlo.multiply %arg1, %arg0 : tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n"}
{"id": 16, "function_name": "compound_vpsa", "python": "def compound_vpsa(a, b, scale):\n    \"\"\"Compound operation with multiple steps.\"\"\"\n    result = (a + b) * scale\n    result = result - jnp.floor(result)\n    return jnp.abs(result)", "type": "generate_compound_kernel", "jaxpr": "{ lambda ; a:f32[4] b:f32[4] c:f32[]. let\n    d:f32[4] = add a b\n    e:f32[] = convert_element_type[new_dtype=float32 weak_type=False] c\n    f:f32[4] = mul d e\n    g:f32[4] = floor f\n    h:f32[4] = sub f g\n    i:f32[4] = abs h\n  in (i,) }\n\n# Gradient extraction failed: Gradient only defined for scalar-output functions. Output had shape: (4,).", "hlo": "module @jit_compound_vpsa attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>, %arg2: tensor<f32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<4xf32>\n    %1 = stablehlo.convert %arg2 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %3 = stablehlo.multiply %0, %2 : tensor<4xf32>\n    %4 = stablehlo.floor %3 : tensor<4xf32>\n    %5 = stablehlo.subtract %3, %4 : tensor<4xf32>\n    %6 = stablehlo.abs %5 : tensor<4xf32>\n    return %6 : tensor<4xf32>\n  }\n}\n"}
{"id": 17, "function_name": "branch_geke", "python": "def branch_geke(a):\n    \"\"\"Branching operation based on threshold.\"\"\"\n    return jnp.where(a > -0.19, a + 1.4, a * 2.5)", "type": "generate_branch_kernel", "jaxpr": "{ lambda ; a:f32[4]. let\n    b:bool[4] = gt a -0.1899999976158142:f32[]\n    c:f32[4] = add a 1.399999976158142:f32[]\n    d:f32[4] = mul a 2.5:f32[]\n    e:f32[4] = jit[\n      name=_where\n      jaxpr={ lambda ; b:bool[4] c:f32[4] d:f32[4]. let\n          e:f32[4] = select_n b d c\n        in (e,) }\n    ] b c d\n  in (e,) }\n\n# Gradient extraction failed: Gradient only defined for scalar-output functions. Output had shape: (4,).", "hlo": "module @jit_branch_geke attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<-1.900000e-01> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %1 = stablehlo.compare  GT, %arg0, %0,  FLOAT : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xi1>\n    %cst_0 = stablehlo.constant dense<1.400000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %3 = stablehlo.add %arg0, %2 : tensor<4xf32>\n    %cst_1 = stablehlo.constant dense<2.500000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %5 = stablehlo.multiply %arg0, %4 : tensor<4xf32>\n    %6 = call @_where(%1, %3, %5) : (tensor<4xi1>, tensor<4xf32>, tensor<4xf32>) -> tensor<4xf32>\n    return %6 : tensor<4xf32>\n  }\n  func.func private @_where(%arg0: tensor<4xi1>, %arg1: tensor<4xf32>, %arg2: tensor<4xf32>) -> tensor<4xf32> {\n    %0 = stablehlo.select %arg0, %arg1, %arg2 : tensor<4xi1>, tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n"}
{"id": 18, "function_name": "unary_secj", "python": "def unary_secj(a):\n    \"\"\"Unary jnp.sqrt operation.\"\"\"\n    return jnp.sqrt(a)", "type": "generate_unary_kernel", "jaxpr": "{ lambda ; a:f32[4]. let b:f32[4] = sqrt a in (b,) }\n\n# Gradient extraction failed: Gradient only defined for scalar-output functions. Output had shape: (4,).", "hlo": "module @jit_unary_secj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sqrt %arg0 : tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n"}
{"id": 19, "function_name": "loop_yeuo", "python": "def loop_yeuo(a):\n    \"\"\"Loop operation using jax.lax.scan.\"\"\"\n    def body_fun(carry, _):\n        return carry / a, None\n    result, _ = jax.lax.scan(body_fun, jnp.zeros_like(a), jnp.arange(4))\n    return result", "type": "generate_loop_kernel", "jaxpr": "{ lambda ; a:f32[4]. let\n    b:f32[4] = broadcast_in_dim[\n      broadcast_dimensions=()\n      shape=(4,)\n      sharding=None\n    ] 0.0:f32[]\n    c:i32[4] = iota[dimension=0 dtype=int32 shape=(4,) sharding=None] \n    d:f32[4] = scan[\n      _split_transpose=False\n      jaxpr={ lambda ; e:f32[4] f:f32[4] g:i32[]. let\n          h:f32[4] = div f e\n        in (h,) }\n      length=4\n      linear=(False, False, False)\n      num_carry=1\n      num_consts=1\n      reverse=False\n      unroll=1\n    ] a b c\n  in (d,) }\n\n# Gradient extraction failed: Gradient only defined for scalar-output functions. Output had shape: (4,).", "hlo": "module @jit_loop_yeuo attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:3 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %0) : tensor<4xf32>, tensor<i32>, tensor<4xf32>\n    cond {\n      %c_2 = stablehlo.constant dense<4> : tensor<i32>\n      %2 = stablehlo.compare  LT, %iterArg_0, %c_2,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %2 = func.call @closed_call(%iterArg, %iterArg_1) : (tensor<4xf32>, tensor<4xf32>) -> tensor<4xf32>\n      %c_2 = stablehlo.constant dense<1> : tensor<i32>\n      %3 = stablehlo.add %iterArg_0, %c_2 : tensor<i32>\n      stablehlo.return %iterArg, %3, %2 : tensor<4xf32>, tensor<i32>, tensor<4xf32>\n    }\n    return %1#2 : tensor<4xf32>\n  }\n  func.func private @closed_call(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> tensor<4xf32> {\n    %0 = stablehlo.divide %arg1, %arg0 : tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n"}
