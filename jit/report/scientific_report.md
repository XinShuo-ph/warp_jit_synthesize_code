# JIT Code Synthesis for LLM Training Data
## Scientific Report

**Date**: December 28, 2025  
**Purpose**: Technical overview of the JIT code synthesis dataset for training code-generation LLMs

---

## 1. Executive Summary

This report describes a synthetic training dataset of **80,000 Python→IR code pairs** (40,000 CPU + 40,000 CUDA) generated using NVIDIA's Warp framework. The dataset provides aligned pairs of high-level Python kernel code and their corresponding low-level Intermediate Representation (IR), suitable for training Large Language Models (LLMs) on code translation and optimization tasks.

**Key Metrics**:
- **CPU Dataset**: 40,000 samples, 207 MB on disk
- **CUDA Dataset**: 40,000 samples, 213 MB on disk
- **Total**: 80,000 samples, 420 MB combined
- **Kernel Types**: 11 distinct categories
- **Generation Rate**: ~80-85 pairs/second

---

## 2. Just-In-Time (JIT) Compilation

### 2.1 What is JIT Compilation?

Just-In-Time (JIT) compilation is a method of code execution that combines the benefits of interpretation and ahead-of-time (AOT) compilation. Instead of compiling code before execution (like C/C++) or interpreting it line-by-line (like traditional Python), JIT compilers translate code to machine instructions at runtime, enabling:

- **Dynamic optimization**: Code can be optimized based on actual runtime behavior
- **Platform portability**: Same source code runs on different hardware architectures  
- **Rapid development**: High-level language syntax with low-level performance

### 2.2 JIT in Numerical Computing

For numerical computing and scientific simulations, JIT compilation enables:

1. **Writing kernels in Python syntax** while achieving C++/CUDA-level performance
2. **Automatic differentiation** for gradient-based optimization
3. **Seamless GPU acceleration** without manual CUDA programming
4. **Type specialization** that eliminates Python's dynamic typing overhead

### 2.3 Python JIT Frameworks

| Framework | Vendor | Primary Use Case |
|-----------|--------|------------------|
| **Numba** | Anaconda | General numerical computing |
| **JAX** | Google | ML research, autodiff |
| **PyTorch JIT** | Meta | Deep learning |
| **NVIDIA Warp** | NVIDIA | Physics simulation, robotics |
| **Triton** | OpenAI | Custom GPU kernels |

---

## 3. Intermediate Representation (IR)

### 3.1 What is IR?

Intermediate Representation (IR) is a data structure or code representation used internally by a compiler to represent the source code during compilation. IR serves as an abstraction layer between:

- **High-level source code** (human-readable, platform-independent)
- **Low-level machine code** (CPU/GPU-specific binary instructions)

### 3.2 Role of IR in Compilation

```
┌──────────────┐     ┌─────────────┐     ┌──────────────┐
│ Python Source│ ──► │     IR      │ ──► │ Machine Code │
│  (@wp.kernel)│     │ (C++/CUDA)  │     │  (Binary)    │
└──────────────┘     └─────────────┘     └──────────────┘
     Front-end          Middle-end          Back-end
```

The IR in our dataset is the **C++/CUDA code** generated by Warp's compiler, which represents the Python kernel logic in a form ready for final compilation.

### 3.3 Why Python→IR Pairs are Valuable

Training LLMs on Python→IR pairs enables:

1. **Code Translation Models**: Learning to translate between abstraction levels
2. **Compiler Optimization Research**: Understanding transformation patterns
3. **Performance Prediction**: Estimating compute cost from high-level code
4. **Decompilation**: Recovering high-level intent from low-level code
5. **Cross-platform Targeting**: Learning CPU↔GPU code transformations

---

## 4. NVIDIA Warp

### 4.1 Overview

NVIDIA Warp is a Python framework designed for high-performance simulation and spatial computing. It provides:

- **Python-native kernel programming** with familiar syntax
- **Automatic GPU acceleration** with CUDA backend
- **Differentiable programming** for gradient-based optimization
- **Rich type system** for vectors, matrices, and spatial data
- **FEM (Finite Element Method)** support for physics simulation

### 4.2 Kernel Programming Model

Warp kernels are Python functions decorated with `@wp.kernel`:

```python
import warp as wp

@wp.kernel
def saxpy(a: wp.array(dtype=float),
          b: wp.array(dtype=float),
          out: wp.array(dtype=float),
          alpha: float):
    tid = wp.tid()  # Thread index
    out[tid] = alpha * a[tid] + b[tid]
```

Key features:
- **Thread-parallel execution**: Each thread processes one element
- **Type annotations**: Required for compilation
- **Built-in functions**: `wp.sin`, `wp.dot`, `wp.normalize`, etc.
- **Vector types**: `wp.vec3`, `wp.mat33`, `wp.quat`

### 4.3 IR Extraction Mechanism

Warp's compilation pipeline exposes IR at the C++/CUDA level:

```python
import warp._src.context as ctx

# Get module builder
builder = ctx.ModuleBuilder(kernel.module, options, hasher)

# Generate IR for target device
cpp_code = builder.codegen("cpu")   # C++ for CPU
cuda_code = builder.codegen("cuda") # CUDA for GPU
```

### 4.4 CPU vs CUDA IR Differences

| Aspect | CPU IR (.cpp) | CUDA IR (.cu) |
|--------|---------------|---------------|
| Thread indexing | `builtin_tid1d()` | `blockIdx.x * blockDim.x + threadIdx.x` |
| Memory model | Sequential access | Shared memory, coalescing |
| Parallelism | Process-level | Massive thread-level |
| Launch bounds | `task_index` parameter | `dim, blockDim, gridDim` |
| Sync primitives | None | `__syncthreads()` |

---

## 5. Dataset Overview

### 5.1 CPU Dataset Statistics

| Metric | Value |
|--------|-------|
| **Total Samples** | 40,000 |
| **Disk Size** | 207 MB |
| **Average Sample Size** | ~5.2 KB |
| **File Format** | JSON |

**Category Distribution**:

| Category | Count | Percentage |
|----------|-------|------------|
| arithmetic | 3,698 | 9.2% |
| atomic | 3,644 | 9.1% |
| combined | 3,603 | 9.0% |
| conditional | 3,670 | 9.2% |
| loop | 3,669 | 9.2% |
| math | 3,739 | 9.3% |
| multi_cond | 3,596 | 9.0% |
| nested | 3,636 | 9.1% |
| random_math | 3,618 | 9.0% |
| scalar_param | 3,599 | 9.0% |
| vector | 3,528 | 8.8% |

### 5.2 CUDA Dataset Statistics

| Metric | Value |
|--------|-------|
| **Total Samples** | 40,000 |
| **Disk Size** | 213 MB |
| **Average Sample Size** | ~5.3 KB |
| **File Format** | JSON |

**Category Distribution**:

| Category | Count | Percentage |
|----------|-------|------------|
| arithmetic | 3,619 | 9.0% |
| atomic | 3,721 | 9.3% |
| combined | 3,553 | 8.9% |
| conditional | 3,607 | 9.0% |
| loop | 3,705 | 9.3% |
| math | 3,613 | 9.0% |
| multi_cond | 3,568 | 8.9% |
| nested | 3,722 | 9.3% |
| random_math | 3,675 | 9.2% |
| scalar_param | 3,600 | 9.0% |
| vector | 3,617 | 9.0% |

### 5.3 Kernel Category Descriptions

| Category | Description | Example Operations |
|----------|-------------|-------------------|
| **arithmetic** | Basic math operations | `+`, `-`, `*`, `/` |
| **conditional** | If-else branching | `if x > threshold: ...` |
| **loop** | For loop patterns | `for i in range(n): acc += ...` |
| **math** | Math function calls | `wp.sin`, `wp.cos`, `wp.exp` |
| **vector** | Vector operations | `wp.vec3`, `wp.dot`, `wp.normalize` |
| **atomic** | Atomic operations | `wp.atomic_add` |
| **nested** | Nested loops | Double `for` loops |
| **multi_cond** | Multiple conditions | `if/elif/else` chains |
| **combined** | Mixed patterns | Loops + conditionals + math |
| **scalar_param** | Scalar parameters | Non-array kernel arguments |
| **random_math** | Random expressions | Randomly generated formulas |

---

## 6. Data Format

### 6.1 JSON Structure

Each sample is a JSON file with the following structure:

```json
{
  "python_source": "@wp.kernel\ndef kernel_name(...):\n    ...",
  "cpp_forward": "void kernel_name_..._forward(...) {\n    ...\n}",
  "metadata": {
    "kernel_name": "kernel_name",
    "category": "arithmetic",
    "device": "cpu"
  }
}
```

### 6.2 Field Descriptions

| Field | Type | Description |
|-------|------|-------------|
| `python_source` | string | Original Python kernel with `@wp.kernel` decorator |
| `cpp_forward` | string | Generated C++/CUDA forward pass function |
| `metadata.kernel_name` | string | Unique kernel identifier |
| `metadata.category` | string | Kernel category (one of 11 types) |
| `metadata.device` | string | Target device ("cpu" or "cuda") |

### 6.3 Example: CPU Sample

**Python Source**:
```python
@wp.kernel
def arith_bgxtbm(a: wp.array(dtype=float), b: wp.array(dtype=float), out: wp.array(dtype=float)):
    tid = wp.tid()
    out[tid] = (a[tid] - b[tid]) * 1.92
```

**Generated C++ IR**:
```cpp
void arith_bgxtbm_78c04fe9_cpu_kernel_forward(
    wp::launch_bounds_t dim,
    size_t task_index,
    wp_args_arith_bgxtbm_78c04fe9 *_wp_args)
{
    // argument vars
    wp::array_t<wp::float32> var_a = _wp_args->a;
    wp::array_t<wp::float32> var_b = _wp_args->b;
    wp::array_t<wp::float32> var_out = _wp_args->out;
    
    // primal vars
    wp::int32 var_0;
    wp::float32 var_3, var_4, var_5, var_7;
    const wp::float32 var_6 = 1.92;
    
    // forward
    var_0 = builtin_tid1d();
    var_4 = wp::load(wp::address(var_a, var_0));
    var_5 = wp::load(wp::address(var_b, var_0));
    var_3 = wp::sub(var_4, var_5);
    var_7 = wp::mul(var_3, var_6);
    wp::array_store(var_out, var_0, var_7);
}
```

### 6.4 Example: CUDA Sample

**Python Source**:
```python
@wp.kernel
def combined_mauchw(a: wp.array(dtype=float), b: wp.array(dtype=float), out: wp.array(dtype=float)):
    tid = wp.tid()
    acc = float(0.0)
    for i in range(3):
        if a[tid] * float(i) > 1.63:
            acc = acc + wp.abs(b[tid])
        else:
            acc = acc + b[tid]
    out[tid] = acc
```

**Generated CUDA IR** (excerpt):
```cuda
void combined_mauchw_c144e405_cuda_kernel_forward(
    wp::launch_bounds_t dim,
    wp::array_t<wp::float32> var_a,
    wp::array_t<wp::float32> var_b,
    wp::array_t<wp::float32> var_out)
{
    for (size_t _idx = blockDim.x * blockIdx.x + threadIdx.x;
         _idx < dim.size;
         _idx += blockDim.x * gridDim.x)
    {
        // Thread-parallel loop body
        var_0 = builtin_tid1d();
        // ... unrolled loop with conditionals ...
        wp::array_store(var_out, var_0, var_50);
    }
}
```

---

## 7. Potential Applications

### 7.1 LLM Training for Code Generation

The dataset enables training models that:
- **Translate Python to optimized IR**: Given high-level Python, generate efficient low-level code
- **Predict performance characteristics**: Estimate compute/memory cost from source
- **Suggest optimizations**: Recommend code transformations for better performance

### 7.2 IR Understanding and Optimization

Research applications include:
- **Compiler optimization learning**: Training models to perform IR-level optimizations
- **Pattern recognition**: Identifying common code patterns and their IR equivalents
- **Vectorization prediction**: Learning when loops can be parallelized

### 7.3 Cross-Device Code Translation

The paired CPU/CUDA datasets enable:
- **CPU↔GPU translation**: Learning to port code between architectures
- **Performance portability**: Understanding device-specific code patterns
- **Heterogeneous computing**: Training models for multi-device code generation

### 7.4 Code Understanding Tasks

Additional use cases:
- **Code summarization**: Describing what compiled code does
- **Decompilation**: Recovering high-level intent from low-level code
- **Bug detection**: Identifying potential issues in generated code

---

## 8. Generation Pipeline Architecture

### 8.1 Pipeline Overview

```
┌─────────────────┐     ┌─────────────────┐     ┌─────────────────┐
│ KernelGenerator │ ──► │ Warp Compiler   │ ──► │ IR Extractor    │
│ (Python source) │     │ (JIT compile)   │     │ (C++/CUDA code) │
└─────────────────┘     └─────────────────┘     └─────────────────┘
         │                       │                       │
         ▼                       ▼                       ▼
    Random kernel         Module building         Forward function
    specification         with codegen            extraction
```

### 8.2 Key Components

| Component | File | Purpose |
|-----------|------|---------|
| `generator.py` | Kernel templates | Generates varied Python kernel source code |
| `pipeline.py` | End-to-end synthesis | Orchestrates generation→compile→extract flow |
| `batch_generator.py` | Scalable generation | Efficient batched generation for large datasets |
| `ir_extractor.py` | IR extraction | Extracts forward/backward functions from compiled code |

### 8.3 Generation Parameters

| Parameter | Value | Notes |
|-----------|-------|-------|
| Kernels per batch | 10 | Optimizes import overhead |
| Chunk size | 1000 | Memory management |
| Generation rate | 80-85/sec | Single CPU core |
| Random seed | Configurable | For reproducibility |

---

## 9. Validation Methodology

### 9.1 Syntactic Validation

All generated pairs are validated for:
- **Valid Python syntax**: Kernel compiles without errors
- **Valid C++/CUDA syntax**: Generated IR is syntactically correct
- **Complete function extraction**: Forward function fully captured

### 9.2 Semantic Validation

Kernel categories are validated by:
- **Category-specific patterns**: Each category uses expected operations
- **Balanced distribution**: ~9% per category (11 categories)
- **Unique kernel names**: No duplicate identifiers

### 9.3 Quality Metrics

| Metric | CPU Dataset | CUDA Dataset |
|--------|-------------|--------------|
| Compilation success rate | 100% | 100% |
| IR extraction success | 100% | 100% |
| Category coverage | 11/11 | 11/11 |

---

## 10. Appendix

### A. Kernel Type Catalog

#### A.1 Arithmetic Kernels
```python
@wp.kernel
def arith_example(a: wp.array(dtype=float), b: wp.array(dtype=float), out: wp.array(dtype=float)):
    tid = wp.tid()
    out[tid] = (a[tid] + b[tid]) * 2.5
```

#### A.2 Conditional Kernels
```python
@wp.kernel
def cond_example(x: wp.array(dtype=float), out: wp.array(dtype=float)):
    tid = wp.tid()
    if x[tid] > 0.5:
        out[tid] = x[tid] * 2.0
    else:
        out[tid] = x[tid] * 0.5
```

#### A.3 Loop Kernels
```python
@wp.kernel
def loop_example(arr: wp.array(dtype=float), out: wp.array(dtype=float)):
    tid = wp.tid()
    acc = float(0.0)
    for i in range(5):
        acc = acc + arr[tid] * float(i + 1)
    out[tid] = acc
```

#### A.4 Vector Kernels
```python
@wp.kernel
def vec_example(pos: wp.array(dtype=wp.vec3), vel: wp.array(dtype=wp.vec3), acc: wp.array(dtype=wp.vec3)):
    tid = wp.tid()
    dt = 0.01
    new_vel = vel[tid] + acc[tid] * dt
    pos[tid] = pos[tid] + new_vel * dt
    vel[tid] = new_vel
```

### B. File Naming Convention

```
data/
├── cpu/
│   ├── pair_000000.json
│   ├── pair_000001.json
│   └── ...
└── cuda/
    ├── pair_000000.json
    ├── pair_000001.json
    └── ...
```

### C. References

1. NVIDIA Warp Documentation: https://nvidia.github.io/warp/
2. NVIDIA Warp GitHub: https://github.com/NVIDIA/warp
3. CUDA Programming Guide: https://docs.nvidia.com/cuda/cuda-c-programming-guide/

---

*Report generated as part of the JIT Code Synthesis project for LLM training data.*
