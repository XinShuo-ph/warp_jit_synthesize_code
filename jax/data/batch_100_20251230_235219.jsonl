{"id": 0, "name": "nn_layer_ctgd", "python_source": "def nn_layer_ctgd(x, w, b):\n    \"\"\"Neural network layer with relu activation.\"\"\"\n    return jax.nn.relu(jnp.dot(x, w) + b)", "jaxpr": "{ lambda ; a:f32[2,25] b:f32[25,27] c:f32[27]. let\n    d:f32[2,27] = dot_general[\n      dimension_numbers=(([1], [0]), ([], []))\n      preferred_element_type=float32\n    ] a b\n    e:f32[1,27] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 27)\n      sharding=None\n    ] c\n    f:f32[2,27] = add d e\n    g:f32[2,27] = custom_jvp_call[\n      name=relu\n      call_jaxpr={ lambda ; h:f32[2,27]. let\n          i:f32[2,27] = jit[\n            name=relu\n            jaxpr={ lambda ; h:f32[2,27]. let\n                i:f32[2,27] = max h 0.0:f32[]\n              in (i,) }\n          ] h\n        in (i,) }\n      jvp=jvp\n      symbolic_zeros=False\n    ] f\n  in (g,) }", "stablehlo": "module @jit_nn_layer_ctgd attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x25xf32>, %arg1: tensor<25x27xf32>, %arg2: tensor<27xf32>) -> (tensor<2x27xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.dot_general %arg0, %arg1, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x25xf32>, tensor<25x27xf32>) -> tensor<2x27xf32>\n    %1 = stablehlo.broadcast_in_dim %arg2, dims = [1] : (tensor<27xf32>) -> tensor<1x27xf32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x27xf32>) -> tensor<2x27xf32>\n    %3 = stablehlo.add %0, %2 : tensor<2x27xf32>\n    %4 = call @relu(%3) : (tensor<2x27xf32>) -> tensor<2x27xf32>\n    return %4 : tensor<2x27xf32>\n  }\n  func.func private @relu(%arg0: tensor<2x27xf32>) -> tensor<2x27xf32> {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<2x27xf32>\n    %1 = stablehlo.maximum %arg0, %0 : tensor<2x27xf32>\n    return %1 : tensor<2x27xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_nn_layer_ctgd, entry_computation_layout={(f32[2,25]{1,0}, f32[25,27]{1,0}, f32[27]{0})->f32[2,27]{1,0}}\n\nrelu.1 {\n  Arg_0.1 = f32[2,27]{1,0} parameter(0)\n  constant.1 = f32[] constant(0)\n  max.2 = f32[2,27]{1,0} broadcast(constant.1), dimensions={}\n  ROOT max.3 = f32[2,27]{1,0} maximum(Arg_0.1, max.2)\n}\n\nENTRY main.2 {\n  x.1 = f32[2,25]{1,0} parameter(0)\n  w.1 = f32[25,27]{1,0} parameter(1)\n  dot_general.1 = f32[2,27]{1,0} dot(x.1, w.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n  b.1 = f32[27]{0} parameter(2)\n  broadcast_in_dim.1 = f32[1,27]{1,0} reshape(b.1)\n  add.4 = f32[1,27]{1,0} broadcast(broadcast_in_dim.1), dimensions={0,1}\n  add.5 = f32[27]{0} reshape(add.4)\n  add.6 = f32[2,27]{1,0} broadcast(add.5), dimensions={1}\n  add.7 = f32[2,27]{1,0} add(dot_general.1, add.6)\n  ROOT jit_relu_.1 = f32[2,27]{1,0} call(add.7), to_apply=relu.1\n}\n\n", "input_shapes": [[2, 25], [25, 27], [27]], "output_shape": [2, 27]}
{"id": 1, "name": "composite_kafn", "python_source": "def composite_kafn(x):\n    \"\"\"Composite function with multiple operations.\"\"\"\n    return jnp.exp(-jnp.abs(x)) + x ** 2", "jaxpr": "{ lambda ; a:f32[2,10]. let\n    b:f32[2,10] = abs a\n    c:f32[2,10] = neg b\n    d:f32[2,10] = exp c\n    e:f32[2,10] = integer_pow[y=2] a\n    f:f32[2,10] = add d e\n  in (f,) }", "stablehlo": "module @jit_composite_kafn attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x10xf32>) -> (tensor<2x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.abs %arg0 : tensor<2x10xf32>\n    %1 = stablehlo.negate %0 : tensor<2x10xf32>\n    %2 = stablehlo.exponential %1 : tensor<2x10xf32>\n    %3 = stablehlo.multiply %arg0, %arg0 : tensor<2x10xf32>\n    %4 = stablehlo.add %2, %3 : tensor<2x10xf32>\n    return %4 : tensor<2x10xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_composite_kafn, entry_computation_layout={(f32[2,10]{1,0})->f32[2,10]{1,0}}\n\nENTRY main.1 {\n  x.1 = f32[2,10]{1,0} parameter(0)\n  abs.1 = f32[2,10]{1,0} abs(x.1)\n  neg.1 = f32[2,10]{1,0} negate(abs.1)\n  exp.1 = f32[2,10]{1,0} exponential(neg.1)\n  integer_pow.1 = f32[2,10]{1,0} multiply(x.1, x.1)\n  ROOT add.1 = f32[2,10]{1,0} add(exp.1, integer_pow.1)\n}\n\n", "input_shapes": [[2, 10]], "output_shape": [2, 10]}
{"id": 2, "name": "div_pvau", "python_source": "def div_pvau(x, y):\n    \"\"\"Division (safe) operation.\"\"\"\n    return x / (y + 1e-6)", "jaxpr": "{ lambda ; a:f32[8,7,6] b:f32[8,7,6]. let\n    c:f32[8,7,6] = add b 9.999999974752427e-07:f32[]\n    d:f32[8,7,6] = div a c\n  in (d,) }", "stablehlo": "module @jit_div_pvau attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x7x6xf32>, %arg1: tensor<8x7x6xf32>) -> (tensor<8x7x6xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<9.99999997E-7> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<8x7x6xf32>\n    %1 = stablehlo.add %arg1, %0 : tensor<8x7x6xf32>\n    %2 = stablehlo.divide %arg0, %1 : tensor<8x7x6xf32>\n    return %2 : tensor<8x7x6xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_div_pvau, entry_computation_layout={(f32[8,7,6]{2,1,0}, f32[8,7,6]{2,1,0})->f32[8,7,6]{2,1,0}}\n\nENTRY main.1 {\n  x.1 = f32[8,7,6]{2,1,0} parameter(0)\n  y.1 = f32[8,7,6]{2,1,0} parameter(1)\n  constant.1 = f32[] constant(1e-06)\n  add.2 = f32[8,7,6]{2,1,0} broadcast(constant.1), dimensions={}\n  add.3 = f32[8,7,6]{2,1,0} add(y.1, add.2)\n  ROOT div.1 = f32[8,7,6]{2,1,0} divide(x.1, add.3)\n}\n\n", "input_shapes": [[8, 7, 6], [8, 7, 6]], "output_shape": [8, 7, 6]}
{"id": 3, "name": "sub_yicc", "python_source": "def sub_yicc(x, y):\n    \"\"\"Subtraction operation.\"\"\"\n    return x - y", "jaxpr": "{ lambda ; a:f32[11,6] b:f32[11,6]. let c:f32[11,6] = sub a b in (c,) }", "stablehlo": "module @jit_sub_yicc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<11x6xf32>, %arg1: tensor<11x6xf32>) -> (tensor<11x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<11x6xf32>\n    return %0 : tensor<11x6xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_sub_yicc, entry_computation_layout={(f32[11,6]{1,0}, f32[11,6]{1,0})->f32[11,6]{1,0}}\n\nENTRY main.1 {\n  x.1 = f32[11,6]{1,0} parameter(0)\n  y.1 = f32[11,6]{1,0} parameter(1)\n  ROOT sub.1 = f32[11,6]{1,0} subtract(x.1, y.1)\n}\n\n", "input_shapes": [[11, 6], [11, 6]], "output_shape": [11, 6]}
{"id": 4, "name": "vmap_bldx", "python_source": "def vmap_bldx(a_batch, b_batch):\n    \"\"\"Batched operation using vmap.\"\"\"\n    def inner(a, b):\n        return jnp.sum(a * b)\n    return jax.vmap(inner)(a_batch, b_batch)", "jaxpr": "{ lambda ; a:f32[2,12] b:f32[2,12]. let\n    c:f32[2,12] = mul a b\n    d:f32[2] = reduce_sum[axes=(np.int64(1),) out_sharding=None] c\n  in (d,) }", "stablehlo": "module @jit_vmap_bldx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x12xf32>, %arg1: tensor<2x12xf32>) -> (tensor<2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<2x12xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<2x12xf32>, tensor<f32>) -> tensor<2xf32>\n    return %1 : tensor<2xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_vmap_bldx, entry_computation_layout={(f32[2,12]{1,0}, f32[2,12]{1,0})->f32[2]{0}}\n\nregion_0.1 {\n  reduce_sum.3 = f32[] parameter(0)\n  reduce_sum.4 = f32[] parameter(1)\n  ROOT reduce_sum.5 = f32[] add(reduce_sum.3, reduce_sum.4)\n}\n\nENTRY main.2 {\n  a_batch.1 = f32[2,12]{1,0} parameter(0)\n  b_batch.1 = f32[2,12]{1,0} parameter(1)\n  mul.1 = f32[2,12]{1,0} multiply(a_batch.1, b_batch.1)\n  constant.1 = f32[] constant(0)\n  ROOT reduce_sum.7 = f32[2]{0} reduce(mul.1, constant.1), dimensions={1}, to_apply=region_0.1\n}\n\n", "input_shapes": [[2, 12], [2, 12]], "output_shape": [2]}
{"id": 5, "name": "vmap_qxje", "python_source": "def vmap_qxje(a_batch, b_batch):\n    \"\"\"Batched operation using vmap.\"\"\"\n    def inner(a, b):\n        return jnp.maximum(a, b)\n    return jax.vmap(inner)(a_batch, b_batch)", "jaxpr": "{ lambda ; a:f32[2,4] b:f32[2,4]. let c:f32[2,4] = max a b in (c,) }", "stablehlo": "module @jit_vmap_qxje attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x4xf32>, %arg1: tensor<2x4xf32>) -> (tensor<2x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.maximum %arg0, %arg1 : tensor<2x4xf32>\n    return %0 : tensor<2x4xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_vmap_qxje, entry_computation_layout={(f32[2,4]{1,0}, f32[2,4]{1,0})->f32[2,4]{1,0}}\n\nENTRY main.1 {\n  a_batch.1 = f32[2,4]{1,0} parameter(0)\n  b_batch.1 = f32[2,4]{1,0} parameter(1)\n  ROOT max.1 = f32[2,4]{1,0} maximum(a_batch.1, b_batch.1)\n}\n\n", "input_shapes": [[2, 4], [2, 4]], "output_shape": [2, 4]}
{"id": 6, "name": "min_hcgc", "python_source": "def min_hcgc(x, y):\n    \"\"\"Element-wise min operation.\"\"\"\n    return jnp.minimum(x, y)", "jaxpr": "{ lambda ; a:f32[9,12] b:f32[9,12]. let c:f32[9,12] = min a b in (c,) }", "stablehlo": "module @jit_min_hcgc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x12xf32>, %arg1: tensor<9x12xf32>) -> (tensor<9x12xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.minimum %arg0, %arg1 : tensor<9x12xf32>\n    return %0 : tensor<9x12xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_min_hcgc, entry_computation_layout={(f32[9,12]{1,0}, f32[9,12]{1,0})->f32[9,12]{1,0}}\n\nENTRY main.1 {\n  x.1 = f32[9,12]{1,0} parameter(0)\n  y.1 = f32[9,12]{1,0} parameter(1)\n  ROOT min.1 = f32[9,12]{1,0} minimum(x.1, y.1)\n}\n\n", "input_shapes": [[9, 12], [9, 12]], "output_shape": [9, 12]}
{"id": 7, "name": "vmap_jjfg", "python_source": "def vmap_jjfg(a_batch, b_batch):\n    \"\"\"Batched operation using vmap.\"\"\"\n    def inner(a, b):\n        return jnp.dot(a, b)\n    return jax.vmap(inner)(a_batch, b_batch)", "jaxpr": "{ lambda ; a:f32[7,14] b:f32[7,14]. let\n    c:f32[7] = dot_general[\n      dimension_numbers=(([1], [1]), ([0], [0]))\n      preferred_element_type=float32\n    ] a b\n  in (c,) }", "stablehlo": "module @jit_vmap_jjfg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x14xf32>, %arg1: tensor<7x14xf32>) -> (tensor<7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.dot_general %arg0, %arg1, batching_dims = [0] x [0], contracting_dims = [1] x [1], precision = [DEFAULT, DEFAULT] : (tensor<7x14xf32>, tensor<7x14xf32>) -> tensor<7xf32>\n    return %0 : tensor<7xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_vmap_jjfg, entry_computation_layout={(f32[7,14]{1,0}, f32[7,14]{1,0})->f32[7]{0}}\n\nENTRY main.1 {\n  a_batch.1 = f32[7,14]{1,0} parameter(0)\n  b_batch.1 = f32[7,14]{1,0} parameter(1)\n  ROOT dot_general.1 = f32[7]{0} dot(a_batch.1, b_batch.1), lhs_batch_dims={0}, lhs_contracting_dims={1}, rhs_batch_dims={0}, rhs_contracting_dims={1}\n}\n\n", "input_shapes": [[7, 14], [7, 14]], "output_shape": [7]}
{"id": 8, "name": "composite_qngm", "python_source": "def composite_qngm(x):\n    \"\"\"Composite function with multiple operations.\"\"\"\n    return jnp.sin(x) + jax.nn.relu(x)", "jaxpr": "{ lambda ; a:f32[6,16]. let\n    b:f32[6,16] = sin a\n    c:f32[6,16] = custom_jvp_call[\n      name=relu\n      call_jaxpr={ lambda ; d:f32[6,16]. let\n          e:f32[6,16] = jit[\n            name=relu\n            jaxpr={ lambda ; d:f32[6,16]. let\n                e:f32[6,16] = max d 0.0:f32[]\n              in (e,) }\n          ] d\n        in (e,) }\n      jvp=jvp\n      symbolic_zeros=False\n    ] a\n    f:f32[6,16] = add b c\n  in (f,) }", "stablehlo": "module @jit_composite_qngm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x16xf32>) -> (tensor<6x16xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<6x16xf32>\n    %1 = call @relu(%arg0) : (tensor<6x16xf32>) -> tensor<6x16xf32>\n    %2 = stablehlo.add %0, %1 : tensor<6x16xf32>\n    return %2 : tensor<6x16xf32>\n  }\n  func.func private @relu(%arg0: tensor<6x16xf32>) -> tensor<6x16xf32> {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<6x16xf32>\n    %1 = stablehlo.maximum %arg0, %0 : tensor<6x16xf32>\n    return %1 : tensor<6x16xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_composite_qngm, entry_computation_layout={(f32[6,16]{1,0})->f32[6,16]{1,0}}\n\nrelu.1 {\n  Arg_0.1 = f32[6,16]{1,0} parameter(0)\n  constant.1 = f32[] constant(0)\n  max.2 = f32[6,16]{1,0} broadcast(constant.1), dimensions={}\n  ROOT max.3 = f32[6,16]{1,0} maximum(Arg_0.1, max.2)\n}\n\nENTRY main.2 {\n  x.1 = f32[6,16]{1,0} parameter(0)\n  sin.1 = f32[6,16]{1,0} sine(x.1)\n  jit_relu_.1 = f32[6,16]{1,0} call(x.1), to_apply=relu.1\n  ROOT add.1 = f32[6,16]{1,0} add(sin.1, jit_relu_.1)\n}\n\n", "input_shapes": [[6, 16]], "output_shape": [6, 16]}
{"id": 9, "name": "min_aigf", "python_source": "def min_aigf(x, y):\n    \"\"\"Element-wise min operation.\"\"\"\n    return jnp.minimum(x, y)", "jaxpr": "{ lambda ; a:f32[16,13,7] b:f32[16,13,7]. let c:f32[16,13,7] = min a b in (c,) }", "stablehlo": "module @jit_min_aigf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<16x13x7xf32>, %arg1: tensor<16x13x7xf32>) -> (tensor<16x13x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.minimum %arg0, %arg1 : tensor<16x13x7xf32>\n    return %0 : tensor<16x13x7xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_min_aigf, entry_computation_layout={(f32[16,13,7]{2,1,0}, f32[16,13,7]{2,1,0})->f32[16,13,7]{2,1,0}}\n\nENTRY main.1 {\n  x.1 = f32[16,13,7]{2,1,0} parameter(0)\n  y.1 = f32[16,13,7]{2,1,0} parameter(1)\n  ROOT min.1 = f32[16,13,7]{2,1,0} minimum(x.1, y.1)\n}\n\n", "input_shapes": [[16, 13, 7], [16, 13, 7]], "output_shape": [16, 13, 7]}
{"id": 10, "name": "max_mxqd", "python_source": "def max_mxqd(x, y):\n    \"\"\"Element-wise max operation.\"\"\"\n    return jnp.maximum(x, y)", "jaxpr": "{ lambda ; a:f32[5] b:f32[5]. let c:f32[5] = max a b in (c,) }", "stablehlo": "module @jit_max_mxqd attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>, %arg1: tensor<5xf32>) -> (tensor<5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.maximum %arg0, %arg1 : tensor<5xf32>\n    return %0 : tensor<5xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_max_mxqd, entry_computation_layout={(f32[5]{0}, f32[5]{0})->f32[5]{0}}\n\nENTRY main.1 {\n  x.1 = f32[5]{0} parameter(0)\n  y.1 = f32[5]{0} parameter(1)\n  ROOT max.1 = f32[5]{0} maximum(x.1, y.1)\n}\n\n", "input_shapes": [[5], [5]], "output_shape": [5]}
{"id": 11, "name": "nn_layer_ogpx", "python_source": "def nn_layer_ogpx(x, w, b):\n    \"\"\"Neural network layer with sigmoid activation.\"\"\"\n    return jax.nn.sigmoid(jnp.dot(x, w) + b)", "jaxpr": "{ lambda ; a:f32[7,15] b:f32[15,11] c:f32[11]. let\n    d:f32[7,11] = dot_general[\n      dimension_numbers=(([1], [0]), ([], []))\n      preferred_element_type=float32\n    ] a b\n    e:f32[1,11] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 11)\n      sharding=None\n    ] c\n    f:f32[7,11] = add d e\n    g:f32[7,11] = logistic f\n  in (g,) }", "stablehlo": "module @jit_nn_layer_ogpx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x15xf32>, %arg1: tensor<15x11xf32>, %arg2: tensor<11xf32>) -> (tensor<7x11xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.dot_general %arg0, %arg1, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<7x15xf32>, tensor<15x11xf32>) -> tensor<7x11xf32>\n    %1 = stablehlo.broadcast_in_dim %arg2, dims = [1] : (tensor<11xf32>) -> tensor<1x11xf32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x11xf32>) -> tensor<7x11xf32>\n    %3 = stablehlo.add %0, %2 : tensor<7x11xf32>\n    %4 = stablehlo.negate %3 : tensor<7x11xf32>\n    %5 = stablehlo.exponential %4 : tensor<7x11xf32>\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %6 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<7x11xf32>\n    %7 = stablehlo.add %6, %5 : tensor<7x11xf32>\n    %cst_0 = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %8 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<7x11xf32>\n    %9 = stablehlo.divide %8, %7 : tensor<7x11xf32>\n    return %9 : tensor<7x11xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_nn_layer_ogpx, entry_computation_layout={(f32[7,15]{1,0}, f32[15,11]{1,0}, f32[11]{0})->f32[7,11]{1,0}}\n\nENTRY main.1 {\n  constant.1 = f32[] constant(1)\n  broadcast.1 = f32[7,11]{1,0} broadcast(constant.1), dimensions={}\n  x.1 = f32[7,15]{1,0} parameter(0)\n  w.1 = f32[15,11]{1,0} parameter(1)\n  dot_general.1 = f32[7,11]{1,0} dot(x.1, w.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n  b.1 = f32[11]{0} parameter(2)\n  broadcast_in_dim.1 = f32[1,11]{1,0} reshape(b.1)\n  add.5 = f32[1,11]{1,0} broadcast(broadcast_in_dim.1), dimensions={0,1}\n  add.6 = f32[11]{0} reshape(add.5)\n  add.7 = f32[7,11]{1,0} broadcast(add.6), dimensions={1}\n  add.8 = f32[7,11]{1,0} add(dot_general.1, add.7)\n  neg.1 = f32[7,11]{1,0} negate(add.8)\n  exp.1 = f32[7,11]{1,0} exponential(neg.1)\n  add.9 = f32[7,11]{1,0} add(exp.1, broadcast.1)\n  ROOT div.1 = f32[7,11]{1,0} divide(broadcast.1, add.9)\n}\n\n", "input_shapes": [[7, 15], [15, 11], [11]], "output_shape": [7, 11]}
{"id": 12, "name": "composite_mtwd", "python_source": "def composite_mtwd(x):\n    \"\"\"Composite function with multiple operations.\"\"\"\n    return jnp.exp(-jnp.abs(x)) + jax.nn.relu(x) + jnp.tanh(x)", "jaxpr": "{ lambda ; a:f32[4,14]. let\n    b:f32[4,14] = abs a\n    c:f32[4,14] = neg b\n    d:f32[4,14] = exp c\n    e:f32[4,14] = custom_jvp_call[\n      name=relu\n      call_jaxpr={ lambda ; f:f32[4,14]. let\n          g:f32[4,14] = jit[\n            name=relu\n            jaxpr={ lambda ; f:f32[4,14]. let\n                g:f32[4,14] = max f 0.0:f32[]\n              in (g,) }\n          ] f\n        in (g,) }\n      jvp=jvp\n      symbolic_zeros=False\n    ] a\n    h:f32[4,14] = add d e\n    i:f32[4,14] = tanh a\n    j:f32[4,14] = add h i\n  in (j,) }", "stablehlo": "module @jit_composite_mtwd attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x14xf32>) -> (tensor<4x14xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.abs %arg0 : tensor<4x14xf32>\n    %1 = stablehlo.negate %0 : tensor<4x14xf32>\n    %2 = stablehlo.exponential %1 : tensor<4x14xf32>\n    %3 = call @relu(%arg0) : (tensor<4x14xf32>) -> tensor<4x14xf32>\n    %4 = stablehlo.add %2, %3 : tensor<4x14xf32>\n    %5 = stablehlo.tanh %arg0 : tensor<4x14xf32>\n    %6 = stablehlo.add %4, %5 : tensor<4x14xf32>\n    return %6 : tensor<4x14xf32>\n  }\n  func.func private @relu(%arg0: tensor<4x14xf32>) -> tensor<4x14xf32> {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<4x14xf32>\n    %1 = stablehlo.maximum %arg0, %0 : tensor<4x14xf32>\n    return %1 : tensor<4x14xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_composite_mtwd, entry_computation_layout={(f32[4,14]{1,0})->f32[4,14]{1,0}}\n\nrelu.1 {\n  Arg_0.1 = f32[4,14]{1,0} parameter(0)\n  constant.1 = f32[] constant(0)\n  max.2 = f32[4,14]{1,0} broadcast(constant.1), dimensions={}\n  ROOT max.3 = f32[4,14]{1,0} maximum(Arg_0.1, max.2)\n}\n\nENTRY main.2 {\n  x.1 = f32[4,14]{1,0} parameter(0)\n  abs.1 = f32[4,14]{1,0} abs(x.1)\n  neg.1 = f32[4,14]{1,0} negate(abs.1)\n  exp.1 = f32[4,14]{1,0} exponential(neg.1)\n  jit_relu_.1 = f32[4,14]{1,0} call(x.1), to_apply=relu.1\n  add.2 = f32[4,14]{1,0} add(exp.1, jit_relu_.1)\n  tanh.1 = f32[4,14]{1,0} tanh(x.1)\n  ROOT add.3 = f32[4,14]{1,0} add(add.2, tanh.1)\n}\n\n", "input_shapes": [[4, 14]], "output_shape": [4, 14]}
{"id": 13, "name": "matmul_pmgo", "python_source": "def matmul_pmgo(a, b):\n    \"\"\"Matrix multiplication of shapes (2,12) @ (12,13).\"\"\"\n    return jnp.dot(a, b)", "jaxpr": "{ lambda ; a:f32[2,12] b:f32[12,13]. let\n    c:f32[2,13] = dot_general[\n      dimension_numbers=(([1], [0]), ([], []))\n      preferred_element_type=float32\n    ] a b\n  in (c,) }", "stablehlo": "module @jit_matmul_pmgo attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x12xf32>, %arg1: tensor<12x13xf32>) -> (tensor<2x13xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.dot_general %arg0, %arg1, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x12xf32>, tensor<12x13xf32>) -> tensor<2x13xf32>\n    return %0 : tensor<2x13xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_matmul_pmgo, entry_computation_layout={(f32[2,12]{1,0}, f32[12,13]{1,0})->f32[2,13]{1,0}}\n\nENTRY main.1 {\n  a.1 = f32[2,12]{1,0} parameter(0)\n  b.1 = f32[12,13]{1,0} parameter(1)\n  ROOT dot_general.1 = f32[2,13]{1,0} dot(a.1, b.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n}\n\n", "input_shapes": [[2, 12], [12, 13]], "output_shape": [2, 13]}
{"id": 14, "name": "softplus_xtti", "python_source": "def softplus_xtti(x):\n    \"\"\"Softplus operation.\"\"\"\n    return jax.nn.softplus(x)", "jaxpr": "{ lambda ; a:f32[8,4]. let\n    b:f32[8,4] = jit[\n      name=softplus\n      jaxpr={ lambda ; a:f32[8,4]. let\n          b:f32[8,4] = custom_jvp_call[\n            name=logaddexp\n            call_jaxpr={ lambda ; c:f32[8,4] d:f32[]. let\n                e:f32[8,4] = max c d\n                f:f32[8,4] = sub c d\n                g:bool[8,4] = ne f f\n                h:f32[8,4] = add c d\n                i:f32[8,4] = abs f\n                j:f32[8,4] = neg i\n                k:f32[8,4] = exp j\n                l:f32[8,4] = log1p k\n                m:f32[8,4] = add e l\n                n:f32[8,4] = select_n g m h\n              in (n,) }\n            jvp=_logaddexp_jvp\n            symbolic_zeros=False\n          ] a 0.0:f32[]\n        in (b,) }\n    ] a\n  in (b,) }", "stablehlo": "module @jit_softplus_xtti attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x4xf32>) -> (tensor<8x4xf32> {jax.result_info = \"result\"}) {\n    %0 = call @softplus(%arg0) : (tensor<8x4xf32>) -> tensor<8x4xf32>\n    return %0 : tensor<8x4xf32>\n  }\n  func.func private @softplus(%arg0: tensor<8x4xf32>) -> tensor<8x4xf32> {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<8x4xf32>\n    %1 = stablehlo.maximum %arg0, %0 : tensor<8x4xf32>\n    %2 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<8x4xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<8x4xf32>\n    %4 = stablehlo.compare  NE, %3, %3,  FLOAT : (tensor<8x4xf32>, tensor<8x4xf32>) -> tensor<8x4xi1>\n    %5 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<8x4xf32>\n    %6 = stablehlo.add %arg0, %5 : tensor<8x4xf32>\n    %7 = stablehlo.abs %3 : tensor<8x4xf32>\n    %8 = stablehlo.negate %7 : tensor<8x4xf32>\n    %9 = stablehlo.exponential %8 : tensor<8x4xf32>\n    %10 = stablehlo.log_plus_one %9 : tensor<8x4xf32>\n    %11 = stablehlo.add %1, %10 : tensor<8x4xf32>\n    %12 = stablehlo.select %4, %6, %11 : tensor<8x4xi1>, tensor<8x4xf32>\n    return %12 : tensor<8x4xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_softplus_xtti, entry_computation_layout={(f32[8,4]{1,0})->f32[8,4]{1,0}}\n\nsoftplus.1 {\n  Arg_0.1 = f32[8,4]{1,0} parameter(0)\n  ne.1 = pred[8,4]{1,0} compare(Arg_0.1, Arg_0.1), direction=NE\n  constant.1 = f32[] constant(0)\n  max.2 = f32[8,4]{1,0} broadcast(constant.1), dimensions={}\n  max.3 = f32[8,4]{1,0} maximum(Arg_0.1, max.2)\n  abs.1 = f32[8,4]{1,0} abs(Arg_0.1)\n  neg.1 = f32[8,4]{1,0} negate(abs.1)\n  exp.1 = f32[8,4]{1,0} exponential(neg.1)\n  log1p.1 = f32[8,4]{1,0} log-plus-one(exp.1)\n  add.1 = f32[8,4]{1,0} add(max.3, log1p.1)\n  ROOT select_n.1 = f32[8,4]{1,0} select(ne.1, Arg_0.1, add.1)\n}\n\nENTRY main.2 {\n  x.1 = f32[8,4]{1,0} parameter(0)\n  ROOT jit_softplus_.1 = f32[8,4]{1,0} call(x.1), to_apply=softplus.1\n}\n\n", "input_shapes": [[8, 4]], "output_shape": [8, 4]}
{"id": 15, "name": "matmul_assz", "python_source": "def matmul_assz(a, b):\n    \"\"\"Matrix multiplication of shapes (14,4) @ (4,10).\"\"\"\n    return jnp.dot(a, b)", "jaxpr": "{ lambda ; a:f32[14,4] b:f32[4,10]. let\n    c:f32[14,10] = dot_general[\n      dimension_numbers=(([1], [0]), ([], []))\n      preferred_element_type=float32\n    ] a b\n  in (c,) }", "stablehlo": "module @jit_matmul_assz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<14x4xf32>, %arg1: tensor<4x10xf32>) -> (tensor<14x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.dot_general %arg0, %arg1, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<14x4xf32>, tensor<4x10xf32>) -> tensor<14x10xf32>\n    return %0 : tensor<14x10xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_matmul_assz, entry_computation_layout={(f32[14,4]{1,0}, f32[4,10]{1,0})->f32[14,10]{1,0}}\n\nENTRY main.1 {\n  a.1 = f32[14,4]{1,0} parameter(0)\n  b.1 = f32[4,10]{1,0} parameter(1)\n  ROOT dot_general.1 = f32[14,10]{1,0} dot(a.1, b.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n}\n\n", "input_shapes": [[14, 4], [4, 10]], "output_shape": [14, 10]}
{"id": 16, "name": "softplus_hqpd", "python_source": "def softplus_hqpd(x):\n    \"\"\"Softplus operation.\"\"\"\n    return jax.nn.softplus(x)", "jaxpr": "{ lambda ; a:f32[10]. let\n    b:f32[10] = jit[\n      name=softplus\n      jaxpr={ lambda ; a:f32[10]. let\n          b:f32[10] = custom_jvp_call[\n            name=logaddexp\n            call_jaxpr={ lambda ; c:f32[10] d:f32[]. let\n                e:f32[10] = max c d\n                f:f32[10] = sub c d\n                g:bool[10] = ne f f\n                h:f32[10] = add c d\n                i:f32[10] = abs f\n                j:f32[10] = neg i\n                k:f32[10] = exp j\n                l:f32[10] = log1p k\n                m:f32[10] = add e l\n                n:f32[10] = select_n g m h\n              in (n,) }\n            jvp=_logaddexp_jvp\n            symbolic_zeros=False\n          ] a 0.0:f32[]\n        in (b,) }\n    ] a\n  in (b,) }", "stablehlo": "module @jit_softplus_hqpd attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = call @softplus(%arg0) : (tensor<10xf32>) -> tensor<10xf32>\n    return %0 : tensor<10xf32>\n  }\n  func.func private @softplus(%arg0: tensor<10xf32>) -> tensor<10xf32> {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %1 = stablehlo.maximum %arg0, %0 : tensor<10xf32>\n    %2 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<10xf32>\n    %4 = stablehlo.compare  NE, %3, %3,  FLOAT : (tensor<10xf32>, tensor<10xf32>) -> tensor<10xi1>\n    %5 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %6 = stablehlo.add %arg0, %5 : tensor<10xf32>\n    %7 = stablehlo.abs %3 : tensor<10xf32>\n    %8 = stablehlo.negate %7 : tensor<10xf32>\n    %9 = stablehlo.exponential %8 : tensor<10xf32>\n    %10 = stablehlo.log_plus_one %9 : tensor<10xf32>\n    %11 = stablehlo.add %1, %10 : tensor<10xf32>\n    %12 = stablehlo.select %4, %6, %11 : tensor<10xi1>, tensor<10xf32>\n    return %12 : tensor<10xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_softplus_hqpd, entry_computation_layout={(f32[10]{0})->f32[10]{0}}\n\nsoftplus.1 {\n  Arg_0.1 = f32[10]{0} parameter(0)\n  ne.1 = pred[10]{0} compare(Arg_0.1, Arg_0.1), direction=NE\n  constant.1 = f32[] constant(0)\n  broadcast.1 = f32[10]{0} broadcast(constant.1), dimensions={}\n  max.1 = f32[10]{0} maximum(Arg_0.1, broadcast.1)\n  abs.1 = f32[10]{0} abs(Arg_0.1)\n  neg.1 = f32[10]{0} negate(abs.1)\n  exp.1 = f32[10]{0} exponential(neg.1)\n  log1p.1 = f32[10]{0} log-plus-one(exp.1)\n  add.1 = f32[10]{0} add(max.1, log1p.1)\n  ROOT select_n.1 = f32[10]{0} select(ne.1, Arg_0.1, add.1)\n}\n\nENTRY main.2 {\n  x.1 = f32[10]{0} parameter(0)\n  ROOT jit_softplus_.1 = f32[10]{0} call(x.1), to_apply=softplus.1\n}\n\n", "input_shapes": [[10]], "output_shape": [10]}
{"id": 17, "name": "vmap_xxpm", "python_source": "def vmap_xxpm(a_batch, b_batch):\n    \"\"\"Batched operation using vmap.\"\"\"\n    def inner(a, b):\n        return jnp.sum(a * b)\n    return jax.vmap(inner)(a_batch, b_batch)", "jaxpr": "{ lambda ; a:f32[2,9] b:f32[2,9]. let\n    c:f32[2,9] = mul a b\n    d:f32[2] = reduce_sum[axes=(np.int64(1),) out_sharding=None] c\n  in (d,) }", "stablehlo": "module @jit_vmap_xxpm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x9xf32>, %arg1: tensor<2x9xf32>) -> (tensor<2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<2x9xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<2x9xf32>, tensor<f32>) -> tensor<2xf32>\n    return %1 : tensor<2xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_vmap_xxpm, entry_computation_layout={(f32[2,9]{1,0}, f32[2,9]{1,0})->f32[2]{0}}\n\nregion_0.1 {\n  reduce_sum.3 = f32[] parameter(0)\n  reduce_sum.4 = f32[] parameter(1)\n  ROOT reduce_sum.5 = f32[] add(reduce_sum.3, reduce_sum.4)\n}\n\nENTRY main.2 {\n  a_batch.1 = f32[2,9]{1,0} parameter(0)\n  b_batch.1 = f32[2,9]{1,0} parameter(1)\n  mul.1 = f32[2,9]{1,0} multiply(a_batch.1, b_batch.1)\n  constant.1 = f32[] constant(0)\n  ROOT reduce_sum.7 = f32[2]{0} reduce(mul.1, constant.1), dimensions={1}, to_apply=region_0.1\n}\n\n", "input_shapes": [[2, 9], [2, 9]], "output_shape": [2]}
{"id": 18, "name": "add_goct", "python_source": "def add_goct(x, y):\n    \"\"\"Addition operation.\"\"\"\n    return x + y", "jaxpr": "{ lambda ; a:f32[14] b:f32[14]. let c:f32[14] = add a b in (c,) }", "stablehlo": "module @jit_add_goct attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<14xf32>, %arg1: tensor<14xf32>) -> (tensor<14xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<14xf32>\n    return %0 : tensor<14xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_add_goct, entry_computation_layout={(f32[14]{0}, f32[14]{0})->f32[14]{0}}\n\nENTRY main.1 {\n  x.1 = f32[14]{0} parameter(0)\n  y.1 = f32[14]{0} parameter(1)\n  ROOT add.1 = f32[14]{0} add(x.1, y.1)\n}\n\n", "input_shapes": [[14], [14]], "output_shape": [14]}
{"id": 19, "name": "composite_tdmo", "python_source": "def composite_tdmo(x):\n    \"\"\"Composite function with multiple operations.\"\"\"\n    return jnp.tanh(x) + jnp.exp(-jnp.abs(x)) + jnp.sin(x)", "jaxpr": "{ lambda ; a:f32[6,10]. let\n    b:f32[6,10] = tanh a\n    c:f32[6,10] = abs a\n    d:f32[6,10] = neg c\n    e:f32[6,10] = exp d\n    f:f32[6,10] = add b e\n    g:f32[6,10] = sin a\n    h:f32[6,10] = add f g\n  in (h,) }", "stablehlo": "module @jit_composite_tdmo attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x10xf32>) -> (tensor<6x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.tanh %arg0 : tensor<6x10xf32>\n    %1 = stablehlo.abs %arg0 : tensor<6x10xf32>\n    %2 = stablehlo.negate %1 : tensor<6x10xf32>\n    %3 = stablehlo.exponential %2 : tensor<6x10xf32>\n    %4 = stablehlo.add %0, %3 : tensor<6x10xf32>\n    %5 = stablehlo.sine %arg0 : tensor<6x10xf32>\n    %6 = stablehlo.add %4, %5 : tensor<6x10xf32>\n    return %6 : tensor<6x10xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_composite_tdmo, entry_computation_layout={(f32[6,10]{1,0})->f32[6,10]{1,0}}\n\nENTRY main.1 {\n  x.1 = f32[6,10]{1,0} parameter(0)\n  tanh.1 = f32[6,10]{1,0} tanh(x.1)\n  abs.1 = f32[6,10]{1,0} abs(x.1)\n  neg.1 = f32[6,10]{1,0} negate(abs.1)\n  exp.1 = f32[6,10]{1,0} exponential(neg.1)\n  add.2 = f32[6,10]{1,0} add(tanh.1, exp.1)\n  sin.1 = f32[6,10]{1,0} sine(x.1)\n  ROOT add.3 = f32[6,10]{1,0} add(add.2, sin.1)\n}\n\n", "input_shapes": [[6, 10]], "output_shape": [6, 10]}
{"id": 20, "name": "min_zqln", "python_source": "def min_zqln(x):\n    \"\"\"Min reduction along axis 0.\"\"\"\n    return jnp.min(x, axis=0)", "jaxpr": "{ lambda ; a:f32[5,5]. let b:f32[5] = reduce_min[axes=(0,)] a in (b,) }", "stablehlo": "module @jit_min_zqln attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x5xf32>) -> (tensor<5xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0x7F800000> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.minimum across dimensions = [0] : (tensor<5x5xf32>, tensor<f32>) -> tensor<5xf32>\n    return %0 : tensor<5xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_min_zqln, entry_computation_layout={(f32[5,5]{1,0})->f32[5]{0}}\n\nregion_0.1 {\n  reduce_min.3 = f32[] parameter(0)\n  reduce_min.4 = f32[] parameter(1)\n  ROOT reduce_min.5 = f32[] minimum(reduce_min.3, reduce_min.4)\n}\n\nENTRY main.2 {\n  x.1 = f32[5,5]{1,0} parameter(0)\n  constant.1 = f32[] constant(inf)\n  ROOT reduce_min.7 = f32[5]{0} reduce(x.1, constant.1), dimensions={0}, to_apply=region_0.1\n}\n\n", "input_shapes": [[5, 5]], "output_shape": [5]}
{"id": 21, "name": "sum_pffb", "python_source": "def sum_pffb(x):\n    \"\"\"Sum reduction along axis 0.\"\"\"\n    return jnp.sum(x, axis=0)", "jaxpr": "{ lambda ; a:f32[2,5,3,16]. let\n    b:f32[5,3,16] = reduce_sum[axes=(0,) out_sharding=None] a\n  in (b,) }", "stablehlo": "module @jit_sum_pffb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x5x3x16xf32>) -> (tensor<5x3x16xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<2x5x3x16xf32>, tensor<f32>) -> tensor<5x3x16xf32>\n    return %0 : tensor<5x3x16xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_sum_pffb, entry_computation_layout={(f32[2,5,3,16]{3,2,1,0})->f32[5,3,16]{2,1,0}}\n\nregion_0.1 {\n  reduce_sum.3 = f32[] parameter(0)\n  reduce_sum.4 = f32[] parameter(1)\n  ROOT reduce_sum.5 = f32[] add(reduce_sum.3, reduce_sum.4)\n}\n\nENTRY main.2 {\n  x.1 = f32[2,5,3,16]{3,2,1,0} parameter(0)\n  constant.1 = f32[] constant(0)\n  ROOT reduce_sum.7 = f32[5,3,16]{2,1,0} reduce(x.1, constant.1), dimensions={0}, to_apply=region_0.1\n}\n\n", "input_shapes": [[2, 5, 3, 16]], "output_shape": [5, 3, 16]}
{"id": 22, "name": "vmap_inhm", "python_source": "def vmap_inhm(a_batch, b_batch):\n    \"\"\"Batched operation using vmap.\"\"\"\n    def inner(a, b):\n        return jnp.maximum(a, b)\n    return jax.vmap(inner)(a_batch, b_batch)", "jaxpr": "{ lambda ; a:f32[6,6] b:f32[6,6]. let c:f32[6,6] = max a b in (c,) }", "stablehlo": "module @jit_vmap_inhm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x6xf32>, %arg1: tensor<6x6xf32>) -> (tensor<6x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.maximum %arg0, %arg1 : tensor<6x6xf32>\n    return %0 : tensor<6x6xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_vmap_inhm, entry_computation_layout={(f32[6,6]{1,0}, f32[6,6]{1,0})->f32[6,6]{1,0}}\n\nENTRY main.1 {\n  a_batch.1 = f32[6,6]{1,0} parameter(0)\n  b_batch.1 = f32[6,6]{1,0} parameter(1)\n  ROOT max.1 = f32[6,6]{1,0} maximum(a_batch.1, b_batch.1)\n}\n\n", "input_shapes": [[6, 6], [6, 6]], "output_shape": [6, 6]}
{"id": 23, "name": "composite_ogmk", "python_source": "def composite_ogmk(x):\n    \"\"\"Composite function with multiple operations.\"\"\"\n    return jnp.sin(x) + x ** 2 + jnp.tanh(x)", "jaxpr": "{ lambda ; a:f32[3,3]. let\n    b:f32[3,3] = sin a\n    c:f32[3,3] = integer_pow[y=2] a\n    d:f32[3,3] = add b c\n    e:f32[3,3] = tanh a\n    f:f32[3,3] = add d e\n  in (f,) }", "stablehlo": "module @jit_composite_ogmk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x3xf32>) -> (tensor<3x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<3x3xf32>\n    %1 = stablehlo.multiply %arg0, %arg0 : tensor<3x3xf32>\n    %2 = stablehlo.add %0, %1 : tensor<3x3xf32>\n    %3 = stablehlo.tanh %arg0 : tensor<3x3xf32>\n    %4 = stablehlo.add %2, %3 : tensor<3x3xf32>\n    return %4 : tensor<3x3xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_composite_ogmk, entry_computation_layout={(f32[3,3]{1,0})->f32[3,3]{1,0}}\n\nENTRY main.1 {\n  x.1 = f32[3,3]{1,0} parameter(0)\n  sin.1 = f32[3,3]{1,0} sine(x.1)\n  integer_pow.1 = f32[3,3]{1,0} multiply(x.1, x.1)\n  add.2 = f32[3,3]{1,0} add(sin.1, integer_pow.1)\n  tanh.1 = f32[3,3]{1,0} tanh(x.1)\n  ROOT add.3 = f32[3,3]{1,0} add(add.2, tanh.1)\n}\n\n", "input_shapes": [[3, 3]], "output_shape": [3, 3]}
{"id": 24, "name": "matmul_wbqq", "python_source": "def matmul_wbqq(a, b):\n    \"\"\"Matrix multiplication of shapes (2,8) @ (8,13).\"\"\"\n    return jnp.dot(a, b)", "jaxpr": "{ lambda ; a:f32[2,8] b:f32[8,13]. let\n    c:f32[2,13] = dot_general[\n      dimension_numbers=(([1], [0]), ([], []))\n      preferred_element_type=float32\n    ] a b\n  in (c,) }", "stablehlo": "module @jit_matmul_wbqq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x8xf32>, %arg1: tensor<8x13xf32>) -> (tensor<2x13xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.dot_general %arg0, %arg1, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x8xf32>, tensor<8x13xf32>) -> tensor<2x13xf32>\n    return %0 : tensor<2x13xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_matmul_wbqq, entry_computation_layout={(f32[2,8]{1,0}, f32[8,13]{1,0})->f32[2,13]{1,0}}\n\nENTRY main.1 {\n  a.1 = f32[2,8]{1,0} parameter(0)\n  b.1 = f32[8,13]{1,0} parameter(1)\n  ROOT dot_general.1 = f32[2,13]{1,0} dot(a.1, b.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n}\n\n", "input_shapes": [[2, 8], [8, 13]], "output_shape": [2, 13]}
{"id": 25, "name": "sum_gelk", "python_source": "def sum_gelk(x):\n    \"\"\"Sum reduction along axis 0.\"\"\"\n    return jnp.sum(x, axis=0)", "jaxpr": "{ lambda ; a:f32[9,5,15]. let\n    b:f32[5,15] = reduce_sum[axes=(0,) out_sharding=None] a\n  in (b,) }", "stablehlo": "module @jit_sum_gelk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x5x15xf32>) -> (tensor<5x15xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<9x5x15xf32>, tensor<f32>) -> tensor<5x15xf32>\n    return %0 : tensor<5x15xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_sum_gelk, entry_computation_layout={(f32[9,5,15]{2,1,0})->f32[5,15]{1,0}}\n\nregion_0.1 {\n  reduce_sum.3 = f32[] parameter(0)\n  reduce_sum.4 = f32[] parameter(1)\n  ROOT reduce_sum.5 = f32[] add(reduce_sum.3, reduce_sum.4)\n}\n\nENTRY main.2 {\n  x.1 = f32[9,5,15]{2,1,0} parameter(0)\n  constant.1 = f32[] constant(0)\n  ROOT reduce_sum.7 = f32[5,15]{1,0} reduce(x.1, constant.1), dimensions={0}, to_apply=region_0.1\n}\n\n", "input_shapes": [[9, 5, 15]], "output_shape": [5, 15]}
{"id": 26, "name": "matmul_vwcq", "python_source": "def matmul_vwcq(a, b):\n    \"\"\"Matrix multiplication of shapes (10,15) @ (15,2).\"\"\"\n    return jnp.dot(a, b)", "jaxpr": "{ lambda ; a:f32[10,15] b:f32[15,2]. let\n    c:f32[10,2] = dot_general[\n      dimension_numbers=(([1], [0]), ([], []))\n      preferred_element_type=float32\n    ] a b\n  in (c,) }", "stablehlo": "module @jit_matmul_vwcq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x15xf32>, %arg1: tensor<15x2xf32>) -> (tensor<10x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.dot_general %arg0, %arg1, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<10x15xf32>, tensor<15x2xf32>) -> tensor<10x2xf32>\n    return %0 : tensor<10x2xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_matmul_vwcq, entry_computation_layout={(f32[10,15]{1,0}, f32[15,2]{1,0})->f32[10,2]{1,0}}\n\nENTRY main.1 {\n  a.1 = f32[10,15]{1,0} parameter(0)\n  b.1 = f32[15,2]{1,0} parameter(1)\n  ROOT dot_general.1 = f32[10,2]{1,0} dot(a.1, b.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n}\n\n", "input_shapes": [[10, 15], [15, 2]], "output_shape": [10, 2]}
{"id": 27, "name": "log_emfk", "python_source": "def log_emfk(x):\n    \"\"\"Log (with safety) operation.\"\"\"\n    return jnp.log(jnp.abs(x) + 1e-6)", "jaxpr": "{ lambda ; a:f32[4]. let\n    b:f32[4] = abs a\n    c:f32[4] = add b 9.999999974752427e-07:f32[]\n    d:f32[4] = log c\n  in (d,) }", "stablehlo": "module @jit_log_emfk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.abs %arg0 : tensor<4xf32>\n    %cst = stablehlo.constant dense<9.99999997E-7> : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %2 = stablehlo.add %0, %1 : tensor<4xf32>\n    %3 = stablehlo.log %2 : tensor<4xf32>\n    return %3 : tensor<4xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_log_emfk, entry_computation_layout={(f32[4]{0})->f32[4]{0}}\n\nENTRY main.1 {\n  x.1 = f32[4]{0} parameter(0)\n  abs.1 = f32[4]{0} abs(x.1)\n  constant.1 = f32[] constant(1e-06)\n  broadcast.1 = f32[4]{0} broadcast(constant.1), dimensions={}\n  add.1 = f32[4]{0} add(abs.1, broadcast.1)\n  ROOT log.1 = f32[4]{0} log(add.1)\n}\n\n", "input_shapes": [[4]], "output_shape": [4]}
{"id": 28, "name": "matmul_akyu", "python_source": "def matmul_akyu(a, b):\n    \"\"\"Matrix multiplication of shapes (6,8) @ (8,13).\"\"\"\n    return jnp.dot(a, b)", "jaxpr": "{ lambda ; a:f32[6,8] b:f32[8,13]. let\n    c:f32[6,13] = dot_general[\n      dimension_numbers=(([1], [0]), ([], []))\n      preferred_element_type=float32\n    ] a b\n  in (c,) }", "stablehlo": "module @jit_matmul_akyu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x8xf32>, %arg1: tensor<8x13xf32>) -> (tensor<6x13xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.dot_general %arg0, %arg1, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<6x8xf32>, tensor<8x13xf32>) -> tensor<6x13xf32>\n    return %0 : tensor<6x13xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_matmul_akyu, entry_computation_layout={(f32[6,8]{1,0}, f32[8,13]{1,0})->f32[6,13]{1,0}}\n\nENTRY main.1 {\n  a.1 = f32[6,8]{1,0} parameter(0)\n  b.1 = f32[8,13]{1,0} parameter(1)\n  ROOT dot_general.1 = f32[6,13]{1,0} dot(a.1, b.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n}\n\n", "input_shapes": [[6, 8], [8, 13]], "output_shape": [6, 13]}
{"id": 29, "name": "nn_layer_zose", "python_source": "def nn_layer_zose(x, w, b):\n    \"\"\"Neural network layer with relu activation.\"\"\"\n    return jax.nn.relu(jnp.dot(x, w) + b)", "jaxpr": "{ lambda ; a:f32[5,10] b:f32[10,5] c:f32[5]. let\n    d:f32[5,5] = dot_general[\n      dimension_numbers=(([1], [0]), ([], []))\n      preferred_element_type=float32\n    ] a b\n    e:f32[1,5] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 5)\n      sharding=None\n    ] c\n    f:f32[5,5] = add d e\n    g:f32[5,5] = custom_jvp_call[\n      name=relu\n      call_jaxpr={ lambda ; h:f32[5,5]. let\n          i:f32[5,5] = jit[\n            name=relu\n            jaxpr={ lambda ; h:f32[5,5]. let\n                i:f32[5,5] = max h 0.0:f32[]\n              in (i,) }\n          ] h\n        in (i,) }\n      jvp=jvp\n      symbolic_zeros=False\n    ] f\n  in (g,) }", "stablehlo": "module @jit_nn_layer_zose attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x10xf32>, %arg1: tensor<10x5xf32>, %arg2: tensor<5xf32>) -> (tensor<5x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.dot_general %arg0, %arg1, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<5x10xf32>, tensor<10x5xf32>) -> tensor<5x5xf32>\n    %1 = stablehlo.broadcast_in_dim %arg2, dims = [1] : (tensor<5xf32>) -> tensor<1x5xf32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x5xf32>) -> tensor<5x5xf32>\n    %3 = stablehlo.add %0, %2 : tensor<5x5xf32>\n    %4 = call @relu(%3) : (tensor<5x5xf32>) -> tensor<5x5xf32>\n    return %4 : tensor<5x5xf32>\n  }\n  func.func private @relu(%arg0: tensor<5x5xf32>) -> tensor<5x5xf32> {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<5x5xf32>\n    %1 = stablehlo.maximum %arg0, %0 : tensor<5x5xf32>\n    return %1 : tensor<5x5xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_nn_layer_zose, entry_computation_layout={(f32[5,10]{1,0}, f32[10,5]{1,0}, f32[5]{0})->f32[5,5]{1,0}}\n\nrelu.1 {\n  Arg_0.1 = f32[5,5]{1,0} parameter(0)\n  constant.1 = f32[] constant(0)\n  broadcast.1 = f32[5,5]{1,0} broadcast(constant.1), dimensions={}\n  ROOT max.1 = f32[5,5]{1,0} maximum(Arg_0.1, broadcast.1)\n}\n\nENTRY main.2 {\n  x.1 = f32[5,10]{1,0} parameter(0)\n  w.1 = f32[10,5]{1,0} parameter(1)\n  dot_general.1 = f32[5,5]{1,0} dot(x.1, w.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n  b.1 = f32[5]{0} parameter(2)\n  broadcast_in_dim.1 = f32[1,5]{1,0} reshape(b.1)\n  add.4 = f32[1,5]{1,0} broadcast(broadcast_in_dim.1), dimensions={0,1}\n  add.5 = f32[5]{0} reshape(add.4)\n  add.6 = f32[5,5]{1,0} broadcast(add.5), dimensions={1}\n  add.7 = f32[5,5]{1,0} add(dot_general.1, add.6)\n  ROOT jit_relu_.1 = f32[5,5]{1,0} call(add.7), to_apply=relu.1\n}\n\n", "input_shapes": [[5, 10], [10, 5], [5]], "output_shape": [5, 5]}
{"id": 30, "name": "nn_layer_ibmx", "python_source": "def nn_layer_ibmx(x, w, b):\n    \"\"\"Neural network layer with relu activation.\"\"\"\n    return jax.nn.relu(jnp.dot(x, w) + b)", "jaxpr": "{ lambda ; a:f32[3,5] b:f32[5,20] c:f32[20]. let\n    d:f32[3,20] = dot_general[\n      dimension_numbers=(([1], [0]), ([], []))\n      preferred_element_type=float32\n    ] a b\n    e:f32[1,20] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 20)\n      sharding=None\n    ] c\n    f:f32[3,20] = add d e\n    g:f32[3,20] = custom_jvp_call[\n      name=relu\n      call_jaxpr={ lambda ; h:f32[3,20]. let\n          i:f32[3,20] = jit[\n            name=relu\n            jaxpr={ lambda ; h:f32[3,20]. let\n                i:f32[3,20] = max h 0.0:f32[]\n              in (i,) }\n          ] h\n        in (i,) }\n      jvp=jvp\n      symbolic_zeros=False\n    ] f\n  in (g,) }", "stablehlo": "module @jit_nn_layer_ibmx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x5xf32>, %arg1: tensor<5x20xf32>, %arg2: tensor<20xf32>) -> (tensor<3x20xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.dot_general %arg0, %arg1, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<3x5xf32>, tensor<5x20xf32>) -> tensor<3x20xf32>\n    %1 = stablehlo.broadcast_in_dim %arg2, dims = [1] : (tensor<20xf32>) -> tensor<1x20xf32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x20xf32>) -> tensor<3x20xf32>\n    %3 = stablehlo.add %0, %2 : tensor<3x20xf32>\n    %4 = call @relu(%3) : (tensor<3x20xf32>) -> tensor<3x20xf32>\n    return %4 : tensor<3x20xf32>\n  }\n  func.func private @relu(%arg0: tensor<3x20xf32>) -> tensor<3x20xf32> {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<3x20xf32>\n    %1 = stablehlo.maximum %arg0, %0 : tensor<3x20xf32>\n    return %1 : tensor<3x20xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_nn_layer_ibmx, entry_computation_layout={(f32[3,5]{1,0}, f32[5,20]{1,0}, f32[20]{0})->f32[3,20]{1,0}}\n\nrelu.1 {\n  Arg_0.1 = f32[3,20]{1,0} parameter(0)\n  constant.1 = f32[] constant(0)\n  max.2 = f32[3,20]{1,0} broadcast(constant.1), dimensions={}\n  ROOT max.3 = f32[3,20]{1,0} maximum(Arg_0.1, max.2)\n}\n\nENTRY main.2 {\n  x.1 = f32[3,5]{1,0} parameter(0)\n  w.1 = f32[5,20]{1,0} parameter(1)\n  dot_general.1 = f32[3,20]{1,0} dot(x.1, w.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n  b.1 = f32[20]{0} parameter(2)\n  broadcast_in_dim.1 = f32[1,20]{1,0} reshape(b.1)\n  add.4 = f32[1,20]{1,0} broadcast(broadcast_in_dim.1), dimensions={0,1}\n  add.5 = f32[20]{0} reshape(add.4)\n  add.6 = f32[3,20]{1,0} broadcast(add.5), dimensions={1}\n  add.7 = f32[3,20]{1,0} add(dot_general.1, add.6)\n  ROOT jit_relu_.1 = f32[3,20]{1,0} call(add.7), to_apply=relu.1\n}\n\n", "input_shapes": [[3, 5], [5, 20], [20]], "output_shape": [3, 20]}
{"id": 31, "name": "vmap_eprg", "python_source": "def vmap_eprg(a_batch, b_batch):\n    \"\"\"Batched operation using vmap.\"\"\"\n    def inner(a, b):\n        return jnp.dot(a, b)\n    return jax.vmap(inner)(a_batch, b_batch)", "jaxpr": "{ lambda ; a:f32[2,13] b:f32[2,13]. let\n    c:f32[2] = dot_general[\n      dimension_numbers=(([1], [1]), ([0], [0]))\n      preferred_element_type=float32\n    ] a b\n  in (c,) }", "stablehlo": "module @jit_vmap_eprg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x13xf32>, %arg1: tensor<2x13xf32>) -> (tensor<2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.dot_general %arg0, %arg1, batching_dims = [0] x [0], contracting_dims = [1] x [1], precision = [DEFAULT, DEFAULT] : (tensor<2x13xf32>, tensor<2x13xf32>) -> tensor<2xf32>\n    return %0 : tensor<2xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_vmap_eprg, entry_computation_layout={(f32[2,13]{1,0}, f32[2,13]{1,0})->f32[2]{0}}\n\nENTRY main.1 {\n  a_batch.1 = f32[2,13]{1,0} parameter(0)\n  b_batch.1 = f32[2,13]{1,0} parameter(1)\n  ROOT dot_general.1 = f32[2]{0} dot(a_batch.1, b_batch.1), lhs_batch_dims={0}, lhs_contracting_dims={1}, rhs_batch_dims={0}, rhs_contracting_dims={1}\n}\n\n", "input_shapes": [[2, 13], [2, 13]], "output_shape": [2]}
{"id": 32, "name": "composite_pqkp", "python_source": "def composite_pqkp(x):\n    \"\"\"Composite function with multiple operations.\"\"\"\n    return jnp.tanh(x) + jnp.sin(x) + jax.nn.relu(x)", "jaxpr": "{ lambda ; a:f32[10,7]. let\n    b:f32[10,7] = tanh a\n    c:f32[10,7] = sin a\n    d:f32[10,7] = add b c\n    e:f32[10,7] = custom_jvp_call[\n      name=relu\n      call_jaxpr={ lambda ; f:f32[10,7]. let\n          g:f32[10,7] = jit[\n            name=relu\n            jaxpr={ lambda ; f:f32[10,7]. let\n                g:f32[10,7] = max f 0.0:f32[]\n              in (g,) }\n          ] f\n        in (g,) }\n      jvp=jvp\n      symbolic_zeros=False\n    ] a\n    h:f32[10,7] = add d e\n  in (h,) }", "stablehlo": "module @jit_composite_pqkp attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x7xf32>) -> (tensor<10x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.tanh %arg0 : tensor<10x7xf32>\n    %1 = stablehlo.sine %arg0 : tensor<10x7xf32>\n    %2 = stablehlo.add %0, %1 : tensor<10x7xf32>\n    %3 = call @relu(%arg0) : (tensor<10x7xf32>) -> tensor<10x7xf32>\n    %4 = stablehlo.add %2, %3 : tensor<10x7xf32>\n    return %4 : tensor<10x7xf32>\n  }\n  func.func private @relu(%arg0: tensor<10x7xf32>) -> tensor<10x7xf32> {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10x7xf32>\n    %1 = stablehlo.maximum %arg0, %0 : tensor<10x7xf32>\n    return %1 : tensor<10x7xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_composite_pqkp, entry_computation_layout={(f32[10,7]{1,0})->f32[10,7]{1,0}}\n\nrelu.1 {\n  Arg_0.1 = f32[10,7]{1,0} parameter(0)\n  constant.1 = f32[] constant(0)\n  max.2 = f32[10,7]{1,0} broadcast(constant.1), dimensions={}\n  ROOT max.3 = f32[10,7]{1,0} maximum(Arg_0.1, max.2)\n}\n\nENTRY main.2 {\n  x.1 = f32[10,7]{1,0} parameter(0)\n  tanh.1 = f32[10,7]{1,0} tanh(x.1)\n  sin.1 = f32[10,7]{1,0} sine(x.1)\n  add.2 = f32[10,7]{1,0} add(tanh.1, sin.1)\n  jit_relu_.1 = f32[10,7]{1,0} call(x.1), to_apply=relu.1\n  ROOT add.3 = f32[10,7]{1,0} add(add.2, jit_relu_.1)\n}\n\n", "input_shapes": [[10, 7]], "output_shape": [10, 7]}
{"id": 33, "name": "min_dqly", "python_source": "def min_dqly(x):\n    \"\"\"Min reduction along axis 0.\"\"\"\n    return jnp.min(x, axis=0)", "jaxpr": "{ lambda ; a:f32[2,9]. let b:f32[9] = reduce_min[axes=(0,)] a in (b,) }", "stablehlo": "module @jit_min_dqly attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x9xf32>) -> (tensor<9xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0x7F800000> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.minimum across dimensions = [0] : (tensor<2x9xf32>, tensor<f32>) -> tensor<9xf32>\n    return %0 : tensor<9xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_min_dqly, entry_computation_layout={(f32[2,9]{1,0})->f32[9]{0}}\n\nregion_0.1 {\n  reduce_min.3 = f32[] parameter(0)\n  reduce_min.4 = f32[] parameter(1)\n  ROOT reduce_min.5 = f32[] minimum(reduce_min.3, reduce_min.4)\n}\n\nENTRY main.2 {\n  x.1 = f32[2,9]{1,0} parameter(0)\n  constant.1 = f32[] constant(inf)\n  ROOT reduce_min.7 = f32[9]{0} reduce(x.1, constant.1), dimensions={0}, to_apply=region_0.1\n}\n\n", "input_shapes": [[2, 9]], "output_shape": [9]}
{"id": 34, "name": "relu_fgyw", "python_source": "def relu_fgyw(x):\n    \"\"\"ReLU activation operation.\"\"\"\n    return jax.nn.relu(x)", "jaxpr": "{ lambda ; a:f32[7]. let\n    b:f32[7] = custom_jvp_call[\n      name=relu\n      call_jaxpr={ lambda ; c:f32[7]. let\n          d:f32[7] = jit[\n            name=relu\n            jaxpr={ lambda ; c:f32[7]. let d:f32[7] = max c 0.0:f32[] in (d,) }\n          ] c\n        in (d,) }\n      jvp=jvp\n      symbolic_zeros=False\n    ] a\n  in (b,) }", "stablehlo": "module @jit_relu_fgyw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>) -> (tensor<7xf32> {jax.result_info = \"result\"}) {\n    %0 = call @relu(%arg0) : (tensor<7xf32>) -> tensor<7xf32>\n    return %0 : tensor<7xf32>\n  }\n  func.func private @relu(%arg0: tensor<7xf32>) -> tensor<7xf32> {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<7xf32>\n    %1 = stablehlo.maximum %arg0, %0 : tensor<7xf32>\n    return %1 : tensor<7xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_relu_fgyw, entry_computation_layout={(f32[7]{0})->f32[7]{0}}\n\nrelu.1 {\n  Arg_0.1 = f32[7]{0} parameter(0)\n  constant.1 = f32[] constant(0)\n  broadcast.1 = f32[7]{0} broadcast(constant.1), dimensions={}\n  ROOT max.1 = f32[7]{0} maximum(Arg_0.1, broadcast.1)\n}\n\nENTRY main.2 {\n  x.1 = f32[7]{0} parameter(0)\n  ROOT jit_relu_.1 = f32[7]{0} call(x.1), to_apply=relu.1\n}\n\n", "input_shapes": [[7]], "output_shape": [7]}
{"id": 35, "name": "mean_lohz", "python_source": "def mean_lohz(x):\n    \"\"\"Mean reduction along axis 2.\"\"\"\n    return jnp.mean(x, axis=2)", "jaxpr": "{ lambda ; a:f32[10,2,12,15]. let\n    b:f32[10,2,15] = reduce_sum[axes=(2,) out_sharding=None] a\n    c:f32[10,2,15] = div b 12.0:f32[]\n  in (c,) }", "stablehlo": "module @jit_mean_lohz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x2x12x15xf32>) -> (tensor<10x2x15xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<10x2x12x15xf32>, tensor<f32>) -> tensor<10x2x15xf32>\n    %cst_0 = stablehlo.constant dense<1.200000e+01> : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<10x2x15xf32>\n    %2 = stablehlo.divide %0, %1 : tensor<10x2x15xf32>\n    return %2 : tensor<10x2x15xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_mean_lohz, entry_computation_layout={(f32[10,2,12,15]{3,2,1,0})->f32[10,2,15]{2,1,0}}\n\nregion_0.1 {\n  reduce_sum.3 = f32[] parameter(0)\n  reduce_sum.4 = f32[] parameter(1)\n  ROOT reduce_sum.5 = f32[] add(reduce_sum.3, reduce_sum.4)\n}\n\nENTRY main.2 {\n  x.1 = f32[10,2,12,15]{3,2,1,0} parameter(0)\n  constant.3 = f32[] constant(0)\n  reduce_sum.7 = f32[10,2,15]{2,1,0} reduce(x.1, constant.3), dimensions={2}, to_apply=region_0.1\n  constant.2 = f32[] constant(12)\n  div.2 = f32[10,2,15]{2,1,0} broadcast(constant.2), dimensions={}\n  ROOT div.3 = f32[10,2,15]{2,1,0} divide(reduce_sum.7, div.2)\n}\n\n", "input_shapes": [[10, 2, 12, 15]], "output_shape": [10, 2, 15]}
{"id": 36, "name": "nn_layer_cwgx", "python_source": "def nn_layer_cwgx(x, w, b):\n    \"\"\"Neural network layer with sigmoid activation.\"\"\"\n    return jax.nn.sigmoid(jnp.dot(x, w) + b)", "jaxpr": "{ lambda ; a:f32[3,12] b:f32[12,13] c:f32[13]. let\n    d:f32[3,13] = dot_general[\n      dimension_numbers=(([1], [0]), ([], []))\n      preferred_element_type=float32\n    ] a b\n    e:f32[1,13] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 13)\n      sharding=None\n    ] c\n    f:f32[3,13] = add d e\n    g:f32[3,13] = logistic f\n  in (g,) }", "stablehlo": "module @jit_nn_layer_cwgx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x12xf32>, %arg1: tensor<12x13xf32>, %arg2: tensor<13xf32>) -> (tensor<3x13xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.dot_general %arg0, %arg1, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<3x12xf32>, tensor<12x13xf32>) -> tensor<3x13xf32>\n    %1 = stablehlo.broadcast_in_dim %arg2, dims = [1] : (tensor<13xf32>) -> tensor<1x13xf32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<3x13xf32>\n    %3 = stablehlo.add %0, %2 : tensor<3x13xf32>\n    %4 = stablehlo.negate %3 : tensor<3x13xf32>\n    %5 = stablehlo.exponential %4 : tensor<3x13xf32>\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %6 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<3x13xf32>\n    %7 = stablehlo.add %6, %5 : tensor<3x13xf32>\n    %cst_0 = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %8 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<3x13xf32>\n    %9 = stablehlo.divide %8, %7 : tensor<3x13xf32>\n    return %9 : tensor<3x13xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_nn_layer_cwgx, entry_computation_layout={(f32[3,12]{1,0}, f32[12,13]{1,0}, f32[13]{0})->f32[3,13]{1,0}}\n\nENTRY main.1 {\n  constant.1 = f32[] constant(1)\n  broadcast.1 = f32[3,13]{1,0} broadcast(constant.1), dimensions={}\n  x.1 = f32[3,12]{1,0} parameter(0)\n  w.1 = f32[12,13]{1,0} parameter(1)\n  dot_general.1 = f32[3,13]{1,0} dot(x.1, w.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n  b.1 = f32[13]{0} parameter(2)\n  broadcast_in_dim.1 = f32[1,13]{1,0} reshape(b.1)\n  add.5 = f32[1,13]{1,0} broadcast(broadcast_in_dim.1), dimensions={0,1}\n  add.6 = f32[13]{0} reshape(add.5)\n  add.7 = f32[3,13]{1,0} broadcast(add.6), dimensions={1}\n  add.8 = f32[3,13]{1,0} add(dot_general.1, add.7)\n  neg.1 = f32[3,13]{1,0} negate(add.8)\n  exp.1 = f32[3,13]{1,0} exponential(neg.1)\n  add.9 = f32[3,13]{1,0} add(exp.1, broadcast.1)\n  ROOT div.1 = f32[3,13]{1,0} divide(broadcast.1, add.9)\n}\n\n", "input_shapes": [[3, 12], [12, 13], [13]], "output_shape": [3, 13]}
{"id": 37, "name": "nn_layer_irwn", "python_source": "def nn_layer_irwn(x, w, b):\n    \"\"\"Neural network layer with relu activation.\"\"\"\n    return jax.nn.relu(jnp.dot(x, w) + b)", "jaxpr": "{ lambda ; a:f32[5,32] b:f32[32,31] c:f32[31]. let\n    d:f32[5,31] = dot_general[\n      dimension_numbers=(([1], [0]), ([], []))\n      preferred_element_type=float32\n    ] a b\n    e:f32[1,31] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 31)\n      sharding=None\n    ] c\n    f:f32[5,31] = add d e\n    g:f32[5,31] = custom_jvp_call[\n      name=relu\n      call_jaxpr={ lambda ; h:f32[5,31]. let\n          i:f32[5,31] = jit[\n            name=relu\n            jaxpr={ lambda ; h:f32[5,31]. let\n                i:f32[5,31] = max h 0.0:f32[]\n              in (i,) }\n          ] h\n        in (i,) }\n      jvp=jvp\n      symbolic_zeros=False\n    ] f\n  in (g,) }", "stablehlo": "module @jit_nn_layer_irwn attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x32xf32>, %arg1: tensor<32x31xf32>, %arg2: tensor<31xf32>) -> (tensor<5x31xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.dot_general %arg0, %arg1, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<5x32xf32>, tensor<32x31xf32>) -> tensor<5x31xf32>\n    %1 = stablehlo.broadcast_in_dim %arg2, dims = [1] : (tensor<31xf32>) -> tensor<1x31xf32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x31xf32>) -> tensor<5x31xf32>\n    %3 = stablehlo.add %0, %2 : tensor<5x31xf32>\n    %4 = call @relu(%3) : (tensor<5x31xf32>) -> tensor<5x31xf32>\n    return %4 : tensor<5x31xf32>\n  }\n  func.func private @relu(%arg0: tensor<5x31xf32>) -> tensor<5x31xf32> {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<5x31xf32>\n    %1 = stablehlo.maximum %arg0, %0 : tensor<5x31xf32>\n    return %1 : tensor<5x31xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_nn_layer_irwn, entry_computation_layout={(f32[5,32]{1,0}, f32[32,31]{1,0}, f32[31]{0})->f32[5,31]{1,0}}\n\nrelu.1 {\n  Arg_0.1 = f32[5,31]{1,0} parameter(0)\n  constant.1 = f32[] constant(0)\n  max.2 = f32[5,31]{1,0} broadcast(constant.1), dimensions={}\n  ROOT max.3 = f32[5,31]{1,0} maximum(Arg_0.1, max.2)\n}\n\nENTRY main.2 {\n  x.1 = f32[5,32]{1,0} parameter(0)\n  w.1 = f32[32,31]{1,0} parameter(1)\n  dot_general.1 = f32[5,31]{1,0} dot(x.1, w.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n  b.1 = f32[31]{0} parameter(2)\n  broadcast_in_dim.1 = f32[1,31]{1,0} reshape(b.1)\n  add.4 = f32[1,31]{1,0} broadcast(broadcast_in_dim.1), dimensions={0,1}\n  add.5 = f32[31]{0} reshape(add.4)\n  add.6 = f32[5,31]{1,0} broadcast(add.5), dimensions={1}\n  add.7 = f32[5,31]{1,0} add(dot_general.1, add.6)\n  ROOT jit_relu_.1 = f32[5,31]{1,0} call(add.7), to_apply=relu.1\n}\n\n", "input_shapes": [[5, 32], [32, 31], [31]], "output_shape": [5, 31]}
{"id": 38, "name": "softplus_lhau", "python_source": "def softplus_lhau(x):\n    \"\"\"Softplus operation.\"\"\"\n    return jax.nn.softplus(x)", "jaxpr": "{ lambda ; a:f32[6,4,13]. let\n    b:f32[6,4,13] = jit[\n      name=softplus\n      jaxpr={ lambda ; a:f32[6,4,13]. let\n          b:f32[6,4,13] = custom_jvp_call[\n            name=logaddexp\n            call_jaxpr={ lambda ; c:f32[6,4,13] d:f32[]. let\n                e:f32[6,4,13] = max c d\n                f:f32[6,4,13] = sub c d\n                g:bool[6,4,13] = ne f f\n                h:f32[6,4,13] = add c d\n                i:f32[6,4,13] = abs f\n                j:f32[6,4,13] = neg i\n                k:f32[6,4,13] = exp j\n                l:f32[6,4,13] = log1p k\n                m:f32[6,4,13] = add e l\n                n:f32[6,4,13] = select_n g m h\n              in (n,) }\n            jvp=_logaddexp_jvp\n            symbolic_zeros=False\n          ] a 0.0:f32[]\n        in (b,) }\n    ] a\n  in (b,) }", "stablehlo": "module @jit_softplus_lhau attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x4x13xf32>) -> (tensor<6x4x13xf32> {jax.result_info = \"result\"}) {\n    %0 = call @softplus(%arg0) : (tensor<6x4x13xf32>) -> tensor<6x4x13xf32>\n    return %0 : tensor<6x4x13xf32>\n  }\n  func.func private @softplus(%arg0: tensor<6x4x13xf32>) -> tensor<6x4x13xf32> {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<6x4x13xf32>\n    %1 = stablehlo.maximum %arg0, %0 : tensor<6x4x13xf32>\n    %2 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<6x4x13xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<6x4x13xf32>\n    %4 = stablehlo.compare  NE, %3, %3,  FLOAT : (tensor<6x4x13xf32>, tensor<6x4x13xf32>) -> tensor<6x4x13xi1>\n    %5 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<6x4x13xf32>\n    %6 = stablehlo.add %arg0, %5 : tensor<6x4x13xf32>\n    %7 = stablehlo.abs %3 : tensor<6x4x13xf32>\n    %8 = stablehlo.negate %7 : tensor<6x4x13xf32>\n    %9 = stablehlo.exponential %8 : tensor<6x4x13xf32>\n    %10 = stablehlo.log_plus_one %9 : tensor<6x4x13xf32>\n    %11 = stablehlo.add %1, %10 : tensor<6x4x13xf32>\n    %12 = stablehlo.select %4, %6, %11 : tensor<6x4x13xi1>, tensor<6x4x13xf32>\n    return %12 : tensor<6x4x13xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_softplus_lhau, entry_computation_layout={(f32[6,4,13]{2,1,0})->f32[6,4,13]{2,1,0}}\n\nsoftplus.1 {\n  Arg_0.1 = f32[6,4,13]{2,1,0} parameter(0)\n  ne.1 = pred[6,4,13]{2,1,0} compare(Arg_0.1, Arg_0.1), direction=NE\n  constant.1 = f32[] constant(0)\n  max.2 = f32[6,4,13]{2,1,0} broadcast(constant.1), dimensions={}\n  max.3 = f32[6,4,13]{2,1,0} maximum(Arg_0.1, max.2)\n  abs.1 = f32[6,4,13]{2,1,0} abs(Arg_0.1)\n  neg.1 = f32[6,4,13]{2,1,0} negate(abs.1)\n  exp.1 = f32[6,4,13]{2,1,0} exponential(neg.1)\n  log1p.1 = f32[6,4,13]{2,1,0} log-plus-one(exp.1)\n  add.1 = f32[6,4,13]{2,1,0} add(max.3, log1p.1)\n  ROOT select_n.1 = f32[6,4,13]{2,1,0} select(ne.1, Arg_0.1, add.1)\n}\n\nENTRY main.2 {\n  x.1 = f32[6,4,13]{2,1,0} parameter(0)\n  ROOT jit_softplus_.1 = f32[6,4,13]{2,1,0} call(x.1), to_apply=softplus.1\n}\n\n", "input_shapes": [[6, 4, 13]], "output_shape": [6, 4, 13]}
{"id": 39, "name": "matmul_olab", "python_source": "def matmul_olab(a, b):\n    \"\"\"Matrix multiplication of shapes (16,13) @ (13,16).\"\"\"\n    return jnp.dot(a, b)", "jaxpr": "{ lambda ; a:f32[16,13] b:f32[13,16]. let\n    c:f32[16,16] = dot_general[\n      dimension_numbers=(([1], [0]), ([], []))\n      preferred_element_type=float32\n    ] a b\n  in (c,) }", "stablehlo": "module @jit_matmul_olab attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<16x13xf32>, %arg1: tensor<13x16xf32>) -> (tensor<16x16xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.dot_general %arg0, %arg1, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<16x13xf32>, tensor<13x16xf32>) -> tensor<16x16xf32>\n    return %0 : tensor<16x16xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_matmul_olab, entry_computation_layout={(f32[16,13]{1,0}, f32[13,16]{1,0})->f32[16,16]{1,0}}\n\nENTRY main.1 {\n  a.1 = f32[16,13]{1,0} parameter(0)\n  b.1 = f32[13,16]{1,0} parameter(1)\n  ROOT dot_general.1 = f32[16,16]{1,0} dot(a.1, b.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n}\n\n", "input_shapes": [[16, 13], [13, 16]], "output_shape": [16, 16]}
{"id": 40, "name": "pow_ajol", "python_source": "def pow_ajol(x, y):\n    \"\"\"Power operation.\"\"\"\n    return jnp.power(jnp.abs(x), jnp.abs(y) % 3 + 1)", "jaxpr": "{ lambda ; a:f32[6] b:f32[6]. let\n    c:f32[6] = abs a\n    d:f32[6] = abs b\n    e:f32[6] = jit[\n      name=remainder\n      jaxpr={ lambda ; d:f32[6] f:i32[]. let\n          g:f32[] = convert_element_type[new_dtype=float32 weak_type=False] f\n          h:f32[6] = rem d g\n          i:bool[6] = ne h 0.0:f32[]\n          j:bool[6] = lt h 0.0:f32[]\n          k:bool[] = lt g 0.0:f32[]\n          l:bool[6] = ne j k\n          m:bool[6] = and l i\n          n:f32[6] = add h g\n          e:f32[6] = select_n m h n\n        in (e,) }\n    ] d 3:i32[]\n    o:f32[6] = add e 1.0:f32[]\n    p:f32[6] = pow c o\n  in (p,) }", "stablehlo": "module @jit_pow_ajol attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>, %arg1: tensor<6xf32>) -> (tensor<6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.abs %arg0 : tensor<6xf32>\n    %1 = stablehlo.abs %arg1 : tensor<6xf32>\n    %c = stablehlo.constant dense<3> : tensor<i32>\n    %2 = call @remainder(%1, %c) : (tensor<6xf32>, tensor<i32>) -> tensor<6xf32>\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %3 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<6xf32>\n    %4 = stablehlo.add %2, %3 : tensor<6xf32>\n    %5 = stablehlo.power %0, %4 : tensor<6xf32>\n    return %5 : tensor<6xf32>\n  }\n  func.func private @remainder(%arg0: tensor<6xf32>, %arg1: tensor<i32>) -> tensor<6xf32> {\n    %0 = stablehlo.convert %arg1 : (tensor<i32>) -> tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<6xf32>\n    %2 = stablehlo.remainder %arg0, %1 : tensor<6xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %3 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<6xf32>\n    %4 = stablehlo.compare  NE, %2, %3,  FLOAT : (tensor<6xf32>, tensor<6xf32>) -> tensor<6xi1>\n    %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %5 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<6xf32>\n    %6 = stablehlo.compare  LT, %2, %5,  FLOAT : (tensor<6xf32>, tensor<6xf32>) -> tensor<6xi1>\n    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %7 = stablehlo.compare  LT, %0, %cst_1,  FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n    %8 = stablehlo.broadcast_in_dim %7, dims = [] : (tensor<i1>) -> tensor<6xi1>\n    %9 = stablehlo.compare  NE, %6, %8,  UNSIGNED : (tensor<6xi1>, tensor<6xi1>) -> tensor<6xi1>\n    %10 = stablehlo.and %9, %4 : tensor<6xi1>\n    %11 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<6xf32>\n    %12 = stablehlo.add %2, %11 : tensor<6xf32>\n    %13 = stablehlo.select %10, %12, %2 : tensor<6xi1>, tensor<6xf32>\n    return %13 : tensor<6xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_pow_ajol, entry_computation_layout={(f32[6]{0}, f32[6]{0})->f32[6]{0}}\n\nremainder.1 {\n  Arg_0.1 = f32[6]{0} parameter(0)\n  Arg_1.1 = s32[] parameter(1)\n  convert_element_type.1 = f32[] convert(Arg_1.1)\n  rem.2 = f32[6]{0} broadcast(convert_element_type.1), dimensions={}\n  rem.3 = f32[6]{0} remainder(Arg_0.1, rem.2)\n  constant.4 = f32[] constant(0)\n  broadcast.2 = f32[6]{0} broadcast(constant.4), dimensions={}\n  lt.2 = pred[6]{0} compare(rem.3, broadcast.2), direction=LT\n  constant.5 = f32[] constant(0)\n  lt.3 = pred[] compare(convert_element_type.1, constant.5), direction=LT\n  ne.4 = pred[6]{0} broadcast(lt.3), dimensions={}\n  ne.5 = pred[6]{0} compare(lt.2, ne.4), direction=NE\n  ne.3 = pred[6]{0} compare(rem.3, broadcast.2), direction=NE\n  and.1 = pred[6]{0} and(ne.5, ne.3)\n  add.2 = f32[6]{0} broadcast(convert_element_type.1), dimensions={}\n  add.3 = f32[6]{0} add(rem.3, add.2)\n  ROOT select_n.1 = f32[6]{0} select(and.1, add.3, rem.3)\n}\n\nENTRY main.2 {\n  x.1 = f32[6]{0} parameter(0)\n  abs.2 = f32[6]{0} abs(x.1)\n  y.1 = f32[6]{0} parameter(1)\n  abs.3 = f32[6]{0} abs(y.1)\n  constant.7 = s32[] constant(3)\n  jit_remainder_.1 = f32[6]{0} call(abs.3, constant.7), to_apply=remainder.1\n  constant.6 = f32[] constant(1)\n  broadcast.3 = f32[6]{0} broadcast(constant.6), dimensions={}\n  add.5 = f32[6]{0} add(jit_remainder_.1, broadcast.3)\n  ROOT pow.1 = f32[6]{0} power(abs.2, add.5)\n}\n\n", "input_shapes": [[6], [6]], "output_shape": [6]}
{"id": 41, "name": "sum_xfgc", "python_source": "def sum_xfgc(x):\n    \"\"\"Sum reduction along axis 1.\"\"\"\n    return jnp.sum(x, axis=1)", "jaxpr": "{ lambda ; a:f32[16,15,8,11]. let\n    b:f32[16,8,11] = reduce_sum[axes=(1,) out_sharding=None] a\n  in (b,) }", "stablehlo": "module @jit_sum_xfgc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<16x15x8x11xf32>) -> (tensor<16x8x11xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<16x15x8x11xf32>, tensor<f32>) -> tensor<16x8x11xf32>\n    return %0 : tensor<16x8x11xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_sum_xfgc, entry_computation_layout={(f32[16,15,8,11]{3,2,1,0})->f32[16,8,11]{2,1,0}}\n\nregion_0.1 {\n  reduce_sum.3 = f32[] parameter(0)\n  reduce_sum.4 = f32[] parameter(1)\n  ROOT reduce_sum.5 = f32[] add(reduce_sum.3, reduce_sum.4)\n}\n\nENTRY main.2 {\n  x.1 = f32[16,15,8,11]{3,2,1,0} parameter(0)\n  constant.1 = f32[] constant(0)\n  ROOT reduce_sum.7 = f32[16,8,11]{2,1,0} reduce(x.1, constant.1), dimensions={1}, to_apply=region_0.1\n}\n\n", "input_shapes": [[16, 15, 8, 11]], "output_shape": [16, 8, 11]}
{"id": 42, "name": "min_euek", "python_source": "def min_euek(x, y):\n    \"\"\"Element-wise min operation.\"\"\"\n    return jnp.minimum(x, y)", "jaxpr": "{ lambda ; a:f32[13] b:f32[13]. let c:f32[13] = min a b in (c,) }", "stablehlo": "module @jit_min_euek attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<13xf32>, %arg1: tensor<13xf32>) -> (tensor<13xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.minimum %arg0, %arg1 : tensor<13xf32>\n    return %0 : tensor<13xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_min_euek, entry_computation_layout={(f32[13]{0}, f32[13]{0})->f32[13]{0}}\n\nENTRY main.1 {\n  x.1 = f32[13]{0} parameter(0)\n  y.1 = f32[13]{0} parameter(1)\n  ROOT min.1 = f32[13]{0} minimum(x.1, y.1)\n}\n\n", "input_shapes": [[13], [13]], "output_shape": [13]}
{"id": 43, "name": "min_uwvg", "python_source": "def min_uwvg(x):\n    \"\"\"Min reduction along axis 3.\"\"\"\n    return jnp.min(x, axis=3)", "jaxpr": "{ lambda ; a:f32[3,8,15,2]. let\n    b:f32[3,8,15] = reduce_min[axes=(3,)] a\n  in (b,) }", "stablehlo": "module @jit_min_uwvg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x8x15x2xf32>) -> (tensor<3x8x15xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0x7F800000> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.minimum across dimensions = [3] : (tensor<3x8x15x2xf32>, tensor<f32>) -> tensor<3x8x15xf32>\n    return %0 : tensor<3x8x15xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_min_uwvg, entry_computation_layout={(f32[3,8,15,2]{3,2,1,0})->f32[3,8,15]{2,1,0}}\n\nregion_0.1 {\n  reduce_min.3 = f32[] parameter(0)\n  reduce_min.4 = f32[] parameter(1)\n  ROOT reduce_min.5 = f32[] minimum(reduce_min.3, reduce_min.4)\n}\n\nENTRY main.2 {\n  x.1 = f32[3,8,15,2]{3,2,1,0} parameter(0)\n  constant.1 = f32[] constant(inf)\n  ROOT reduce_min.7 = f32[3,8,15]{2,1,0} reduce(x.1, constant.1), dimensions={3}, to_apply=region_0.1\n}\n\n", "input_shapes": [[3, 8, 15, 2]], "output_shape": [3, 8, 15]}
{"id": 44, "name": "sub_vlhu", "python_source": "def sub_vlhu(x, y):\n    \"\"\"Subtraction operation.\"\"\"\n    return x - y", "jaxpr": "{ lambda ; a:f32[5] b:f32[5]. let c:f32[5] = sub a b in (c,) }", "stablehlo": "module @jit_sub_vlhu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5xf32>, %arg1: tensor<5xf32>) -> (tensor<5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<5xf32>\n    return %0 : tensor<5xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_sub_vlhu, entry_computation_layout={(f32[5]{0}, f32[5]{0})->f32[5]{0}}\n\nENTRY main.1 {\n  x.1 = f32[5]{0} parameter(0)\n  y.1 = f32[5]{0} parameter(1)\n  ROOT sub.1 = f32[5]{0} subtract(x.1, y.1)\n}\n\n", "input_shapes": [[5], [5]], "output_shape": [5]}
{"id": 45, "name": "softplus_fiwz", "python_source": "def softplus_fiwz(x):\n    \"\"\"Softplus operation.\"\"\"\n    return jax.nn.softplus(x)", "jaxpr": "{ lambda ; a:f32[7,12]. let\n    b:f32[7,12] = jit[\n      name=softplus\n      jaxpr={ lambda ; a:f32[7,12]. let\n          b:f32[7,12] = custom_jvp_call[\n            name=logaddexp\n            call_jaxpr={ lambda ; c:f32[7,12] d:f32[]. let\n                e:f32[7,12] = max c d\n                f:f32[7,12] = sub c d\n                g:bool[7,12] = ne f f\n                h:f32[7,12] = add c d\n                i:f32[7,12] = abs f\n                j:f32[7,12] = neg i\n                k:f32[7,12] = exp j\n                l:f32[7,12] = log1p k\n                m:f32[7,12] = add e l\n                n:f32[7,12] = select_n g m h\n              in (n,) }\n            jvp=_logaddexp_jvp\n            symbolic_zeros=False\n          ] a 0.0:f32[]\n        in (b,) }\n    ] a\n  in (b,) }", "stablehlo": "module @jit_softplus_fiwz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x12xf32>) -> (tensor<7x12xf32> {jax.result_info = \"result\"}) {\n    %0 = call @softplus(%arg0) : (tensor<7x12xf32>) -> tensor<7x12xf32>\n    return %0 : tensor<7x12xf32>\n  }\n  func.func private @softplus(%arg0: tensor<7x12xf32>) -> tensor<7x12xf32> {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<7x12xf32>\n    %1 = stablehlo.maximum %arg0, %0 : tensor<7x12xf32>\n    %2 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<7x12xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<7x12xf32>\n    %4 = stablehlo.compare  NE, %3, %3,  FLOAT : (tensor<7x12xf32>, tensor<7x12xf32>) -> tensor<7x12xi1>\n    %5 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<7x12xf32>\n    %6 = stablehlo.add %arg0, %5 : tensor<7x12xf32>\n    %7 = stablehlo.abs %3 : tensor<7x12xf32>\n    %8 = stablehlo.negate %7 : tensor<7x12xf32>\n    %9 = stablehlo.exponential %8 : tensor<7x12xf32>\n    %10 = stablehlo.log_plus_one %9 : tensor<7x12xf32>\n    %11 = stablehlo.add %1, %10 : tensor<7x12xf32>\n    %12 = stablehlo.select %4, %6, %11 : tensor<7x12xi1>, tensor<7x12xf32>\n    return %12 : tensor<7x12xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_softplus_fiwz, entry_computation_layout={(f32[7,12]{1,0})->f32[7,12]{1,0}}\n\nsoftplus.1 {\n  Arg_0.1 = f32[7,12]{1,0} parameter(0)\n  ne.1 = pred[7,12]{1,0} compare(Arg_0.1, Arg_0.1), direction=NE\n  constant.1 = f32[] constant(0)\n  max.2 = f32[7,12]{1,0} broadcast(constant.1), dimensions={}\n  max.3 = f32[7,12]{1,0} maximum(Arg_0.1, max.2)\n  abs.1 = f32[7,12]{1,0} abs(Arg_0.1)\n  neg.1 = f32[7,12]{1,0} negate(abs.1)\n  exp.1 = f32[7,12]{1,0} exponential(neg.1)\n  log1p.1 = f32[7,12]{1,0} log-plus-one(exp.1)\n  add.1 = f32[7,12]{1,0} add(max.3, log1p.1)\n  ROOT select_n.1 = f32[7,12]{1,0} select(ne.1, Arg_0.1, add.1)\n}\n\nENTRY main.2 {\n  x.1 = f32[7,12]{1,0} parameter(0)\n  ROOT jit_softplus_.1 = f32[7,12]{1,0} call(x.1), to_apply=softplus.1\n}\n\n", "input_shapes": [[7, 12]], "output_shape": [7, 12]}
{"id": 46, "name": "composite_kzny", "python_source": "def composite_kzny(x):\n    \"\"\"Composite function with multiple operations.\"\"\"\n    return jnp.tanh(x) + jnp.sin(x) + jax.nn.relu(x)", "jaxpr": "{ lambda ; a:f32[3,16]. let\n    b:f32[3,16] = tanh a\n    c:f32[3,16] = sin a\n    d:f32[3,16] = add b c\n    e:f32[3,16] = custom_jvp_call[\n      name=relu\n      call_jaxpr={ lambda ; f:f32[3,16]. let\n          g:f32[3,16] = jit[\n            name=relu\n            jaxpr={ lambda ; f:f32[3,16]. let\n                g:f32[3,16] = max f 0.0:f32[]\n              in (g,) }\n          ] f\n        in (g,) }\n      jvp=jvp\n      symbolic_zeros=False\n    ] a\n    h:f32[3,16] = add d e\n  in (h,) }", "stablehlo": "module @jit_composite_kzny attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x16xf32>) -> (tensor<3x16xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.tanh %arg0 : tensor<3x16xf32>\n    %1 = stablehlo.sine %arg0 : tensor<3x16xf32>\n    %2 = stablehlo.add %0, %1 : tensor<3x16xf32>\n    %3 = call @relu(%arg0) : (tensor<3x16xf32>) -> tensor<3x16xf32>\n    %4 = stablehlo.add %2, %3 : tensor<3x16xf32>\n    return %4 : tensor<3x16xf32>\n  }\n  func.func private @relu(%arg0: tensor<3x16xf32>) -> tensor<3x16xf32> {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<3x16xf32>\n    %1 = stablehlo.maximum %arg0, %0 : tensor<3x16xf32>\n    return %1 : tensor<3x16xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_composite_kzny, entry_computation_layout={(f32[3,16]{1,0})->f32[3,16]{1,0}}\n\nrelu.1 {\n  Arg_0.1 = f32[3,16]{1,0} parameter(0)\n  constant.1 = f32[] constant(0)\n  max.2 = f32[3,16]{1,0} broadcast(constant.1), dimensions={}\n  ROOT max.3 = f32[3,16]{1,0} maximum(Arg_0.1, max.2)\n}\n\nENTRY main.2 {\n  x.1 = f32[3,16]{1,0} parameter(0)\n  tanh.1 = f32[3,16]{1,0} tanh(x.1)\n  sin.1 = f32[3,16]{1,0} sine(x.1)\n  add.2 = f32[3,16]{1,0} add(tanh.1, sin.1)\n  jit_relu_.1 = f32[3,16]{1,0} call(x.1), to_apply=relu.1\n  ROOT add.3 = f32[3,16]{1,0} add(add.2, jit_relu_.1)\n}\n\n", "input_shapes": [[3, 16]], "output_shape": [3, 16]}
{"id": 47, "name": "sigmoid_lsip", "python_source": "def sigmoid_lsip(x):\n    \"\"\"Sigmoid activation operation.\"\"\"\n    return jax.nn.sigmoid(x)", "jaxpr": "{ lambda ; a:f32[3,8,16]. let b:f32[3,8,16] = logistic a in (b,) }", "stablehlo": "module @jit_sigmoid_lsip attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x8x16xf32>) -> (tensor<3x8x16xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.negate %arg0 : tensor<3x8x16xf32>\n    %1 = stablehlo.exponential %0 : tensor<3x8x16xf32>\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<3x8x16xf32>\n    %3 = stablehlo.add %2, %1 : tensor<3x8x16xf32>\n    %cst_0 = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<3x8x16xf32>\n    %5 = stablehlo.divide %4, %3 : tensor<3x8x16xf32>\n    return %5 : tensor<3x8x16xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_sigmoid_lsip, entry_computation_layout={(f32[3,8,16]{2,1,0})->f32[3,8,16]{2,1,0}}\n\nENTRY main.1 {\n  constant.1 = f32[] constant(1)\n  broadcast.1 = f32[3,8,16]{2,1,0} broadcast(constant.1), dimensions={}\n  x.1 = f32[3,8,16]{2,1,0} parameter(0)\n  neg.1 = f32[3,8,16]{2,1,0} negate(x.1)\n  exp.1 = f32[3,8,16]{2,1,0} exponential(neg.1)\n  add.1 = f32[3,8,16]{2,1,0} add(exp.1, broadcast.1)\n  ROOT div.1 = f32[3,8,16]{2,1,0} divide(broadcast.1, add.1)\n}\n\n", "input_shapes": [[3, 8, 16]], "output_shape": [3, 8, 16]}
{"id": 48, "name": "composite_ebln", "python_source": "def composite_ebln(x):\n    \"\"\"Composite function with multiple operations.\"\"\"\n    return jnp.sin(x) + x ** 2", "jaxpr": "{ lambda ; a:f32[14,10]. let\n    b:f32[14,10] = sin a\n    c:f32[14,10] = integer_pow[y=2] a\n    d:f32[14,10] = add b c\n  in (d,) }", "stablehlo": "module @jit_composite_ebln attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<14x10xf32>) -> (tensor<14x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<14x10xf32>\n    %1 = stablehlo.multiply %arg0, %arg0 : tensor<14x10xf32>\n    %2 = stablehlo.add %0, %1 : tensor<14x10xf32>\n    return %2 : tensor<14x10xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_composite_ebln, entry_computation_layout={(f32[14,10]{1,0})->f32[14,10]{1,0}}\n\nENTRY main.1 {\n  x.1 = f32[14,10]{1,0} parameter(0)\n  sin.1 = f32[14,10]{1,0} sine(x.1)\n  integer_pow.1 = f32[14,10]{1,0} multiply(x.1, x.1)\n  ROOT add.1 = f32[14,10]{1,0} add(sin.1, integer_pow.1)\n}\n\n", "input_shapes": [[14, 10]], "output_shape": [14, 10]}
{"id": 49, "name": "softplus_xqrd", "python_source": "def softplus_xqrd(x):\n    \"\"\"Softplus operation.\"\"\"\n    return jax.nn.softplus(x)", "jaxpr": "{ lambda ; a:f32[10,6]. let\n    b:f32[10,6] = jit[\n      name=softplus\n      jaxpr={ lambda ; a:f32[10,6]. let\n          b:f32[10,6] = custom_jvp_call[\n            name=logaddexp\n            call_jaxpr={ lambda ; c:f32[10,6] d:f32[]. let\n                e:f32[10,6] = max c d\n                f:f32[10,6] = sub c d\n                g:bool[10,6] = ne f f\n                h:f32[10,6] = add c d\n                i:f32[10,6] = abs f\n                j:f32[10,6] = neg i\n                k:f32[10,6] = exp j\n                l:f32[10,6] = log1p k\n                m:f32[10,6] = add e l\n                n:f32[10,6] = select_n g m h\n              in (n,) }\n            jvp=_logaddexp_jvp\n            symbolic_zeros=False\n          ] a 0.0:f32[]\n        in (b,) }\n    ] a\n  in (b,) }", "stablehlo": "module @jit_softplus_xqrd attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x6xf32>) -> (tensor<10x6xf32> {jax.result_info = \"result\"}) {\n    %0 = call @softplus(%arg0) : (tensor<10x6xf32>) -> tensor<10x6xf32>\n    return %0 : tensor<10x6xf32>\n  }\n  func.func private @softplus(%arg0: tensor<10x6xf32>) -> tensor<10x6xf32> {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10x6xf32>\n    %1 = stablehlo.maximum %arg0, %0 : tensor<10x6xf32>\n    %2 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10x6xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<10x6xf32>\n    %4 = stablehlo.compare  NE, %3, %3,  FLOAT : (tensor<10x6xf32>, tensor<10x6xf32>) -> tensor<10x6xi1>\n    %5 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10x6xf32>\n    %6 = stablehlo.add %arg0, %5 : tensor<10x6xf32>\n    %7 = stablehlo.abs %3 : tensor<10x6xf32>\n    %8 = stablehlo.negate %7 : tensor<10x6xf32>\n    %9 = stablehlo.exponential %8 : tensor<10x6xf32>\n    %10 = stablehlo.log_plus_one %9 : tensor<10x6xf32>\n    %11 = stablehlo.add %1, %10 : tensor<10x6xf32>\n    %12 = stablehlo.select %4, %6, %11 : tensor<10x6xi1>, tensor<10x6xf32>\n    return %12 : tensor<10x6xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_softplus_xqrd, entry_computation_layout={(f32[10,6]{1,0})->f32[10,6]{1,0}}\n\nsoftplus.1 {\n  Arg_0.1 = f32[10,6]{1,0} parameter(0)\n  ne.1 = pred[10,6]{1,0} compare(Arg_0.1, Arg_0.1), direction=NE\n  constant.1 = f32[] constant(0)\n  max.2 = f32[10,6]{1,0} broadcast(constant.1), dimensions={}\n  max.3 = f32[10,6]{1,0} maximum(Arg_0.1, max.2)\n  abs.1 = f32[10,6]{1,0} abs(Arg_0.1)\n  neg.1 = f32[10,6]{1,0} negate(abs.1)\n  exp.1 = f32[10,6]{1,0} exponential(neg.1)\n  log1p.1 = f32[10,6]{1,0} log-plus-one(exp.1)\n  add.1 = f32[10,6]{1,0} add(max.3, log1p.1)\n  ROOT select_n.1 = f32[10,6]{1,0} select(ne.1, Arg_0.1, add.1)\n}\n\nENTRY main.2 {\n  x.1 = f32[10,6]{1,0} parameter(0)\n  ROOT jit_softplus_.1 = f32[10,6]{1,0} call(x.1), to_apply=softplus.1\n}\n\n", "input_shapes": [[10, 6]], "output_shape": [10, 6]}
{"id": 50, "name": "nn_layer_kkhd", "python_source": "def nn_layer_kkhd(x, w, b):\n    \"\"\"Neural network layer with sigmoid activation.\"\"\"\n    return jax.nn.sigmoid(jnp.dot(x, w) + b)", "jaxpr": "{ lambda ; a:f32[7,25] b:f32[25,16] c:f32[16]. let\n    d:f32[7,16] = dot_general[\n      dimension_numbers=(([1], [0]), ([], []))\n      preferred_element_type=float32\n    ] a b\n    e:f32[1,16] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 16)\n      sharding=None\n    ] c\n    f:f32[7,16] = add d e\n    g:f32[7,16] = logistic f\n  in (g,) }", "stablehlo": "module @jit_nn_layer_kkhd attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x25xf32>, %arg1: tensor<25x16xf32>, %arg2: tensor<16xf32>) -> (tensor<7x16xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.dot_general %arg0, %arg1, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<7x25xf32>, tensor<25x16xf32>) -> tensor<7x16xf32>\n    %1 = stablehlo.broadcast_in_dim %arg2, dims = [1] : (tensor<16xf32>) -> tensor<1x16xf32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x16xf32>) -> tensor<7x16xf32>\n    %3 = stablehlo.add %0, %2 : tensor<7x16xf32>\n    %4 = stablehlo.negate %3 : tensor<7x16xf32>\n    %5 = stablehlo.exponential %4 : tensor<7x16xf32>\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %6 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<7x16xf32>\n    %7 = stablehlo.add %6, %5 : tensor<7x16xf32>\n    %cst_0 = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %8 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<7x16xf32>\n    %9 = stablehlo.divide %8, %7 : tensor<7x16xf32>\n    return %9 : tensor<7x16xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_nn_layer_kkhd, entry_computation_layout={(f32[7,25]{1,0}, f32[25,16]{1,0}, f32[16]{0})->f32[7,16]{1,0}}\n\nENTRY main.1 {\n  constant.1 = f32[] constant(1)\n  broadcast.1 = f32[7,16]{1,0} broadcast(constant.1), dimensions={}\n  x.1 = f32[7,25]{1,0} parameter(0)\n  w.1 = f32[25,16]{1,0} parameter(1)\n  dot_general.1 = f32[7,16]{1,0} dot(x.1, w.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n  b.1 = f32[16]{0} parameter(2)\n  broadcast_in_dim.1 = f32[1,16]{1,0} reshape(b.1)\n  add.5 = f32[1,16]{1,0} broadcast(broadcast_in_dim.1), dimensions={0,1}\n  add.6 = f32[16]{0} reshape(add.5)\n  add.7 = f32[7,16]{1,0} broadcast(add.6), dimensions={1}\n  add.8 = f32[7,16]{1,0} add(dot_general.1, add.7)\n  neg.1 = f32[7,16]{1,0} negate(add.8)\n  exp.1 = f32[7,16]{1,0} exponential(neg.1)\n  add.9 = f32[7,16]{1,0} add(exp.1, broadcast.1)\n  ROOT div.1 = f32[7,16]{1,0} divide(broadcast.1, add.9)\n}\n\n", "input_shapes": [[7, 25], [25, 16], [16]], "output_shape": [7, 16]}
{"id": 51, "name": "composite_okvh", "python_source": "def composite_okvh(x):\n    \"\"\"Composite function with multiple operations.\"\"\"\n    return x ** 2 + jnp.exp(-jnp.abs(x)) + jnp.tanh(x)", "jaxpr": "{ lambda ; a:f32[5,8]. let\n    b:f32[5,8] = integer_pow[y=2] a\n    c:f32[5,8] = abs a\n    d:f32[5,8] = neg c\n    e:f32[5,8] = exp d\n    f:f32[5,8] = add b e\n    g:f32[5,8] = tanh a\n    h:f32[5,8] = add f g\n  in (h,) }", "stablehlo": "module @jit_composite_okvh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x8xf32>) -> (tensor<5x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<5x8xf32>\n    %1 = stablehlo.abs %arg0 : tensor<5x8xf32>\n    %2 = stablehlo.negate %1 : tensor<5x8xf32>\n    %3 = stablehlo.exponential %2 : tensor<5x8xf32>\n    %4 = stablehlo.add %0, %3 : tensor<5x8xf32>\n    %5 = stablehlo.tanh %arg0 : tensor<5x8xf32>\n    %6 = stablehlo.add %4, %5 : tensor<5x8xf32>\n    return %6 : tensor<5x8xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_composite_okvh, entry_computation_layout={(f32[5,8]{1,0})->f32[5,8]{1,0}}\n\nENTRY main.1 {\n  x.1 = f32[5,8]{1,0} parameter(0)\n  integer_pow.1 = f32[5,8]{1,0} multiply(x.1, x.1)\n  abs.1 = f32[5,8]{1,0} abs(x.1)\n  neg.1 = f32[5,8]{1,0} negate(abs.1)\n  exp.1 = f32[5,8]{1,0} exponential(neg.1)\n  add.2 = f32[5,8]{1,0} add(integer_pow.1, exp.1)\n  tanh.1 = f32[5,8]{1,0} tanh(x.1)\n  ROOT add.3 = f32[5,8]{1,0} add(add.2, tanh.1)\n}\n\n", "input_shapes": [[5, 8]], "output_shape": [5, 8]}
{"id": 52, "name": "nn_layer_fmxu", "python_source": "def nn_layer_fmxu(x, w, b):\n    \"\"\"Neural network layer with tanh activation.\"\"\"\n    return jax.nn.tanh(jnp.dot(x, w) + b)", "jaxpr": "{ lambda ; a:f32[3,25] b:f32[25,6] c:f32[6]. let\n    d:f32[3,6] = dot_general[\n      dimension_numbers=(([1], [0]), ([], []))\n      preferred_element_type=float32\n    ] a b\n    e:f32[1,6] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 6)\n      sharding=None\n    ] c\n    f:f32[3,6] = add d e\n    g:f32[3,6] = tanh f\n  in (g,) }", "stablehlo": "module @jit_nn_layer_fmxu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x25xf32>, %arg1: tensor<25x6xf32>, %arg2: tensor<6xf32>) -> (tensor<3x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.dot_general %arg0, %arg1, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<3x25xf32>, tensor<25x6xf32>) -> tensor<3x6xf32>\n    %1 = stablehlo.broadcast_in_dim %arg2, dims = [1] : (tensor<6xf32>) -> tensor<1x6xf32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x6xf32>) -> tensor<3x6xf32>\n    %3 = stablehlo.add %0, %2 : tensor<3x6xf32>\n    %4 = stablehlo.tanh %3 : tensor<3x6xf32>\n    return %4 : tensor<3x6xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_nn_layer_fmxu, entry_computation_layout={(f32[3,25]{1,0}, f32[25,6]{1,0}, f32[6]{0})->f32[3,6]{1,0}}\n\nENTRY main.1 {\n  x.1 = f32[3,25]{1,0} parameter(0)\n  w.1 = f32[25,6]{1,0} parameter(1)\n  dot_general.1 = f32[3,6]{1,0} dot(x.1, w.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n  b.1 = f32[6]{0} parameter(2)\n  broadcast_in_dim.1 = f32[1,6]{1,0} reshape(b.1)\n  add.4 = f32[1,6]{1,0} broadcast(broadcast_in_dim.1), dimensions={0,1}\n  add.5 = f32[6]{0} reshape(add.4)\n  add.6 = f32[3,6]{1,0} broadcast(add.5), dimensions={1}\n  add.7 = f32[3,6]{1,0} add(dot_general.1, add.6)\n  ROOT tanh.1 = f32[3,6]{1,0} tanh(add.7)\n}\n\n", "input_shapes": [[3, 25], [25, 6], [6]], "output_shape": [3, 6]}
{"id": 53, "name": "composite_rqcy", "python_source": "def composite_rqcy(x):\n    \"\"\"Composite function with multiple operations.\"\"\"\n    return jnp.tanh(x) + jnp.exp(-jnp.abs(x)) + jax.nn.relu(x)", "jaxpr": "{ lambda ; a:f32[5,12]. let\n    b:f32[5,12] = tanh a\n    c:f32[5,12] = abs a\n    d:f32[5,12] = neg c\n    e:f32[5,12] = exp d\n    f:f32[5,12] = add b e\n    g:f32[5,12] = custom_jvp_call[\n      name=relu\n      call_jaxpr={ lambda ; h:f32[5,12]. let\n          i:f32[5,12] = jit[\n            name=relu\n            jaxpr={ lambda ; h:f32[5,12]. let\n                i:f32[5,12] = max h 0.0:f32[]\n              in (i,) }\n          ] h\n        in (i,) }\n      jvp=jvp\n      symbolic_zeros=False\n    ] a\n    j:f32[5,12] = add f g\n  in (j,) }", "stablehlo": "module @jit_composite_rqcy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x12xf32>) -> (tensor<5x12xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.tanh %arg0 : tensor<5x12xf32>\n    %1 = stablehlo.abs %arg0 : tensor<5x12xf32>\n    %2 = stablehlo.negate %1 : tensor<5x12xf32>\n    %3 = stablehlo.exponential %2 : tensor<5x12xf32>\n    %4 = stablehlo.add %0, %3 : tensor<5x12xf32>\n    %5 = call @relu(%arg0) : (tensor<5x12xf32>) -> tensor<5x12xf32>\n    %6 = stablehlo.add %4, %5 : tensor<5x12xf32>\n    return %6 : tensor<5x12xf32>\n  }\n  func.func private @relu(%arg0: tensor<5x12xf32>) -> tensor<5x12xf32> {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<5x12xf32>\n    %1 = stablehlo.maximum %arg0, %0 : tensor<5x12xf32>\n    return %1 : tensor<5x12xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_composite_rqcy, entry_computation_layout={(f32[5,12]{1,0})->f32[5,12]{1,0}}\n\nrelu.1 {\n  Arg_0.1 = f32[5,12]{1,0} parameter(0)\n  constant.1 = f32[] constant(0)\n  max.2 = f32[5,12]{1,0} broadcast(constant.1), dimensions={}\n  ROOT max.3 = f32[5,12]{1,0} maximum(Arg_0.1, max.2)\n}\n\nENTRY main.2 {\n  x.1 = f32[5,12]{1,0} parameter(0)\n  tanh.1 = f32[5,12]{1,0} tanh(x.1)\n  abs.1 = f32[5,12]{1,0} abs(x.1)\n  neg.1 = f32[5,12]{1,0} negate(abs.1)\n  exp.1 = f32[5,12]{1,0} exponential(neg.1)\n  add.2 = f32[5,12]{1,0} add(tanh.1, exp.1)\n  jit_relu_.1 = f32[5,12]{1,0} call(x.1), to_apply=relu.1\n  ROOT add.3 = f32[5,12]{1,0} add(add.2, jit_relu_.1)\n}\n\n", "input_shapes": [[5, 12]], "output_shape": [5, 12]}
{"id": 54, "name": "neg_gmwb", "python_source": "def neg_gmwb(x):\n    \"\"\"Negate operation.\"\"\"\n    return jnp.negative(x)", "jaxpr": "{ lambda ; a:f32[16,12]. let b:f32[16,12] = neg a in (b,) }", "stablehlo": "module @jit_neg_gmwb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<16x12xf32>) -> (tensor<16x12xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.negate %arg0 : tensor<16x12xf32>\n    return %0 : tensor<16x12xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_neg_gmwb, entry_computation_layout={(f32[16,12]{1,0})->f32[16,12]{1,0}}\n\nENTRY main.1 {\n  x.1 = f32[16,12]{1,0} parameter(0)\n  ROOT neg.1 = f32[16,12]{1,0} negate(x.1)\n}\n\n", "input_shapes": [[16, 12]], "output_shape": [16, 12]}
{"id": 55, "name": "composite_fsmg", "python_source": "def composite_fsmg(x):\n    \"\"\"Composite function with multiple operations.\"\"\"\n    return jax.nn.relu(x) + x ** 2", "jaxpr": "{ lambda ; a:f32[12,13]. let\n    b:f32[12,13] = custom_jvp_call[\n      name=relu\n      call_jaxpr={ lambda ; c:f32[12,13]. let\n          d:f32[12,13] = jit[\n            name=relu\n            jaxpr={ lambda ; c:f32[12,13]. let\n                d:f32[12,13] = max c 0.0:f32[]\n              in (d,) }\n          ] c\n        in (d,) }\n      jvp=jvp\n      symbolic_zeros=False\n    ] a\n    e:f32[12,13] = integer_pow[y=2] a\n    f:f32[12,13] = add b e\n  in (f,) }", "stablehlo": "module @jit_composite_fsmg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<12x13xf32>) -> (tensor<12x13xf32> {jax.result_info = \"result\"}) {\n    %0 = call @relu(%arg0) : (tensor<12x13xf32>) -> tensor<12x13xf32>\n    %1 = stablehlo.multiply %arg0, %arg0 : tensor<12x13xf32>\n    %2 = stablehlo.add %0, %1 : tensor<12x13xf32>\n    return %2 : tensor<12x13xf32>\n  }\n  func.func private @relu(%arg0: tensor<12x13xf32>) -> tensor<12x13xf32> {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<12x13xf32>\n    %1 = stablehlo.maximum %arg0, %0 : tensor<12x13xf32>\n    return %1 : tensor<12x13xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_composite_fsmg, entry_computation_layout={(f32[12,13]{1,0})->f32[12,13]{1,0}}\n\nrelu.1 {\n  Arg_0.1 = f32[12,13]{1,0} parameter(0)\n  constant.1 = f32[] constant(0)\n  max.2 = f32[12,13]{1,0} broadcast(constant.1), dimensions={}\n  ROOT max.3 = f32[12,13]{1,0} maximum(Arg_0.1, max.2)\n}\n\nENTRY main.2 {\n  x.1 = f32[12,13]{1,0} parameter(0)\n  jit_relu_.1 = f32[12,13]{1,0} call(x.1), to_apply=relu.1\n  integer_pow.1 = f32[12,13]{1,0} multiply(x.1, x.1)\n  ROOT add.1 = f32[12,13]{1,0} add(jit_relu_.1, integer_pow.1)\n}\n\n", "input_shapes": [[12, 13]], "output_shape": [12, 13]}
{"id": 56, "name": "sub_usmo", "python_source": "def sub_usmo(x, y):\n    \"\"\"Subtraction operation.\"\"\"\n    return x - y", "jaxpr": "{ lambda ; a:f32[9] b:f32[9]. let c:f32[9] = sub a b in (c,) }", "stablehlo": "module @jit_sub_usmo attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>, %arg1: tensor<9xf32>) -> (tensor<9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<9xf32>\n    return %0 : tensor<9xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_sub_usmo, entry_computation_layout={(f32[9]{0}, f32[9]{0})->f32[9]{0}}\n\nENTRY main.1 {\n  x.1 = f32[9]{0} parameter(0)\n  y.1 = f32[9]{0} parameter(1)\n  ROOT sub.1 = f32[9]{0} subtract(x.1, y.1)\n}\n\n", "input_shapes": [[9], [9]], "output_shape": [9]}
{"id": 57, "name": "min_mnoi", "python_source": "def min_mnoi(x, y):\n    \"\"\"Element-wise min operation.\"\"\"\n    return jnp.minimum(x, y)", "jaxpr": "{ lambda ; a:f32[11,15] b:f32[11,15]. let c:f32[11,15] = min a b in (c,) }", "stablehlo": "module @jit_min_mnoi attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<11x15xf32>, %arg1: tensor<11x15xf32>) -> (tensor<11x15xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.minimum %arg0, %arg1 : tensor<11x15xf32>\n    return %0 : tensor<11x15xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_min_mnoi, entry_computation_layout={(f32[11,15]{1,0}, f32[11,15]{1,0})->f32[11,15]{1,0}}\n\nENTRY main.1 {\n  x.1 = f32[11,15]{1,0} parameter(0)\n  y.1 = f32[11,15]{1,0} parameter(1)\n  ROOT min.1 = f32[11,15]{1,0} minimum(x.1, y.1)\n}\n\n", "input_shapes": [[11, 15], [11, 15]], "output_shape": [11, 15]}
{"id": 58, "name": "nn_layer_xlxl", "python_source": "def nn_layer_xlxl(x, w, b):\n    \"\"\"Neural network layer with gelu activation.\"\"\"\n    return jax.nn.gelu(jnp.dot(x, w) + b)", "jaxpr": "{ lambda ; a:f32[3,27] b:f32[27,31] c:f32[31]. let\n    d:f32[3,31] = dot_general[\n      dimension_numbers=(([1], [0]), ([], []))\n      preferred_element_type=float32\n    ] a b\n    e:f32[1,31] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 31)\n      sharding=None\n    ] c\n    f:f32[3,31] = add d e\n    g:f32[3,31] = integer_pow[y=3] f\n    h:f32[3,31] = mul 0.044714998453855515:f32[] g\n    i:f32[3,31] = add f h\n    j:f32[3,31] = mul 0.7978845834732056:f32[] i\n    k:f32[3,31] = tanh j\n    l:f32[3,31] = add 1.0:f32[] k\n    m:f32[3,31] = mul 0.5:f32[] l\n    n:f32[3,31] = mul f m\n  in (n,) }", "stablehlo": "module @jit_nn_layer_xlxl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x27xf32>, %arg1: tensor<27x31xf32>, %arg2: tensor<31xf32>) -> (tensor<3x31xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.dot_general %arg0, %arg1, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<3x27xf32>, tensor<27x31xf32>) -> tensor<3x31xf32>\n    %1 = stablehlo.broadcast_in_dim %arg2, dims = [1] : (tensor<31xf32>) -> tensor<1x31xf32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x31xf32>) -> tensor<3x31xf32>\n    %3 = stablehlo.add %0, %2 : tensor<3x31xf32>\n    %4 = stablehlo.multiply %3, %3 : tensor<3x31xf32>\n    %5 = stablehlo.multiply %4, %3 : tensor<3x31xf32>\n    %cst = stablehlo.constant dense<4.471500e-02> : tensor<f32>\n    %6 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<3x31xf32>\n    %7 = stablehlo.multiply %6, %5 : tensor<3x31xf32>\n    %8 = stablehlo.add %3, %7 : tensor<3x31xf32>\n    %cst_0 = stablehlo.constant dense<0.797884583> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<3x31xf32>\n    %10 = stablehlo.multiply %9, %8 : tensor<3x31xf32>\n    %11 = stablehlo.tanh %10 : tensor<3x31xf32>\n    %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %12 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<3x31xf32>\n    %13 = stablehlo.add %12, %11 : tensor<3x31xf32>\n    %cst_2 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %14 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<3x31xf32>\n    %15 = stablehlo.multiply %14, %13 : tensor<3x31xf32>\n    %16 = stablehlo.multiply %3, %15 : tensor<3x31xf32>\n    return %16 : tensor<3x31xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_nn_layer_xlxl, entry_computation_layout={(f32[3,27]{1,0}, f32[27,31]{1,0}, f32[31]{0})->f32[3,31]{1,0}}\n\nENTRY main.1 {\n  x.1 = f32[3,27]{1,0} parameter(0)\n  w.1 = f32[27,31]{1,0} parameter(1)\n  dot_general.1 = f32[3,31]{1,0} dot(x.1, w.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n  b.1 = f32[31]{0} parameter(2)\n  broadcast_in_dim.1 = f32[1,31]{1,0} reshape(b.1)\n  add.8 = f32[1,31]{1,0} broadcast(broadcast_in_dim.1), dimensions={0,1}\n  add.9 = f32[31]{0} reshape(add.8)\n  add.10 = f32[3,31]{1,0} broadcast(add.9), dimensions={1}\n  add.11 = f32[3,31]{1,0} add(dot_general.1, add.10)\n  integer_pow.2 = f32[3,31]{1,0} multiply(add.11, add.11)\n  integer_pow.3 = f32[3,31]{1,0} multiply(integer_pow.2, add.11)\n  constant.7 = f32[] constant(0.044715)\n  mul.9 = f32[3,31]{1,0} broadcast(constant.7), dimensions={}\n  mul.10 = f32[3,31]{1,0} multiply(integer_pow.3, mul.9)\n  add.12 = f32[3,31]{1,0} add(add.11, mul.10)\n  constant.6 = f32[] constant(0.797884583)\n  mul.8 = f32[3,31]{1,0} broadcast(constant.6), dimensions={}\n  mul.11 = f32[3,31]{1,0} multiply(add.12, mul.8)\n  tanh.1 = f32[3,31]{1,0} tanh(mul.11)\n  constant.5 = f32[] constant(1)\n  add.7 = f32[3,31]{1,0} broadcast(constant.5), dimensions={}\n  add.13 = f32[3,31]{1,0} add(tanh.1, add.7)\n  constant.4 = f32[] constant(0.5)\n  mul.7 = f32[3,31]{1,0} broadcast(constant.4), dimensions={}\n  mul.12 = f32[3,31]{1,0} multiply(add.13, mul.7)\n  ROOT mul.13 = f32[3,31]{1,0} multiply(add.11, mul.12)\n}\n\n", "input_shapes": [[3, 27], [27, 31], [31]], "output_shape": [3, 31]}
{"id": 59, "name": "matmul_ggqt", "python_source": "def matmul_ggqt(a, b):\n    \"\"\"Matrix multiplication of shapes (10,9) @ (9,12).\"\"\"\n    return jnp.dot(a, b)", "jaxpr": "{ lambda ; a:f32[10,9] b:f32[9,12]. let\n    c:f32[10,12] = dot_general[\n      dimension_numbers=(([1], [0]), ([], []))\n      preferred_element_type=float32\n    ] a b\n  in (c,) }", "stablehlo": "module @jit_matmul_ggqt attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x9xf32>, %arg1: tensor<9x12xf32>) -> (tensor<10x12xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.dot_general %arg0, %arg1, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<10x9xf32>, tensor<9x12xf32>) -> tensor<10x12xf32>\n    return %0 : tensor<10x12xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_matmul_ggqt, entry_computation_layout={(f32[10,9]{1,0}, f32[9,12]{1,0})->f32[10,12]{1,0}}\n\nENTRY main.1 {\n  a.1 = f32[10,9]{1,0} parameter(0)\n  b.1 = f32[9,12]{1,0} parameter(1)\n  ROOT dot_general.1 = f32[10,12]{1,0} dot(a.1, b.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n}\n\n", "input_shapes": [[10, 9], [9, 12]], "output_shape": [10, 12]}
{"id": 60, "name": "mul_lsgi", "python_source": "def mul_lsgi(x, y):\n    \"\"\"Multiplication operation.\"\"\"\n    return x * y", "jaxpr": "{ lambda ; a:f32[3,4,4] b:f32[3,4,4]. let c:f32[3,4,4] = mul a b in (c,) }", "stablehlo": "module @jit_mul_lsgi attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x4x4xf32>, %arg1: tensor<3x4x4xf32>) -> (tensor<3x4x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<3x4x4xf32>\n    return %0 : tensor<3x4x4xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_mul_lsgi, entry_computation_layout={(f32[3,4,4]{2,1,0}, f32[3,4,4]{2,1,0})->f32[3,4,4]{2,1,0}}\n\nENTRY main.1 {\n  x.1 = f32[3,4,4]{2,1,0} parameter(0)\n  y.1 = f32[3,4,4]{2,1,0} parameter(1)\n  ROOT mul.1 = f32[3,4,4]{2,1,0} multiply(x.1, y.1)\n}\n\n", "input_shapes": [[3, 4, 4], [3, 4, 4]], "output_shape": [3, 4, 4]}
{"id": 61, "name": "div_ssbk", "python_source": "def div_ssbk(x, y):\n    \"\"\"Division (safe) operation.\"\"\"\n    return x / (y + 1e-6)", "jaxpr": "{ lambda ; a:f32[9,8,2] b:f32[9,8,2]. let\n    c:f32[9,8,2] = add b 9.999999974752427e-07:f32[]\n    d:f32[9,8,2] = div a c\n  in (d,) }", "stablehlo": "module @jit_div_ssbk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x8x2xf32>, %arg1: tensor<9x8x2xf32>) -> (tensor<9x8x2xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<9.99999997E-7> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<9x8x2xf32>\n    %1 = stablehlo.add %arg1, %0 : tensor<9x8x2xf32>\n    %2 = stablehlo.divide %arg0, %1 : tensor<9x8x2xf32>\n    return %2 : tensor<9x8x2xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_div_ssbk, entry_computation_layout={(f32[9,8,2]{2,1,0}, f32[9,8,2]{2,1,0})->f32[9,8,2]{2,1,0}}\n\nENTRY main.1 {\n  x.1 = f32[9,8,2]{2,1,0} parameter(0)\n  y.1 = f32[9,8,2]{2,1,0} parameter(1)\n  constant.1 = f32[] constant(1e-06)\n  add.2 = f32[9,8,2]{2,1,0} broadcast(constant.1), dimensions={}\n  add.3 = f32[9,8,2]{2,1,0} add(y.1, add.2)\n  ROOT div.1 = f32[9,8,2]{2,1,0} divide(x.1, add.3)\n}\n\n", "input_shapes": [[9, 8, 2], [9, 8, 2]], "output_shape": [9, 8, 2]}
{"id": 62, "name": "min_kxps", "python_source": "def min_kxps(x, y):\n    \"\"\"Element-wise min operation.\"\"\"\n    return jnp.minimum(x, y)", "jaxpr": "{ lambda ; a:f32[8,9,2] b:f32[8,9,2]. let c:f32[8,9,2] = min a b in (c,) }", "stablehlo": "module @jit_min_kxps attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x9x2xf32>, %arg1: tensor<8x9x2xf32>) -> (tensor<8x9x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.minimum %arg0, %arg1 : tensor<8x9x2xf32>\n    return %0 : tensor<8x9x2xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_min_kxps, entry_computation_layout={(f32[8,9,2]{2,1,0}, f32[8,9,2]{2,1,0})->f32[8,9,2]{2,1,0}}\n\nENTRY main.1 {\n  x.1 = f32[8,9,2]{2,1,0} parameter(0)\n  y.1 = f32[8,9,2]{2,1,0} parameter(1)\n  ROOT min.1 = f32[8,9,2]{2,1,0} minimum(x.1, y.1)\n}\n\n", "input_shapes": [[8, 9, 2], [8, 9, 2]], "output_shape": [8, 9, 2]}
{"id": 63, "name": "max_twyk", "python_source": "def max_twyk(x):\n    \"\"\"Max reduction along axis 1.\"\"\"\n    return jnp.max(x, axis=1)", "jaxpr": "{ lambda ; a:f32[13,10,14,11]. let\n    b:f32[13,14,11] = reduce_max[axes=(1,)] a\n  in (b,) }", "stablehlo": "module @jit_max_twyk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<13x10x14x11xf32>) -> (tensor<13x14x11xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0xFF800000> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.maximum across dimensions = [1] : (tensor<13x10x14x11xf32>, tensor<f32>) -> tensor<13x14x11xf32>\n    return %0 : tensor<13x14x11xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_max_twyk, entry_computation_layout={(f32[13,10,14,11]{3,2,1,0})->f32[13,14,11]{2,1,0}}\n\nregion_0.1 {\n  reduce_max.3 = f32[] parameter(0)\n  reduce_max.4 = f32[] parameter(1)\n  ROOT reduce_max.5 = f32[] maximum(reduce_max.3, reduce_max.4)\n}\n\nENTRY main.2 {\n  x.1 = f32[13,10,14,11]{3,2,1,0} parameter(0)\n  constant.1 = f32[] constant(-inf)\n  ROOT reduce_max.7 = f32[13,14,11]{2,1,0} reduce(x.1, constant.1), dimensions={1}, to_apply=region_0.1\n}\n\n", "input_shapes": [[13, 10, 14, 11]], "output_shape": [13, 14, 11]}
{"id": 64, "name": "matmul_flai", "python_source": "def matmul_flai(a, b):\n    \"\"\"Matrix multiplication of shapes (12,14) @ (14,8).\"\"\"\n    return jnp.dot(a, b)", "jaxpr": "{ lambda ; a:f32[12,14] b:f32[14,8]. let\n    c:f32[12,8] = dot_general[\n      dimension_numbers=(([1], [0]), ([], []))\n      preferred_element_type=float32\n    ] a b\n  in (c,) }", "stablehlo": "module @jit_matmul_flai attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<12x14xf32>, %arg1: tensor<14x8xf32>) -> (tensor<12x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.dot_general %arg0, %arg1, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<12x14xf32>, tensor<14x8xf32>) -> tensor<12x8xf32>\n    return %0 : tensor<12x8xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_matmul_flai, entry_computation_layout={(f32[12,14]{1,0}, f32[14,8]{1,0})->f32[12,8]{1,0}}\n\nENTRY main.1 {\n  a.1 = f32[12,14]{1,0} parameter(0)\n  b.1 = f32[14,8]{1,0} parameter(1)\n  ROOT dot_general.1 = f32[12,8]{1,0} dot(a.1, b.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n}\n\n", "input_shapes": [[12, 14], [14, 8]], "output_shape": [12, 8]}
{"id": 65, "name": "nn_layer_emdq", "python_source": "def nn_layer_emdq(x, w, b):\n    \"\"\"Neural network layer with relu activation.\"\"\"\n    return jax.nn.relu(jnp.dot(x, w) + b)", "jaxpr": "{ lambda ; a:f32[1,16] b:f32[16,22] c:f32[22]. let\n    d:f32[1,22] = dot_general[\n      dimension_numbers=(([1], [0]), ([], []))\n      preferred_element_type=float32\n    ] a b\n    e:f32[1,22] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 22)\n      sharding=None\n    ] c\n    f:f32[1,22] = add d e\n    g:f32[1,22] = custom_jvp_call[\n      name=relu\n      call_jaxpr={ lambda ; h:f32[1,22]. let\n          i:f32[1,22] = jit[\n            name=relu\n            jaxpr={ lambda ; h:f32[1,22]. let\n                i:f32[1,22] = max h 0.0:f32[]\n              in (i,) }\n          ] h\n        in (i,) }\n      jvp=jvp\n      symbolic_zeros=False\n    ] f\n  in (g,) }", "stablehlo": "module @jit_nn_layer_emdq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1x16xf32>, %arg1: tensor<16x22xf32>, %arg2: tensor<22xf32>) -> (tensor<1x22xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.dot_general %arg0, %arg1, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<1x16xf32>, tensor<16x22xf32>) -> tensor<1x22xf32>\n    %1 = stablehlo.broadcast_in_dim %arg2, dims = [1] : (tensor<22xf32>) -> tensor<1x22xf32>\n    %2 = stablehlo.add %0, %1 : tensor<1x22xf32>\n    %3 = call @relu(%2) : (tensor<1x22xf32>) -> tensor<1x22xf32>\n    return %3 : tensor<1x22xf32>\n  }\n  func.func private @relu(%arg0: tensor<1x22xf32>) -> tensor<1x22xf32> {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<1x22xf32>\n    %1 = stablehlo.maximum %arg0, %0 : tensor<1x22xf32>\n    return %1 : tensor<1x22xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_nn_layer_emdq, entry_computation_layout={(f32[1,16]{1,0}, f32[16,22]{1,0}, f32[22]{0})->f32[1,22]{1,0}}\n\nrelu.1 {\n  Arg_0.1 = f32[1,22]{1,0} parameter(0)\n  constant.1 = f32[] constant(0)\n  broadcast.1 = f32[1,22]{1,0} broadcast(constant.1), dimensions={}\n  ROOT max.1 = f32[1,22]{1,0} maximum(Arg_0.1, broadcast.1)\n}\n\nENTRY main.2 {\n  x.1 = f32[1,16]{1,0} parameter(0)\n  w.1 = f32[16,22]{1,0} parameter(1)\n  dot_general.1 = f32[1,22]{1,0} dot(x.1, w.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n  b.1 = f32[22]{0} parameter(2)\n  broadcast_in_dim.1 = f32[1,22]{1,0} reshape(b.1)\n  add.1 = f32[1,22]{1,0} add(dot_general.1, broadcast_in_dim.1)\n  ROOT jit_relu_.1 = f32[1,22]{1,0} call(add.1), to_apply=relu.1\n}\n\n", "input_shapes": [[1, 16], [16, 22], [22]], "output_shape": [1, 22]}
{"id": 66, "name": "softplus_lweg", "python_source": "def softplus_lweg(x):\n    \"\"\"Softplus operation.\"\"\"\n    return jax.nn.softplus(x)", "jaxpr": "{ lambda ; a:f32[5,9]. let\n    b:f32[5,9] = jit[\n      name=softplus\n      jaxpr={ lambda ; a:f32[5,9]. let\n          b:f32[5,9] = custom_jvp_call[\n            name=logaddexp\n            call_jaxpr={ lambda ; c:f32[5,9] d:f32[]. let\n                e:f32[5,9] = max c d\n                f:f32[5,9] = sub c d\n                g:bool[5,9] = ne f f\n                h:f32[5,9] = add c d\n                i:f32[5,9] = abs f\n                j:f32[5,9] = neg i\n                k:f32[5,9] = exp j\n                l:f32[5,9] = log1p k\n                m:f32[5,9] = add e l\n                n:f32[5,9] = select_n g m h\n              in (n,) }\n            jvp=_logaddexp_jvp\n            symbolic_zeros=False\n          ] a 0.0:f32[]\n        in (b,) }\n    ] a\n  in (b,) }", "stablehlo": "module @jit_softplus_lweg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x9xf32>) -> (tensor<5x9xf32> {jax.result_info = \"result\"}) {\n    %0 = call @softplus(%arg0) : (tensor<5x9xf32>) -> tensor<5x9xf32>\n    return %0 : tensor<5x9xf32>\n  }\n  func.func private @softplus(%arg0: tensor<5x9xf32>) -> tensor<5x9xf32> {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<5x9xf32>\n    %1 = stablehlo.maximum %arg0, %0 : tensor<5x9xf32>\n    %2 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<5x9xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<5x9xf32>\n    %4 = stablehlo.compare  NE, %3, %3,  FLOAT : (tensor<5x9xf32>, tensor<5x9xf32>) -> tensor<5x9xi1>\n    %5 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<5x9xf32>\n    %6 = stablehlo.add %arg0, %5 : tensor<5x9xf32>\n    %7 = stablehlo.abs %3 : tensor<5x9xf32>\n    %8 = stablehlo.negate %7 : tensor<5x9xf32>\n    %9 = stablehlo.exponential %8 : tensor<5x9xf32>\n    %10 = stablehlo.log_plus_one %9 : tensor<5x9xf32>\n    %11 = stablehlo.add %1, %10 : tensor<5x9xf32>\n    %12 = stablehlo.select %4, %6, %11 : tensor<5x9xi1>, tensor<5x9xf32>\n    return %12 : tensor<5x9xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_softplus_lweg, entry_computation_layout={(f32[5,9]{1,0})->f32[5,9]{1,0}}\n\nsoftplus.1 {\n  Arg_0.1 = f32[5,9]{1,0} parameter(0)\n  ne.1 = pred[5,9]{1,0} compare(Arg_0.1, Arg_0.1), direction=NE\n  constant.1 = f32[] constant(0)\n  max.2 = f32[5,9]{1,0} broadcast(constant.1), dimensions={}\n  max.3 = f32[5,9]{1,0} maximum(Arg_0.1, max.2)\n  abs.1 = f32[5,9]{1,0} abs(Arg_0.1)\n  neg.1 = f32[5,9]{1,0} negate(abs.1)\n  exp.1 = f32[5,9]{1,0} exponential(neg.1)\n  log1p.1 = f32[5,9]{1,0} log-plus-one(exp.1)\n  add.1 = f32[5,9]{1,0} add(max.3, log1p.1)\n  ROOT select_n.1 = f32[5,9]{1,0} select(ne.1, Arg_0.1, add.1)\n}\n\nENTRY main.2 {\n  x.1 = f32[5,9]{1,0} parameter(0)\n  ROOT jit_softplus_.1 = f32[5,9]{1,0} call(x.1), to_apply=softplus.1\n}\n\n", "input_shapes": [[5, 9]], "output_shape": [5, 9]}
{"id": 67, "name": "max_tjtv", "python_source": "def max_tjtv(x):\n    \"\"\"Max reduction along axis 0.\"\"\"\n    return jnp.max(x, axis=0)", "jaxpr": "{ lambda ; a:f32[15,3,9]. let b:f32[3,9] = reduce_max[axes=(0,)] a in (b,) }", "stablehlo": "module @jit_max_tjtv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<15x3x9xf32>) -> (tensor<3x9xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0xFF800000> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.maximum across dimensions = [0] : (tensor<15x3x9xf32>, tensor<f32>) -> tensor<3x9xf32>\n    return %0 : tensor<3x9xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_max_tjtv, entry_computation_layout={(f32[15,3,9]{2,1,0})->f32[3,9]{1,0}}\n\nregion_0.1 {\n  reduce_max.3 = f32[] parameter(0)\n  reduce_max.4 = f32[] parameter(1)\n  ROOT reduce_max.5 = f32[] maximum(reduce_max.3, reduce_max.4)\n}\n\nENTRY main.2 {\n  x.1 = f32[15,3,9]{2,1,0} parameter(0)\n  constant.1 = f32[] constant(-inf)\n  ROOT reduce_max.7 = f32[3,9]{1,0} reduce(x.1, constant.1), dimensions={0}, to_apply=region_0.1\n}\n\n", "input_shapes": [[15, 3, 9]], "output_shape": [3, 9]}
{"id": 68, "name": "nn_layer_ozjq", "python_source": "def nn_layer_ozjq(x, w, b):\n    \"\"\"Neural network layer with sigmoid activation.\"\"\"\n    return jax.nn.sigmoid(jnp.dot(x, w) + b)", "jaxpr": "{ lambda ; a:f32[1,28] b:f32[28,4] c:f32[4]. let\n    d:f32[1,4] = dot_general[\n      dimension_numbers=(([1], [0]), ([], []))\n      preferred_element_type=float32\n    ] a b\n    e:f32[1,4] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 4)\n      sharding=None\n    ] c\n    f:f32[1,4] = add d e\n    g:f32[1,4] = logistic f\n  in (g,) }", "stablehlo": "module @jit_nn_layer_ozjq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<1x28xf32>, %arg1: tensor<28x4xf32>, %arg2: tensor<4xf32>) -> (tensor<1x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.dot_general %arg0, %arg1, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<1x28xf32>, tensor<28x4xf32>) -> tensor<1x4xf32>\n    %1 = stablehlo.broadcast_in_dim %arg2, dims = [1] : (tensor<4xf32>) -> tensor<1x4xf32>\n    %2 = stablehlo.add %0, %1 : tensor<1x4xf32>\n    %3 = stablehlo.negate %2 : tensor<1x4xf32>\n    %4 = stablehlo.exponential %3 : tensor<1x4xf32>\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %5 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<1x4xf32>\n    %6 = stablehlo.add %5, %4 : tensor<1x4xf32>\n    %cst_0 = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %7 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<1x4xf32>\n    %8 = stablehlo.divide %7, %6 : tensor<1x4xf32>\n    return %8 : tensor<1x4xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_nn_layer_ozjq, entry_computation_layout={(f32[1,28]{1,0}, f32[28,4]{1,0}, f32[4]{0})->f32[1,4]{1,0}}\n\nENTRY main.1 {\n  constant.1 = f32[] constant(1)\n  broadcast.1 = f32[1,4]{1,0} broadcast(constant.1), dimensions={}\n  x.1 = f32[1,28]{1,0} parameter(0)\n  w.1 = f32[28,4]{1,0} parameter(1)\n  dot_general.1 = f32[1,4]{1,0} dot(x.1, w.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n  b.1 = f32[4]{0} parameter(2)\n  broadcast_in_dim.1 = f32[1,4]{1,0} reshape(b.1)\n  add.2 = f32[1,4]{1,0} add(dot_general.1, broadcast_in_dim.1)\n  neg.1 = f32[1,4]{1,0} negate(add.2)\n  exp.1 = f32[1,4]{1,0} exponential(neg.1)\n  add.3 = f32[1,4]{1,0} add(exp.1, broadcast.1)\n  ROOT div.1 = f32[1,4]{1,0} divide(broadcast.1, add.3)\n}\n\n", "input_shapes": [[1, 28], [28, 4], [4]], "output_shape": [1, 4]}
{"id": 69, "name": "min_addr", "python_source": "def min_addr(x, y):\n    \"\"\"Element-wise min operation.\"\"\"\n    return jnp.minimum(x, y)", "jaxpr": "{ lambda ; a:f32[5,9,13] b:f32[5,9,13]. let c:f32[5,9,13] = min a b in (c,) }", "stablehlo": "module @jit_min_addr attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x9x13xf32>, %arg1: tensor<5x9x13xf32>) -> (tensor<5x9x13xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.minimum %arg0, %arg1 : tensor<5x9x13xf32>\n    return %0 : tensor<5x9x13xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_min_addr, entry_computation_layout={(f32[5,9,13]{2,1,0}, f32[5,9,13]{2,1,0})->f32[5,9,13]{2,1,0}}\n\nENTRY main.1 {\n  x.1 = f32[5,9,13]{2,1,0} parameter(0)\n  y.1 = f32[5,9,13]{2,1,0} parameter(1)\n  ROOT min.1 = f32[5,9,13]{2,1,0} minimum(x.1, y.1)\n}\n\n", "input_shapes": [[5, 9, 13], [5, 9, 13]], "output_shape": [5, 9, 13]}
{"id": 70, "name": "max_eptc", "python_source": "def max_eptc(x):\n    \"\"\"Max reduction along axis 0.\"\"\"\n    return jnp.max(x, axis=0)", "jaxpr": "{ lambda ; a:f32[6,3]. let b:f32[3] = reduce_max[axes=(0,)] a in (b,) }", "stablehlo": "module @jit_max_eptc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x3xf32>) -> (tensor<3xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0xFF800000> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.maximum across dimensions = [0] : (tensor<6x3xf32>, tensor<f32>) -> tensor<3xf32>\n    return %0 : tensor<3xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_max_eptc, entry_computation_layout={(f32[6,3]{1,0})->f32[3]{0}}\n\nregion_0.1 {\n  reduce_max.3 = f32[] parameter(0)\n  reduce_max.4 = f32[] parameter(1)\n  ROOT reduce_max.5 = f32[] maximum(reduce_max.3, reduce_max.4)\n}\n\nENTRY main.2 {\n  x.1 = f32[6,3]{1,0} parameter(0)\n  constant.1 = f32[] constant(-inf)\n  ROOT reduce_max.7 = f32[3]{0} reduce(x.1, constant.1), dimensions={0}, to_apply=region_0.1\n}\n\n", "input_shapes": [[6, 3]], "output_shape": [3]}
{"id": 71, "name": "prod_ryks", "python_source": "def prod_ryks(x):\n    \"\"\"Product reduction along axis 0.\"\"\"\n    return jnp.prod(x, axis=0)", "jaxpr": "{ lambda ; a:f32[11,13]. let b:f32[13] = reduce_prod[axes=(0,)] a in (b,) }", "stablehlo": "module @jit_prod_ryks attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<11x13xf32>) -> (tensor<13xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.multiply across dimensions = [0] : (tensor<11x13xf32>, tensor<f32>) -> tensor<13xf32>\n    return %0 : tensor<13xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_prod_ryks, entry_computation_layout={(f32[11,13]{1,0})->f32[13]{0}}\n\nregion_0.1 {\n  reduce_prod.3 = f32[] parameter(0)\n  reduce_prod.4 = f32[] parameter(1)\n  ROOT reduce_prod.5 = f32[] multiply(reduce_prod.3, reduce_prod.4)\n}\n\nENTRY main.2 {\n  x.1 = f32[11,13]{1,0} parameter(0)\n  constant.1 = f32[] constant(1)\n  ROOT reduce_prod.7 = f32[13]{0} reduce(x.1, constant.1), dimensions={0}, to_apply=region_0.1\n}\n\n", "input_shapes": [[11, 13]], "output_shape": [13]}
{"id": 72, "name": "square_uwpd", "python_source": "def square_uwpd(x):\n    \"\"\"Square operation.\"\"\"\n    return x ** 2", "jaxpr": "{ lambda ; a:f32[14,2,7]. let b:f32[14,2,7] = integer_pow[y=2] a in (b,) }", "stablehlo": "module @jit_square_uwpd attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<14x2x7xf32>) -> (tensor<14x2x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<14x2x7xf32>\n    return %0 : tensor<14x2x7xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_square_uwpd, entry_computation_layout={(f32[14,2,7]{2,1,0})->f32[14,2,7]{2,1,0}}\n\nENTRY main.1 {\n  x.1 = f32[14,2,7]{2,1,0} parameter(0)\n  ROOT integer_pow.1 = f32[14,2,7]{2,1,0} multiply(x.1, x.1)\n}\n\n", "input_shapes": [[14, 2, 7]], "output_shape": [14, 2, 7]}
{"id": 73, "name": "composite_ljni", "python_source": "def composite_ljni(x):\n    \"\"\"Composite function with multiple operations.\"\"\"\n    return jax.nn.relu(x) + x ** 2 + jnp.tanh(x)", "jaxpr": "{ lambda ; a:f32[15,8]. let\n    b:f32[15,8] = custom_jvp_call[\n      name=relu\n      call_jaxpr={ lambda ; c:f32[15,8]. let\n          d:f32[15,8] = jit[\n            name=relu\n            jaxpr={ lambda ; c:f32[15,8]. let\n                d:f32[15,8] = max c 0.0:f32[]\n              in (d,) }\n          ] c\n        in (d,) }\n      jvp=jvp\n      symbolic_zeros=False\n    ] a\n    e:f32[15,8] = integer_pow[y=2] a\n    f:f32[15,8] = add b e\n    g:f32[15,8] = tanh a\n    h:f32[15,8] = add f g\n  in (h,) }", "stablehlo": "module @jit_composite_ljni attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<15x8xf32>) -> (tensor<15x8xf32> {jax.result_info = \"result\"}) {\n    %0 = call @relu(%arg0) : (tensor<15x8xf32>) -> tensor<15x8xf32>\n    %1 = stablehlo.multiply %arg0, %arg0 : tensor<15x8xf32>\n    %2 = stablehlo.add %0, %1 : tensor<15x8xf32>\n    %3 = stablehlo.tanh %arg0 : tensor<15x8xf32>\n    %4 = stablehlo.add %2, %3 : tensor<15x8xf32>\n    return %4 : tensor<15x8xf32>\n  }\n  func.func private @relu(%arg0: tensor<15x8xf32>) -> tensor<15x8xf32> {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<15x8xf32>\n    %1 = stablehlo.maximum %arg0, %0 : tensor<15x8xf32>\n    return %1 : tensor<15x8xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_composite_ljni, entry_computation_layout={(f32[15,8]{1,0})->f32[15,8]{1,0}}\n\nrelu.1 {\n  Arg_0.1 = f32[15,8]{1,0} parameter(0)\n  constant.1 = f32[] constant(0)\n  max.2 = f32[15,8]{1,0} broadcast(constant.1), dimensions={}\n  ROOT max.3 = f32[15,8]{1,0} maximum(Arg_0.1, max.2)\n}\n\nENTRY main.2 {\n  x.1 = f32[15,8]{1,0} parameter(0)\n  jit_relu_.1 = f32[15,8]{1,0} call(x.1), to_apply=relu.1\n  integer_pow.1 = f32[15,8]{1,0} multiply(x.1, x.1)\n  add.2 = f32[15,8]{1,0} add(jit_relu_.1, integer_pow.1)\n  tanh.1 = f32[15,8]{1,0} tanh(x.1)\n  ROOT add.3 = f32[15,8]{1,0} add(add.2, tanh.1)\n}\n\n", "input_shapes": [[15, 8]], "output_shape": [15, 8]}
{"id": 74, "name": "nn_layer_xlde", "python_source": "def nn_layer_xlde(x, w, b):\n    \"\"\"Neural network layer with gelu activation.\"\"\"\n    return jax.nn.gelu(jnp.dot(x, w) + b)", "jaxpr": "{ lambda ; a:f32[5,23] b:f32[23,29] c:f32[29]. let\n    d:f32[5,29] = dot_general[\n      dimension_numbers=(([1], [0]), ([], []))\n      preferred_element_type=float32\n    ] a b\n    e:f32[1,29] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 29)\n      sharding=None\n    ] c\n    f:f32[5,29] = add d e\n    g:f32[5,29] = integer_pow[y=3] f\n    h:f32[5,29] = mul 0.044714998453855515:f32[] g\n    i:f32[5,29] = add f h\n    j:f32[5,29] = mul 0.7978845834732056:f32[] i\n    k:f32[5,29] = tanh j\n    l:f32[5,29] = add 1.0:f32[] k\n    m:f32[5,29] = mul 0.5:f32[] l\n    n:f32[5,29] = mul f m\n  in (n,) }", "stablehlo": "module @jit_nn_layer_xlde attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x23xf32>, %arg1: tensor<23x29xf32>, %arg2: tensor<29xf32>) -> (tensor<5x29xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.dot_general %arg0, %arg1, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<5x23xf32>, tensor<23x29xf32>) -> tensor<5x29xf32>\n    %1 = stablehlo.broadcast_in_dim %arg2, dims = [1] : (tensor<29xf32>) -> tensor<1x29xf32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x29xf32>) -> tensor<5x29xf32>\n    %3 = stablehlo.add %0, %2 : tensor<5x29xf32>\n    %4 = stablehlo.multiply %3, %3 : tensor<5x29xf32>\n    %5 = stablehlo.multiply %4, %3 : tensor<5x29xf32>\n    %cst = stablehlo.constant dense<4.471500e-02> : tensor<f32>\n    %6 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<5x29xf32>\n    %7 = stablehlo.multiply %6, %5 : tensor<5x29xf32>\n    %8 = stablehlo.add %3, %7 : tensor<5x29xf32>\n    %cst_0 = stablehlo.constant dense<0.797884583> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<5x29xf32>\n    %10 = stablehlo.multiply %9, %8 : tensor<5x29xf32>\n    %11 = stablehlo.tanh %10 : tensor<5x29xf32>\n    %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %12 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<5x29xf32>\n    %13 = stablehlo.add %12, %11 : tensor<5x29xf32>\n    %cst_2 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %14 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<5x29xf32>\n    %15 = stablehlo.multiply %14, %13 : tensor<5x29xf32>\n    %16 = stablehlo.multiply %3, %15 : tensor<5x29xf32>\n    return %16 : tensor<5x29xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_nn_layer_xlde, entry_computation_layout={(f32[5,23]{1,0}, f32[23,29]{1,0}, f32[29]{0})->f32[5,29]{1,0}}\n\nENTRY main.1 {\n  x.1 = f32[5,23]{1,0} parameter(0)\n  w.1 = f32[23,29]{1,0} parameter(1)\n  dot_general.1 = f32[5,29]{1,0} dot(x.1, w.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n  b.1 = f32[29]{0} parameter(2)\n  broadcast_in_dim.1 = f32[1,29]{1,0} reshape(b.1)\n  add.8 = f32[1,29]{1,0} broadcast(broadcast_in_dim.1), dimensions={0,1}\n  add.9 = f32[29]{0} reshape(add.8)\n  add.10 = f32[5,29]{1,0} broadcast(add.9), dimensions={1}\n  add.11 = f32[5,29]{1,0} add(dot_general.1, add.10)\n  integer_pow.2 = f32[5,29]{1,0} multiply(add.11, add.11)\n  integer_pow.3 = f32[5,29]{1,0} multiply(integer_pow.2, add.11)\n  constant.7 = f32[] constant(0.044715)\n  mul.9 = f32[5,29]{1,0} broadcast(constant.7), dimensions={}\n  mul.10 = f32[5,29]{1,0} multiply(integer_pow.3, mul.9)\n  add.12 = f32[5,29]{1,0} add(add.11, mul.10)\n  constant.6 = f32[] constant(0.797884583)\n  mul.8 = f32[5,29]{1,0} broadcast(constant.6), dimensions={}\n  mul.11 = f32[5,29]{1,0} multiply(add.12, mul.8)\n  tanh.1 = f32[5,29]{1,0} tanh(mul.11)\n  constant.5 = f32[] constant(1)\n  add.7 = f32[5,29]{1,0} broadcast(constant.5), dimensions={}\n  add.13 = f32[5,29]{1,0} add(tanh.1, add.7)\n  constant.4 = f32[] constant(0.5)\n  mul.7 = f32[5,29]{1,0} broadcast(constant.4), dimensions={}\n  mul.12 = f32[5,29]{1,0} multiply(add.13, mul.7)\n  ROOT mul.13 = f32[5,29]{1,0} multiply(add.11, mul.12)\n}\n\n", "input_shapes": [[5, 23], [23, 29], [29]], "output_shape": [5, 29]}
{"id": 75, "name": "matmul_ltgw", "python_source": "def matmul_ltgw(a, b):\n    \"\"\"Matrix multiplication of shapes (15,16) @ (16,3).\"\"\"\n    return jnp.dot(a, b)", "jaxpr": "{ lambda ; a:f32[15,16] b:f32[16,3]. let\n    c:f32[15,3] = dot_general[\n      dimension_numbers=(([1], [0]), ([], []))\n      preferred_element_type=float32\n    ] a b\n  in (c,) }", "stablehlo": "module @jit_matmul_ltgw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<15x16xf32>, %arg1: tensor<16x3xf32>) -> (tensor<15x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.dot_general %arg0, %arg1, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<15x16xf32>, tensor<16x3xf32>) -> tensor<15x3xf32>\n    return %0 : tensor<15x3xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_matmul_ltgw, entry_computation_layout={(f32[15,16]{1,0}, f32[16,3]{1,0})->f32[15,3]{1,0}}\n\nENTRY main.1 {\n  a.1 = f32[15,16]{1,0} parameter(0)\n  b.1 = f32[16,3]{1,0} parameter(1)\n  ROOT dot_general.1 = f32[15,3]{1,0} dot(a.1, b.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n}\n\n", "input_shapes": [[15, 16], [16, 3]], "output_shape": [15, 3]}
{"id": 76, "name": "min_gmpj", "python_source": "def min_gmpj(x):\n    \"\"\"Min reduction along axis 1.\"\"\"\n    return jnp.min(x, axis=1)", "jaxpr": "{ lambda ; a:f32[9,15]. let b:f32[9] = reduce_min[axes=(1,)] a in (b,) }", "stablehlo": "module @jit_min_gmpj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x15xf32>) -> (tensor<9xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0x7F800000> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.minimum across dimensions = [1] : (tensor<9x15xf32>, tensor<f32>) -> tensor<9xf32>\n    return %0 : tensor<9xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_min_gmpj, entry_computation_layout={(f32[9,15]{1,0})->f32[9]{0}}\n\nregion_0.1 {\n  reduce_min.3 = f32[] parameter(0)\n  reduce_min.4 = f32[] parameter(1)\n  ROOT reduce_min.5 = f32[] minimum(reduce_min.3, reduce_min.4)\n}\n\nENTRY main.2 {\n  x.1 = f32[9,15]{1,0} parameter(0)\n  constant.1 = f32[] constant(inf)\n  ROOT reduce_min.7 = f32[9]{0} reduce(x.1, constant.1), dimensions={1}, to_apply=region_0.1\n}\n\n", "input_shapes": [[9, 15]], "output_shape": [9]}
{"id": 77, "name": "div_fuiw", "python_source": "def div_fuiw(x, y):\n    \"\"\"Division (safe) operation.\"\"\"\n    return x / (y + 1e-6)", "jaxpr": "{ lambda ; a:f32[16,6,10] b:f32[16,6,10]. let\n    c:f32[16,6,10] = add b 9.999999974752427e-07:f32[]\n    d:f32[16,6,10] = div a c\n  in (d,) }", "stablehlo": "module @jit_div_fuiw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<16x6x10xf32>, %arg1: tensor<16x6x10xf32>) -> (tensor<16x6x10xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<9.99999997E-7> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<16x6x10xf32>\n    %1 = stablehlo.add %arg1, %0 : tensor<16x6x10xf32>\n    %2 = stablehlo.divide %arg0, %1 : tensor<16x6x10xf32>\n    return %2 : tensor<16x6x10xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_div_fuiw, entry_computation_layout={(f32[16,6,10]{2,1,0}, f32[16,6,10]{2,1,0})->f32[16,6,10]{2,1,0}}\n\nENTRY main.1 {\n  x.1 = f32[16,6,10]{2,1,0} parameter(0)\n  y.1 = f32[16,6,10]{2,1,0} parameter(1)\n  constant.1 = f32[] constant(1e-06)\n  add.2 = f32[16,6,10]{2,1,0} broadcast(constant.1), dimensions={}\n  add.3 = f32[16,6,10]{2,1,0} add(y.1, add.2)\n  ROOT div.1 = f32[16,6,10]{2,1,0} divide(x.1, add.3)\n}\n\n", "input_shapes": [[16, 6, 10], [16, 6, 10]], "output_shape": [16, 6, 10]}
{"id": 78, "name": "relu_ycsm", "python_source": "def relu_ycsm(x):\n    \"\"\"ReLU activation operation.\"\"\"\n    return jax.nn.relu(x)", "jaxpr": "{ lambda ; a:f32[13]. let\n    b:f32[13] = custom_jvp_call[\n      name=relu\n      call_jaxpr={ lambda ; c:f32[13]. let\n          d:f32[13] = jit[\n            name=relu\n            jaxpr={ lambda ; c:f32[13]. let\n                d:f32[13] = max c 0.0:f32[]\n              in (d,) }\n          ] c\n        in (d,) }\n      jvp=jvp\n      symbolic_zeros=False\n    ] a\n  in (b,) }", "stablehlo": "module @jit_relu_ycsm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<13xf32>) -> (tensor<13xf32> {jax.result_info = \"result\"}) {\n    %0 = call @relu(%arg0) : (tensor<13xf32>) -> tensor<13xf32>\n    return %0 : tensor<13xf32>\n  }\n  func.func private @relu(%arg0: tensor<13xf32>) -> tensor<13xf32> {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<13xf32>\n    %1 = stablehlo.maximum %arg0, %0 : tensor<13xf32>\n    return %1 : tensor<13xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_relu_ycsm, entry_computation_layout={(f32[13]{0})->f32[13]{0}}\n\nrelu.1 {\n  Arg_0.1 = f32[13]{0} parameter(0)\n  constant.1 = f32[] constant(0)\n  broadcast.1 = f32[13]{0} broadcast(constant.1), dimensions={}\n  ROOT max.1 = f32[13]{0} maximum(Arg_0.1, broadcast.1)\n}\n\nENTRY main.2 {\n  x.1 = f32[13]{0} parameter(0)\n  ROOT jit_relu_.1 = f32[13]{0} call(x.1), to_apply=relu.1\n}\n\n", "input_shapes": [[13]], "output_shape": [13]}
{"id": 79, "name": "matmul_qmuc", "python_source": "def matmul_qmuc(a, b):\n    \"\"\"Matrix multiplication of shapes (5,8) @ (8,13).\"\"\"\n    return jnp.dot(a, b)", "jaxpr": "{ lambda ; a:f32[5,8] b:f32[8,13]. let\n    c:f32[5,13] = dot_general[\n      dimension_numbers=(([1], [0]), ([], []))\n      preferred_element_type=float32\n    ] a b\n  in (c,) }", "stablehlo": "module @jit_matmul_qmuc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x8xf32>, %arg1: tensor<8x13xf32>) -> (tensor<5x13xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.dot_general %arg0, %arg1, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<5x8xf32>, tensor<8x13xf32>) -> tensor<5x13xf32>\n    return %0 : tensor<5x13xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_matmul_qmuc, entry_computation_layout={(f32[5,8]{1,0}, f32[8,13]{1,0})->f32[5,13]{1,0}}\n\nENTRY main.1 {\n  a.1 = f32[5,8]{1,0} parameter(0)\n  b.1 = f32[8,13]{1,0} parameter(1)\n  ROOT dot_general.1 = f32[5,13]{1,0} dot(a.1, b.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n}\n\n", "input_shapes": [[5, 8], [8, 13]], "output_shape": [5, 13]}
{"id": 80, "name": "mul_rjoi", "python_source": "def mul_rjoi(x, y):\n    \"\"\"Multiplication operation.\"\"\"\n    return x * y", "jaxpr": "{ lambda ; a:f32[10,7,7] b:f32[10,7,7]. let c:f32[10,7,7] = mul a b in (c,) }", "stablehlo": "module @jit_mul_rjoi attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x7x7xf32>, %arg1: tensor<10x7x7xf32>) -> (tensor<10x7x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<10x7x7xf32>\n    return %0 : tensor<10x7x7xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_mul_rjoi, entry_computation_layout={(f32[10,7,7]{2,1,0}, f32[10,7,7]{2,1,0})->f32[10,7,7]{2,1,0}}\n\nENTRY main.1 {\n  x.1 = f32[10,7,7]{2,1,0} parameter(0)\n  y.1 = f32[10,7,7]{2,1,0} parameter(1)\n  ROOT mul.1 = f32[10,7,7]{2,1,0} multiply(x.1, y.1)\n}\n\n", "input_shapes": [[10, 7, 7], [10, 7, 7]], "output_shape": [10, 7, 7]}
{"id": 81, "name": "nn_layer_lhfs", "python_source": "def nn_layer_lhfs(x, w, b):\n    \"\"\"Neural network layer with sigmoid activation.\"\"\"\n    return jax.nn.sigmoid(jnp.dot(x, w) + b)", "jaxpr": "{ lambda ; a:f32[6,7] b:f32[7,27] c:f32[27]. let\n    d:f32[6,27] = dot_general[\n      dimension_numbers=(([1], [0]), ([], []))\n      preferred_element_type=float32\n    ] a b\n    e:f32[1,27] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 27)\n      sharding=None\n    ] c\n    f:f32[6,27] = add d e\n    g:f32[6,27] = logistic f\n  in (g,) }", "stablehlo": "module @jit_nn_layer_lhfs attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x7xf32>, %arg1: tensor<7x27xf32>, %arg2: tensor<27xf32>) -> (tensor<6x27xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.dot_general %arg0, %arg1, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<6x7xf32>, tensor<7x27xf32>) -> tensor<6x27xf32>\n    %1 = stablehlo.broadcast_in_dim %arg2, dims = [1] : (tensor<27xf32>) -> tensor<1x27xf32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x27xf32>) -> tensor<6x27xf32>\n    %3 = stablehlo.add %0, %2 : tensor<6x27xf32>\n    %4 = stablehlo.negate %3 : tensor<6x27xf32>\n    %5 = stablehlo.exponential %4 : tensor<6x27xf32>\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %6 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<6x27xf32>\n    %7 = stablehlo.add %6, %5 : tensor<6x27xf32>\n    %cst_0 = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %8 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<6x27xf32>\n    %9 = stablehlo.divide %8, %7 : tensor<6x27xf32>\n    return %9 : tensor<6x27xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_nn_layer_lhfs, entry_computation_layout={(f32[6,7]{1,0}, f32[7,27]{1,0}, f32[27]{0})->f32[6,27]{1,0}}\n\nENTRY main.1 {\n  constant.1 = f32[] constant(1)\n  broadcast.1 = f32[6,27]{1,0} broadcast(constant.1), dimensions={}\n  x.1 = f32[6,7]{1,0} parameter(0)\n  w.1 = f32[7,27]{1,0} parameter(1)\n  dot_general.1 = f32[6,27]{1,0} dot(x.1, w.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n  b.1 = f32[27]{0} parameter(2)\n  broadcast_in_dim.1 = f32[1,27]{1,0} reshape(b.1)\n  add.5 = f32[1,27]{1,0} broadcast(broadcast_in_dim.1), dimensions={0,1}\n  add.6 = f32[27]{0} reshape(add.5)\n  add.7 = f32[6,27]{1,0} broadcast(add.6), dimensions={1}\n  add.8 = f32[6,27]{1,0} add(dot_general.1, add.7)\n  neg.1 = f32[6,27]{1,0} negate(add.8)\n  exp.1 = f32[6,27]{1,0} exponential(neg.1)\n  add.9 = f32[6,27]{1,0} add(exp.1, broadcast.1)\n  ROOT div.1 = f32[6,27]{1,0} divide(broadcast.1, add.9)\n}\n\n", "input_shapes": [[6, 7], [7, 27], [27]], "output_shape": [6, 27]}
{"id": 82, "name": "sub_thpt", "python_source": "def sub_thpt(x, y):\n    \"\"\"Subtraction operation.\"\"\"\n    return x - y", "jaxpr": "{ lambda ; a:f32[6,3,15] b:f32[6,3,15]. let c:f32[6,3,15] = sub a b in (c,) }", "stablehlo": "module @jit_sub_thpt attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x3x15xf32>, %arg1: tensor<6x3x15xf32>) -> (tensor<6x3x15xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<6x3x15xf32>\n    return %0 : tensor<6x3x15xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_sub_thpt, entry_computation_layout={(f32[6,3,15]{2,1,0}, f32[6,3,15]{2,1,0})->f32[6,3,15]{2,1,0}}\n\nENTRY main.1 {\n  x.1 = f32[6,3,15]{2,1,0} parameter(0)\n  y.1 = f32[6,3,15]{2,1,0} parameter(1)\n  ROOT sub.1 = f32[6,3,15]{2,1,0} subtract(x.1, y.1)\n}\n\n", "input_shapes": [[6, 3, 15], [6, 3, 15]], "output_shape": [6, 3, 15]}
{"id": 83, "name": "mul_fean", "python_source": "def mul_fean(x, y):\n    \"\"\"Multiplication operation.\"\"\"\n    return x * y", "jaxpr": "{ lambda ; a:f32[2,2] b:f32[2,2]. let c:f32[2,2] = mul a b in (c,) }", "stablehlo": "module @jit_mul_fean attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x2xf32>, %arg1: tensor<2x2xf32>) -> (tensor<2x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<2x2xf32>\n    return %0 : tensor<2x2xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_mul_fean, entry_computation_layout={(f32[2,2]{1,0}, f32[2,2]{1,0})->f32[2,2]{1,0}}\n\nENTRY main.1 {\n  x.1 = f32[2,2]{1,0} parameter(0)\n  y.1 = f32[2,2]{1,0} parameter(1)\n  ROOT mul.1 = f32[2,2]{1,0} multiply(x.1, y.1)\n}\n\n", "input_shapes": [[2, 2], [2, 2]], "output_shape": [2, 2]}
{"id": 84, "name": "composite_hyqt", "python_source": "def composite_hyqt(x):\n    \"\"\"Composite function with multiple operations.\"\"\"\n    return jnp.exp(-jnp.abs(x)) + jnp.sin(x)", "jaxpr": "{ lambda ; a:f32[3,15]. let\n    b:f32[3,15] = abs a\n    c:f32[3,15] = neg b\n    d:f32[3,15] = exp c\n    e:f32[3,15] = sin a\n    f:f32[3,15] = add d e\n  in (f,) }", "stablehlo": "module @jit_composite_hyqt attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x15xf32>) -> (tensor<3x15xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.abs %arg0 : tensor<3x15xf32>\n    %1 = stablehlo.negate %0 : tensor<3x15xf32>\n    %2 = stablehlo.exponential %1 : tensor<3x15xf32>\n    %3 = stablehlo.sine %arg0 : tensor<3x15xf32>\n    %4 = stablehlo.add %2, %3 : tensor<3x15xf32>\n    return %4 : tensor<3x15xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_composite_hyqt, entry_computation_layout={(f32[3,15]{1,0})->f32[3,15]{1,0}}\n\nENTRY main.1 {\n  x.1 = f32[3,15]{1,0} parameter(0)\n  abs.1 = f32[3,15]{1,0} abs(x.1)\n  neg.1 = f32[3,15]{1,0} negate(abs.1)\n  exp.1 = f32[3,15]{1,0} exponential(neg.1)\n  sin.1 = f32[3,15]{1,0} sine(x.1)\n  ROOT add.1 = f32[3,15]{1,0} add(exp.1, sin.1)\n}\n\n", "input_shapes": [[3, 15]], "output_shape": [3, 15]}
{"id": 85, "name": "matmul_mizg", "python_source": "def matmul_mizg(a, b):\n    \"\"\"Matrix multiplication of shapes (15,9) @ (9,3).\"\"\"\n    return jnp.dot(a, b)", "jaxpr": "{ lambda ; a:f32[15,9] b:f32[9,3]. let\n    c:f32[15,3] = dot_general[\n      dimension_numbers=(([1], [0]), ([], []))\n      preferred_element_type=float32\n    ] a b\n  in (c,) }", "stablehlo": "module @jit_matmul_mizg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<15x9xf32>, %arg1: tensor<9x3xf32>) -> (tensor<15x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.dot_general %arg0, %arg1, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<15x9xf32>, tensor<9x3xf32>) -> tensor<15x3xf32>\n    return %0 : tensor<15x3xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_matmul_mizg, entry_computation_layout={(f32[15,9]{1,0}, f32[9,3]{1,0})->f32[15,3]{1,0}}\n\nENTRY main.1 {\n  a.1 = f32[15,9]{1,0} parameter(0)\n  b.1 = f32[9,3]{1,0} parameter(1)\n  ROOT dot_general.1 = f32[15,3]{1,0} dot(a.1, b.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n}\n\n", "input_shapes": [[15, 9], [9, 3]], "output_shape": [15, 3]}
{"id": 86, "name": "vmap_bmpr", "python_source": "def vmap_bmpr(a_batch, b_batch):\n    \"\"\"Batched operation using vmap.\"\"\"\n    def inner(a, b):\n        return jnp.maximum(a, b)\n    return jax.vmap(inner)(a_batch, b_batch)", "jaxpr": "{ lambda ; a:f32[3,6] b:f32[3,6]. let c:f32[3,6] = max a b in (c,) }", "stablehlo": "module @jit_vmap_bmpr attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x6xf32>, %arg1: tensor<3x6xf32>) -> (tensor<3x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.maximum %arg0, %arg1 : tensor<3x6xf32>\n    return %0 : tensor<3x6xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_vmap_bmpr, entry_computation_layout={(f32[3,6]{1,0}, f32[3,6]{1,0})->f32[3,6]{1,0}}\n\nENTRY main.1 {\n  a_batch.1 = f32[3,6]{1,0} parameter(0)\n  b_batch.1 = f32[3,6]{1,0} parameter(1)\n  ROOT max.1 = f32[3,6]{1,0} maximum(a_batch.1, b_batch.1)\n}\n\n", "input_shapes": [[3, 6], [3, 6]], "output_shape": [3, 6]}
{"id": 87, "name": "sum_zdtp", "python_source": "def sum_zdtp(x):\n    \"\"\"Sum reduction along axis 3.\"\"\"\n    return jnp.sum(x, axis=3)", "jaxpr": "{ lambda ; a:f32[5,14,10,8]. let\n    b:f32[5,14,10] = reduce_sum[axes=(3,) out_sharding=None] a\n  in (b,) }", "stablehlo": "module @jit_sum_zdtp attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x14x10x8xf32>) -> (tensor<5x14x10xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [3] : (tensor<5x14x10x8xf32>, tensor<f32>) -> tensor<5x14x10xf32>\n    return %0 : tensor<5x14x10xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_sum_zdtp, entry_computation_layout={(f32[5,14,10,8]{3,2,1,0})->f32[5,14,10]{2,1,0}}\n\nregion_0.1 {\n  reduce_sum.3 = f32[] parameter(0)\n  reduce_sum.4 = f32[] parameter(1)\n  ROOT reduce_sum.5 = f32[] add(reduce_sum.3, reduce_sum.4)\n}\n\nENTRY main.2 {\n  x.1 = f32[5,14,10,8]{3,2,1,0} parameter(0)\n  constant.1 = f32[] constant(0)\n  ROOT reduce_sum.7 = f32[5,14,10]{2,1,0} reduce(x.1, constant.1), dimensions={3}, to_apply=region_0.1\n}\n\n", "input_shapes": [[5, 14, 10, 8]], "output_shape": [5, 14, 10]}
{"id": 88, "name": "matmul_hplo", "python_source": "def matmul_hplo(a, b):\n    \"\"\"Matrix multiplication of shapes (2,11) @ (11,13).\"\"\"\n    return jnp.dot(a, b)", "jaxpr": "{ lambda ; a:f32[2,11] b:f32[11,13]. let\n    c:f32[2,13] = dot_general[\n      dimension_numbers=(([1], [0]), ([], []))\n      preferred_element_type=float32\n    ] a b\n  in (c,) }", "stablehlo": "module @jit_matmul_hplo attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x11xf32>, %arg1: tensor<11x13xf32>) -> (tensor<2x13xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.dot_general %arg0, %arg1, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x11xf32>, tensor<11x13xf32>) -> tensor<2x13xf32>\n    return %0 : tensor<2x13xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_matmul_hplo, entry_computation_layout={(f32[2,11]{1,0}, f32[11,13]{1,0})->f32[2,13]{1,0}}\n\nENTRY main.1 {\n  a.1 = f32[2,11]{1,0} parameter(0)\n  b.1 = f32[11,13]{1,0} parameter(1)\n  ROOT dot_general.1 = f32[2,13]{1,0} dot(a.1, b.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n}\n\n", "input_shapes": [[2, 11], [11, 13]], "output_shape": [2, 13]}
{"id": 89, "name": "log_qgcg", "python_source": "def log_qgcg(x):\n    \"\"\"Log (with safety) operation.\"\"\"\n    return jnp.log(jnp.abs(x) + 1e-6)", "jaxpr": "{ lambda ; a:f32[3,4,2]. let\n    b:f32[3,4,2] = abs a\n    c:f32[3,4,2] = add b 9.999999974752427e-07:f32[]\n    d:f32[3,4,2] = log c\n  in (d,) }", "stablehlo": "module @jit_log_qgcg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x4x2xf32>) -> (tensor<3x4x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.abs %arg0 : tensor<3x4x2xf32>\n    %cst = stablehlo.constant dense<9.99999997E-7> : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<3x4x2xf32>\n    %2 = stablehlo.add %0, %1 : tensor<3x4x2xf32>\n    %3 = stablehlo.log %2 : tensor<3x4x2xf32>\n    return %3 : tensor<3x4x2xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_log_qgcg, entry_computation_layout={(f32[3,4,2]{2,1,0})->f32[3,4,2]{2,1,0}}\n\nENTRY main.1 {\n  x.1 = f32[3,4,2]{2,1,0} parameter(0)\n  abs.1 = f32[3,4,2]{2,1,0} abs(x.1)\n  constant.1 = f32[] constant(1e-06)\n  broadcast.1 = f32[3,4,2]{2,1,0} broadcast(constant.1), dimensions={}\n  add.1 = f32[3,4,2]{2,1,0} add(abs.1, broadcast.1)\n  ROOT log.1 = f32[3,4,2]{2,1,0} log(add.1)\n}\n\n", "input_shapes": [[3, 4, 2]], "output_shape": [3, 4, 2]}
{"id": 90, "name": "matmul_lphg", "python_source": "def matmul_lphg(a, b):\n    \"\"\"Matrix multiplication of shapes (13,6) @ (6,13).\"\"\"\n    return jnp.dot(a, b)", "jaxpr": "{ lambda ; a:f32[13,6] b:f32[6,13]. let\n    c:f32[13,13] = dot_general[\n      dimension_numbers=(([1], [0]), ([], []))\n      preferred_element_type=float32\n    ] a b\n  in (c,) }", "stablehlo": "module @jit_matmul_lphg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<13x6xf32>, %arg1: tensor<6x13xf32>) -> (tensor<13x13xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.dot_general %arg0, %arg1, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<13x6xf32>, tensor<6x13xf32>) -> tensor<13x13xf32>\n    return %0 : tensor<13x13xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_matmul_lphg, entry_computation_layout={(f32[13,6]{1,0}, f32[6,13]{1,0})->f32[13,13]{1,0}}\n\nENTRY main.1 {\n  a.1 = f32[13,6]{1,0} parameter(0)\n  b.1 = f32[6,13]{1,0} parameter(1)\n  ROOT dot_general.1 = f32[13,13]{1,0} dot(a.1, b.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n}\n\n", "input_shapes": [[13, 6], [6, 13]], "output_shape": [13, 13]}
{"id": 91, "name": "vmap_lryu", "python_source": "def vmap_lryu(a_batch, b_batch):\n    \"\"\"Batched operation using vmap.\"\"\"\n    def inner(a, b):\n        return jnp.maximum(a, b)\n    return jax.vmap(inner)(a_batch, b_batch)", "jaxpr": "{ lambda ; a:f32[7,13] b:f32[7,13]. let c:f32[7,13] = max a b in (c,) }", "stablehlo": "module @jit_vmap_lryu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x13xf32>, %arg1: tensor<7x13xf32>) -> (tensor<7x13xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.maximum %arg0, %arg1 : tensor<7x13xf32>\n    return %0 : tensor<7x13xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_vmap_lryu, entry_computation_layout={(f32[7,13]{1,0}, f32[7,13]{1,0})->f32[7,13]{1,0}}\n\nENTRY main.1 {\n  a_batch.1 = f32[7,13]{1,0} parameter(0)\n  b_batch.1 = f32[7,13]{1,0} parameter(1)\n  ROOT max.1 = f32[7,13]{1,0} maximum(a_batch.1, b_batch.1)\n}\n\n", "input_shapes": [[7, 13], [7, 13]], "output_shape": [7, 13]}
{"id": 92, "name": "vmap_yloq", "python_source": "def vmap_yloq(a_batch, b_batch):\n    \"\"\"Batched operation using vmap.\"\"\"\n    def inner(a, b):\n        return jnp.dot(a, b)\n    return jax.vmap(inner)(a_batch, b_batch)", "jaxpr": "{ lambda ; a:f32[4,6] b:f32[4,6]. let\n    c:f32[4] = dot_general[\n      dimension_numbers=(([1], [1]), ([0], [0]))\n      preferred_element_type=float32\n    ] a b\n  in (c,) }", "stablehlo": "module @jit_vmap_yloq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x6xf32>, %arg1: tensor<4x6xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.dot_general %arg0, %arg1, batching_dims = [0] x [0], contracting_dims = [1] x [1], precision = [DEFAULT, DEFAULT] : (tensor<4x6xf32>, tensor<4x6xf32>) -> tensor<4xf32>\n    return %0 : tensor<4xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_vmap_yloq, entry_computation_layout={(f32[4,6]{1,0}, f32[4,6]{1,0})->f32[4]{0}}\n\nENTRY main.1 {\n  a_batch.1 = f32[4,6]{1,0} parameter(0)\n  b_batch.1 = f32[4,6]{1,0} parameter(1)\n  ROOT dot_general.1 = f32[4]{0} dot(a_batch.1, b_batch.1), lhs_batch_dims={0}, lhs_contracting_dims={1}, rhs_batch_dims={0}, rhs_contracting_dims={1}\n}\n\n", "input_shapes": [[4, 6], [4, 6]], "output_shape": [4]}
{"id": 93, "name": "exp_upvx", "python_source": "def exp_upvx(x):\n    \"\"\"Exponential operation.\"\"\"\n    return jnp.exp(x)", "jaxpr": "{ lambda ; a:f32[3,9]. let b:f32[3,9] = exp a in (b,) }", "stablehlo": "module @jit_exp_upvx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x9xf32>) -> (tensor<3x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.exponential %arg0 : tensor<3x9xf32>\n    return %0 : tensor<3x9xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_exp_upvx, entry_computation_layout={(f32[3,9]{1,0})->f32[3,9]{1,0}}\n\nENTRY main.1 {\n  x.1 = f32[3,9]{1,0} parameter(0)\n  ROOT exp.1 = f32[3,9]{1,0} exponential(x.1)\n}\n\n", "input_shapes": [[3, 9]], "output_shape": [3, 9]}
{"id": 94, "name": "nn_layer_hkho", "python_source": "def nn_layer_hkho(x, w, b):\n    \"\"\"Neural network layer with relu activation.\"\"\"\n    return jax.nn.relu(jnp.dot(x, w) + b)", "jaxpr": "{ lambda ; a:f32[8,6] b:f32[6,23] c:f32[23]. let\n    d:f32[8,23] = dot_general[\n      dimension_numbers=(([1], [0]), ([], []))\n      preferred_element_type=float32\n    ] a b\n    e:f32[1,23] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 23)\n      sharding=None\n    ] c\n    f:f32[8,23] = add d e\n    g:f32[8,23] = custom_jvp_call[\n      name=relu\n      call_jaxpr={ lambda ; h:f32[8,23]. let\n          i:f32[8,23] = jit[\n            name=relu\n            jaxpr={ lambda ; h:f32[8,23]. let\n                i:f32[8,23] = max h 0.0:f32[]\n              in (i,) }\n          ] h\n        in (i,) }\n      jvp=jvp\n      symbolic_zeros=False\n    ] f\n  in (g,) }", "stablehlo": "module @jit_nn_layer_hkho attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x6xf32>, %arg1: tensor<6x23xf32>, %arg2: tensor<23xf32>) -> (tensor<8x23xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.dot_general %arg0, %arg1, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<8x6xf32>, tensor<6x23xf32>) -> tensor<8x23xf32>\n    %1 = stablehlo.broadcast_in_dim %arg2, dims = [1] : (tensor<23xf32>) -> tensor<1x23xf32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x23xf32>) -> tensor<8x23xf32>\n    %3 = stablehlo.add %0, %2 : tensor<8x23xf32>\n    %4 = call @relu(%3) : (tensor<8x23xf32>) -> tensor<8x23xf32>\n    return %4 : tensor<8x23xf32>\n  }\n  func.func private @relu(%arg0: tensor<8x23xf32>) -> tensor<8x23xf32> {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<8x23xf32>\n    %1 = stablehlo.maximum %arg0, %0 : tensor<8x23xf32>\n    return %1 : tensor<8x23xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_nn_layer_hkho, entry_computation_layout={(f32[8,6]{1,0}, f32[6,23]{1,0}, f32[23]{0})->f32[8,23]{1,0}}\n\nrelu.1 {\n  Arg_0.1 = f32[8,23]{1,0} parameter(0)\n  constant.1 = f32[] constant(0)\n  max.2 = f32[8,23]{1,0} broadcast(constant.1), dimensions={}\n  ROOT max.3 = f32[8,23]{1,0} maximum(Arg_0.1, max.2)\n}\n\nENTRY main.2 {\n  x.1 = f32[8,6]{1,0} parameter(0)\n  w.1 = f32[6,23]{1,0} parameter(1)\n  dot_general.1 = f32[8,23]{1,0} dot(x.1, w.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n  b.1 = f32[23]{0} parameter(2)\n  broadcast_in_dim.1 = f32[1,23]{1,0} reshape(b.1)\n  add.4 = f32[1,23]{1,0} broadcast(broadcast_in_dim.1), dimensions={0,1}\n  add.5 = f32[23]{0} reshape(add.4)\n  add.6 = f32[8,23]{1,0} broadcast(add.5), dimensions={1}\n  add.7 = f32[8,23]{1,0} add(dot_general.1, add.6)\n  ROOT jit_relu_.1 = f32[8,23]{1,0} call(add.7), to_apply=relu.1\n}\n\n", "input_shapes": [[8, 6], [6, 23], [23]], "output_shape": [8, 23]}
{"id": 95, "name": "matmul_tpaf", "python_source": "def matmul_tpaf(a, b):\n    \"\"\"Matrix multiplication of shapes (12,15) @ (15,15).\"\"\"\n    return jnp.dot(a, b)", "jaxpr": "{ lambda ; a:f32[12,15] b:f32[15,15]. let\n    c:f32[12,15] = dot_general[\n      dimension_numbers=(([1], [0]), ([], []))\n      preferred_element_type=float32\n    ] a b\n  in (c,) }", "stablehlo": "module @jit_matmul_tpaf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<12x15xf32>, %arg1: tensor<15x15xf32>) -> (tensor<12x15xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.dot_general %arg0, %arg1, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<12x15xf32>, tensor<15x15xf32>) -> tensor<12x15xf32>\n    return %0 : tensor<12x15xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_matmul_tpaf, entry_computation_layout={(f32[12,15]{1,0}, f32[15,15]{1,0})->f32[12,15]{1,0}}\n\nENTRY main.1 {\n  a.1 = f32[12,15]{1,0} parameter(0)\n  b.1 = f32[15,15]{1,0} parameter(1)\n  ROOT dot_general.1 = f32[12,15]{1,0} dot(a.1, b.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n}\n\n", "input_shapes": [[12, 15], [15, 15]], "output_shape": [12, 15]}
{"id": 96, "name": "composite_patv", "python_source": "def composite_patv(x):\n    \"\"\"Composite function with multiple operations.\"\"\"\n    return x ** 2 + jnp.exp(-jnp.abs(x))", "jaxpr": "{ lambda ; a:f32[11,2]. let\n    b:f32[11,2] = integer_pow[y=2] a\n    c:f32[11,2] = abs a\n    d:f32[11,2] = neg c\n    e:f32[11,2] = exp d\n    f:f32[11,2] = add b e\n  in (f,) }", "stablehlo": "module @jit_composite_patv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<11x2xf32>) -> (tensor<11x2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<11x2xf32>\n    %1 = stablehlo.abs %arg0 : tensor<11x2xf32>\n    %2 = stablehlo.negate %1 : tensor<11x2xf32>\n    %3 = stablehlo.exponential %2 : tensor<11x2xf32>\n    %4 = stablehlo.add %0, %3 : tensor<11x2xf32>\n    return %4 : tensor<11x2xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_composite_patv, entry_computation_layout={(f32[11,2]{1,0})->f32[11,2]{1,0}}\n\nENTRY main.1 {\n  x.1 = f32[11,2]{1,0} parameter(0)\n  integer_pow.1 = f32[11,2]{1,0} multiply(x.1, x.1)\n  abs.1 = f32[11,2]{1,0} abs(x.1)\n  neg.1 = f32[11,2]{1,0} negate(abs.1)\n  exp.1 = f32[11,2]{1,0} exponential(neg.1)\n  ROOT add.1 = f32[11,2]{1,0} add(integer_pow.1, exp.1)\n}\n\n", "input_shapes": [[11, 2]], "output_shape": [11, 2]}
{"id": 97, "name": "mean_zlvz", "python_source": "def mean_zlvz(x):\n    \"\"\"Mean reduction along axis 1.\"\"\"\n    return jnp.mean(x, axis=1)", "jaxpr": "{ lambda ; a:f32[7,8,7]. let\n    b:f32[7,7] = reduce_sum[axes=(1,) out_sharding=None] a\n    c:f32[7,7] = div b 8.0:f32[]\n  in (c,) }", "stablehlo": "module @jit_mean_zlvz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x8x7xf32>) -> (tensor<7x7xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<7x8x7xf32>, tensor<f32>) -> tensor<7x7xf32>\n    %cst_0 = stablehlo.constant dense<8.000000e+00> : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<7x7xf32>\n    %2 = stablehlo.divide %0, %1 : tensor<7x7xf32>\n    return %2 : tensor<7x7xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_mean_zlvz, entry_computation_layout={(f32[7,8,7]{2,1,0})->f32[7,7]{1,0}}\n\nregion_0.1 {\n  reduce_sum.3 = f32[] parameter(0)\n  reduce_sum.4 = f32[] parameter(1)\n  ROOT reduce_sum.5 = f32[] add(reduce_sum.3, reduce_sum.4)\n}\n\nENTRY main.2 {\n  x.1 = f32[7,8,7]{2,1,0} parameter(0)\n  constant.3 = f32[] constant(0)\n  reduce_sum.7 = f32[7,7]{1,0} reduce(x.1, constant.3), dimensions={1}, to_apply=region_0.1\n  constant.2 = f32[] constant(8)\n  div.2 = f32[7,7]{1,0} broadcast(constant.2), dimensions={}\n  ROOT div.3 = f32[7,7]{1,0} divide(reduce_sum.7, div.2)\n}\n\n", "input_shapes": [[7, 8, 7]], "output_shape": [7, 7]}
{"id": 98, "name": "nn_layer_cekm", "python_source": "def nn_layer_cekm(x, w, b):\n    \"\"\"Neural network layer with relu activation.\"\"\"\n    return jax.nn.relu(jnp.dot(x, w) + b)", "jaxpr": "{ lambda ; a:f32[7,30] b:f32[30,28] c:f32[28]. let\n    d:f32[7,28] = dot_general[\n      dimension_numbers=(([1], [0]), ([], []))\n      preferred_element_type=float32\n    ] a b\n    e:f32[1,28] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 28)\n      sharding=None\n    ] c\n    f:f32[7,28] = add d e\n    g:f32[7,28] = custom_jvp_call[\n      name=relu\n      call_jaxpr={ lambda ; h:f32[7,28]. let\n          i:f32[7,28] = jit[\n            name=relu\n            jaxpr={ lambda ; h:f32[7,28]. let\n                i:f32[7,28] = max h 0.0:f32[]\n              in (i,) }\n          ] h\n        in (i,) }\n      jvp=jvp\n      symbolic_zeros=False\n    ] f\n  in (g,) }", "stablehlo": "module @jit_nn_layer_cekm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x30xf32>, %arg1: tensor<30x28xf32>, %arg2: tensor<28xf32>) -> (tensor<7x28xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.dot_general %arg0, %arg1, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<7x30xf32>, tensor<30x28xf32>) -> tensor<7x28xf32>\n    %1 = stablehlo.broadcast_in_dim %arg2, dims = [1] : (tensor<28xf32>) -> tensor<1x28xf32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x28xf32>) -> tensor<7x28xf32>\n    %3 = stablehlo.add %0, %2 : tensor<7x28xf32>\n    %4 = call @relu(%3) : (tensor<7x28xf32>) -> tensor<7x28xf32>\n    return %4 : tensor<7x28xf32>\n  }\n  func.func private @relu(%arg0: tensor<7x28xf32>) -> tensor<7x28xf32> {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<7x28xf32>\n    %1 = stablehlo.maximum %arg0, %0 : tensor<7x28xf32>\n    return %1 : tensor<7x28xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_nn_layer_cekm, entry_computation_layout={(f32[7,30]{1,0}, f32[30,28]{1,0}, f32[28]{0})->f32[7,28]{1,0}}\n\nrelu.1 {\n  Arg_0.1 = f32[7,28]{1,0} parameter(0)\n  constant.1 = f32[] constant(0)\n  max.2 = f32[7,28]{1,0} broadcast(constant.1), dimensions={}\n  ROOT max.3 = f32[7,28]{1,0} maximum(Arg_0.1, max.2)\n}\n\nENTRY main.2 {\n  x.1 = f32[7,30]{1,0} parameter(0)\n  w.1 = f32[30,28]{1,0} parameter(1)\n  dot_general.1 = f32[7,28]{1,0} dot(x.1, w.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n  b.1 = f32[28]{0} parameter(2)\n  broadcast_in_dim.1 = f32[1,28]{1,0} reshape(b.1)\n  add.4 = f32[1,28]{1,0} broadcast(broadcast_in_dim.1), dimensions={0,1}\n  add.5 = f32[28]{0} reshape(add.4)\n  add.6 = f32[7,28]{1,0} broadcast(add.5), dimensions={1}\n  add.7 = f32[7,28]{1,0} add(dot_general.1, add.6)\n  ROOT jit_relu_.1 = f32[7,28]{1,0} call(add.7), to_apply=relu.1\n}\n\n", "input_shapes": [[7, 30], [30, 28], [28]], "output_shape": [7, 28]}
{"id": 99, "name": "matmul_cgdu", "python_source": "def matmul_cgdu(a, b):\n    \"\"\"Matrix multiplication of shapes (15,10) @ (10,15).\"\"\"\n    return jnp.dot(a, b)", "jaxpr": "{ lambda ; a:f32[15,10] b:f32[10,15]. let\n    c:f32[15,15] = dot_general[\n      dimension_numbers=(([1], [0]), ([], []))\n      preferred_element_type=float32\n    ] a b\n  in (c,) }", "stablehlo": "module @jit_matmul_cgdu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<15x10xf32>, %arg1: tensor<10x15xf32>) -> (tensor<15x15xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.dot_general %arg0, %arg1, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<15x10xf32>, tensor<10x15xf32>) -> tensor<15x15xf32>\n    return %0 : tensor<15x15xf32>\n  }\n}\n", "xla_hlo": "HloModule jit_matmul_cgdu, entry_computation_layout={(f32[15,10]{1,0}, f32[10,15]{1,0})->f32[15,15]{1,0}}\n\nENTRY main.1 {\n  a.1 = f32[15,10]{1,0} parameter(0)\n  b.1 = f32[10,15]{1,0} parameter(1)\n  ROOT dot_general.1 = f32[15,15]{1,0} dot(a.1, b.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n}\n\n", "input_shapes": [[15, 10], [10, 15]], "output_shape": [15, 15]}
