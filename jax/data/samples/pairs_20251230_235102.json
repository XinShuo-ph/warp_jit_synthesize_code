[
  {
    "name": "matmul_tvuw",
    "python_source": "def matmul_tvuw(a, b):\n    \"\"\"Matrix multiplication of shapes (16,5) @ (5,6).\"\"\"\n    return jnp.dot(a, b)",
    "jaxpr": "{ lambda ; a:f32[16,5] b:f32[5,6]. let\n    c:f32[16,6] = dot_general[\n      dimension_numbers=(([1], [0]), ([], []))\n      preferred_element_type=float32\n    ] a b\n  in (c,) }",
    "stablehlo": "module @jit_matmul_tvuw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<16x5xf32>, %arg1: tensor<5x6xf32>) -> (tensor<16x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.dot_general %arg0, %arg1, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<16x5xf32>, tensor<5x6xf32>) -> tensor<16x6xf32>\n    return %0 : tensor<16x6xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_matmul_tvuw, entry_computation_layout={(f32[16,5]{1,0}, f32[5,6]{1,0})->f32[16,6]{1,0}}\n\nENTRY main.1 {\n  a.1 = f32[16,5]{1,0} parameter(0)\n  b.1 = f32[5,6]{1,0} parameter(1)\n  ROOT dot_general.1 = f32[16,6]{1,0} dot(a.1, b.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n}\n\n",
    "input_shapes": [
      [
        16,
        5
      ],
      [
        5,
        6
      ]
    ],
    "output_shape": [
      16,
      6
    ]
  },
  {
    "name": "composite_ljwg",
    "python_source": "def composite_ljwg(x):\n    \"\"\"Composite function with multiple operations.\"\"\"\n    return jnp.sin(x) + jax.nn.relu(x)",
    "jaxpr": "{ lambda ; a:f32[12,4]. let\n    b:f32[12,4] = sin a\n    c:f32[12,4] = custom_jvp_call[\n      name=relu\n      call_jaxpr={ lambda ; d:f32[12,4]. let\n          e:f32[12,4] = jit[\n            name=relu\n            jaxpr={ lambda ; d:f32[12,4]. let\n                e:f32[12,4] = max d 0.0:f32[]\n              in (e,) }\n          ] d\n        in (e,) }\n      jvp=jvp\n      symbolic_zeros=False\n    ] a\n    f:f32[12,4] = add b c\n  in (f,) }",
    "stablehlo": "module @jit_composite_ljwg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<12x4xf32>) -> (tensor<12x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<12x4xf32>\n    %1 = call @relu(%arg0) : (tensor<12x4xf32>) -> tensor<12x4xf32>\n    %2 = stablehlo.add %0, %1 : tensor<12x4xf32>\n    return %2 : tensor<12x4xf32>\n  }\n  func.func private @relu(%arg0: tensor<12x4xf32>) -> tensor<12x4xf32> {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<12x4xf32>\n    %1 = stablehlo.maximum %arg0, %0 : tensor<12x4xf32>\n    return %1 : tensor<12x4xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_composite_ljwg, entry_computation_layout={(f32[12,4]{1,0})->f32[12,4]{1,0}}\n\nrelu.1 {\n  Arg_0.1 = f32[12,4]{1,0} parameter(0)\n  constant.1 = f32[] constant(0)\n  max.2 = f32[12,4]{1,0} broadcast(constant.1), dimensions={}\n  ROOT max.3 = f32[12,4]{1,0} maximum(Arg_0.1, max.2)\n}\n\nENTRY main.2 {\n  x.1 = f32[12,4]{1,0} parameter(0)\n  sin.1 = f32[12,4]{1,0} sine(x.1)\n  jit_relu_.1 = f32[12,4]{1,0} call(x.1), to_apply=relu.1\n  ROOT add.1 = f32[12,4]{1,0} add(sin.1, jit_relu_.1)\n}\n\n",
    "input_shapes": [
      [
        12,
        4
      ]
    ],
    "output_shape": [
      12,
      4
    ]
  },
  {
    "name": "composite_zknd",
    "python_source": "def composite_zknd(x):\n    \"\"\"Composite function with multiple operations.\"\"\"\n    return jnp.sin(x) + jnp.exp(-jnp.abs(x))",
    "jaxpr": "{ lambda ; a:f32[13,3]. let\n    b:f32[13,3] = sin a\n    c:f32[13,3] = abs a\n    d:f32[13,3] = neg c\n    e:f32[13,3] = exp d\n    f:f32[13,3] = add b e\n  in (f,) }",
    "stablehlo": "module @jit_composite_zknd attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<13x3xf32>) -> (tensor<13x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<13x3xf32>\n    %1 = stablehlo.abs %arg0 : tensor<13x3xf32>\n    %2 = stablehlo.negate %1 : tensor<13x3xf32>\n    %3 = stablehlo.exponential %2 : tensor<13x3xf32>\n    %4 = stablehlo.add %0, %3 : tensor<13x3xf32>\n    return %4 : tensor<13x3xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_composite_zknd, entry_computation_layout={(f32[13,3]{1,0})->f32[13,3]{1,0}}\n\nENTRY main.1 {\n  x.1 = f32[13,3]{1,0} parameter(0)\n  sin.1 = f32[13,3]{1,0} sine(x.1)\n  abs.1 = f32[13,3]{1,0} abs(x.1)\n  neg.1 = f32[13,3]{1,0} negate(abs.1)\n  exp.1 = f32[13,3]{1,0} exponential(neg.1)\n  ROOT add.1 = f32[13,3]{1,0} add(sin.1, exp.1)\n}\n\n",
    "input_shapes": [
      [
        13,
        3
      ]
    ],
    "output_shape": [
      13,
      3
    ]
  },
  {
    "name": "tanh_izzt",
    "python_source": "def tanh_izzt(x):\n    \"\"\"Hyperbolic tangent operation.\"\"\"\n    return jnp.tanh(x)",
    "jaxpr": "{ lambda ; a:f32[2]. let b:f32[2] = tanh a in (b,) }",
    "stablehlo": "module @jit_tanh_izzt attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>) -> (tensor<2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.tanh %arg0 : tensor<2xf32>\n    return %0 : tensor<2xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_tanh_izzt, entry_computation_layout={(f32[2]{0})->f32[2]{0}}\n\nENTRY main.1 {\n  x.1 = f32[2]{0} parameter(0)\n  ROOT tanh.1 = f32[2]{0} tanh(x.1)\n}\n\n",
    "input_shapes": [
      [
        2
      ]
    ],
    "output_shape": [
      2
    ]
  },
  {
    "name": "nn_layer_wuec",
    "python_source": "def nn_layer_wuec(x, w, b):\n    \"\"\"Neural network layer with sigmoid activation.\"\"\"\n    return jax.nn.sigmoid(jnp.dot(x, w) + b)",
    "jaxpr": "{ lambda ; a:f32[7,7] b:f32[7,32] c:f32[32]. let\n    d:f32[7,32] = dot_general[\n      dimension_numbers=(([1], [0]), ([], []))\n      preferred_element_type=float32\n    ] a b\n    e:f32[1,32] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 32)\n      sharding=None\n    ] c\n    f:f32[7,32] = add d e\n    g:f32[7,32] = logistic f\n  in (g,) }",
    "stablehlo": "module @jit_nn_layer_wuec attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x7xf32>, %arg1: tensor<7x32xf32>, %arg2: tensor<32xf32>) -> (tensor<7x32xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.dot_general %arg0, %arg1, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<7x7xf32>, tensor<7x32xf32>) -> tensor<7x32xf32>\n    %1 = stablehlo.broadcast_in_dim %arg2, dims = [1] : (tensor<32xf32>) -> tensor<1x32xf32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x32xf32>) -> tensor<7x32xf32>\n    %3 = stablehlo.add %0, %2 : tensor<7x32xf32>\n    %4 = stablehlo.negate %3 : tensor<7x32xf32>\n    %5 = stablehlo.exponential %4 : tensor<7x32xf32>\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %6 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<7x32xf32>\n    %7 = stablehlo.add %6, %5 : tensor<7x32xf32>\n    %cst_0 = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %8 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<7x32xf32>\n    %9 = stablehlo.divide %8, %7 : tensor<7x32xf32>\n    return %9 : tensor<7x32xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_nn_layer_wuec, entry_computation_layout={(f32[7,7]{1,0}, f32[7,32]{1,0}, f32[32]{0})->f32[7,32]{1,0}}\n\nENTRY main.1 {\n  constant.1 = f32[] constant(1)\n  broadcast.1 = f32[7,32]{1,0} broadcast(constant.1), dimensions={}\n  x.1 = f32[7,7]{1,0} parameter(0)\n  w.1 = f32[7,32]{1,0} parameter(1)\n  dot_general.1 = f32[7,32]{1,0} dot(x.1, w.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n  b.1 = f32[32]{0} parameter(2)\n  broadcast_in_dim.1 = f32[1,32]{1,0} reshape(b.1)\n  add.5 = f32[1,32]{1,0} broadcast(broadcast_in_dim.1), dimensions={0,1}\n  add.6 = f32[32]{0} reshape(add.5)\n  add.7 = f32[7,32]{1,0} broadcast(add.6), dimensions={1}\n  add.8 = f32[7,32]{1,0} add(dot_general.1, add.7)\n  neg.1 = f32[7,32]{1,0} negate(add.8)\n  exp.1 = f32[7,32]{1,0} exponential(neg.1)\n  add.9 = f32[7,32]{1,0} add(exp.1, broadcast.1)\n  ROOT div.1 = f32[7,32]{1,0} divide(broadcast.1, add.9)\n}\n\n",
    "input_shapes": [
      [
        7,
        7
      ],
      [
        7,
        32
      ],
      [
        32
      ]
    ],
    "output_shape": [
      7,
      32
    ]
  },
  {
    "name": "nn_layer_jypx",
    "python_source": "def nn_layer_jypx(x, w, b):\n    \"\"\"Neural network layer with sigmoid activation.\"\"\"\n    return jax.nn.sigmoid(jnp.dot(x, w) + b)",
    "jaxpr": "{ lambda ; a:f32[6,29] b:f32[29,5] c:f32[5]. let\n    d:f32[6,5] = dot_general[\n      dimension_numbers=(([1], [0]), ([], []))\n      preferred_element_type=float32\n    ] a b\n    e:f32[1,5] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 5)\n      sharding=None\n    ] c\n    f:f32[6,5] = add d e\n    g:f32[6,5] = logistic f\n  in (g,) }",
    "stablehlo": "module @jit_nn_layer_jypx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x29xf32>, %arg1: tensor<29x5xf32>, %arg2: tensor<5xf32>) -> (tensor<6x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.dot_general %arg0, %arg1, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<6x29xf32>, tensor<29x5xf32>) -> tensor<6x5xf32>\n    %1 = stablehlo.broadcast_in_dim %arg2, dims = [1] : (tensor<5xf32>) -> tensor<1x5xf32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x5xf32>) -> tensor<6x5xf32>\n    %3 = stablehlo.add %0, %2 : tensor<6x5xf32>\n    %4 = stablehlo.negate %3 : tensor<6x5xf32>\n    %5 = stablehlo.exponential %4 : tensor<6x5xf32>\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %6 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<6x5xf32>\n    %7 = stablehlo.add %6, %5 : tensor<6x5xf32>\n    %cst_0 = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %8 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<6x5xf32>\n    %9 = stablehlo.divide %8, %7 : tensor<6x5xf32>\n    return %9 : tensor<6x5xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_nn_layer_jypx, entry_computation_layout={(f32[6,29]{1,0}, f32[29,5]{1,0}, f32[5]{0})->f32[6,5]{1,0}}\n\nENTRY main.1 {\n  constant.1 = f32[] constant(1)\n  broadcast.1 = f32[6,5]{1,0} broadcast(constant.1), dimensions={}\n  x.1 = f32[6,29]{1,0} parameter(0)\n  w.1 = f32[29,5]{1,0} parameter(1)\n  dot_general.1 = f32[6,5]{1,0} dot(x.1, w.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n  b.1 = f32[5]{0} parameter(2)\n  broadcast_in_dim.1 = f32[1,5]{1,0} reshape(b.1)\n  add.5 = f32[1,5]{1,0} broadcast(broadcast_in_dim.1), dimensions={0,1}\n  add.6 = f32[5]{0} reshape(add.5)\n  add.7 = f32[6,5]{1,0} broadcast(add.6), dimensions={1}\n  add.8 = f32[6,5]{1,0} add(dot_general.1, add.7)\n  neg.1 = f32[6,5]{1,0} negate(add.8)\n  exp.1 = f32[6,5]{1,0} exponential(neg.1)\n  add.9 = f32[6,5]{1,0} add(exp.1, broadcast.1)\n  ROOT div.1 = f32[6,5]{1,0} divide(broadcast.1, add.9)\n}\n\n",
    "input_shapes": [
      [
        6,
        29
      ],
      [
        29,
        5
      ],
      [
        5
      ]
    ],
    "output_shape": [
      6,
      5
    ]
  },
  {
    "name": "tanh_fuoe",
    "python_source": "def tanh_fuoe(x):\n    \"\"\"Hyperbolic tangent operation.\"\"\"\n    return jnp.tanh(x)",
    "jaxpr": "{ lambda ; a:f32[8,10,16]. let b:f32[8,10,16] = tanh a in (b,) }",
    "stablehlo": "module @jit_tanh_fuoe attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x10x16xf32>) -> (tensor<8x10x16xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.tanh %arg0 : tensor<8x10x16xf32>\n    return %0 : tensor<8x10x16xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_tanh_fuoe, entry_computation_layout={(f32[8,10,16]{2,1,0})->f32[8,10,16]{2,1,0}}\n\nENTRY main.1 {\n  x.1 = f32[8,10,16]{2,1,0} parameter(0)\n  ROOT tanh.1 = f32[8,10,16]{2,1,0} tanh(x.1)\n}\n\n",
    "input_shapes": [
      [
        8,
        10,
        16
      ]
    ],
    "output_shape": [
      8,
      10,
      16
    ]
  },
  {
    "name": "add_zhwa",
    "python_source": "def add_zhwa(x, y):\n    \"\"\"Addition operation.\"\"\"\n    return x + y",
    "jaxpr": "{ lambda ; a:f32[7] b:f32[7]. let c:f32[7] = add a b in (c,) }",
    "stablehlo": "module @jit_add_zhwa attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>, %arg1: tensor<7xf32>) -> (tensor<7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.add %arg0, %arg1 : tensor<7xf32>\n    return %0 : tensor<7xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_add_zhwa, entry_computation_layout={(f32[7]{0}, f32[7]{0})->f32[7]{0}}\n\nENTRY main.1 {\n  x.1 = f32[7]{0} parameter(0)\n  y.1 = f32[7]{0} parameter(1)\n  ROOT add.1 = f32[7]{0} add(x.1, y.1)\n}\n\n",
    "input_shapes": [
      [
        7
      ],
      [
        7
      ]
    ],
    "output_shape": [
      7
    ]
  },
  {
    "name": "composite_ikab",
    "python_source": "def composite_ikab(x):\n    \"\"\"Composite function with multiple operations.\"\"\"\n    return jnp.tanh(x) + jax.nn.relu(x) + jnp.sin(x)",
    "jaxpr": "{ lambda ; a:f32[9,3]. let\n    b:f32[9,3] = tanh a\n    c:f32[9,3] = custom_jvp_call[\n      name=relu\n      call_jaxpr={ lambda ; d:f32[9,3]. let\n          e:f32[9,3] = jit[\n            name=relu\n            jaxpr={ lambda ; d:f32[9,3]. let\n                e:f32[9,3] = max d 0.0:f32[]\n              in (e,) }\n          ] d\n        in (e,) }\n      jvp=jvp\n      symbolic_zeros=False\n    ] a\n    f:f32[9,3] = add b c\n    g:f32[9,3] = sin a\n    h:f32[9,3] = add f g\n  in (h,) }",
    "stablehlo": "module @jit_composite_ikab attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x3xf32>) -> (tensor<9x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.tanh %arg0 : tensor<9x3xf32>\n    %1 = call @relu(%arg0) : (tensor<9x3xf32>) -> tensor<9x3xf32>\n    %2 = stablehlo.add %0, %1 : tensor<9x3xf32>\n    %3 = stablehlo.sine %arg0 : tensor<9x3xf32>\n    %4 = stablehlo.add %2, %3 : tensor<9x3xf32>\n    return %4 : tensor<9x3xf32>\n  }\n  func.func private @relu(%arg0: tensor<9x3xf32>) -> tensor<9x3xf32> {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<9x3xf32>\n    %1 = stablehlo.maximum %arg0, %0 : tensor<9x3xf32>\n    return %1 : tensor<9x3xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_composite_ikab, entry_computation_layout={(f32[9,3]{1,0})->f32[9,3]{1,0}}\n\nrelu.1 {\n  Arg_0.1 = f32[9,3]{1,0} parameter(0)\n  constant.1 = f32[] constant(0)\n  broadcast.1 = f32[9,3]{1,0} broadcast(constant.1), dimensions={}\n  ROOT max.1 = f32[9,3]{1,0} maximum(Arg_0.1, broadcast.1)\n}\n\nENTRY main.2 {\n  x.1 = f32[9,3]{1,0} parameter(0)\n  tanh.1 = f32[9,3]{1,0} tanh(x.1)\n  jit_relu_.1 = f32[9,3]{1,0} call(x.1), to_apply=relu.1\n  add.2 = f32[9,3]{1,0} add(tanh.1, jit_relu_.1)\n  sin.1 = f32[9,3]{1,0} sine(x.1)\n  ROOT add.3 = f32[9,3]{1,0} add(add.2, sin.1)\n}\n\n",
    "input_shapes": [
      [
        9,
        3
      ]
    ],
    "output_shape": [
      9,
      3
    ]
  },
  {
    "name": "mean_rtmb",
    "python_source": "def mean_rtmb(x):\n    \"\"\"Mean reduction along axis 3.\"\"\"\n    return jnp.mean(x, axis=3)",
    "jaxpr": "{ lambda ; a:f32[9,7,14,11]. let\n    b:f32[9,7,14] = reduce_sum[axes=(3,) out_sharding=None] a\n    c:f32[9,7,14] = div b 11.0:f32[]\n  in (c,) }",
    "stablehlo": "module @jit_mean_rtmb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x7x14x11xf32>) -> (tensor<9x7x14xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [3] : (tensor<9x7x14x11xf32>, tensor<f32>) -> tensor<9x7x14xf32>\n    %cst_0 = stablehlo.constant dense<1.100000e+01> : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<9x7x14xf32>\n    %2 = stablehlo.divide %0, %1 : tensor<9x7x14xf32>\n    return %2 : tensor<9x7x14xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_mean_rtmb, entry_computation_layout={(f32[9,7,14,11]{3,2,1,0})->f32[9,7,14]{2,1,0}}\n\nregion_0.1 {\n  reduce_sum.3 = f32[] parameter(0)\n  reduce_sum.4 = f32[] parameter(1)\n  ROOT reduce_sum.5 = f32[] add(reduce_sum.3, reduce_sum.4)\n}\n\nENTRY main.2 {\n  x.1 = f32[9,7,14,11]{3,2,1,0} parameter(0)\n  constant.3 = f32[] constant(0)\n  reduce_sum.7 = f32[9,7,14]{2,1,0} reduce(x.1, constant.3), dimensions={3}, to_apply=region_0.1\n  constant.2 = f32[] constant(11)\n  div.2 = f32[9,7,14]{2,1,0} broadcast(constant.2), dimensions={}\n  ROOT div.3 = f32[9,7,14]{2,1,0} divide(reduce_sum.7, div.2)\n}\n\n",
    "input_shapes": [
      [
        9,
        7,
        14,
        11
      ]
    ],
    "output_shape": [
      9,
      7,
      14
    ]
  },
  {
    "name": "composite_ftso",
    "python_source": "def composite_ftso(x):\n    \"\"\"Composite function with multiple operations.\"\"\"\n    return jnp.sin(x) + jnp.tanh(x)",
    "jaxpr": "{ lambda ; a:f32[3,9]. let\n    b:f32[3,9] = sin a\n    c:f32[3,9] = tanh a\n    d:f32[3,9] = add b c\n  in (d,) }",
    "stablehlo": "module @jit_composite_ftso attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x9xf32>) -> (tensor<3x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<3x9xf32>\n    %1 = stablehlo.tanh %arg0 : tensor<3x9xf32>\n    %2 = stablehlo.add %0, %1 : tensor<3x9xf32>\n    return %2 : tensor<3x9xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_composite_ftso, entry_computation_layout={(f32[3,9]{1,0})->f32[3,9]{1,0}}\n\nENTRY main.1 {\n  x.1 = f32[3,9]{1,0} parameter(0)\n  sin.1 = f32[3,9]{1,0} sine(x.1)\n  tanh.1 = f32[3,9]{1,0} tanh(x.1)\n  ROOT add.1 = f32[3,9]{1,0} add(sin.1, tanh.1)\n}\n\n",
    "input_shapes": [
      [
        3,
        9
      ]
    ],
    "output_shape": [
      3,
      9
    ]
  },
  {
    "name": "sub_edcs",
    "python_source": "def sub_edcs(x, y):\n    \"\"\"Subtraction operation.\"\"\"\n    return x - y",
    "jaxpr": "{ lambda ; a:f32[2] b:f32[2]. let c:f32[2] = sub a b in (c,) }",
    "stablehlo": "module @jit_sub_edcs attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2xf32>, %arg1: tensor<2xf32>) -> (tensor<2xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<2xf32>\n    return %0 : tensor<2xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_sub_edcs, entry_computation_layout={(f32[2]{0}, f32[2]{0})->f32[2]{0}}\n\nENTRY main.1 {\n  x.1 = f32[2]{0} parameter(0)\n  y.1 = f32[2]{0} parameter(1)\n  ROOT sub.1 = f32[2]{0} subtract(x.1, y.1)\n}\n\n",
    "input_shapes": [
      [
        2
      ],
      [
        2
      ]
    ],
    "output_shape": [
      2
    ]
  },
  {
    "name": "nn_layer_mepu",
    "python_source": "def nn_layer_mepu(x, w, b):\n    \"\"\"Neural network layer with gelu activation.\"\"\"\n    return jax.nn.gelu(jnp.dot(x, w) + b)",
    "jaxpr": "{ lambda ; a:f32[2,12] b:f32[12,20] c:f32[20]. let\n    d:f32[2,20] = dot_general[\n      dimension_numbers=(([1], [0]), ([], []))\n      preferred_element_type=float32\n    ] a b\n    e:f32[1,20] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 20)\n      sharding=None\n    ] c\n    f:f32[2,20] = add d e\n    g:f32[2,20] = integer_pow[y=3] f\n    h:f32[2,20] = mul 0.044714998453855515:f32[] g\n    i:f32[2,20] = add f h\n    j:f32[2,20] = mul 0.7978845834732056:f32[] i\n    k:f32[2,20] = tanh j\n    l:f32[2,20] = add 1.0:f32[] k\n    m:f32[2,20] = mul 0.5:f32[] l\n    n:f32[2,20] = mul f m\n  in (n,) }",
    "stablehlo": "module @jit_nn_layer_mepu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x12xf32>, %arg1: tensor<12x20xf32>, %arg2: tensor<20xf32>) -> (tensor<2x20xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.dot_general %arg0, %arg1, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x12xf32>, tensor<12x20xf32>) -> tensor<2x20xf32>\n    %1 = stablehlo.broadcast_in_dim %arg2, dims = [1] : (tensor<20xf32>) -> tensor<1x20xf32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x20xf32>) -> tensor<2x20xf32>\n    %3 = stablehlo.add %0, %2 : tensor<2x20xf32>\n    %4 = stablehlo.multiply %3, %3 : tensor<2x20xf32>\n    %5 = stablehlo.multiply %4, %3 : tensor<2x20xf32>\n    %cst = stablehlo.constant dense<4.471500e-02> : tensor<f32>\n    %6 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<2x20xf32>\n    %7 = stablehlo.multiply %6, %5 : tensor<2x20xf32>\n    %8 = stablehlo.add %3, %7 : tensor<2x20xf32>\n    %cst_0 = stablehlo.constant dense<0.797884583> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<2x20xf32>\n    %10 = stablehlo.multiply %9, %8 : tensor<2x20xf32>\n    %11 = stablehlo.tanh %10 : tensor<2x20xf32>\n    %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %12 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<2x20xf32>\n    %13 = stablehlo.add %12, %11 : tensor<2x20xf32>\n    %cst_2 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %14 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<2x20xf32>\n    %15 = stablehlo.multiply %14, %13 : tensor<2x20xf32>\n    %16 = stablehlo.multiply %3, %15 : tensor<2x20xf32>\n    return %16 : tensor<2x20xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_nn_layer_mepu, entry_computation_layout={(f32[2,12]{1,0}, f32[12,20]{1,0}, f32[20]{0})->f32[2,20]{1,0}}\n\nENTRY main.1 {\n  x.1 = f32[2,12]{1,0} parameter(0)\n  w.1 = f32[12,20]{1,0} parameter(1)\n  dot_general.1 = f32[2,20]{1,0} dot(x.1, w.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n  b.1 = f32[20]{0} parameter(2)\n  broadcast_in_dim.1 = f32[1,20]{1,0} reshape(b.1)\n  add.8 = f32[1,20]{1,0} broadcast(broadcast_in_dim.1), dimensions={0,1}\n  add.9 = f32[20]{0} reshape(add.8)\n  add.10 = f32[2,20]{1,0} broadcast(add.9), dimensions={1}\n  add.11 = f32[2,20]{1,0} add(dot_general.1, add.10)\n  integer_pow.2 = f32[2,20]{1,0} multiply(add.11, add.11)\n  integer_pow.3 = f32[2,20]{1,0} multiply(integer_pow.2, add.11)\n  constant.7 = f32[] constant(0.044715)\n  mul.9 = f32[2,20]{1,0} broadcast(constant.7), dimensions={}\n  mul.10 = f32[2,20]{1,0} multiply(integer_pow.3, mul.9)\n  add.12 = f32[2,20]{1,0} add(add.11, mul.10)\n  constant.6 = f32[] constant(0.797884583)\n  mul.8 = f32[2,20]{1,0} broadcast(constant.6), dimensions={}\n  mul.11 = f32[2,20]{1,0} multiply(add.12, mul.8)\n  tanh.1 = f32[2,20]{1,0} tanh(mul.11)\n  constant.5 = f32[] constant(1)\n  add.7 = f32[2,20]{1,0} broadcast(constant.5), dimensions={}\n  add.13 = f32[2,20]{1,0} add(tanh.1, add.7)\n  constant.4 = f32[] constant(0.5)\n  mul.7 = f32[2,20]{1,0} broadcast(constant.4), dimensions={}\n  mul.12 = f32[2,20]{1,0} multiply(add.13, mul.7)\n  ROOT mul.13 = f32[2,20]{1,0} multiply(add.11, mul.12)\n}\n\n",
    "input_shapes": [
      [
        2,
        12
      ],
      [
        12,
        20
      ],
      [
        20
      ]
    ],
    "output_shape": [
      2,
      20
    ]
  },
  {
    "name": "tanh_kgwa",
    "python_source": "def tanh_kgwa(x):\n    \"\"\"Hyperbolic tangent operation.\"\"\"\n    return jnp.tanh(x)",
    "jaxpr": "{ lambda ; a:f32[4,8]. let b:f32[4,8] = tanh a in (b,) }",
    "stablehlo": "module @jit_tanh_kgwa attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x8xf32>) -> (tensor<4x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.tanh %arg0 : tensor<4x8xf32>\n    return %0 : tensor<4x8xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_tanh_kgwa, entry_computation_layout={(f32[4,8]{1,0})->f32[4,8]{1,0}}\n\nENTRY main.1 {\n  x.1 = f32[4,8]{1,0} parameter(0)\n  ROOT tanh.1 = f32[4,8]{1,0} tanh(x.1)\n}\n\n",
    "input_shapes": [
      [
        4,
        8
      ]
    ],
    "output_shape": [
      4,
      8
    ]
  },
  {
    "name": "sub_ovxy",
    "python_source": "def sub_ovxy(x, y):\n    \"\"\"Subtraction operation.\"\"\"\n    return x - y",
    "jaxpr": "{ lambda ; a:f32[13,8,7] b:f32[13,8,7]. let c:f32[13,8,7] = sub a b in (c,) }",
    "stablehlo": "module @jit_sub_ovxy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<13x8x7xf32>, %arg1: tensor<13x8x7xf32>) -> (tensor<13x8x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<13x8x7xf32>\n    return %0 : tensor<13x8x7xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_sub_ovxy, entry_computation_layout={(f32[13,8,7]{2,1,0}, f32[13,8,7]{2,1,0})->f32[13,8,7]{2,1,0}}\n\nENTRY main.1 {\n  x.1 = f32[13,8,7]{2,1,0} parameter(0)\n  y.1 = f32[13,8,7]{2,1,0} parameter(1)\n  ROOT sub.1 = f32[13,8,7]{2,1,0} subtract(x.1, y.1)\n}\n\n",
    "input_shapes": [
      [
        13,
        8,
        7
      ],
      [
        13,
        8,
        7
      ]
    ],
    "output_shape": [
      13,
      8,
      7
    ]
  },
  {
    "name": "abs_qtqo",
    "python_source": "def abs_qtqo(x):\n    \"\"\"Absolute value operation.\"\"\"\n    return jnp.abs(x)",
    "jaxpr": "{ lambda ; a:f32[4,10,16]. let b:f32[4,10,16] = abs a in (b,) }",
    "stablehlo": "module @jit_abs_qtqo attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x10x16xf32>) -> (tensor<4x10x16xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.abs %arg0 : tensor<4x10x16xf32>\n    return %0 : tensor<4x10x16xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_abs_qtqo, entry_computation_layout={(f32[4,10,16]{2,1,0})->f32[4,10,16]{2,1,0}}\n\nENTRY main.1 {\n  x.1 = f32[4,10,16]{2,1,0} parameter(0)\n  ROOT abs.1 = f32[4,10,16]{2,1,0} abs(x.1)\n}\n\n",
    "input_shapes": [
      [
        4,
        10,
        16
      ]
    ],
    "output_shape": [
      4,
      10,
      16
    ]
  },
  {
    "name": "mul_odnu",
    "python_source": "def mul_odnu(x, y):\n    \"\"\"Multiplication operation.\"\"\"\n    return x * y",
    "jaxpr": "{ lambda ; a:f32[6] b:f32[6]. let c:f32[6] = mul a b in (c,) }",
    "stablehlo": "module @jit_mul_odnu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>, %arg1: tensor<6xf32>) -> (tensor<6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<6xf32>\n    return %0 : tensor<6xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_mul_odnu, entry_computation_layout={(f32[6]{0}, f32[6]{0})->f32[6]{0}}\n\nENTRY main.1 {\n  x.1 = f32[6]{0} parameter(0)\n  y.1 = f32[6]{0} parameter(1)\n  ROOT mul.1 = f32[6]{0} multiply(x.1, y.1)\n}\n\n",
    "input_shapes": [
      [
        6
      ],
      [
        6
      ]
    ],
    "output_shape": [
      6
    ]
  },
  {
    "name": "vmap_ljno",
    "python_source": "def vmap_ljno(a_batch, b_batch):\n    \"\"\"Batched operation using vmap.\"\"\"\n    def inner(a, b):\n        return jnp.dot(a, b)\n    return jax.vmap(inner)(a_batch, b_batch)",
    "jaxpr": "{ lambda ; a:f32[7,16] b:f32[7,16]. let\n    c:f32[7] = dot_general[\n      dimension_numbers=(([1], [1]), ([0], [0]))\n      preferred_element_type=float32\n    ] a b\n  in (c,) }",
    "stablehlo": "module @jit_vmap_ljno attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x16xf32>, %arg1: tensor<7x16xf32>) -> (tensor<7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.dot_general %arg0, %arg1, batching_dims = [0] x [0], contracting_dims = [1] x [1], precision = [DEFAULT, DEFAULT] : (tensor<7x16xf32>, tensor<7x16xf32>) -> tensor<7xf32>\n    return %0 : tensor<7xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_vmap_ljno, entry_computation_layout={(f32[7,16]{1,0}, f32[7,16]{1,0})->f32[7]{0}}\n\nENTRY main.1 {\n  a_batch.1 = f32[7,16]{1,0} parameter(0)\n  b_batch.1 = f32[7,16]{1,0} parameter(1)\n  ROOT dot_general.1 = f32[7]{0} dot(a_batch.1, b_batch.1), lhs_batch_dims={0}, lhs_contracting_dims={1}, rhs_batch_dims={0}, rhs_contracting_dims={1}\n}\n\n",
    "input_shapes": [
      [
        7,
        16
      ],
      [
        7,
        16
      ]
    ],
    "output_shape": [
      7
    ]
  },
  {
    "name": "mul_wqhl",
    "python_source": "def mul_wqhl(x, y):\n    \"\"\"Multiplication operation.\"\"\"\n    return x * y",
    "jaxpr": "{ lambda ; a:f32[8,6] b:f32[8,6]. let c:f32[8,6] = mul a b in (c,) }",
    "stablehlo": "module @jit_mul_wqhl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x6xf32>, %arg1: tensor<8x6xf32>) -> (tensor<8x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<8x6xf32>\n    return %0 : tensor<8x6xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_mul_wqhl, entry_computation_layout={(f32[8,6]{1,0}, f32[8,6]{1,0})->f32[8,6]{1,0}}\n\nENTRY main.1 {\n  x.1 = f32[8,6]{1,0} parameter(0)\n  y.1 = f32[8,6]{1,0} parameter(1)\n  ROOT mul.1 = f32[8,6]{1,0} multiply(x.1, y.1)\n}\n\n",
    "input_shapes": [
      [
        8,
        6
      ],
      [
        8,
        6
      ]
    ],
    "output_shape": [
      8,
      6
    ]
  },
  {
    "name": "matmul_pimb",
    "python_source": "def matmul_pimb(a, b):\n    \"\"\"Matrix multiplication of shapes (9,2) @ (2,5).\"\"\"\n    return jnp.dot(a, b)",
    "jaxpr": "{ lambda ; a:f32[9,2] b:f32[2,5]. let\n    c:f32[9,5] = dot_general[\n      dimension_numbers=(([1], [0]), ([], []))\n      preferred_element_type=float32\n    ] a b\n  in (c,) }",
    "stablehlo": "module @jit_matmul_pimb attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x2xf32>, %arg1: tensor<2x5xf32>) -> (tensor<9x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.dot_general %arg0, %arg1, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<9x2xf32>, tensor<2x5xf32>) -> tensor<9x5xf32>\n    return %0 : tensor<9x5xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_matmul_pimb, entry_computation_layout={(f32[9,2]{1,0}, f32[2,5]{1,0})->f32[9,5]{1,0}}\n\nENTRY main.1 {\n  a.1 = f32[9,2]{1,0} parameter(0)\n  b.1 = f32[2,5]{1,0} parameter(1)\n  ROOT dot_general.1 = f32[9,5]{1,0} dot(a.1, b.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n}\n\n",
    "input_shapes": [
      [
        9,
        2
      ],
      [
        2,
        5
      ]
    ],
    "output_shape": [
      9,
      5
    ]
  },
  {
    "name": "mean_xrqy",
    "python_source": "def mean_xrqy(x):\n    \"\"\"Mean reduction along axis 0.\"\"\"\n    return jnp.mean(x, axis=0)",
    "jaxpr": "{ lambda ; a:f32[3,11]. let\n    b:f32[11] = reduce_sum[axes=(0,) out_sharding=None] a\n    c:f32[11] = div b 3.0:f32[]\n  in (c,) }",
    "stablehlo": "module @jit_mean_xrqy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x11xf32>) -> (tensor<11xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<3x11xf32>, tensor<f32>) -> tensor<11xf32>\n    %cst_0 = stablehlo.constant dense<3.000000e+00> : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<11xf32>\n    %2 = stablehlo.divide %0, %1 : tensor<11xf32>\n    return %2 : tensor<11xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_mean_xrqy, entry_computation_layout={(f32[3,11]{1,0})->f32[11]{0}}\n\nregion_0.1 {\n  reduce_sum.3 = f32[] parameter(0)\n  reduce_sum.4 = f32[] parameter(1)\n  ROOT reduce_sum.5 = f32[] add(reduce_sum.3, reduce_sum.4)\n}\n\nENTRY main.2 {\n  x.1 = f32[3,11]{1,0} parameter(0)\n  constant.3 = f32[] constant(0)\n  reduce_sum.7 = f32[11]{0} reduce(x.1, constant.3), dimensions={0}, to_apply=region_0.1\n  constant.2 = f32[] constant(3)\n  broadcast.1 = f32[11]{0} broadcast(constant.2), dimensions={}\n  ROOT div.1 = f32[11]{0} divide(reduce_sum.7, broadcast.1)\n}\n\n",
    "input_shapes": [
      [
        3,
        11
      ]
    ],
    "output_shape": [
      11
    ]
  },
  {
    "name": "vmap_kejx",
    "python_source": "def vmap_kejx(a_batch, b_batch):\n    \"\"\"Batched operation using vmap.\"\"\"\n    def inner(a, b):\n        return jnp.sum(a * b)\n    return jax.vmap(inner)(a_batch, b_batch)",
    "jaxpr": "{ lambda ; a:f32[7,9] b:f32[7,9]. let\n    c:f32[7,9] = mul a b\n    d:f32[7] = reduce_sum[axes=(np.int64(1),) out_sharding=None] c\n  in (d,) }",
    "stablehlo": "module @jit_vmap_kejx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x9xf32>, %arg1: tensor<7x9xf32>) -> (tensor<7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<7x9xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<7x9xf32>, tensor<f32>) -> tensor<7xf32>\n    return %1 : tensor<7xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_vmap_kejx, entry_computation_layout={(f32[7,9]{1,0}, f32[7,9]{1,0})->f32[7]{0}}\n\nregion_0.1 {\n  reduce_sum.3 = f32[] parameter(0)\n  reduce_sum.4 = f32[] parameter(1)\n  ROOT reduce_sum.5 = f32[] add(reduce_sum.3, reduce_sum.4)\n}\n\nENTRY main.2 {\n  a_batch.1 = f32[7,9]{1,0} parameter(0)\n  b_batch.1 = f32[7,9]{1,0} parameter(1)\n  mul.1 = f32[7,9]{1,0} multiply(a_batch.1, b_batch.1)\n  constant.1 = f32[] constant(0)\n  ROOT reduce_sum.7 = f32[7]{0} reduce(mul.1, constant.1), dimensions={1}, to_apply=region_0.1\n}\n\n",
    "input_shapes": [
      [
        7,
        9
      ],
      [
        7,
        9
      ]
    ],
    "output_shape": [
      7
    ]
  },
  {
    "name": "sigmoid_hube",
    "python_source": "def sigmoid_hube(x):\n    \"\"\"Sigmoid activation operation.\"\"\"\n    return jax.nn.sigmoid(x)",
    "jaxpr": "{ lambda ; a:f32[4]. let b:f32[4] = logistic a in (b,) }",
    "stablehlo": "module @jit_sigmoid_hube attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.negate %arg0 : tensor<4xf32>\n    %1 = stablehlo.exponential %0 : tensor<4xf32>\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %3 = stablehlo.add %2, %1 : tensor<4xf32>\n    %cst_0 = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %5 = stablehlo.divide %4, %3 : tensor<4xf32>\n    return %5 : tensor<4xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_sigmoid_hube, entry_computation_layout={(f32[4]{0})->f32[4]{0}}\n\nENTRY main.1 {\n  constant.1 = f32[] constant(1)\n  broadcast.1 = f32[4]{0} broadcast(constant.1), dimensions={}\n  x.1 = f32[4]{0} parameter(0)\n  neg.1 = f32[4]{0} negate(x.1)\n  exp.1 = f32[4]{0} exponential(neg.1)\n  add.1 = f32[4]{0} add(exp.1, broadcast.1)\n  ROOT div.1 = f32[4]{0} divide(broadcast.1, add.1)\n}\n\n",
    "input_shapes": [
      [
        4
      ]
    ],
    "output_shape": [
      4
    ]
  },
  {
    "name": "vmap_hifd",
    "python_source": "def vmap_hifd(a_batch, b_batch):\n    \"\"\"Batched operation using vmap.\"\"\"\n    def inner(a, b):\n        return jnp.sum(a * b)\n    return jax.vmap(inner)(a_batch, b_batch)",
    "jaxpr": "{ lambda ; a:f32[6,4] b:f32[6,4]. let\n    c:f32[6,4] = mul a b\n    d:f32[6] = reduce_sum[axes=(np.int64(1),) out_sharding=None] c\n  in (d,) }",
    "stablehlo": "module @jit_vmap_hifd attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x4xf32>, %arg1: tensor<6x4xf32>) -> (tensor<6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<6x4xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<6x4xf32>, tensor<f32>) -> tensor<6xf32>\n    return %1 : tensor<6xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_vmap_hifd, entry_computation_layout={(f32[6,4]{1,0}, f32[6,4]{1,0})->f32[6]{0}}\n\nregion_0.1 {\n  reduce_sum.3 = f32[] parameter(0)\n  reduce_sum.4 = f32[] parameter(1)\n  ROOT reduce_sum.5 = f32[] add(reduce_sum.3, reduce_sum.4)\n}\n\nENTRY main.2 {\n  a_batch.1 = f32[6,4]{1,0} parameter(0)\n  b_batch.1 = f32[6,4]{1,0} parameter(1)\n  mul.1 = f32[6,4]{1,0} multiply(a_batch.1, b_batch.1)\n  constant.1 = f32[] constant(0)\n  ROOT reduce_sum.7 = f32[6]{0} reduce(mul.1, constant.1), dimensions={1}, to_apply=region_0.1\n}\n\n",
    "input_shapes": [
      [
        6,
        4
      ],
      [
        6,
        4
      ]
    ],
    "output_shape": [
      6
    ]
  },
  {
    "name": "matmul_ibcd",
    "python_source": "def matmul_ibcd(a, b):\n    \"\"\"Matrix multiplication of shapes (5,6) @ (6,11).\"\"\"\n    return jnp.dot(a, b)",
    "jaxpr": "{ lambda ; a:f32[5,6] b:f32[6,11]. let\n    c:f32[5,11] = dot_general[\n      dimension_numbers=(([1], [0]), ([], []))\n      preferred_element_type=float32\n    ] a b\n  in (c,) }",
    "stablehlo": "module @jit_matmul_ibcd attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x6xf32>, %arg1: tensor<6x11xf32>) -> (tensor<5x11xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.dot_general %arg0, %arg1, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<5x6xf32>, tensor<6x11xf32>) -> tensor<5x11xf32>\n    return %0 : tensor<5x11xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_matmul_ibcd, entry_computation_layout={(f32[5,6]{1,0}, f32[6,11]{1,0})->f32[5,11]{1,0}}\n\nENTRY main.1 {\n  a.1 = f32[5,6]{1,0} parameter(0)\n  b.1 = f32[6,11]{1,0} parameter(1)\n  ROOT dot_general.1 = f32[5,11]{1,0} dot(a.1, b.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n}\n\n",
    "input_shapes": [
      [
        5,
        6
      ],
      [
        6,
        11
      ]
    ],
    "output_shape": [
      5,
      11
    ]
  },
  {
    "name": "composite_jzhw",
    "python_source": "def composite_jzhw(x):\n    \"\"\"Composite function with multiple operations.\"\"\"\n    return jnp.exp(-jnp.abs(x)) + x ** 2",
    "jaxpr": "{ lambda ; a:f32[3,3]. let\n    b:f32[3,3] = abs a\n    c:f32[3,3] = neg b\n    d:f32[3,3] = exp c\n    e:f32[3,3] = integer_pow[y=2] a\n    f:f32[3,3] = add d e\n  in (f,) }",
    "stablehlo": "module @jit_composite_jzhw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x3xf32>) -> (tensor<3x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.abs %arg0 : tensor<3x3xf32>\n    %1 = stablehlo.negate %0 : tensor<3x3xf32>\n    %2 = stablehlo.exponential %1 : tensor<3x3xf32>\n    %3 = stablehlo.multiply %arg0, %arg0 : tensor<3x3xf32>\n    %4 = stablehlo.add %2, %3 : tensor<3x3xf32>\n    return %4 : tensor<3x3xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_composite_jzhw, entry_computation_layout={(f32[3,3]{1,0})->f32[3,3]{1,0}}\n\nENTRY main.1 {\n  x.1 = f32[3,3]{1,0} parameter(0)\n  abs.1 = f32[3,3]{1,0} abs(x.1)\n  neg.1 = f32[3,3]{1,0} negate(abs.1)\n  exp.1 = f32[3,3]{1,0} exponential(neg.1)\n  integer_pow.1 = f32[3,3]{1,0} multiply(x.1, x.1)\n  ROOT add.1 = f32[3,3]{1,0} add(exp.1, integer_pow.1)\n}\n\n",
    "input_shapes": [
      [
        3,
        3
      ]
    ],
    "output_shape": [
      3,
      3
    ]
  },
  {
    "name": "composite_anpi",
    "python_source": "def composite_anpi(x):\n    \"\"\"Composite function with multiple operations.\"\"\"\n    return x ** 2 + jnp.exp(-jnp.abs(x))",
    "jaxpr": "{ lambda ; a:f32[14,14]. let\n    b:f32[14,14] = integer_pow[y=2] a\n    c:f32[14,14] = abs a\n    d:f32[14,14] = neg c\n    e:f32[14,14] = exp d\n    f:f32[14,14] = add b e\n  in (f,) }",
    "stablehlo": "module @jit_composite_anpi attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<14x14xf32>) -> (tensor<14x14xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<14x14xf32>\n    %1 = stablehlo.abs %arg0 : tensor<14x14xf32>\n    %2 = stablehlo.negate %1 : tensor<14x14xf32>\n    %3 = stablehlo.exponential %2 : tensor<14x14xf32>\n    %4 = stablehlo.add %0, %3 : tensor<14x14xf32>\n    return %4 : tensor<14x14xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_composite_anpi, entry_computation_layout={(f32[14,14]{1,0})->f32[14,14]{1,0}}\n\nENTRY main.1 {\n  x.1 = f32[14,14]{1,0} parameter(0)\n  integer_pow.1 = f32[14,14]{1,0} multiply(x.1, x.1)\n  abs.1 = f32[14,14]{1,0} abs(x.1)\n  neg.1 = f32[14,14]{1,0} negate(abs.1)\n  exp.1 = f32[14,14]{1,0} exponential(neg.1)\n  ROOT add.1 = f32[14,14]{1,0} add(integer_pow.1, exp.1)\n}\n\n",
    "input_shapes": [
      [
        14,
        14
      ]
    ],
    "output_shape": [
      14,
      14
    ]
  },
  {
    "name": "matmul_arwj",
    "python_source": "def matmul_arwj(a, b):\n    \"\"\"Matrix multiplication of shapes (15,10) @ (10,12).\"\"\"\n    return jnp.dot(a, b)",
    "jaxpr": "{ lambda ; a:f32[15,10] b:f32[10,12]. let\n    c:f32[15,12] = dot_general[\n      dimension_numbers=(([1], [0]), ([], []))\n      preferred_element_type=float32\n    ] a b\n  in (c,) }",
    "stablehlo": "module @jit_matmul_arwj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<15x10xf32>, %arg1: tensor<10x12xf32>) -> (tensor<15x12xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.dot_general %arg0, %arg1, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<15x10xf32>, tensor<10x12xf32>) -> tensor<15x12xf32>\n    return %0 : tensor<15x12xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_matmul_arwj, entry_computation_layout={(f32[15,10]{1,0}, f32[10,12]{1,0})->f32[15,12]{1,0}}\n\nENTRY main.1 {\n  a.1 = f32[15,10]{1,0} parameter(0)\n  b.1 = f32[10,12]{1,0} parameter(1)\n  ROOT dot_general.1 = f32[15,12]{1,0} dot(a.1, b.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n}\n\n",
    "input_shapes": [
      [
        15,
        10
      ],
      [
        10,
        12
      ]
    ],
    "output_shape": [
      15,
      12
    ]
  },
  {
    "name": "composite_ambi",
    "python_source": "def composite_ambi(x):\n    \"\"\"Composite function with multiple operations.\"\"\"\n    return x ** 2 + jax.nn.relu(x)",
    "jaxpr": "{ lambda ; a:f32[14,6]. let\n    b:f32[14,6] = integer_pow[y=2] a\n    c:f32[14,6] = custom_jvp_call[\n      name=relu\n      call_jaxpr={ lambda ; d:f32[14,6]. let\n          e:f32[14,6] = jit[\n            name=relu\n            jaxpr={ lambda ; d:f32[14,6]. let\n                e:f32[14,6] = max d 0.0:f32[]\n              in (e,) }\n          ] d\n        in (e,) }\n      jvp=jvp\n      symbolic_zeros=False\n    ] a\n    f:f32[14,6] = add b c\n  in (f,) }",
    "stablehlo": "module @jit_composite_ambi attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<14x6xf32>) -> (tensor<14x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<14x6xf32>\n    %1 = call @relu(%arg0) : (tensor<14x6xf32>) -> tensor<14x6xf32>\n    %2 = stablehlo.add %0, %1 : tensor<14x6xf32>\n    return %2 : tensor<14x6xf32>\n  }\n  func.func private @relu(%arg0: tensor<14x6xf32>) -> tensor<14x6xf32> {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<14x6xf32>\n    %1 = stablehlo.maximum %arg0, %0 : tensor<14x6xf32>\n    return %1 : tensor<14x6xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_composite_ambi, entry_computation_layout={(f32[14,6]{1,0})->f32[14,6]{1,0}}\n\nrelu.1 {\n  Arg_0.1 = f32[14,6]{1,0} parameter(0)\n  constant.1 = f32[] constant(0)\n  max.2 = f32[14,6]{1,0} broadcast(constant.1), dimensions={}\n  ROOT max.3 = f32[14,6]{1,0} maximum(Arg_0.1, max.2)\n}\n\nENTRY main.2 {\n  x.1 = f32[14,6]{1,0} parameter(0)\n  integer_pow.1 = f32[14,6]{1,0} multiply(x.1, x.1)\n  jit_relu_.1 = f32[14,6]{1,0} call(x.1), to_apply=relu.1\n  ROOT add.1 = f32[14,6]{1,0} add(integer_pow.1, jit_relu_.1)\n}\n\n",
    "input_shapes": [
      [
        14,
        6
      ]
    ],
    "output_shape": [
      14,
      6
    ]
  },
  {
    "name": "composite_cyce",
    "python_source": "def composite_cyce(x):\n    \"\"\"Composite function with multiple operations.\"\"\"\n    return jnp.exp(-jnp.abs(x)) + jax.nn.relu(x)",
    "jaxpr": "{ lambda ; a:f32[2,6]. let\n    b:f32[2,6] = abs a\n    c:f32[2,6] = neg b\n    d:f32[2,6] = exp c\n    e:f32[2,6] = custom_jvp_call[\n      name=relu\n      call_jaxpr={ lambda ; f:f32[2,6]. let\n          g:f32[2,6] = jit[\n            name=relu\n            jaxpr={ lambda ; f:f32[2,6]. let\n                g:f32[2,6] = max f 0.0:f32[]\n              in (g,) }\n          ] f\n        in (g,) }\n      jvp=jvp\n      symbolic_zeros=False\n    ] a\n    h:f32[2,6] = add d e\n  in (h,) }",
    "stablehlo": "module @jit_composite_cyce attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x6xf32>) -> (tensor<2x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.abs %arg0 : tensor<2x6xf32>\n    %1 = stablehlo.negate %0 : tensor<2x6xf32>\n    %2 = stablehlo.exponential %1 : tensor<2x6xf32>\n    %3 = call @relu(%arg0) : (tensor<2x6xf32>) -> tensor<2x6xf32>\n    %4 = stablehlo.add %2, %3 : tensor<2x6xf32>\n    return %4 : tensor<2x6xf32>\n  }\n  func.func private @relu(%arg0: tensor<2x6xf32>) -> tensor<2x6xf32> {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<2x6xf32>\n    %1 = stablehlo.maximum %arg0, %0 : tensor<2x6xf32>\n    return %1 : tensor<2x6xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_composite_cyce, entry_computation_layout={(f32[2,6]{1,0})->f32[2,6]{1,0}}\n\nrelu.1 {\n  Arg_0.1 = f32[2,6]{1,0} parameter(0)\n  constant.1 = f32[] constant(0)\n  broadcast.1 = f32[2,6]{1,0} broadcast(constant.1), dimensions={}\n  ROOT max.1 = f32[2,6]{1,0} maximum(Arg_0.1, broadcast.1)\n}\n\nENTRY main.2 {\n  x.1 = f32[2,6]{1,0} parameter(0)\n  abs.1 = f32[2,6]{1,0} abs(x.1)\n  neg.1 = f32[2,6]{1,0} negate(abs.1)\n  exp.1 = f32[2,6]{1,0} exponential(neg.1)\n  jit_relu_.1 = f32[2,6]{1,0} call(x.1), to_apply=relu.1\n  ROOT add.1 = f32[2,6]{1,0} add(exp.1, jit_relu_.1)\n}\n\n",
    "input_shapes": [
      [
        2,
        6
      ]
    ],
    "output_shape": [
      2,
      6
    ]
  },
  {
    "name": "matmul_mbff",
    "python_source": "def matmul_mbff(a, b):\n    \"\"\"Matrix multiplication of shapes (13,14) @ (14,4).\"\"\"\n    return jnp.dot(a, b)",
    "jaxpr": "{ lambda ; a:f32[13,14] b:f32[14,4]. let\n    c:f32[13,4] = dot_general[\n      dimension_numbers=(([1], [0]), ([], []))\n      preferred_element_type=float32\n    ] a b\n  in (c,) }",
    "stablehlo": "module @jit_matmul_mbff attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<13x14xf32>, %arg1: tensor<14x4xf32>) -> (tensor<13x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.dot_general %arg0, %arg1, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<13x14xf32>, tensor<14x4xf32>) -> tensor<13x4xf32>\n    return %0 : tensor<13x4xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_matmul_mbff, entry_computation_layout={(f32[13,14]{1,0}, f32[14,4]{1,0})->f32[13,4]{1,0}}\n\nENTRY main.1 {\n  a.1 = f32[13,14]{1,0} parameter(0)\n  b.1 = f32[14,4]{1,0} parameter(1)\n  ROOT dot_general.1 = f32[13,4]{1,0} dot(a.1, b.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n}\n\n",
    "input_shapes": [
      [
        13,
        14
      ],
      [
        14,
        4
      ]
    ],
    "output_shape": [
      13,
      4
    ]
  },
  {
    "name": "matmul_pgzz",
    "python_source": "def matmul_pgzz(a, b):\n    \"\"\"Matrix multiplication of shapes (16,4) @ (4,16).\"\"\"\n    return jnp.dot(a, b)",
    "jaxpr": "{ lambda ; a:f32[16,4] b:f32[4,16]. let\n    c:f32[16,16] = dot_general[\n      dimension_numbers=(([1], [0]), ([], []))\n      preferred_element_type=float32\n    ] a b\n  in (c,) }",
    "stablehlo": "module @jit_matmul_pgzz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<16x4xf32>, %arg1: tensor<4x16xf32>) -> (tensor<16x16xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.dot_general %arg0, %arg1, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<16x4xf32>, tensor<4x16xf32>) -> tensor<16x16xf32>\n    return %0 : tensor<16x16xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_matmul_pgzz, entry_computation_layout={(f32[16,4]{1,0}, f32[4,16]{1,0})->f32[16,16]{1,0}}\n\nENTRY main.1 {\n  a.1 = f32[16,4]{1,0} parameter(0)\n  b.1 = f32[4,16]{1,0} parameter(1)\n  ROOT dot_general.1 = f32[16,16]{1,0} dot(a.1, b.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n}\n\n",
    "input_shapes": [
      [
        16,
        4
      ],
      [
        4,
        16
      ]
    ],
    "output_shape": [
      16,
      16
    ]
  },
  {
    "name": "pow_oofr",
    "python_source": "def pow_oofr(x, y):\n    \"\"\"Power operation.\"\"\"\n    return jnp.power(jnp.abs(x), jnp.abs(y) % 3 + 1)",
    "jaxpr": "{ lambda ; a:f32[15,14] b:f32[15,14]. let\n    c:f32[15,14] = abs a\n    d:f32[15,14] = abs b\n    e:f32[15,14] = jit[\n      name=remainder\n      jaxpr={ lambda ; d:f32[15,14] f:i32[]. let\n          g:f32[] = convert_element_type[new_dtype=float32 weak_type=False] f\n          h:f32[15,14] = rem d g\n          i:bool[15,14] = ne h 0.0:f32[]\n          j:bool[15,14] = lt h 0.0:f32[]\n          k:bool[] = lt g 0.0:f32[]\n          l:bool[15,14] = ne j k\n          m:bool[15,14] = and l i\n          n:f32[15,14] = add h g\n          e:f32[15,14] = select_n m h n\n        in (e,) }\n    ] d 3:i32[]\n    o:f32[15,14] = add e 1.0:f32[]\n    p:f32[15,14] = pow c o\n  in (p,) }",
    "stablehlo": "module @jit_pow_oofr attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<15x14xf32>, %arg1: tensor<15x14xf32>) -> (tensor<15x14xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.abs %arg0 : tensor<15x14xf32>\n    %1 = stablehlo.abs %arg1 : tensor<15x14xf32>\n    %c = stablehlo.constant dense<3> : tensor<i32>\n    %2 = call @remainder(%1, %c) : (tensor<15x14xf32>, tensor<i32>) -> tensor<15x14xf32>\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %3 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<15x14xf32>\n    %4 = stablehlo.add %2, %3 : tensor<15x14xf32>\n    %5 = stablehlo.power %0, %4 : tensor<15x14xf32>\n    return %5 : tensor<15x14xf32>\n  }\n  func.func private @remainder(%arg0: tensor<15x14xf32>, %arg1: tensor<i32>) -> tensor<15x14xf32> {\n    %0 = stablehlo.convert %arg1 : (tensor<i32>) -> tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<15x14xf32>\n    %2 = stablehlo.remainder %arg0, %1 : tensor<15x14xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %3 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<15x14xf32>\n    %4 = stablehlo.compare  NE, %2, %3,  FLOAT : (tensor<15x14xf32>, tensor<15x14xf32>) -> tensor<15x14xi1>\n    %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %5 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<15x14xf32>\n    %6 = stablehlo.compare  LT, %2, %5,  FLOAT : (tensor<15x14xf32>, tensor<15x14xf32>) -> tensor<15x14xi1>\n    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %7 = stablehlo.compare  LT, %0, %cst_1,  FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n    %8 = stablehlo.broadcast_in_dim %7, dims = [] : (tensor<i1>) -> tensor<15x14xi1>\n    %9 = stablehlo.compare  NE, %6, %8,  UNSIGNED : (tensor<15x14xi1>, tensor<15x14xi1>) -> tensor<15x14xi1>\n    %10 = stablehlo.and %9, %4 : tensor<15x14xi1>\n    %11 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<15x14xf32>\n    %12 = stablehlo.add %2, %11 : tensor<15x14xf32>\n    %13 = stablehlo.select %10, %12, %2 : tensor<15x14xi1>, tensor<15x14xf32>\n    return %13 : tensor<15x14xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_pow_oofr, entry_computation_layout={(f32[15,14]{1,0}, f32[15,14]{1,0})->f32[15,14]{1,0}}\n\nremainder.1 {\n  Arg_0.1 = f32[15,14]{1,0} parameter(0)\n  Arg_1.1 = s32[] parameter(1)\n  convert_element_type.1 = f32[] convert(Arg_1.1)\n  rem.2 = f32[15,14]{1,0} broadcast(convert_element_type.1), dimensions={}\n  rem.3 = f32[15,14]{1,0} remainder(Arg_0.1, rem.2)\n  constant.4 = f32[] constant(0)\n  broadcast.1 = f32[15,14]{1,0} broadcast(constant.4), dimensions={}\n  lt.2 = pred[15,14]{1,0} compare(rem.3, broadcast.1), direction=LT\n  constant.5 = f32[] constant(0)\n  lt.3 = pred[] compare(convert_element_type.1, constant.5), direction=LT\n  ne.4 = pred[15,14]{1,0} broadcast(lt.3), dimensions={}\n  ne.5 = pred[15,14]{1,0} compare(lt.2, ne.4), direction=NE\n  ne.3 = pred[15,14]{1,0} compare(rem.3, broadcast.1), direction=NE\n  and.1 = pred[15,14]{1,0} and(ne.5, ne.3)\n  add.3 = f32[15,14]{1,0} broadcast(convert_element_type.1), dimensions={}\n  add.4 = f32[15,14]{1,0} add(rem.3, add.3)\n  ROOT select_n.1 = f32[15,14]{1,0} select(and.1, add.4, rem.3)\n}\n\nENTRY main.2 {\n  x.1 = f32[15,14]{1,0} parameter(0)\n  abs.2 = f32[15,14]{1,0} abs(x.1)\n  y.1 = f32[15,14]{1,0} parameter(1)\n  abs.3 = f32[15,14]{1,0} abs(y.1)\n  constant.7 = s32[] constant(3)\n  jit_remainder_.1 = f32[15,14]{1,0} call(abs.3, constant.7), to_apply=remainder.1\n  constant.6 = f32[] constant(1)\n  add.6 = f32[15,14]{1,0} broadcast(constant.6), dimensions={}\n  add.7 = f32[15,14]{1,0} add(jit_remainder_.1, add.6)\n  ROOT pow.1 = f32[15,14]{1,0} power(abs.2, add.7)\n}\n\n",
    "input_shapes": [
      [
        15,
        14
      ],
      [
        15,
        14
      ]
    ],
    "output_shape": [
      15,
      14
    ]
  },
  {
    "name": "softplus_futo",
    "python_source": "def softplus_futo(x):\n    \"\"\"Softplus operation.\"\"\"\n    return jax.nn.softplus(x)",
    "jaxpr": "{ lambda ; a:f32[3]. let\n    b:f32[3] = jit[\n      name=softplus\n      jaxpr={ lambda ; a:f32[3]. let\n          b:f32[3] = custom_jvp_call[\n            name=logaddexp\n            call_jaxpr={ lambda ; c:f32[3] d:f32[]. let\n                e:f32[3] = max c d\n                f:f32[3] = sub c d\n                g:bool[3] = ne f f\n                h:f32[3] = add c d\n                i:f32[3] = abs f\n                j:f32[3] = neg i\n                k:f32[3] = exp j\n                l:f32[3] = log1p k\n                m:f32[3] = add e l\n                n:f32[3] = select_n g m h\n              in (n,) }\n            jvp=_logaddexp_jvp\n            symbolic_zeros=False\n          ] a 0.0:f32[]\n        in (b,) }\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit_softplus_futo attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3xf32>) -> (tensor<3xf32> {jax.result_info = \"result\"}) {\n    %0 = call @softplus(%arg0) : (tensor<3xf32>) -> tensor<3xf32>\n    return %0 : tensor<3xf32>\n  }\n  func.func private @softplus(%arg0: tensor<3xf32>) -> tensor<3xf32> {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<3xf32>\n    %1 = stablehlo.maximum %arg0, %0 : tensor<3xf32>\n    %2 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<3xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<3xf32>\n    %4 = stablehlo.compare  NE, %3, %3,  FLOAT : (tensor<3xf32>, tensor<3xf32>) -> tensor<3xi1>\n    %5 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<3xf32>\n    %6 = stablehlo.add %arg0, %5 : tensor<3xf32>\n    %7 = stablehlo.abs %3 : tensor<3xf32>\n    %8 = stablehlo.negate %7 : tensor<3xf32>\n    %9 = stablehlo.exponential %8 : tensor<3xf32>\n    %10 = stablehlo.log_plus_one %9 : tensor<3xf32>\n    %11 = stablehlo.add %1, %10 : tensor<3xf32>\n    %12 = stablehlo.select %4, %6, %11 : tensor<3xi1>, tensor<3xf32>\n    return %12 : tensor<3xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_softplus_futo, entry_computation_layout={(f32[3]{0})->f32[3]{0}}\n\nsoftplus.1 {\n  Arg_0.1 = f32[3]{0} parameter(0)\n  ne.1 = pred[3]{0} compare(Arg_0.1, Arg_0.1), direction=NE\n  constant.1 = f32[] constant(0)\n  broadcast.1 = f32[3]{0} broadcast(constant.1), dimensions={}\n  max.1 = f32[3]{0} maximum(Arg_0.1, broadcast.1)\n  abs.1 = f32[3]{0} abs(Arg_0.1)\n  neg.1 = f32[3]{0} negate(abs.1)\n  exp.1 = f32[3]{0} exponential(neg.1)\n  log1p.1 = f32[3]{0} log-plus-one(exp.1)\n  add.1 = f32[3]{0} add(max.1, log1p.1)\n  ROOT select_n.1 = f32[3]{0} select(ne.1, Arg_0.1, add.1)\n}\n\nENTRY main.2 {\n  x.1 = f32[3]{0} parameter(0)\n  ROOT jit_softplus_.1 = f32[3]{0} call(x.1), to_apply=softplus.1\n}\n\n",
    "input_shapes": [
      [
        3
      ]
    ],
    "output_shape": [
      3
    ]
  },
  {
    "name": "prod_gzfj",
    "python_source": "def prod_gzfj(x):\n    \"\"\"Product reduction along axis 1.\"\"\"\n    return jnp.prod(x, axis=1)",
    "jaxpr": "{ lambda ; a:f32[9,12,8,10]. let\n    b:f32[9,8,10] = reduce_prod[axes=(1,)] a\n  in (b,) }",
    "stablehlo": "module @jit_prod_gzfj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x12x8x10xf32>) -> (tensor<9x8x10xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.multiply across dimensions = [1] : (tensor<9x12x8x10xf32>, tensor<f32>) -> tensor<9x8x10xf32>\n    return %0 : tensor<9x8x10xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_prod_gzfj, entry_computation_layout={(f32[9,12,8,10]{3,2,1,0})->f32[9,8,10]{2,1,0}}\n\nregion_0.1 {\n  reduce_prod.3 = f32[] parameter(0)\n  reduce_prod.4 = f32[] parameter(1)\n  ROOT reduce_prod.5 = f32[] multiply(reduce_prod.3, reduce_prod.4)\n}\n\nENTRY main.2 {\n  x.1 = f32[9,12,8,10]{3,2,1,0} parameter(0)\n  constant.1 = f32[] constant(1)\n  ROOT reduce_prod.7 = f32[9,8,10]{2,1,0} reduce(x.1, constant.1), dimensions={1}, to_apply=region_0.1\n}\n\n",
    "input_shapes": [
      [
        9,
        12,
        8,
        10
      ]
    ],
    "output_shape": [
      9,
      8,
      10
    ]
  },
  {
    "name": "nn_layer_lvnn",
    "python_source": "def nn_layer_lvnn(x, w, b):\n    \"\"\"Neural network layer with gelu activation.\"\"\"\n    return jax.nn.gelu(jnp.dot(x, w) + b)",
    "jaxpr": "{ lambda ; a:f32[6,11] b:f32[11,16] c:f32[16]. let\n    d:f32[6,16] = dot_general[\n      dimension_numbers=(([1], [0]), ([], []))\n      preferred_element_type=float32\n    ] a b\n    e:f32[1,16] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 16)\n      sharding=None\n    ] c\n    f:f32[6,16] = add d e\n    g:f32[6,16] = integer_pow[y=3] f\n    h:f32[6,16] = mul 0.044714998453855515:f32[] g\n    i:f32[6,16] = add f h\n    j:f32[6,16] = mul 0.7978845834732056:f32[] i\n    k:f32[6,16] = tanh j\n    l:f32[6,16] = add 1.0:f32[] k\n    m:f32[6,16] = mul 0.5:f32[] l\n    n:f32[6,16] = mul f m\n  in (n,) }",
    "stablehlo": "module @jit_nn_layer_lvnn attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x11xf32>, %arg1: tensor<11x16xf32>, %arg2: tensor<16xf32>) -> (tensor<6x16xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.dot_general %arg0, %arg1, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<6x11xf32>, tensor<11x16xf32>) -> tensor<6x16xf32>\n    %1 = stablehlo.broadcast_in_dim %arg2, dims = [1] : (tensor<16xf32>) -> tensor<1x16xf32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x16xf32>) -> tensor<6x16xf32>\n    %3 = stablehlo.add %0, %2 : tensor<6x16xf32>\n    %4 = stablehlo.multiply %3, %3 : tensor<6x16xf32>\n    %5 = stablehlo.multiply %4, %3 : tensor<6x16xf32>\n    %cst = stablehlo.constant dense<4.471500e-02> : tensor<f32>\n    %6 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<6x16xf32>\n    %7 = stablehlo.multiply %6, %5 : tensor<6x16xf32>\n    %8 = stablehlo.add %3, %7 : tensor<6x16xf32>\n    %cst_0 = stablehlo.constant dense<0.797884583> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<6x16xf32>\n    %10 = stablehlo.multiply %9, %8 : tensor<6x16xf32>\n    %11 = stablehlo.tanh %10 : tensor<6x16xf32>\n    %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %12 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<6x16xf32>\n    %13 = stablehlo.add %12, %11 : tensor<6x16xf32>\n    %cst_2 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %14 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<6x16xf32>\n    %15 = stablehlo.multiply %14, %13 : tensor<6x16xf32>\n    %16 = stablehlo.multiply %3, %15 : tensor<6x16xf32>\n    return %16 : tensor<6x16xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_nn_layer_lvnn, entry_computation_layout={(f32[6,11]{1,0}, f32[11,16]{1,0}, f32[16]{0})->f32[6,16]{1,0}}\n\nENTRY main.1 {\n  x.1 = f32[6,11]{1,0} parameter(0)\n  w.1 = f32[11,16]{1,0} parameter(1)\n  dot_general.1 = f32[6,16]{1,0} dot(x.1, w.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n  b.1 = f32[16]{0} parameter(2)\n  broadcast_in_dim.1 = f32[1,16]{1,0} reshape(b.1)\n  add.8 = f32[1,16]{1,0} broadcast(broadcast_in_dim.1), dimensions={0,1}\n  add.9 = f32[16]{0} reshape(add.8)\n  add.10 = f32[6,16]{1,0} broadcast(add.9), dimensions={1}\n  add.11 = f32[6,16]{1,0} add(dot_general.1, add.10)\n  integer_pow.2 = f32[6,16]{1,0} multiply(add.11, add.11)\n  integer_pow.3 = f32[6,16]{1,0} multiply(integer_pow.2, add.11)\n  constant.7 = f32[] constant(0.044715)\n  mul.9 = f32[6,16]{1,0} broadcast(constant.7), dimensions={}\n  mul.10 = f32[6,16]{1,0} multiply(integer_pow.3, mul.9)\n  add.12 = f32[6,16]{1,0} add(add.11, mul.10)\n  constant.6 = f32[] constant(0.797884583)\n  mul.8 = f32[6,16]{1,0} broadcast(constant.6), dimensions={}\n  mul.11 = f32[6,16]{1,0} multiply(add.12, mul.8)\n  tanh.1 = f32[6,16]{1,0} tanh(mul.11)\n  constant.5 = f32[] constant(1)\n  add.7 = f32[6,16]{1,0} broadcast(constant.5), dimensions={}\n  add.13 = f32[6,16]{1,0} add(tanh.1, add.7)\n  constant.4 = f32[] constant(0.5)\n  mul.7 = f32[6,16]{1,0} broadcast(constant.4), dimensions={}\n  mul.12 = f32[6,16]{1,0} multiply(add.13, mul.7)\n  ROOT mul.13 = f32[6,16]{1,0} multiply(add.11, mul.12)\n}\n\n",
    "input_shapes": [
      [
        6,
        11
      ],
      [
        11,
        16
      ],
      [
        16
      ]
    ],
    "output_shape": [
      6,
      16
    ]
  },
  {
    "name": "nn_layer_cmil",
    "python_source": "def nn_layer_cmil(x, w, b):\n    \"\"\"Neural network layer with sigmoid activation.\"\"\"\n    return jax.nn.sigmoid(jnp.dot(x, w) + b)",
    "jaxpr": "{ lambda ; a:f32[7,22] b:f32[22,7] c:f32[7]. let\n    d:f32[7,7] = dot_general[\n      dimension_numbers=(([1], [0]), ([], []))\n      preferred_element_type=float32\n    ] a b\n    e:f32[1,7] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 7)\n      sharding=None\n    ] c\n    f:f32[7,7] = add d e\n    g:f32[7,7] = logistic f\n  in (g,) }",
    "stablehlo": "module @jit_nn_layer_cmil attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x22xf32>, %arg1: tensor<22x7xf32>, %arg2: tensor<7xf32>) -> (tensor<7x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.dot_general %arg0, %arg1, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<7x22xf32>, tensor<22x7xf32>) -> tensor<7x7xf32>\n    %1 = stablehlo.broadcast_in_dim %arg2, dims = [1] : (tensor<7xf32>) -> tensor<1x7xf32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x7xf32>) -> tensor<7x7xf32>\n    %3 = stablehlo.add %0, %2 : tensor<7x7xf32>\n    %4 = stablehlo.negate %3 : tensor<7x7xf32>\n    %5 = stablehlo.exponential %4 : tensor<7x7xf32>\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %6 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<7x7xf32>\n    %7 = stablehlo.add %6, %5 : tensor<7x7xf32>\n    %cst_0 = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %8 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<7x7xf32>\n    %9 = stablehlo.divide %8, %7 : tensor<7x7xf32>\n    return %9 : tensor<7x7xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_nn_layer_cmil, entry_computation_layout={(f32[7,22]{1,0}, f32[22,7]{1,0}, f32[7]{0})->f32[7,7]{1,0}}\n\nENTRY main.1 {\n  constant.1 = f32[] constant(1)\n  broadcast.1 = f32[7,7]{1,0} broadcast(constant.1), dimensions={}\n  x.1 = f32[7,22]{1,0} parameter(0)\n  w.1 = f32[22,7]{1,0} parameter(1)\n  dot_general.1 = f32[7,7]{1,0} dot(x.1, w.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n  b.1 = f32[7]{0} parameter(2)\n  broadcast_in_dim.1 = f32[1,7]{1,0} reshape(b.1)\n  add.5 = f32[1,7]{1,0} broadcast(broadcast_in_dim.1), dimensions={0,1}\n  add.6 = f32[7]{0} reshape(add.5)\n  add.7 = f32[7,7]{1,0} broadcast(add.6), dimensions={1}\n  add.8 = f32[7,7]{1,0} add(dot_general.1, add.7)\n  neg.1 = f32[7,7]{1,0} negate(add.8)\n  exp.1 = f32[7,7]{1,0} exponential(neg.1)\n  add.9 = f32[7,7]{1,0} add(exp.1, broadcast.1)\n  ROOT div.1 = f32[7,7]{1,0} divide(broadcast.1, add.9)\n}\n\n",
    "input_shapes": [
      [
        7,
        22
      ],
      [
        22,
        7
      ],
      [
        7
      ]
    ],
    "output_shape": [
      7,
      7
    ]
  },
  {
    "name": "max_qxgw",
    "python_source": "def max_qxgw(x, y):\n    \"\"\"Element-wise max operation.\"\"\"\n    return jnp.maximum(x, y)",
    "jaxpr": "{ lambda ; a:f32[12] b:f32[12]. let c:f32[12] = max a b in (c,) }",
    "stablehlo": "module @jit_max_qxgw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<12xf32>, %arg1: tensor<12xf32>) -> (tensor<12xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.maximum %arg0, %arg1 : tensor<12xf32>\n    return %0 : tensor<12xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_max_qxgw, entry_computation_layout={(f32[12]{0}, f32[12]{0})->f32[12]{0}}\n\nENTRY main.1 {\n  x.1 = f32[12]{0} parameter(0)\n  y.1 = f32[12]{0} parameter(1)\n  ROOT max.1 = f32[12]{0} maximum(x.1, y.1)\n}\n\n",
    "input_shapes": [
      [
        12
      ],
      [
        12
      ]
    ],
    "output_shape": [
      12
    ]
  },
  {
    "name": "relu_thum",
    "python_source": "def relu_thum(x):\n    \"\"\"ReLU activation operation.\"\"\"\n    return jax.nn.relu(x)",
    "jaxpr": "{ lambda ; a:f32[13,12]. let\n    b:f32[13,12] = custom_jvp_call[\n      name=relu\n      call_jaxpr={ lambda ; c:f32[13,12]. let\n          d:f32[13,12] = jit[\n            name=relu\n            jaxpr={ lambda ; c:f32[13,12]. let\n                d:f32[13,12] = max c 0.0:f32[]\n              in (d,) }\n          ] c\n        in (d,) }\n      jvp=jvp\n      symbolic_zeros=False\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit_relu_thum attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<13x12xf32>) -> (tensor<13x12xf32> {jax.result_info = \"result\"}) {\n    %0 = call @relu(%arg0) : (tensor<13x12xf32>) -> tensor<13x12xf32>\n    return %0 : tensor<13x12xf32>\n  }\n  func.func private @relu(%arg0: tensor<13x12xf32>) -> tensor<13x12xf32> {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<13x12xf32>\n    %1 = stablehlo.maximum %arg0, %0 : tensor<13x12xf32>\n    return %1 : tensor<13x12xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_relu_thum, entry_computation_layout={(f32[13,12]{1,0})->f32[13,12]{1,0}}\n\nrelu.1 {\n  Arg_0.1 = f32[13,12]{1,0} parameter(0)\n  constant.1 = f32[] constant(0)\n  max.2 = f32[13,12]{1,0} broadcast(constant.1), dimensions={}\n  ROOT max.3 = f32[13,12]{1,0} maximum(Arg_0.1, max.2)\n}\n\nENTRY main.2 {\n  x.1 = f32[13,12]{1,0} parameter(0)\n  ROOT jit_relu_.1 = f32[13,12]{1,0} call(x.1), to_apply=relu.1\n}\n\n",
    "input_shapes": [
      [
        13,
        12
      ]
    ],
    "output_shape": [
      13,
      12
    ]
  },
  {
    "name": "min_qakf",
    "python_source": "def min_qakf(x, y):\n    \"\"\"Element-wise min operation.\"\"\"\n    return jnp.minimum(x, y)",
    "jaxpr": "{ lambda ; a:f32[6,5,3] b:f32[6,5,3]. let c:f32[6,5,3] = min a b in (c,) }",
    "stablehlo": "module @jit_min_qakf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x5x3xf32>, %arg1: tensor<6x5x3xf32>) -> (tensor<6x5x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.minimum %arg0, %arg1 : tensor<6x5x3xf32>\n    return %0 : tensor<6x5x3xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_min_qakf, entry_computation_layout={(f32[6,5,3]{2,1,0}, f32[6,5,3]{2,1,0})->f32[6,5,3]{2,1,0}}\n\nENTRY main.1 {\n  x.1 = f32[6,5,3]{2,1,0} parameter(0)\n  y.1 = f32[6,5,3]{2,1,0} parameter(1)\n  ROOT min.1 = f32[6,5,3]{2,1,0} minimum(x.1, y.1)\n}\n\n",
    "input_shapes": [
      [
        6,
        5,
        3
      ],
      [
        6,
        5,
        3
      ]
    ],
    "output_shape": [
      6,
      5,
      3
    ]
  },
  {
    "name": "vmap_fxad",
    "python_source": "def vmap_fxad(a_batch, b_batch):\n    \"\"\"Batched operation using vmap.\"\"\"\n    def inner(a, b):\n        return jnp.dot(a, b)\n    return jax.vmap(inner)(a_batch, b_batch)",
    "jaxpr": "{ lambda ; a:f32[6,16] b:f32[6,16]. let\n    c:f32[6] = dot_general[\n      dimension_numbers=(([1], [1]), ([0], [0]))\n      preferred_element_type=float32\n    ] a b\n  in (c,) }",
    "stablehlo": "module @jit_vmap_fxad attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x16xf32>, %arg1: tensor<6x16xf32>) -> (tensor<6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.dot_general %arg0, %arg1, batching_dims = [0] x [0], contracting_dims = [1] x [1], precision = [DEFAULT, DEFAULT] : (tensor<6x16xf32>, tensor<6x16xf32>) -> tensor<6xf32>\n    return %0 : tensor<6xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_vmap_fxad, entry_computation_layout={(f32[6,16]{1,0}, f32[6,16]{1,0})->f32[6]{0}}\n\nENTRY main.1 {\n  a_batch.1 = f32[6,16]{1,0} parameter(0)\n  b_batch.1 = f32[6,16]{1,0} parameter(1)\n  ROOT dot_general.1 = f32[6]{0} dot(a_batch.1, b_batch.1), lhs_batch_dims={0}, lhs_contracting_dims={1}, rhs_batch_dims={0}, rhs_contracting_dims={1}\n}\n\n",
    "input_shapes": [
      [
        6,
        16
      ],
      [
        6,
        16
      ]
    ],
    "output_shape": [
      6
    ]
  },
  {
    "name": "mean_dphh",
    "python_source": "def mean_dphh(x):\n    \"\"\"Mean reduction along axis 1.\"\"\"\n    return jnp.mean(x, axis=1)",
    "jaxpr": "{ lambda ; a:f32[8,8]. let\n    b:f32[8] = reduce_sum[axes=(1,) out_sharding=None] a\n    c:f32[8] = div b 8.0:f32[]\n  in (c,) }",
    "stablehlo": "module @jit_mean_dphh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x8xf32>) -> (tensor<8xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<8x8xf32>, tensor<f32>) -> tensor<8xf32>\n    %cst_0 = stablehlo.constant dense<8.000000e+00> : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<8xf32>\n    %2 = stablehlo.divide %0, %1 : tensor<8xf32>\n    return %2 : tensor<8xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_mean_dphh, entry_computation_layout={(f32[8,8]{1,0})->f32[8]{0}}\n\nregion_0.1 {\n  reduce_sum.3 = f32[] parameter(0)\n  reduce_sum.4 = f32[] parameter(1)\n  ROOT reduce_sum.5 = f32[] add(reduce_sum.3, reduce_sum.4)\n}\n\nENTRY main.2 {\n  x.1 = f32[8,8]{1,0} parameter(0)\n  constant.3 = f32[] constant(0)\n  reduce_sum.7 = f32[8]{0} reduce(x.1, constant.3), dimensions={1}, to_apply=region_0.1\n  constant.2 = f32[] constant(8)\n  broadcast.1 = f32[8]{0} broadcast(constant.2), dimensions={}\n  ROOT div.1 = f32[8]{0} divide(reduce_sum.7, broadcast.1)\n}\n\n",
    "input_shapes": [
      [
        8,
        8
      ]
    ],
    "output_shape": [
      8
    ]
  },
  {
    "name": "vmap_qsvo",
    "python_source": "def vmap_qsvo(a_batch, b_batch):\n    \"\"\"Batched operation using vmap.\"\"\"\n    def inner(a, b):\n        return jnp.sum(a * b)\n    return jax.vmap(inner)(a_batch, b_batch)",
    "jaxpr": "{ lambda ; a:f32[4,12] b:f32[4,12]. let\n    c:f32[4,12] = mul a b\n    d:f32[4] = reduce_sum[axes=(np.int64(1),) out_sharding=None] c\n  in (d,) }",
    "stablehlo": "module @jit_vmap_qsvo attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x12xf32>, %arg1: tensor<4x12xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<4x12xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<4x12xf32>, tensor<f32>) -> tensor<4xf32>\n    return %1 : tensor<4xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_vmap_qsvo, entry_computation_layout={(f32[4,12]{1,0}, f32[4,12]{1,0})->f32[4]{0}}\n\nregion_0.1 {\n  reduce_sum.3 = f32[] parameter(0)\n  reduce_sum.4 = f32[] parameter(1)\n  ROOT reduce_sum.5 = f32[] add(reduce_sum.3, reduce_sum.4)\n}\n\nENTRY main.2 {\n  a_batch.1 = f32[4,12]{1,0} parameter(0)\n  b_batch.1 = f32[4,12]{1,0} parameter(1)\n  mul.1 = f32[4,12]{1,0} multiply(a_batch.1, b_batch.1)\n  constant.1 = f32[] constant(0)\n  ROOT reduce_sum.7 = f32[4]{0} reduce(mul.1, constant.1), dimensions={1}, to_apply=region_0.1\n}\n\n",
    "input_shapes": [
      [
        4,
        12
      ],
      [
        4,
        12
      ]
    ],
    "output_shape": [
      4
    ]
  },
  {
    "name": "composite_bmwm",
    "python_source": "def composite_bmwm(x):\n    \"\"\"Composite function with multiple operations.\"\"\"\n    return jax.nn.relu(x) + jnp.exp(-jnp.abs(x)) + jnp.sin(x)",
    "jaxpr": "{ lambda ; a:f32[12,4]. let\n    b:f32[12,4] = custom_jvp_call[\n      name=relu\n      call_jaxpr={ lambda ; c:f32[12,4]. let\n          d:f32[12,4] = jit[\n            name=relu\n            jaxpr={ lambda ; c:f32[12,4]. let\n                d:f32[12,4] = max c 0.0:f32[]\n              in (d,) }\n          ] c\n        in (d,) }\n      jvp=jvp\n      symbolic_zeros=False\n    ] a\n    e:f32[12,4] = abs a\n    f:f32[12,4] = neg e\n    g:f32[12,4] = exp f\n    h:f32[12,4] = add b g\n    i:f32[12,4] = sin a\n    j:f32[12,4] = add h i\n  in (j,) }",
    "stablehlo": "module @jit_composite_bmwm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<12x4xf32>) -> (tensor<12x4xf32> {jax.result_info = \"result\"}) {\n    %0 = call @relu(%arg0) : (tensor<12x4xf32>) -> tensor<12x4xf32>\n    %1 = stablehlo.abs %arg0 : tensor<12x4xf32>\n    %2 = stablehlo.negate %1 : tensor<12x4xf32>\n    %3 = stablehlo.exponential %2 : tensor<12x4xf32>\n    %4 = stablehlo.add %0, %3 : tensor<12x4xf32>\n    %5 = stablehlo.sine %arg0 : tensor<12x4xf32>\n    %6 = stablehlo.add %4, %5 : tensor<12x4xf32>\n    return %6 : tensor<12x4xf32>\n  }\n  func.func private @relu(%arg0: tensor<12x4xf32>) -> tensor<12x4xf32> {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<12x4xf32>\n    %1 = stablehlo.maximum %arg0, %0 : tensor<12x4xf32>\n    return %1 : tensor<12x4xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_composite_bmwm, entry_computation_layout={(f32[12,4]{1,0})->f32[12,4]{1,0}}\n\nrelu.1 {\n  Arg_0.1 = f32[12,4]{1,0} parameter(0)\n  constant.1 = f32[] constant(0)\n  max.2 = f32[12,4]{1,0} broadcast(constant.1), dimensions={}\n  ROOT max.3 = f32[12,4]{1,0} maximum(Arg_0.1, max.2)\n}\n\nENTRY main.2 {\n  x.1 = f32[12,4]{1,0} parameter(0)\n  jit_relu_.1 = f32[12,4]{1,0} call(x.1), to_apply=relu.1\n  abs.1 = f32[12,4]{1,0} abs(x.1)\n  neg.1 = f32[12,4]{1,0} negate(abs.1)\n  exp.1 = f32[12,4]{1,0} exponential(neg.1)\n  add.2 = f32[12,4]{1,0} add(jit_relu_.1, exp.1)\n  sin.1 = f32[12,4]{1,0} sine(x.1)\n  ROOT add.3 = f32[12,4]{1,0} add(add.2, sin.1)\n}\n\n",
    "input_shapes": [
      [
        12,
        4
      ]
    ],
    "output_shape": [
      12,
      4
    ]
  },
  {
    "name": "vmap_drxt",
    "python_source": "def vmap_drxt(a_batch, b_batch):\n    \"\"\"Batched operation using vmap.\"\"\"\n    def inner(a, b):\n        return jnp.sum(a * b)\n    return jax.vmap(inner)(a_batch, b_batch)",
    "jaxpr": "{ lambda ; a:f32[5,5] b:f32[5,5]. let\n    c:f32[5,5] = mul a b\n    d:f32[5] = reduce_sum[axes=(np.int64(1),) out_sharding=None] c\n  in (d,) }",
    "stablehlo": "module @jit_vmap_drxt attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x5xf32>, %arg1: tensor<5x5xf32>) -> (tensor<5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<5x5xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<5x5xf32>, tensor<f32>) -> tensor<5xf32>\n    return %1 : tensor<5xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_vmap_drxt, entry_computation_layout={(f32[5,5]{1,0}, f32[5,5]{1,0})->f32[5]{0}}\n\nregion_0.1 {\n  reduce_sum.3 = f32[] parameter(0)\n  reduce_sum.4 = f32[] parameter(1)\n  ROOT reduce_sum.5 = f32[] add(reduce_sum.3, reduce_sum.4)\n}\n\nENTRY main.2 {\n  a_batch.1 = f32[5,5]{1,0} parameter(0)\n  b_batch.1 = f32[5,5]{1,0} parameter(1)\n  mul.1 = f32[5,5]{1,0} multiply(a_batch.1, b_batch.1)\n  constant.1 = f32[] constant(0)\n  ROOT reduce_sum.7 = f32[5]{0} reduce(mul.1, constant.1), dimensions={1}, to_apply=region_0.1\n}\n\n",
    "input_shapes": [
      [
        5,
        5
      ],
      [
        5,
        5
      ]
    ],
    "output_shape": [
      5
    ]
  },
  {
    "name": "nn_layer_qtfq",
    "python_source": "def nn_layer_qtfq(x, w, b):\n    \"\"\"Neural network layer with relu activation.\"\"\"\n    return jax.nn.relu(jnp.dot(x, w) + b)",
    "jaxpr": "{ lambda ; a:f32[8,8] b:f32[8,24] c:f32[24]. let\n    d:f32[8,24] = dot_general[\n      dimension_numbers=(([1], [0]), ([], []))\n      preferred_element_type=float32\n    ] a b\n    e:f32[1,24] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 24)\n      sharding=None\n    ] c\n    f:f32[8,24] = add d e\n    g:f32[8,24] = custom_jvp_call[\n      name=relu\n      call_jaxpr={ lambda ; h:f32[8,24]. let\n          i:f32[8,24] = jit[\n            name=relu\n            jaxpr={ lambda ; h:f32[8,24]. let\n                i:f32[8,24] = max h 0.0:f32[]\n              in (i,) }\n          ] h\n        in (i,) }\n      jvp=jvp\n      symbolic_zeros=False\n    ] f\n  in (g,) }",
    "stablehlo": "module @jit_nn_layer_qtfq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x8xf32>, %arg1: tensor<8x24xf32>, %arg2: tensor<24xf32>) -> (tensor<8x24xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.dot_general %arg0, %arg1, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<8x8xf32>, tensor<8x24xf32>) -> tensor<8x24xf32>\n    %1 = stablehlo.broadcast_in_dim %arg2, dims = [1] : (tensor<24xf32>) -> tensor<1x24xf32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x24xf32>) -> tensor<8x24xf32>\n    %3 = stablehlo.add %0, %2 : tensor<8x24xf32>\n    %4 = call @relu(%3) : (tensor<8x24xf32>) -> tensor<8x24xf32>\n    return %4 : tensor<8x24xf32>\n  }\n  func.func private @relu(%arg0: tensor<8x24xf32>) -> tensor<8x24xf32> {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<8x24xf32>\n    %1 = stablehlo.maximum %arg0, %0 : tensor<8x24xf32>\n    return %1 : tensor<8x24xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_nn_layer_qtfq, entry_computation_layout={(f32[8,8]{1,0}, f32[8,24]{1,0}, f32[24]{0})->f32[8,24]{1,0}}\n\nrelu.1 {\n  Arg_0.1 = f32[8,24]{1,0} parameter(0)\n  constant.1 = f32[] constant(0)\n  max.2 = f32[8,24]{1,0} broadcast(constant.1), dimensions={}\n  ROOT max.3 = f32[8,24]{1,0} maximum(Arg_0.1, max.2)\n}\n\nENTRY main.2 {\n  x.1 = f32[8,8]{1,0} parameter(0)\n  w.1 = f32[8,24]{1,0} parameter(1)\n  dot_general.1 = f32[8,24]{1,0} dot(x.1, w.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n  b.1 = f32[24]{0} parameter(2)\n  broadcast_in_dim.1 = f32[1,24]{1,0} reshape(b.1)\n  add.4 = f32[1,24]{1,0} broadcast(broadcast_in_dim.1), dimensions={0,1}\n  add.5 = f32[24]{0} reshape(add.4)\n  add.6 = f32[8,24]{1,0} broadcast(add.5), dimensions={1}\n  add.7 = f32[8,24]{1,0} add(dot_general.1, add.6)\n  ROOT jit_relu_.1 = f32[8,24]{1,0} call(add.7), to_apply=relu.1\n}\n\n",
    "input_shapes": [
      [
        8,
        8
      ],
      [
        8,
        24
      ],
      [
        24
      ]
    ],
    "output_shape": [
      8,
      24
    ]
  },
  {
    "name": "mean_vhde",
    "python_source": "def mean_vhde(x):\n    \"\"\"Mean reduction along axis 1.\"\"\"\n    return jnp.mean(x, axis=1)",
    "jaxpr": "{ lambda ; a:f32[12,11,7]. let\n    b:f32[12,7] = reduce_sum[axes=(1,) out_sharding=None] a\n    c:f32[12,7] = div b 11.0:f32[]\n  in (c,) }",
    "stablehlo": "module @jit_mean_vhde attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<12x11x7xf32>) -> (tensor<12x7xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<12x11x7xf32>, tensor<f32>) -> tensor<12x7xf32>\n    %cst_0 = stablehlo.constant dense<1.100000e+01> : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<12x7xf32>\n    %2 = stablehlo.divide %0, %1 : tensor<12x7xf32>\n    return %2 : tensor<12x7xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_mean_vhde, entry_computation_layout={(f32[12,11,7]{2,1,0})->f32[12,7]{1,0}}\n\nregion_0.1 {\n  reduce_sum.3 = f32[] parameter(0)\n  reduce_sum.4 = f32[] parameter(1)\n  ROOT reduce_sum.5 = f32[] add(reduce_sum.3, reduce_sum.4)\n}\n\nENTRY main.2 {\n  x.1 = f32[12,11,7]{2,1,0} parameter(0)\n  constant.3 = f32[] constant(0)\n  reduce_sum.7 = f32[12,7]{1,0} reduce(x.1, constant.3), dimensions={1}, to_apply=region_0.1\n  constant.2 = f32[] constant(11)\n  div.2 = f32[12,7]{1,0} broadcast(constant.2), dimensions={}\n  ROOT div.3 = f32[12,7]{1,0} divide(reduce_sum.7, div.2)\n}\n\n",
    "input_shapes": [
      [
        12,
        11,
        7
      ]
    ],
    "output_shape": [
      12,
      7
    ]
  },
  {
    "name": "sin_gplj",
    "python_source": "def sin_gplj(x):\n    \"\"\"Sine operation.\"\"\"\n    return jnp.sin(x)",
    "jaxpr": "{ lambda ; a:f32[9,4]. let b:f32[9,4] = sin a in (b,) }",
    "stablehlo": "module @jit_sin_gplj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x4xf32>) -> (tensor<9x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<9x4xf32>\n    return %0 : tensor<9x4xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_sin_gplj, entry_computation_layout={(f32[9,4]{1,0})->f32[9,4]{1,0}}\n\nENTRY main.1 {\n  x.1 = f32[9,4]{1,0} parameter(0)\n  ROOT sin.1 = f32[9,4]{1,0} sine(x.1)\n}\n\n",
    "input_shapes": [
      [
        9,
        4
      ]
    ],
    "output_shape": [
      9,
      4
    ]
  },
  {
    "name": "composite_tjro",
    "python_source": "def composite_tjro(x):\n    \"\"\"Composite function with multiple operations.\"\"\"\n    return x ** 2 + jnp.exp(-jnp.abs(x))",
    "jaxpr": "{ lambda ; a:f32[16,6]. let\n    b:f32[16,6] = integer_pow[y=2] a\n    c:f32[16,6] = abs a\n    d:f32[16,6] = neg c\n    e:f32[16,6] = exp d\n    f:f32[16,6] = add b e\n  in (f,) }",
    "stablehlo": "module @jit_composite_tjro attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<16x6xf32>) -> (tensor<16x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<16x6xf32>\n    %1 = stablehlo.abs %arg0 : tensor<16x6xf32>\n    %2 = stablehlo.negate %1 : tensor<16x6xf32>\n    %3 = stablehlo.exponential %2 : tensor<16x6xf32>\n    %4 = stablehlo.add %0, %3 : tensor<16x6xf32>\n    return %4 : tensor<16x6xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_composite_tjro, entry_computation_layout={(f32[16,6]{1,0})->f32[16,6]{1,0}}\n\nENTRY main.1 {\n  x.1 = f32[16,6]{1,0} parameter(0)\n  integer_pow.1 = f32[16,6]{1,0} multiply(x.1, x.1)\n  abs.1 = f32[16,6]{1,0} abs(x.1)\n  neg.1 = f32[16,6]{1,0} negate(abs.1)\n  exp.1 = f32[16,6]{1,0} exponential(neg.1)\n  ROOT add.1 = f32[16,6]{1,0} add(integer_pow.1, exp.1)\n}\n\n",
    "input_shapes": [
      [
        16,
        6
      ]
    ],
    "output_shape": [
      16,
      6
    ]
  },
  {
    "name": "nn_layer_atfz",
    "python_source": "def nn_layer_atfz(x, w, b):\n    \"\"\"Neural network layer with tanh activation.\"\"\"\n    return jax.nn.tanh(jnp.dot(x, w) + b)",
    "jaxpr": "{ lambda ; a:f32[4,23] b:f32[23,20] c:f32[20]. let\n    d:f32[4,20] = dot_general[\n      dimension_numbers=(([1], [0]), ([], []))\n      preferred_element_type=float32\n    ] a b\n    e:f32[1,20] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 20)\n      sharding=None\n    ] c\n    f:f32[4,20] = add d e\n    g:f32[4,20] = tanh f\n  in (g,) }",
    "stablehlo": "module @jit_nn_layer_atfz attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x23xf32>, %arg1: tensor<23x20xf32>, %arg2: tensor<20xf32>) -> (tensor<4x20xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.dot_general %arg0, %arg1, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<4x23xf32>, tensor<23x20xf32>) -> tensor<4x20xf32>\n    %1 = stablehlo.broadcast_in_dim %arg2, dims = [1] : (tensor<20xf32>) -> tensor<1x20xf32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x20xf32>) -> tensor<4x20xf32>\n    %3 = stablehlo.add %0, %2 : tensor<4x20xf32>\n    %4 = stablehlo.tanh %3 : tensor<4x20xf32>\n    return %4 : tensor<4x20xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_nn_layer_atfz, entry_computation_layout={(f32[4,23]{1,0}, f32[23,20]{1,0}, f32[20]{0})->f32[4,20]{1,0}}\n\nENTRY main.1 {\n  x.1 = f32[4,23]{1,0} parameter(0)\n  w.1 = f32[23,20]{1,0} parameter(1)\n  dot_general.1 = f32[4,20]{1,0} dot(x.1, w.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n  b.1 = f32[20]{0} parameter(2)\n  broadcast_in_dim.1 = f32[1,20]{1,0} reshape(b.1)\n  add.4 = f32[1,20]{1,0} broadcast(broadcast_in_dim.1), dimensions={0,1}\n  add.5 = f32[20]{0} reshape(add.4)\n  add.6 = f32[4,20]{1,0} broadcast(add.5), dimensions={1}\n  add.7 = f32[4,20]{1,0} add(dot_general.1, add.6)\n  ROOT tanh.1 = f32[4,20]{1,0} tanh(add.7)\n}\n\n",
    "input_shapes": [
      [
        4,
        23
      ],
      [
        23,
        20
      ],
      [
        20
      ]
    ],
    "output_shape": [
      4,
      20
    ]
  },
  {
    "name": "matmul_hzfk",
    "python_source": "def matmul_hzfk(a, b):\n    \"\"\"Matrix multiplication of shapes (16,3) @ (3,4).\"\"\"\n    return jnp.dot(a, b)",
    "jaxpr": "{ lambda ; a:f32[16,3] b:f32[3,4]. let\n    c:f32[16,4] = dot_general[\n      dimension_numbers=(([1], [0]), ([], []))\n      preferred_element_type=float32\n    ] a b\n  in (c,) }",
    "stablehlo": "module @jit_matmul_hzfk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<16x3xf32>, %arg1: tensor<3x4xf32>) -> (tensor<16x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.dot_general %arg0, %arg1, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<16x3xf32>, tensor<3x4xf32>) -> tensor<16x4xf32>\n    return %0 : tensor<16x4xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_matmul_hzfk, entry_computation_layout={(f32[16,3]{1,0}, f32[3,4]{1,0})->f32[16,4]{1,0}}\n\nENTRY main.1 {\n  a.1 = f32[16,3]{1,0} parameter(0)\n  b.1 = f32[3,4]{1,0} parameter(1)\n  ROOT dot_general.1 = f32[16,4]{1,0} dot(a.1, b.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n}\n\n",
    "input_shapes": [
      [
        16,
        3
      ],
      [
        3,
        4
      ]
    ],
    "output_shape": [
      16,
      4
    ]
  },
  {
    "name": "pow_bsrt",
    "python_source": "def pow_bsrt(x, y):\n    \"\"\"Power operation.\"\"\"\n    return jnp.power(jnp.abs(x), jnp.abs(y) % 3 + 1)",
    "jaxpr": "{ lambda ; a:f32[4,7] b:f32[4,7]. let\n    c:f32[4,7] = abs a\n    d:f32[4,7] = abs b\n    e:f32[4,7] = jit[\n      name=remainder\n      jaxpr={ lambda ; d:f32[4,7] f:i32[]. let\n          g:f32[] = convert_element_type[new_dtype=float32 weak_type=False] f\n          h:f32[4,7] = rem d g\n          i:bool[4,7] = ne h 0.0:f32[]\n          j:bool[4,7] = lt h 0.0:f32[]\n          k:bool[] = lt g 0.0:f32[]\n          l:bool[4,7] = ne j k\n          m:bool[4,7] = and l i\n          n:f32[4,7] = add h g\n          e:f32[4,7] = select_n m h n\n        in (e,) }\n    ] d 3:i32[]\n    o:f32[4,7] = add e 1.0:f32[]\n    p:f32[4,7] = pow c o\n  in (p,) }",
    "stablehlo": "module @jit_pow_bsrt attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x7xf32>, %arg1: tensor<4x7xf32>) -> (tensor<4x7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.abs %arg0 : tensor<4x7xf32>\n    %1 = stablehlo.abs %arg1 : tensor<4x7xf32>\n    %c = stablehlo.constant dense<3> : tensor<i32>\n    %2 = call @remainder(%1, %c) : (tensor<4x7xf32>, tensor<i32>) -> tensor<4x7xf32>\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %3 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<4x7xf32>\n    %4 = stablehlo.add %2, %3 : tensor<4x7xf32>\n    %5 = stablehlo.power %0, %4 : tensor<4x7xf32>\n    return %5 : tensor<4x7xf32>\n  }\n  func.func private @remainder(%arg0: tensor<4x7xf32>, %arg1: tensor<i32>) -> tensor<4x7xf32> {\n    %0 = stablehlo.convert %arg1 : (tensor<i32>) -> tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<4x7xf32>\n    %2 = stablehlo.remainder %arg0, %1 : tensor<4x7xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %3 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<4x7xf32>\n    %4 = stablehlo.compare  NE, %2, %3,  FLOAT : (tensor<4x7xf32>, tensor<4x7xf32>) -> tensor<4x7xi1>\n    %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %5 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<4x7xf32>\n    %6 = stablehlo.compare  LT, %2, %5,  FLOAT : (tensor<4x7xf32>, tensor<4x7xf32>) -> tensor<4x7xi1>\n    %cst_1 = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %7 = stablehlo.compare  LT, %0, %cst_1,  FLOAT : (tensor<f32>, tensor<f32>) -> tensor<i1>\n    %8 = stablehlo.broadcast_in_dim %7, dims = [] : (tensor<i1>) -> tensor<4x7xi1>\n    %9 = stablehlo.compare  NE, %6, %8,  UNSIGNED : (tensor<4x7xi1>, tensor<4x7xi1>) -> tensor<4x7xi1>\n    %10 = stablehlo.and %9, %4 : tensor<4x7xi1>\n    %11 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<4x7xf32>\n    %12 = stablehlo.add %2, %11 : tensor<4x7xf32>\n    %13 = stablehlo.select %10, %12, %2 : tensor<4x7xi1>, tensor<4x7xf32>\n    return %13 : tensor<4x7xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_pow_bsrt, entry_computation_layout={(f32[4,7]{1,0}, f32[4,7]{1,0})->f32[4,7]{1,0}}\n\nremainder.1 {\n  Arg_0.1 = f32[4,7]{1,0} parameter(0)\n  Arg_1.1 = s32[] parameter(1)\n  convert_element_type.1 = f32[] convert(Arg_1.1)\n  rem.2 = f32[4,7]{1,0} broadcast(convert_element_type.1), dimensions={}\n  rem.3 = f32[4,7]{1,0} remainder(Arg_0.1, rem.2)\n  constant.4 = f32[] constant(0)\n  broadcast.2 = f32[4,7]{1,0} broadcast(constant.4), dimensions={}\n  lt.2 = pred[4,7]{1,0} compare(rem.3, broadcast.2), direction=LT\n  constant.5 = f32[] constant(0)\n  lt.3 = pred[] compare(convert_element_type.1, constant.5), direction=LT\n  ne.4 = pred[4,7]{1,0} broadcast(lt.3), dimensions={}\n  ne.5 = pred[4,7]{1,0} compare(lt.2, ne.4), direction=NE\n  ne.3 = pred[4,7]{1,0} compare(rem.3, broadcast.2), direction=NE\n  and.1 = pred[4,7]{1,0} and(ne.5, ne.3)\n  add.2 = f32[4,7]{1,0} broadcast(convert_element_type.1), dimensions={}\n  add.3 = f32[4,7]{1,0} add(rem.3, add.2)\n  ROOT select_n.1 = f32[4,7]{1,0} select(and.1, add.3, rem.3)\n}\n\nENTRY main.2 {\n  x.1 = f32[4,7]{1,0} parameter(0)\n  abs.2 = f32[4,7]{1,0} abs(x.1)\n  y.1 = f32[4,7]{1,0} parameter(1)\n  abs.3 = f32[4,7]{1,0} abs(y.1)\n  constant.7 = s32[] constant(3)\n  jit_remainder_.1 = f32[4,7]{1,0} call(abs.3, constant.7), to_apply=remainder.1\n  constant.6 = f32[] constant(1)\n  broadcast.3 = f32[4,7]{1,0} broadcast(constant.6), dimensions={}\n  add.5 = f32[4,7]{1,0} add(jit_remainder_.1, broadcast.3)\n  ROOT pow.1 = f32[4,7]{1,0} power(abs.2, add.5)\n}\n\n",
    "input_shapes": [
      [
        4,
        7
      ],
      [
        4,
        7
      ]
    ],
    "output_shape": [
      4,
      7
    ]
  },
  {
    "name": "log_raip",
    "python_source": "def log_raip(x):\n    \"\"\"Log (with safety) operation.\"\"\"\n    return jnp.log(jnp.abs(x) + 1e-6)",
    "jaxpr": "{ lambda ; a:f32[10,11,6]. let\n    b:f32[10,11,6] = abs a\n    c:f32[10,11,6] = add b 9.999999974752427e-07:f32[]\n    d:f32[10,11,6] = log c\n  in (d,) }",
    "stablehlo": "module @jit_log_raip attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x11x6xf32>) -> (tensor<10x11x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.abs %arg0 : tensor<10x11x6xf32>\n    %cst = stablehlo.constant dense<9.99999997E-7> : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10x11x6xf32>\n    %2 = stablehlo.add %0, %1 : tensor<10x11x6xf32>\n    %3 = stablehlo.log %2 : tensor<10x11x6xf32>\n    return %3 : tensor<10x11x6xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_log_raip, entry_computation_layout={(f32[10,11,6]{2,1,0})->f32[10,11,6]{2,1,0}}\n\nENTRY main.1 {\n  x.1 = f32[10,11,6]{2,1,0} parameter(0)\n  abs.1 = f32[10,11,6]{2,1,0} abs(x.1)\n  constant.1 = f32[] constant(1e-06)\n  add.2 = f32[10,11,6]{2,1,0} broadcast(constant.1), dimensions={}\n  add.3 = f32[10,11,6]{2,1,0} add(abs.1, add.2)\n  ROOT log.1 = f32[10,11,6]{2,1,0} log(add.3)\n}\n\n",
    "input_shapes": [
      [
        10,
        11,
        6
      ]
    ],
    "output_shape": [
      10,
      11,
      6
    ]
  },
  {
    "name": "composite_ziia",
    "python_source": "def composite_ziia(x):\n    \"\"\"Composite function with multiple operations.\"\"\"\n    return x ** 2 + jnp.sin(x) + jax.nn.relu(x)",
    "jaxpr": "{ lambda ; a:f32[3,11]. let\n    b:f32[3,11] = integer_pow[y=2] a\n    c:f32[3,11] = sin a\n    d:f32[3,11] = add b c\n    e:f32[3,11] = custom_jvp_call[\n      name=relu\n      call_jaxpr={ lambda ; f:f32[3,11]. let\n          g:f32[3,11] = jit[\n            name=relu\n            jaxpr={ lambda ; f:f32[3,11]. let\n                g:f32[3,11] = max f 0.0:f32[]\n              in (g,) }\n          ] f\n        in (g,) }\n      jvp=jvp\n      symbolic_zeros=False\n    ] a\n    h:f32[3,11] = add d e\n  in (h,) }",
    "stablehlo": "module @jit_composite_ziia attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x11xf32>) -> (tensor<3x11xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<3x11xf32>\n    %1 = stablehlo.sine %arg0 : tensor<3x11xf32>\n    %2 = stablehlo.add %0, %1 : tensor<3x11xf32>\n    %3 = call @relu(%arg0) : (tensor<3x11xf32>) -> tensor<3x11xf32>\n    %4 = stablehlo.add %2, %3 : tensor<3x11xf32>\n    return %4 : tensor<3x11xf32>\n  }\n  func.func private @relu(%arg0: tensor<3x11xf32>) -> tensor<3x11xf32> {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<3x11xf32>\n    %1 = stablehlo.maximum %arg0, %0 : tensor<3x11xf32>\n    return %1 : tensor<3x11xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_composite_ziia, entry_computation_layout={(f32[3,11]{1,0})->f32[3,11]{1,0}}\n\nrelu.1 {\n  Arg_0.1 = f32[3,11]{1,0} parameter(0)\n  constant.1 = f32[] constant(0)\n  max.2 = f32[3,11]{1,0} broadcast(constant.1), dimensions={}\n  ROOT max.3 = f32[3,11]{1,0} maximum(Arg_0.1, max.2)\n}\n\nENTRY main.2 {\n  x.1 = f32[3,11]{1,0} parameter(0)\n  integer_pow.1 = f32[3,11]{1,0} multiply(x.1, x.1)\n  sin.1 = f32[3,11]{1,0} sine(x.1)\n  add.2 = f32[3,11]{1,0} add(integer_pow.1, sin.1)\n  jit_relu_.1 = f32[3,11]{1,0} call(x.1), to_apply=relu.1\n  ROOT add.3 = f32[3,11]{1,0} add(add.2, jit_relu_.1)\n}\n\n",
    "input_shapes": [
      [
        3,
        11
      ]
    ],
    "output_shape": [
      3,
      11
    ]
  },
  {
    "name": "sum_ehbd",
    "python_source": "def sum_ehbd(x):\n    \"\"\"Sum reduction along axis 2.\"\"\"\n    return jnp.sum(x, axis=2)",
    "jaxpr": "{ lambda ; a:f32[5,13,6,5]. let\n    b:f32[5,13,5] = reduce_sum[axes=(2,) out_sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit_sum_ehbd attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x13x6x5xf32>) -> (tensor<5x13x5xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<5x13x6x5xf32>, tensor<f32>) -> tensor<5x13x5xf32>\n    return %0 : tensor<5x13x5xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_sum_ehbd, entry_computation_layout={(f32[5,13,6,5]{3,2,1,0})->f32[5,13,5]{2,1,0}}\n\nregion_0.1 {\n  reduce_sum.3 = f32[] parameter(0)\n  reduce_sum.4 = f32[] parameter(1)\n  ROOT reduce_sum.5 = f32[] add(reduce_sum.3, reduce_sum.4)\n}\n\nENTRY main.2 {\n  x.1 = f32[5,13,6,5]{3,2,1,0} parameter(0)\n  constant.1 = f32[] constant(0)\n  ROOT reduce_sum.7 = f32[5,13,5]{2,1,0} reduce(x.1, constant.1), dimensions={2}, to_apply=region_0.1\n}\n\n",
    "input_shapes": [
      [
        5,
        13,
        6,
        5
      ]
    ],
    "output_shape": [
      5,
      13,
      5
    ]
  },
  {
    "name": "sum_csdt",
    "python_source": "def sum_csdt(x):\n    \"\"\"Sum reduction along axis 0.\"\"\"\n    return jnp.sum(x, axis=0)",
    "jaxpr": "{ lambda ; a:f32[2,14,15,4]. let\n    b:f32[14,15,4] = reduce_sum[axes=(0,) out_sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit_sum_csdt attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x14x15x4xf32>) -> (tensor<14x15x4xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<2x14x15x4xf32>, tensor<f32>) -> tensor<14x15x4xf32>\n    return %0 : tensor<14x15x4xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_sum_csdt, entry_computation_layout={(f32[2,14,15,4]{3,2,1,0})->f32[14,15,4]{2,1,0}}\n\nregion_0.1 {\n  reduce_sum.3 = f32[] parameter(0)\n  reduce_sum.4 = f32[] parameter(1)\n  ROOT reduce_sum.5 = f32[] add(reduce_sum.3, reduce_sum.4)\n}\n\nENTRY main.2 {\n  x.1 = f32[2,14,15,4]{3,2,1,0} parameter(0)\n  constant.1 = f32[] constant(0)\n  ROOT reduce_sum.7 = f32[14,15,4]{2,1,0} reduce(x.1, constant.1), dimensions={0}, to_apply=region_0.1\n}\n\n",
    "input_shapes": [
      [
        2,
        14,
        15,
        4
      ]
    ],
    "output_shape": [
      14,
      15,
      4
    ]
  },
  {
    "name": "exp_jpay",
    "python_source": "def exp_jpay(x):\n    \"\"\"Exponential operation.\"\"\"\n    return jnp.exp(x)",
    "jaxpr": "{ lambda ; a:f32[10,10,4]. let b:f32[10,10,4] = exp a in (b,) }",
    "stablehlo": "module @jit_exp_jpay attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x10x4xf32>) -> (tensor<10x10x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.exponential %arg0 : tensor<10x10x4xf32>\n    return %0 : tensor<10x10x4xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_exp_jpay, entry_computation_layout={(f32[10,10,4]{2,1,0})->f32[10,10,4]{2,1,0}}\n\nENTRY main.1 {\n  x.1 = f32[10,10,4]{2,1,0} parameter(0)\n  ROOT exp.1 = f32[10,10,4]{2,1,0} exponential(x.1)\n}\n\n",
    "input_shapes": [
      [
        10,
        10,
        4
      ]
    ],
    "output_shape": [
      10,
      10,
      4
    ]
  },
  {
    "name": "matmul_vjbe",
    "python_source": "def matmul_vjbe(a, b):\n    \"\"\"Matrix multiplication of shapes (3,14) @ (14,15).\"\"\"\n    return jnp.dot(a, b)",
    "jaxpr": "{ lambda ; a:f32[3,14] b:f32[14,15]. let\n    c:f32[3,15] = dot_general[\n      dimension_numbers=(([1], [0]), ([], []))\n      preferred_element_type=float32\n    ] a b\n  in (c,) }",
    "stablehlo": "module @jit_matmul_vjbe attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x14xf32>, %arg1: tensor<14x15xf32>) -> (tensor<3x15xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.dot_general %arg0, %arg1, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<3x14xf32>, tensor<14x15xf32>) -> tensor<3x15xf32>\n    return %0 : tensor<3x15xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_matmul_vjbe, entry_computation_layout={(f32[3,14]{1,0}, f32[14,15]{1,0})->f32[3,15]{1,0}}\n\nENTRY main.1 {\n  a.1 = f32[3,14]{1,0} parameter(0)\n  b.1 = f32[14,15]{1,0} parameter(1)\n  ROOT dot_general.1 = f32[3,15]{1,0} dot(a.1, b.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n}\n\n",
    "input_shapes": [
      [
        3,
        14
      ],
      [
        14,
        15
      ]
    ],
    "output_shape": [
      3,
      15
    ]
  },
  {
    "name": "mean_yiff",
    "python_source": "def mean_yiff(x):\n    \"\"\"Mean reduction along axis 0.\"\"\"\n    return jnp.mean(x, axis=0)",
    "jaxpr": "{ lambda ; a:f32[8,3,7]. let\n    b:f32[3,7] = reduce_sum[axes=(0,) out_sharding=None] a\n    c:f32[3,7] = div b 8.0:f32[]\n  in (c,) }",
    "stablehlo": "module @jit_mean_yiff attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x3x7xf32>) -> (tensor<3x7xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<8x3x7xf32>, tensor<f32>) -> tensor<3x7xf32>\n    %cst_0 = stablehlo.constant dense<8.000000e+00> : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<3x7xf32>\n    %2 = stablehlo.divide %0, %1 : tensor<3x7xf32>\n    return %2 : tensor<3x7xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_mean_yiff, entry_computation_layout={(f32[8,3,7]{2,1,0})->f32[3,7]{1,0}}\n\nregion_0.1 {\n  reduce_sum.3 = f32[] parameter(0)\n  reduce_sum.4 = f32[] parameter(1)\n  ROOT reduce_sum.5 = f32[] add(reduce_sum.3, reduce_sum.4)\n}\n\nENTRY main.2 {\n  x.1 = f32[8,3,7]{2,1,0} parameter(0)\n  constant.3 = f32[] constant(0)\n  reduce_sum.7 = f32[3,7]{1,0} reduce(x.1, constant.3), dimensions={0}, to_apply=region_0.1\n  constant.2 = f32[] constant(8)\n  broadcast.1 = f32[3,7]{1,0} broadcast(constant.2), dimensions={}\n  ROOT div.1 = f32[3,7]{1,0} divide(reduce_sum.7, broadcast.1)\n}\n\n",
    "input_shapes": [
      [
        8,
        3,
        7
      ]
    ],
    "output_shape": [
      3,
      7
    ]
  },
  {
    "name": "min_yesk",
    "python_source": "def min_yesk(x):\n    \"\"\"Min reduction along axis 1.\"\"\"\n    return jnp.min(x, axis=1)",
    "jaxpr": "{ lambda ; a:f32[4,16,16]. let b:f32[4,16] = reduce_min[axes=(1,)] a in (b,) }",
    "stablehlo": "module @jit_min_yesk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x16x16xf32>) -> (tensor<4x16xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0x7F800000> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.minimum across dimensions = [1] : (tensor<4x16x16xf32>, tensor<f32>) -> tensor<4x16xf32>\n    return %0 : tensor<4x16xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_min_yesk, entry_computation_layout={(f32[4,16,16]{2,1,0})->f32[4,16]{1,0}}\n\nregion_0.1 {\n  reduce_min.3 = f32[] parameter(0)\n  reduce_min.4 = f32[] parameter(1)\n  ROOT reduce_min.5 = f32[] minimum(reduce_min.3, reduce_min.4)\n}\n\nENTRY main.2 {\n  x.1 = f32[4,16,16]{2,1,0} parameter(0)\n  constant.1 = f32[] constant(inf)\n  ROOT reduce_min.7 = f32[4,16]{1,0} reduce(x.1, constant.1), dimensions={1}, to_apply=region_0.1\n}\n\n",
    "input_shapes": [
      [
        4,
        16,
        16
      ]
    ],
    "output_shape": [
      4,
      16
    ]
  },
  {
    "name": "min_ropy",
    "python_source": "def min_ropy(x, y):\n    \"\"\"Element-wise min operation.\"\"\"\n    return jnp.minimum(x, y)",
    "jaxpr": "{ lambda ; a:f32[6,13] b:f32[6,13]. let c:f32[6,13] = min a b in (c,) }",
    "stablehlo": "module @jit_min_ropy attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x13xf32>, %arg1: tensor<6x13xf32>) -> (tensor<6x13xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.minimum %arg0, %arg1 : tensor<6x13xf32>\n    return %0 : tensor<6x13xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_min_ropy, entry_computation_layout={(f32[6,13]{1,0}, f32[6,13]{1,0})->f32[6,13]{1,0}}\n\nENTRY main.1 {\n  x.1 = f32[6,13]{1,0} parameter(0)\n  y.1 = f32[6,13]{1,0} parameter(1)\n  ROOT min.1 = f32[6,13]{1,0} minimum(x.1, y.1)\n}\n\n",
    "input_shapes": [
      [
        6,
        13
      ],
      [
        6,
        13
      ]
    ],
    "output_shape": [
      6,
      13
    ]
  },
  {
    "name": "relu_osoj",
    "python_source": "def relu_osoj(x):\n    \"\"\"ReLU activation operation.\"\"\"\n    return jax.nn.relu(x)",
    "jaxpr": "{ lambda ; a:f32[12]. let\n    b:f32[12] = custom_jvp_call[\n      name=relu\n      call_jaxpr={ lambda ; c:f32[12]. let\n          d:f32[12] = jit[\n            name=relu\n            jaxpr={ lambda ; c:f32[12]. let\n                d:f32[12] = max c 0.0:f32[]\n              in (d,) }\n          ] c\n        in (d,) }\n      jvp=jvp\n      symbolic_zeros=False\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit_relu_osoj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<12xf32>) -> (tensor<12xf32> {jax.result_info = \"result\"}) {\n    %0 = call @relu(%arg0) : (tensor<12xf32>) -> tensor<12xf32>\n    return %0 : tensor<12xf32>\n  }\n  func.func private @relu(%arg0: tensor<12xf32>) -> tensor<12xf32> {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<12xf32>\n    %1 = stablehlo.maximum %arg0, %0 : tensor<12xf32>\n    return %1 : tensor<12xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_relu_osoj, entry_computation_layout={(f32[12]{0})->f32[12]{0}}\n\nrelu.1 {\n  Arg_0.1 = f32[12]{0} parameter(0)\n  constant.1 = f32[] constant(0)\n  broadcast.1 = f32[12]{0} broadcast(constant.1), dimensions={}\n  ROOT max.1 = f32[12]{0} maximum(Arg_0.1, broadcast.1)\n}\n\nENTRY main.2 {\n  x.1 = f32[12]{0} parameter(0)\n  ROOT jit_relu_.1 = f32[12]{0} call(x.1), to_apply=relu.1\n}\n\n",
    "input_shapes": [
      [
        12
      ]
    ],
    "output_shape": [
      12
    ]
  },
  {
    "name": "composite_euer",
    "python_source": "def composite_euer(x):\n    \"\"\"Composite function with multiple operations.\"\"\"\n    return x ** 2 + jnp.tanh(x)",
    "jaxpr": "{ lambda ; a:f32[11,3]. let\n    b:f32[11,3] = integer_pow[y=2] a\n    c:f32[11,3] = tanh a\n    d:f32[11,3] = add b c\n  in (d,) }",
    "stablehlo": "module @jit_composite_euer attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<11x3xf32>) -> (tensor<11x3xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg0 : tensor<11x3xf32>\n    %1 = stablehlo.tanh %arg0 : tensor<11x3xf32>\n    %2 = stablehlo.add %0, %1 : tensor<11x3xf32>\n    return %2 : tensor<11x3xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_composite_euer, entry_computation_layout={(f32[11,3]{1,0})->f32[11,3]{1,0}}\n\nENTRY main.1 {\n  x.1 = f32[11,3]{1,0} parameter(0)\n  integer_pow.1 = f32[11,3]{1,0} multiply(x.1, x.1)\n  tanh.1 = f32[11,3]{1,0} tanh(x.1)\n  ROOT add.1 = f32[11,3]{1,0} add(integer_pow.1, tanh.1)\n}\n\n",
    "input_shapes": [
      [
        11,
        3
      ]
    ],
    "output_shape": [
      11,
      3
    ]
  },
  {
    "name": "vmap_sldg",
    "python_source": "def vmap_sldg(a_batch, b_batch):\n    \"\"\"Batched operation using vmap.\"\"\"\n    def inner(a, b):\n        return jnp.maximum(a, b)\n    return jax.vmap(inner)(a_batch, b_batch)",
    "jaxpr": "{ lambda ; a:f32[3,12] b:f32[3,12]. let c:f32[3,12] = max a b in (c,) }",
    "stablehlo": "module @jit_vmap_sldg attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x12xf32>, %arg1: tensor<3x12xf32>) -> (tensor<3x12xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.maximum %arg0, %arg1 : tensor<3x12xf32>\n    return %0 : tensor<3x12xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_vmap_sldg, entry_computation_layout={(f32[3,12]{1,0}, f32[3,12]{1,0})->f32[3,12]{1,0}}\n\nENTRY main.1 {\n  a_batch.1 = f32[3,12]{1,0} parameter(0)\n  b_batch.1 = f32[3,12]{1,0} parameter(1)\n  ROOT max.1 = f32[3,12]{1,0} maximum(a_batch.1, b_batch.1)\n}\n\n",
    "input_shapes": [
      [
        3,
        12
      ],
      [
        3,
        12
      ]
    ],
    "output_shape": [
      3,
      12
    ]
  },
  {
    "name": "min_xecd",
    "python_source": "def min_xecd(x, y):\n    \"\"\"Element-wise min operation.\"\"\"\n    return jnp.minimum(x, y)",
    "jaxpr": "{ lambda ; a:f32[14] b:f32[14]. let c:f32[14] = min a b in (c,) }",
    "stablehlo": "module @jit_min_xecd attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<14xf32>, %arg1: tensor<14xf32>) -> (tensor<14xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.minimum %arg0, %arg1 : tensor<14xf32>\n    return %0 : tensor<14xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_min_xecd, entry_computation_layout={(f32[14]{0}, f32[14]{0})->f32[14]{0}}\n\nENTRY main.1 {\n  x.1 = f32[14]{0} parameter(0)\n  y.1 = f32[14]{0} parameter(1)\n  ROOT min.1 = f32[14]{0} minimum(x.1, y.1)\n}\n\n",
    "input_shapes": [
      [
        14
      ],
      [
        14
      ]
    ],
    "output_shape": [
      14
    ]
  },
  {
    "name": "softplus_ikyt",
    "python_source": "def softplus_ikyt(x):\n    \"\"\"Softplus operation.\"\"\"\n    return jax.nn.softplus(x)",
    "jaxpr": "{ lambda ; a:f32[5,7]. let\n    b:f32[5,7] = jit[\n      name=softplus\n      jaxpr={ lambda ; a:f32[5,7]. let\n          b:f32[5,7] = custom_jvp_call[\n            name=logaddexp\n            call_jaxpr={ lambda ; c:f32[5,7] d:f32[]. let\n                e:f32[5,7] = max c d\n                f:f32[5,7] = sub c d\n                g:bool[5,7] = ne f f\n                h:f32[5,7] = add c d\n                i:f32[5,7] = abs f\n                j:f32[5,7] = neg i\n                k:f32[5,7] = exp j\n                l:f32[5,7] = log1p k\n                m:f32[5,7] = add e l\n                n:f32[5,7] = select_n g m h\n              in (n,) }\n            jvp=_logaddexp_jvp\n            symbolic_zeros=False\n          ] a 0.0:f32[]\n        in (b,) }\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit_softplus_ikyt attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x7xf32>) -> (tensor<5x7xf32> {jax.result_info = \"result\"}) {\n    %0 = call @softplus(%arg0) : (tensor<5x7xf32>) -> tensor<5x7xf32>\n    return %0 : tensor<5x7xf32>\n  }\n  func.func private @softplus(%arg0: tensor<5x7xf32>) -> tensor<5x7xf32> {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<5x7xf32>\n    %1 = stablehlo.maximum %arg0, %0 : tensor<5x7xf32>\n    %2 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<5x7xf32>\n    %3 = stablehlo.subtract %arg0, %2 : tensor<5x7xf32>\n    %4 = stablehlo.compare  NE, %3, %3,  FLOAT : (tensor<5x7xf32>, tensor<5x7xf32>) -> tensor<5x7xi1>\n    %5 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<5x7xf32>\n    %6 = stablehlo.add %arg0, %5 : tensor<5x7xf32>\n    %7 = stablehlo.abs %3 : tensor<5x7xf32>\n    %8 = stablehlo.negate %7 : tensor<5x7xf32>\n    %9 = stablehlo.exponential %8 : tensor<5x7xf32>\n    %10 = stablehlo.log_plus_one %9 : tensor<5x7xf32>\n    %11 = stablehlo.add %1, %10 : tensor<5x7xf32>\n    %12 = stablehlo.select %4, %6, %11 : tensor<5x7xi1>, tensor<5x7xf32>\n    return %12 : tensor<5x7xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_softplus_ikyt, entry_computation_layout={(f32[5,7]{1,0})->f32[5,7]{1,0}}\n\nsoftplus.1 {\n  Arg_0.1 = f32[5,7]{1,0} parameter(0)\n  ne.1 = pred[5,7]{1,0} compare(Arg_0.1, Arg_0.1), direction=NE\n  constant.1 = f32[] constant(0)\n  max.2 = f32[5,7]{1,0} broadcast(constant.1), dimensions={}\n  max.3 = f32[5,7]{1,0} maximum(Arg_0.1, max.2)\n  abs.1 = f32[5,7]{1,0} abs(Arg_0.1)\n  neg.1 = f32[5,7]{1,0} negate(abs.1)\n  exp.1 = f32[5,7]{1,0} exponential(neg.1)\n  log1p.1 = f32[5,7]{1,0} log-plus-one(exp.1)\n  add.1 = f32[5,7]{1,0} add(max.3, log1p.1)\n  ROOT select_n.1 = f32[5,7]{1,0} select(ne.1, Arg_0.1, add.1)\n}\n\nENTRY main.2 {\n  x.1 = f32[5,7]{1,0} parameter(0)\n  ROOT jit_softplus_.1 = f32[5,7]{1,0} call(x.1), to_apply=softplus.1\n}\n\n",
    "input_shapes": [
      [
        5,
        7
      ]
    ],
    "output_shape": [
      5,
      7
    ]
  },
  {
    "name": "matmul_ihxs",
    "python_source": "def matmul_ihxs(a, b):\n    \"\"\"Matrix multiplication of shapes (11,8) @ (8,9).\"\"\"\n    return jnp.dot(a, b)",
    "jaxpr": "{ lambda ; a:f32[11,8] b:f32[8,9]. let\n    c:f32[11,9] = dot_general[\n      dimension_numbers=(([1], [0]), ([], []))\n      preferred_element_type=float32\n    ] a b\n  in (c,) }",
    "stablehlo": "module @jit_matmul_ihxs attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<11x8xf32>, %arg1: tensor<8x9xf32>) -> (tensor<11x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.dot_general %arg0, %arg1, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<11x8xf32>, tensor<8x9xf32>) -> tensor<11x9xf32>\n    return %0 : tensor<11x9xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_matmul_ihxs, entry_computation_layout={(f32[11,8]{1,0}, f32[8,9]{1,0})->f32[11,9]{1,0}}\n\nENTRY main.1 {\n  a.1 = f32[11,8]{1,0} parameter(0)\n  b.1 = f32[8,9]{1,0} parameter(1)\n  ROOT dot_general.1 = f32[11,9]{1,0} dot(a.1, b.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n}\n\n",
    "input_shapes": [
      [
        11,
        8
      ],
      [
        8,
        9
      ]
    ],
    "output_shape": [
      11,
      9
    ]
  },
  {
    "name": "matmul_mcmp",
    "python_source": "def matmul_mcmp(a, b):\n    \"\"\"Matrix multiplication of shapes (7,4) @ (4,10).\"\"\"\n    return jnp.dot(a, b)",
    "jaxpr": "{ lambda ; a:f32[7,4] b:f32[4,10]. let\n    c:f32[7,10] = dot_general[\n      dimension_numbers=(([1], [0]), ([], []))\n      preferred_element_type=float32\n    ] a b\n  in (c,) }",
    "stablehlo": "module @jit_matmul_mcmp attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x4xf32>, %arg1: tensor<4x10xf32>) -> (tensor<7x10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.dot_general %arg0, %arg1, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<7x4xf32>, tensor<4x10xf32>) -> tensor<7x10xf32>\n    return %0 : tensor<7x10xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_matmul_mcmp, entry_computation_layout={(f32[7,4]{1,0}, f32[4,10]{1,0})->f32[7,10]{1,0}}\n\nENTRY main.1 {\n  a.1 = f32[7,4]{1,0} parameter(0)\n  b.1 = f32[4,10]{1,0} parameter(1)\n  ROOT dot_general.1 = f32[7,10]{1,0} dot(a.1, b.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n}\n\n",
    "input_shapes": [
      [
        7,
        4
      ],
      [
        4,
        10
      ]
    ],
    "output_shape": [
      7,
      10
    ]
  },
  {
    "name": "sub_xcyu",
    "python_source": "def sub_xcyu(x, y):\n    \"\"\"Subtraction operation.\"\"\"\n    return x - y",
    "jaxpr": "{ lambda ; a:f32[5,4,4] b:f32[5,4,4]. let c:f32[5,4,4] = sub a b in (c,) }",
    "stablehlo": "module @jit_sub_xcyu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x4x4xf32>, %arg1: tensor<5x4x4xf32>) -> (tensor<5x4x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<5x4x4xf32>\n    return %0 : tensor<5x4x4xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_sub_xcyu, entry_computation_layout={(f32[5,4,4]{2,1,0}, f32[5,4,4]{2,1,0})->f32[5,4,4]{2,1,0}}\n\nENTRY main.1 {\n  x.1 = f32[5,4,4]{2,1,0} parameter(0)\n  y.1 = f32[5,4,4]{2,1,0} parameter(1)\n  ROOT sub.1 = f32[5,4,4]{2,1,0} subtract(x.1, y.1)\n}\n\n",
    "input_shapes": [
      [
        5,
        4,
        4
      ],
      [
        5,
        4,
        4
      ]
    ],
    "output_shape": [
      5,
      4,
      4
    ]
  },
  {
    "name": "max_awdx",
    "python_source": "def max_awdx(x, y):\n    \"\"\"Element-wise max operation.\"\"\"\n    return jnp.maximum(x, y)",
    "jaxpr": "{ lambda ; a:f32[6] b:f32[6]. let c:f32[6] = max a b in (c,) }",
    "stablehlo": "module @jit_max_awdx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6xf32>, %arg1: tensor<6xf32>) -> (tensor<6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.maximum %arg0, %arg1 : tensor<6xf32>\n    return %0 : tensor<6xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_max_awdx, entry_computation_layout={(f32[6]{0}, f32[6]{0})->f32[6]{0}}\n\nENTRY main.1 {\n  x.1 = f32[6]{0} parameter(0)\n  y.1 = f32[6]{0} parameter(1)\n  ROOT max.1 = f32[6]{0} maximum(x.1, y.1)\n}\n\n",
    "input_shapes": [
      [
        6
      ],
      [
        6
      ]
    ],
    "output_shape": [
      6
    ]
  },
  {
    "name": "relu_hvap",
    "python_source": "def relu_hvap(x):\n    \"\"\"ReLU activation operation.\"\"\"\n    return jax.nn.relu(x)",
    "jaxpr": "{ lambda ; a:f32[7]. let\n    b:f32[7] = custom_jvp_call[\n      name=relu\n      call_jaxpr={ lambda ; c:f32[7]. let\n          d:f32[7] = jit[\n            name=relu\n            jaxpr={ lambda ; c:f32[7]. let d:f32[7] = max c 0.0:f32[] in (d,) }\n          ] c\n        in (d,) }\n      jvp=jvp\n      symbolic_zeros=False\n    ] a\n  in (b,) }",
    "stablehlo": "module @jit_relu_hvap attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7xf32>) -> (tensor<7xf32> {jax.result_info = \"result\"}) {\n    %0 = call @relu(%arg0) : (tensor<7xf32>) -> tensor<7xf32>\n    return %0 : tensor<7xf32>\n  }\n  func.func private @relu(%arg0: tensor<7xf32>) -> tensor<7xf32> {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<7xf32>\n    %1 = stablehlo.maximum %arg0, %0 : tensor<7xf32>\n    return %1 : tensor<7xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_relu_hvap, entry_computation_layout={(f32[7]{0})->f32[7]{0}}\n\nrelu.1 {\n  Arg_0.1 = f32[7]{0} parameter(0)\n  constant.1 = f32[] constant(0)\n  broadcast.1 = f32[7]{0} broadcast(constant.1), dimensions={}\n  ROOT max.1 = f32[7]{0} maximum(Arg_0.1, broadcast.1)\n}\n\nENTRY main.2 {\n  x.1 = f32[7]{0} parameter(0)\n  ROOT jit_relu_.1 = f32[7]{0} call(x.1), to_apply=relu.1\n}\n\n",
    "input_shapes": [
      [
        7
      ]
    ],
    "output_shape": [
      7
    ]
  },
  {
    "name": "nn_layer_caki",
    "python_source": "def nn_layer_caki(x, w, b):\n    \"\"\"Neural network layer with sigmoid activation.\"\"\"\n    return jax.nn.sigmoid(jnp.dot(x, w) + b)",
    "jaxpr": "{ lambda ; a:f32[2,6] b:f32[6,16] c:f32[16]. let\n    d:f32[2,16] = dot_general[\n      dimension_numbers=(([1], [0]), ([], []))\n      preferred_element_type=float32\n    ] a b\n    e:f32[1,16] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 16)\n      sharding=None\n    ] c\n    f:f32[2,16] = add d e\n    g:f32[2,16] = logistic f\n  in (g,) }",
    "stablehlo": "module @jit_nn_layer_caki attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x6xf32>, %arg1: tensor<6x16xf32>, %arg2: tensor<16xf32>) -> (tensor<2x16xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.dot_general %arg0, %arg1, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<2x6xf32>, tensor<6x16xf32>) -> tensor<2x16xf32>\n    %1 = stablehlo.broadcast_in_dim %arg2, dims = [1] : (tensor<16xf32>) -> tensor<1x16xf32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x16xf32>) -> tensor<2x16xf32>\n    %3 = stablehlo.add %0, %2 : tensor<2x16xf32>\n    %4 = stablehlo.negate %3 : tensor<2x16xf32>\n    %5 = stablehlo.exponential %4 : tensor<2x16xf32>\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %6 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<2x16xf32>\n    %7 = stablehlo.add %6, %5 : tensor<2x16xf32>\n    %cst_0 = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %8 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<2x16xf32>\n    %9 = stablehlo.divide %8, %7 : tensor<2x16xf32>\n    return %9 : tensor<2x16xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_nn_layer_caki, entry_computation_layout={(f32[2,6]{1,0}, f32[6,16]{1,0}, f32[16]{0})->f32[2,16]{1,0}}\n\nENTRY main.1 {\n  constant.1 = f32[] constant(1)\n  broadcast.1 = f32[2,16]{1,0} broadcast(constant.1), dimensions={}\n  x.1 = f32[2,6]{1,0} parameter(0)\n  w.1 = f32[6,16]{1,0} parameter(1)\n  dot_general.1 = f32[2,16]{1,0} dot(x.1, w.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n  b.1 = f32[16]{0} parameter(2)\n  broadcast_in_dim.1 = f32[1,16]{1,0} reshape(b.1)\n  add.5 = f32[1,16]{1,0} broadcast(broadcast_in_dim.1), dimensions={0,1}\n  add.6 = f32[16]{0} reshape(add.5)\n  add.7 = f32[2,16]{1,0} broadcast(add.6), dimensions={1}\n  add.8 = f32[2,16]{1,0} add(dot_general.1, add.7)\n  neg.1 = f32[2,16]{1,0} negate(add.8)\n  exp.1 = f32[2,16]{1,0} exponential(neg.1)\n  add.9 = f32[2,16]{1,0} add(exp.1, broadcast.1)\n  ROOT div.1 = f32[2,16]{1,0} divide(broadcast.1, add.9)\n}\n\n",
    "input_shapes": [
      [
        2,
        6
      ],
      [
        6,
        16
      ],
      [
        16
      ]
    ],
    "output_shape": [
      2,
      16
    ]
  },
  {
    "name": "matmul_imrq",
    "python_source": "def matmul_imrq(a, b):\n    \"\"\"Matrix multiplication of shapes (10,8) @ (8,15).\"\"\"\n    return jnp.dot(a, b)",
    "jaxpr": "{ lambda ; a:f32[10,8] b:f32[8,15]. let\n    c:f32[10,15] = dot_general[\n      dimension_numbers=(([1], [0]), ([], []))\n      preferred_element_type=float32\n    ] a b\n  in (c,) }",
    "stablehlo": "module @jit_matmul_imrq attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x8xf32>, %arg1: tensor<8x15xf32>) -> (tensor<10x15xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.dot_general %arg0, %arg1, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<10x8xf32>, tensor<8x15xf32>) -> tensor<10x15xf32>\n    return %0 : tensor<10x15xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_matmul_imrq, entry_computation_layout={(f32[10,8]{1,0}, f32[8,15]{1,0})->f32[10,15]{1,0}}\n\nENTRY main.1 {\n  a.1 = f32[10,8]{1,0} parameter(0)\n  b.1 = f32[8,15]{1,0} parameter(1)\n  ROOT dot_general.1 = f32[10,15]{1,0} dot(a.1, b.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n}\n\n",
    "input_shapes": [
      [
        10,
        8
      ],
      [
        8,
        15
      ]
    ],
    "output_shape": [
      10,
      15
    ]
  },
  {
    "name": "div_wlvs",
    "python_source": "def div_wlvs(x, y):\n    \"\"\"Division (safe) operation.\"\"\"\n    return x / (y + 1e-6)",
    "jaxpr": "{ lambda ; a:f32[8] b:f32[8]. let\n    c:f32[8] = add b 9.999999974752427e-07:f32[]\n    d:f32[8] = div a c\n  in (d,) }",
    "stablehlo": "module @jit_div_wlvs attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8xf32>, %arg1: tensor<8xf32>) -> (tensor<8xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<9.99999997E-7> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<8xf32>\n    %1 = stablehlo.add %arg1, %0 : tensor<8xf32>\n    %2 = stablehlo.divide %arg0, %1 : tensor<8xf32>\n    return %2 : tensor<8xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_div_wlvs, entry_computation_layout={(f32[8]{0}, f32[8]{0})->f32[8]{0}}\n\nENTRY main.1 {\n  x.1 = f32[8]{0} parameter(0)\n  y.1 = f32[8]{0} parameter(1)\n  constant.1 = f32[] constant(1e-06)\n  broadcast.1 = f32[8]{0} broadcast(constant.1), dimensions={}\n  add.1 = f32[8]{0} add(y.1, broadcast.1)\n  ROOT div.1 = f32[8]{0} divide(x.1, add.1)\n}\n\n",
    "input_shapes": [
      [
        8
      ],
      [
        8
      ]
    ],
    "output_shape": [
      8
    ]
  },
  {
    "name": "tanh_iymv",
    "python_source": "def tanh_iymv(x):\n    \"\"\"Hyperbolic tangent operation.\"\"\"\n    return jnp.tanh(x)",
    "jaxpr": "{ lambda ; a:f32[3,11]. let b:f32[3,11] = tanh a in (b,) }",
    "stablehlo": "module @jit_tanh_iymv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x11xf32>) -> (tensor<3x11xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.tanh %arg0 : tensor<3x11xf32>\n    return %0 : tensor<3x11xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_tanh_iymv, entry_computation_layout={(f32[3,11]{1,0})->f32[3,11]{1,0}}\n\nENTRY main.1 {\n  x.1 = f32[3,11]{1,0} parameter(0)\n  ROOT tanh.1 = f32[3,11]{1,0} tanh(x.1)\n}\n\n",
    "input_shapes": [
      [
        3,
        11
      ]
    ],
    "output_shape": [
      3,
      11
    ]
  },
  {
    "name": "sum_nazr",
    "python_source": "def sum_nazr(x):\n    \"\"\"Sum reduction along axis 2.\"\"\"\n    return jnp.sum(x, axis=2)",
    "jaxpr": "{ lambda ; a:f32[9,12,4,5]. let\n    b:f32[9,12,5] = reduce_sum[axes=(2,) out_sharding=None] a\n  in (b,) }",
    "stablehlo": "module @jit_sum_nazr attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x12x4x5xf32>) -> (tensor<9x12x5xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<9x12x4x5xf32>, tensor<f32>) -> tensor<9x12x5xf32>\n    return %0 : tensor<9x12x5xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_sum_nazr, entry_computation_layout={(f32[9,12,4,5]{3,2,1,0})->f32[9,12,5]{2,1,0}}\n\nregion_0.1 {\n  reduce_sum.3 = f32[] parameter(0)\n  reduce_sum.4 = f32[] parameter(1)\n  ROOT reduce_sum.5 = f32[] add(reduce_sum.3, reduce_sum.4)\n}\n\nENTRY main.2 {\n  x.1 = f32[9,12,4,5]{3,2,1,0} parameter(0)\n  constant.1 = f32[] constant(0)\n  ROOT reduce_sum.7 = f32[9,12,5]{2,1,0} reduce(x.1, constant.1), dimensions={2}, to_apply=region_0.1\n}\n\n",
    "input_shapes": [
      [
        9,
        12,
        4,
        5
      ]
    ],
    "output_shape": [
      9,
      12,
      5
    ]
  },
  {
    "name": "matmul_adjx",
    "python_source": "def matmul_adjx(a, b):\n    \"\"\"Matrix multiplication of shapes (5,13) @ (13,8).\"\"\"\n    return jnp.dot(a, b)",
    "jaxpr": "{ lambda ; a:f32[5,13] b:f32[13,8]. let\n    c:f32[5,8] = dot_general[\n      dimension_numbers=(([1], [0]), ([], []))\n      preferred_element_type=float32\n    ] a b\n  in (c,) }",
    "stablehlo": "module @jit_matmul_adjx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<5x13xf32>, %arg1: tensor<13x8xf32>) -> (tensor<5x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.dot_general %arg0, %arg1, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<5x13xf32>, tensor<13x8xf32>) -> tensor<5x8xf32>\n    return %0 : tensor<5x8xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_matmul_adjx, entry_computation_layout={(f32[5,13]{1,0}, f32[13,8]{1,0})->f32[5,8]{1,0}}\n\nENTRY main.1 {\n  a.1 = f32[5,13]{1,0} parameter(0)\n  b.1 = f32[13,8]{1,0} parameter(1)\n  ROOT dot_general.1 = f32[5,8]{1,0} dot(a.1, b.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n}\n\n",
    "input_shapes": [
      [
        5,
        13
      ],
      [
        13,
        8
      ]
    ],
    "output_shape": [
      5,
      8
    ]
  },
  {
    "name": "nn_layer_apki",
    "python_source": "def nn_layer_apki(x, w, b):\n    \"\"\"Neural network layer with gelu activation.\"\"\"\n    return jax.nn.gelu(jnp.dot(x, w) + b)",
    "jaxpr": "{ lambda ; a:f32[7,17] b:f32[17,12] c:f32[12]. let\n    d:f32[7,12] = dot_general[\n      dimension_numbers=(([1], [0]), ([], []))\n      preferred_element_type=float32\n    ] a b\n    e:f32[1,12] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 12)\n      sharding=None\n    ] c\n    f:f32[7,12] = add d e\n    g:f32[7,12] = integer_pow[y=3] f\n    h:f32[7,12] = mul 0.044714998453855515:f32[] g\n    i:f32[7,12] = add f h\n    j:f32[7,12] = mul 0.7978845834732056:f32[] i\n    k:f32[7,12] = tanh j\n    l:f32[7,12] = add 1.0:f32[] k\n    m:f32[7,12] = mul 0.5:f32[] l\n    n:f32[7,12] = mul f m\n  in (n,) }",
    "stablehlo": "module @jit_nn_layer_apki attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x17xf32>, %arg1: tensor<17x12xf32>, %arg2: tensor<12xf32>) -> (tensor<7x12xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.dot_general %arg0, %arg1, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<7x17xf32>, tensor<17x12xf32>) -> tensor<7x12xf32>\n    %1 = stablehlo.broadcast_in_dim %arg2, dims = [1] : (tensor<12xf32>) -> tensor<1x12xf32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x12xf32>) -> tensor<7x12xf32>\n    %3 = stablehlo.add %0, %2 : tensor<7x12xf32>\n    %4 = stablehlo.multiply %3, %3 : tensor<7x12xf32>\n    %5 = stablehlo.multiply %4, %3 : tensor<7x12xf32>\n    %cst = stablehlo.constant dense<4.471500e-02> : tensor<f32>\n    %6 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<7x12xf32>\n    %7 = stablehlo.multiply %6, %5 : tensor<7x12xf32>\n    %8 = stablehlo.add %3, %7 : tensor<7x12xf32>\n    %cst_0 = stablehlo.constant dense<0.797884583> : tensor<f32>\n    %9 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<7x12xf32>\n    %10 = stablehlo.multiply %9, %8 : tensor<7x12xf32>\n    %11 = stablehlo.tanh %10 : tensor<7x12xf32>\n    %cst_1 = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %12 = stablehlo.broadcast_in_dim %cst_1, dims = [] : (tensor<f32>) -> tensor<7x12xf32>\n    %13 = stablehlo.add %12, %11 : tensor<7x12xf32>\n    %cst_2 = stablehlo.constant dense<5.000000e-01> : tensor<f32>\n    %14 = stablehlo.broadcast_in_dim %cst_2, dims = [] : (tensor<f32>) -> tensor<7x12xf32>\n    %15 = stablehlo.multiply %14, %13 : tensor<7x12xf32>\n    %16 = stablehlo.multiply %3, %15 : tensor<7x12xf32>\n    return %16 : tensor<7x12xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_nn_layer_apki, entry_computation_layout={(f32[7,17]{1,0}, f32[17,12]{1,0}, f32[12]{0})->f32[7,12]{1,0}}\n\nENTRY main.1 {\n  x.1 = f32[7,17]{1,0} parameter(0)\n  w.1 = f32[17,12]{1,0} parameter(1)\n  dot_general.1 = f32[7,12]{1,0} dot(x.1, w.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n  b.1 = f32[12]{0} parameter(2)\n  broadcast_in_dim.1 = f32[1,12]{1,0} reshape(b.1)\n  add.8 = f32[1,12]{1,0} broadcast(broadcast_in_dim.1), dimensions={0,1}\n  add.9 = f32[12]{0} reshape(add.8)\n  add.10 = f32[7,12]{1,0} broadcast(add.9), dimensions={1}\n  add.11 = f32[7,12]{1,0} add(dot_general.1, add.10)\n  integer_pow.2 = f32[7,12]{1,0} multiply(add.11, add.11)\n  integer_pow.3 = f32[7,12]{1,0} multiply(integer_pow.2, add.11)\n  constant.7 = f32[] constant(0.044715)\n  mul.9 = f32[7,12]{1,0} broadcast(constant.7), dimensions={}\n  mul.10 = f32[7,12]{1,0} multiply(integer_pow.3, mul.9)\n  add.12 = f32[7,12]{1,0} add(add.11, mul.10)\n  constant.6 = f32[] constant(0.797884583)\n  mul.8 = f32[7,12]{1,0} broadcast(constant.6), dimensions={}\n  mul.11 = f32[7,12]{1,0} multiply(add.12, mul.8)\n  tanh.1 = f32[7,12]{1,0} tanh(mul.11)\n  constant.5 = f32[] constant(1)\n  add.7 = f32[7,12]{1,0} broadcast(constant.5), dimensions={}\n  add.13 = f32[7,12]{1,0} add(tanh.1, add.7)\n  constant.4 = f32[] constant(0.5)\n  mul.7 = f32[7,12]{1,0} broadcast(constant.4), dimensions={}\n  mul.12 = f32[7,12]{1,0} multiply(add.13, mul.7)\n  ROOT mul.13 = f32[7,12]{1,0} multiply(add.11, mul.12)\n}\n\n",
    "input_shapes": [
      [
        7,
        17
      ],
      [
        17,
        12
      ],
      [
        12
      ]
    ],
    "output_shape": [
      7,
      12
    ]
  },
  {
    "name": "exp_rszc",
    "python_source": "def exp_rszc(x):\n    \"\"\"Exponential operation.\"\"\"\n    return jnp.exp(x)",
    "jaxpr": "{ lambda ; a:f32[2,6]. let b:f32[2,6] = exp a in (b,) }",
    "stablehlo": "module @jit_exp_rszc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x6xf32>) -> (tensor<2x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.exponential %arg0 : tensor<2x6xf32>\n    return %0 : tensor<2x6xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_exp_rszc, entry_computation_layout={(f32[2,6]{1,0})->f32[2,6]{1,0}}\n\nENTRY main.1 {\n  x.1 = f32[2,6]{1,0} parameter(0)\n  ROOT exp.1 = f32[2,6]{1,0} exponential(x.1)\n}\n\n",
    "input_shapes": [
      [
        2,
        6
      ]
    ],
    "output_shape": [
      2,
      6
    ]
  },
  {
    "name": "vmap_brvc",
    "python_source": "def vmap_brvc(a_batch, b_batch):\n    \"\"\"Batched operation using vmap.\"\"\"\n    def inner(a, b):\n        return jnp.sum(a * b)\n    return jax.vmap(inner)(a_batch, b_batch)",
    "jaxpr": "{ lambda ; a:f32[8,12] b:f32[8,12]. let\n    c:f32[8,12] = mul a b\n    d:f32[8] = reduce_sum[axes=(np.int64(1),) out_sharding=None] c\n  in (d,) }",
    "stablehlo": "module @jit_vmap_brvc attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x12xf32>, %arg1: tensor<8x12xf32>) -> (tensor<8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<8x12xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<8x12xf32>, tensor<f32>) -> tensor<8xf32>\n    return %1 : tensor<8xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_vmap_brvc, entry_computation_layout={(f32[8,12]{1,0}, f32[8,12]{1,0})->f32[8]{0}}\n\nregion_0.1 {\n  reduce_sum.3 = f32[] parameter(0)\n  reduce_sum.4 = f32[] parameter(1)\n  ROOT reduce_sum.5 = f32[] add(reduce_sum.3, reduce_sum.4)\n}\n\nENTRY main.2 {\n  a_batch.1 = f32[8,12]{1,0} parameter(0)\n  b_batch.1 = f32[8,12]{1,0} parameter(1)\n  mul.1 = f32[8,12]{1,0} multiply(a_batch.1, b_batch.1)\n  constant.1 = f32[] constant(0)\n  ROOT reduce_sum.7 = f32[8]{0} reduce(mul.1, constant.1), dimensions={1}, to_apply=region_0.1\n}\n\n",
    "input_shapes": [
      [
        8,
        12
      ],
      [
        8,
        12
      ]
    ],
    "output_shape": [
      8
    ]
  },
  {
    "name": "matmul_uxcv",
    "python_source": "def matmul_uxcv(a, b):\n    \"\"\"Matrix multiplication of shapes (9,11) @ (11,9).\"\"\"\n    return jnp.dot(a, b)",
    "jaxpr": "{ lambda ; a:f32[9,11] b:f32[11,9]. let\n    c:f32[9,9] = dot_general[\n      dimension_numbers=(([1], [0]), ([], []))\n      preferred_element_type=float32\n    ] a b\n  in (c,) }",
    "stablehlo": "module @jit_matmul_uxcv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x11xf32>, %arg1: tensor<11x9xf32>) -> (tensor<9x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.dot_general %arg0, %arg1, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<9x11xf32>, tensor<11x9xf32>) -> tensor<9x9xf32>\n    return %0 : tensor<9x9xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_matmul_uxcv, entry_computation_layout={(f32[9,11]{1,0}, f32[11,9]{1,0})->f32[9,9]{1,0}}\n\nENTRY main.1 {\n  a.1 = f32[9,11]{1,0} parameter(0)\n  b.1 = f32[11,9]{1,0} parameter(1)\n  ROOT dot_general.1 = f32[9,9]{1,0} dot(a.1, b.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n}\n\n",
    "input_shapes": [
      [
        9,
        11
      ],
      [
        11,
        9
      ]
    ],
    "output_shape": [
      9,
      9
    ]
  },
  {
    "name": "sin_wiio",
    "python_source": "def sin_wiio(x):\n    \"\"\"Sine operation.\"\"\"\n    return jnp.sin(x)",
    "jaxpr": "{ lambda ; a:f32[6,12]. let b:f32[6,12] = sin a in (b,) }",
    "stablehlo": "module @jit_sin_wiio attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x12xf32>) -> (tensor<6x12xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<6x12xf32>\n    return %0 : tensor<6x12xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_sin_wiio, entry_computation_layout={(f32[6,12]{1,0})->f32[6,12]{1,0}}\n\nENTRY main.1 {\n  x.1 = f32[6,12]{1,0} parameter(0)\n  ROOT sin.1 = f32[6,12]{1,0} sine(x.1)\n}\n\n",
    "input_shapes": [
      [
        6,
        12
      ]
    ],
    "output_shape": [
      6,
      12
    ]
  },
  {
    "name": "min_cumt",
    "python_source": "def min_cumt(x):\n    \"\"\"Min reduction along axis 2.\"\"\"\n    return jnp.min(x, axis=2)",
    "jaxpr": "{ lambda ; a:f32[10,3,7,6]. let\n    b:f32[10,3,6] = reduce_min[axes=(2,)] a\n  in (b,) }",
    "stablehlo": "module @jit_min_cumt attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x3x7x6xf32>) -> (tensor<10x3x6xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0x7F800000> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.minimum across dimensions = [2] : (tensor<10x3x7x6xf32>, tensor<f32>) -> tensor<10x3x6xf32>\n    return %0 : tensor<10x3x6xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_min_cumt, entry_computation_layout={(f32[10,3,7,6]{3,2,1,0})->f32[10,3,6]{2,1,0}}\n\nregion_0.1 {\n  reduce_min.3 = f32[] parameter(0)\n  reduce_min.4 = f32[] parameter(1)\n  ROOT reduce_min.5 = f32[] minimum(reduce_min.3, reduce_min.4)\n}\n\nENTRY main.2 {\n  x.1 = f32[10,3,7,6]{3,2,1,0} parameter(0)\n  constant.1 = f32[] constant(inf)\n  ROOT reduce_min.7 = f32[10,3,6]{2,1,0} reduce(x.1, constant.1), dimensions={2}, to_apply=region_0.1\n}\n\n",
    "input_shapes": [
      [
        10,
        3,
        7,
        6
      ]
    ],
    "output_shape": [
      10,
      3,
      6
    ]
  },
  {
    "name": "vmap_ujzu",
    "python_source": "def vmap_ujzu(a_batch, b_batch):\n    \"\"\"Batched operation using vmap.\"\"\"\n    def inner(a, b):\n        return jnp.sum(a * b)\n    return jax.vmap(inner)(a_batch, b_batch)",
    "jaxpr": "{ lambda ; a:f32[7,12] b:f32[7,12]. let\n    c:f32[7,12] = mul a b\n    d:f32[7] = reduce_sum[axes=(np.int64(1),) out_sharding=None] c\n  in (d,) }",
    "stablehlo": "module @jit_vmap_ujzu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x12xf32>, %arg1: tensor<7x12xf32>) -> (tensor<7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<7x12xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<7x12xf32>, tensor<f32>) -> tensor<7xf32>\n    return %1 : tensor<7xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_vmap_ujzu, entry_computation_layout={(f32[7,12]{1,0}, f32[7,12]{1,0})->f32[7]{0}}\n\nregion_0.1 {\n  reduce_sum.3 = f32[] parameter(0)\n  reduce_sum.4 = f32[] parameter(1)\n  ROOT reduce_sum.5 = f32[] add(reduce_sum.3, reduce_sum.4)\n}\n\nENTRY main.2 {\n  a_batch.1 = f32[7,12]{1,0} parameter(0)\n  b_batch.1 = f32[7,12]{1,0} parameter(1)\n  mul.1 = f32[7,12]{1,0} multiply(a_batch.1, b_batch.1)\n  constant.1 = f32[] constant(0)\n  ROOT reduce_sum.7 = f32[7]{0} reduce(mul.1, constant.1), dimensions={1}, to_apply=region_0.1\n}\n\n",
    "input_shapes": [
      [
        7,
        12
      ],
      [
        7,
        12
      ]
    ],
    "output_shape": [
      7
    ]
  },
  {
    "name": "nn_layer_ebll",
    "python_source": "def nn_layer_ebll(x, w, b):\n    \"\"\"Neural network layer with sigmoid activation.\"\"\"\n    return jax.nn.sigmoid(jnp.dot(x, w) + b)",
    "jaxpr": "{ lambda ; a:f32[8,27] b:f32[27,26] c:f32[26]. let\n    d:f32[8,26] = dot_general[\n      dimension_numbers=(([1], [0]), ([], []))\n      preferred_element_type=float32\n    ] a b\n    e:f32[1,26] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 26)\n      sharding=None\n    ] c\n    f:f32[8,26] = add d e\n    g:f32[8,26] = logistic f\n  in (g,) }",
    "stablehlo": "module @jit_nn_layer_ebll attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x27xf32>, %arg1: tensor<27x26xf32>, %arg2: tensor<26xf32>) -> (tensor<8x26xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.dot_general %arg0, %arg1, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<8x27xf32>, tensor<27x26xf32>) -> tensor<8x26xf32>\n    %1 = stablehlo.broadcast_in_dim %arg2, dims = [1] : (tensor<26xf32>) -> tensor<1x26xf32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x26xf32>) -> tensor<8x26xf32>\n    %3 = stablehlo.add %0, %2 : tensor<8x26xf32>\n    %4 = stablehlo.negate %3 : tensor<8x26xf32>\n    %5 = stablehlo.exponential %4 : tensor<8x26xf32>\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %6 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<8x26xf32>\n    %7 = stablehlo.add %6, %5 : tensor<8x26xf32>\n    %cst_0 = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %8 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<8x26xf32>\n    %9 = stablehlo.divide %8, %7 : tensor<8x26xf32>\n    return %9 : tensor<8x26xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_nn_layer_ebll, entry_computation_layout={(f32[8,27]{1,0}, f32[27,26]{1,0}, f32[26]{0})->f32[8,26]{1,0}}\n\nENTRY main.1 {\n  constant.1 = f32[] constant(1)\n  broadcast.1 = f32[8,26]{1,0} broadcast(constant.1), dimensions={}\n  x.1 = f32[8,27]{1,0} parameter(0)\n  w.1 = f32[27,26]{1,0} parameter(1)\n  dot_general.1 = f32[8,26]{1,0} dot(x.1, w.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n  b.1 = f32[26]{0} parameter(2)\n  broadcast_in_dim.1 = f32[1,26]{1,0} reshape(b.1)\n  add.5 = f32[1,26]{1,0} broadcast(broadcast_in_dim.1), dimensions={0,1}\n  add.6 = f32[26]{0} reshape(add.5)\n  add.7 = f32[8,26]{1,0} broadcast(add.6), dimensions={1}\n  add.8 = f32[8,26]{1,0} add(dot_general.1, add.7)\n  neg.1 = f32[8,26]{1,0} negate(add.8)\n  exp.1 = f32[8,26]{1,0} exponential(neg.1)\n  add.9 = f32[8,26]{1,0} add(exp.1, broadcast.1)\n  ROOT div.1 = f32[8,26]{1,0} divide(broadcast.1, add.9)\n}\n\n",
    "input_shapes": [
      [
        8,
        27
      ],
      [
        27,
        26
      ],
      [
        26
      ]
    ],
    "output_shape": [
      8,
      26
    ]
  },
  {
    "name": "matmul_ftvl",
    "python_source": "def matmul_ftvl(a, b):\n    \"\"\"Matrix multiplication of shapes (8,15) @ (15,8).\"\"\"\n    return jnp.dot(a, b)",
    "jaxpr": "{ lambda ; a:f32[8,15] b:f32[15,8]. let\n    c:f32[8,8] = dot_general[\n      dimension_numbers=(([1], [0]), ([], []))\n      preferred_element_type=float32\n    ] a b\n  in (c,) }",
    "stablehlo": "module @jit_matmul_ftvl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x15xf32>, %arg1: tensor<15x8xf32>) -> (tensor<8x8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.dot_general %arg0, %arg1, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<8x15xf32>, tensor<15x8xf32>) -> tensor<8x8xf32>\n    return %0 : tensor<8x8xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_matmul_ftvl, entry_computation_layout={(f32[8,15]{1,0}, f32[15,8]{1,0})->f32[8,8]{1,0}}\n\nENTRY main.1 {\n  a.1 = f32[8,15]{1,0} parameter(0)\n  b.1 = f32[15,8]{1,0} parameter(1)\n  ROOT dot_general.1 = f32[8,8]{1,0} dot(a.1, b.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n}\n\n",
    "input_shapes": [
      [
        8,
        15
      ],
      [
        15,
        8
      ]
    ],
    "output_shape": [
      8,
      8
    ]
  },
  {
    "name": "composite_cajr",
    "python_source": "def composite_cajr(x):\n    \"\"\"Composite function with multiple operations.\"\"\"\n    return jnp.tanh(x) + jax.nn.relu(x)",
    "jaxpr": "{ lambda ; a:f32[9,15]. let\n    b:f32[9,15] = tanh a\n    c:f32[9,15] = custom_jvp_call[\n      name=relu\n      call_jaxpr={ lambda ; d:f32[9,15]. let\n          e:f32[9,15] = jit[\n            name=relu\n            jaxpr={ lambda ; d:f32[9,15]. let\n                e:f32[9,15] = max d 0.0:f32[]\n              in (e,) }\n          ] d\n        in (e,) }\n      jvp=jvp\n      symbolic_zeros=False\n    ] a\n    f:f32[9,15] = add b c\n  in (f,) }",
    "stablehlo": "module @jit_composite_cajr attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9x15xf32>) -> (tensor<9x15xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.tanh %arg0 : tensor<9x15xf32>\n    %1 = call @relu(%arg0) : (tensor<9x15xf32>) -> tensor<9x15xf32>\n    %2 = stablehlo.add %0, %1 : tensor<9x15xf32>\n    return %2 : tensor<9x15xf32>\n  }\n  func.func private @relu(%arg0: tensor<9x15xf32>) -> tensor<9x15xf32> {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<9x15xf32>\n    %1 = stablehlo.maximum %arg0, %0 : tensor<9x15xf32>\n    return %1 : tensor<9x15xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_composite_cajr, entry_computation_layout={(f32[9,15]{1,0})->f32[9,15]{1,0}}\n\nrelu.1 {\n  Arg_0.1 = f32[9,15]{1,0} parameter(0)\n  constant.1 = f32[] constant(0)\n  max.2 = f32[9,15]{1,0} broadcast(constant.1), dimensions={}\n  ROOT max.3 = f32[9,15]{1,0} maximum(Arg_0.1, max.2)\n}\n\nENTRY main.2 {\n  x.1 = f32[9,15]{1,0} parameter(0)\n  tanh.1 = f32[9,15]{1,0} tanh(x.1)\n  jit_relu_.1 = f32[9,15]{1,0} call(x.1), to_apply=relu.1\n  ROOT add.1 = f32[9,15]{1,0} add(tanh.1, jit_relu_.1)\n}\n\n",
    "input_shapes": [
      [
        9,
        15
      ]
    ],
    "output_shape": [
      9,
      15
    ]
  },
  {
    "name": "mul_pqfa",
    "python_source": "def mul_pqfa(x, y):\n    \"\"\"Multiplication operation.\"\"\"\n    return x * y",
    "jaxpr": "{ lambda ; a:f32[8,10,12] b:f32[8,10,12]. let c:f32[8,10,12] = mul a b in (c,) }",
    "stablehlo": "module @jit_mul_pqfa attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x10x12xf32>, %arg1: tensor<8x10x12xf32>) -> (tensor<8x10x12xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<8x10x12xf32>\n    return %0 : tensor<8x10x12xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_mul_pqfa, entry_computation_layout={(f32[8,10,12]{2,1,0}, f32[8,10,12]{2,1,0})->f32[8,10,12]{2,1,0}}\n\nENTRY main.1 {\n  x.1 = f32[8,10,12]{2,1,0} parameter(0)\n  y.1 = f32[8,10,12]{2,1,0} parameter(1)\n  ROOT mul.1 = f32[8,10,12]{2,1,0} multiply(x.1, y.1)\n}\n\n",
    "input_shapes": [
      [
        8,
        10,
        12
      ],
      [
        8,
        10,
        12
      ]
    ],
    "output_shape": [
      8,
      10,
      12
    ]
  },
  {
    "name": "sub_rivk",
    "python_source": "def sub_rivk(x, y):\n    \"\"\"Subtraction operation.\"\"\"\n    return x - y",
    "jaxpr": "{ lambda ; a:f32[14] b:f32[14]. let c:f32[14] = sub a b in (c,) }",
    "stablehlo": "module @jit_sub_rivk attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<14xf32>, %arg1: tensor<14xf32>) -> (tensor<14xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<14xf32>\n    return %0 : tensor<14xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_sub_rivk, entry_computation_layout={(f32[14]{0}, f32[14]{0})->f32[14]{0}}\n\nENTRY main.1 {\n  x.1 = f32[14]{0} parameter(0)\n  y.1 = f32[14]{0} parameter(1)\n  ROOT sub.1 = f32[14]{0} subtract(x.1, y.1)\n}\n\n",
    "input_shapes": [
      [
        14
      ],
      [
        14
      ]
    ],
    "output_shape": [
      14
    ]
  },
  {
    "name": "div_xafp",
    "python_source": "def div_xafp(x, y):\n    \"\"\"Division (safe) operation.\"\"\"\n    return x / (y + 1e-6)",
    "jaxpr": "{ lambda ; a:f32[14] b:f32[14]. let\n    c:f32[14] = add b 9.999999974752427e-07:f32[]\n    d:f32[14] = div a c\n  in (d,) }",
    "stablehlo": "module @jit_div_xafp attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<14xf32>, %arg1: tensor<14xf32>) -> (tensor<14xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<9.99999997E-7> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<14xf32>\n    %1 = stablehlo.add %arg1, %0 : tensor<14xf32>\n    %2 = stablehlo.divide %arg0, %1 : tensor<14xf32>\n    return %2 : tensor<14xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_div_xafp, entry_computation_layout={(f32[14]{0}, f32[14]{0})->f32[14]{0}}\n\nENTRY main.1 {\n  x.1 = f32[14]{0} parameter(0)\n  y.1 = f32[14]{0} parameter(1)\n  constant.1 = f32[] constant(1e-06)\n  broadcast.1 = f32[14]{0} broadcast(constant.1), dimensions={}\n  add.1 = f32[14]{0} add(y.1, broadcast.1)\n  ROOT div.1 = f32[14]{0} divide(x.1, add.1)\n}\n\n",
    "input_shapes": [
      [
        14
      ],
      [
        14
      ]
    ],
    "output_shape": [
      14
    ]
  },
  {
    "name": "vmap_leby",
    "python_source": "def vmap_leby(a_batch, b_batch):\n    \"\"\"Batched operation using vmap.\"\"\"\n    def inner(a, b):\n        return jnp.dot(a, b)\n    return jax.vmap(inner)(a_batch, b_batch)",
    "jaxpr": "{ lambda ; a:f32[8,11] b:f32[8,11]. let\n    c:f32[8] = dot_general[\n      dimension_numbers=(([1], [1]), ([0], [0]))\n      preferred_element_type=float32\n    ] a b\n  in (c,) }",
    "stablehlo": "module @jit_vmap_leby attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8x11xf32>, %arg1: tensor<8x11xf32>) -> (tensor<8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.dot_general %arg0, %arg1, batching_dims = [0] x [0], contracting_dims = [1] x [1], precision = [DEFAULT, DEFAULT] : (tensor<8x11xf32>, tensor<8x11xf32>) -> tensor<8xf32>\n    return %0 : tensor<8xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_vmap_leby, entry_computation_layout={(f32[8,11]{1,0}, f32[8,11]{1,0})->f32[8]{0}}\n\nENTRY main.1 {\n  a_batch.1 = f32[8,11]{1,0} parameter(0)\n  b_batch.1 = f32[8,11]{1,0} parameter(1)\n  ROOT dot_general.1 = f32[8]{0} dot(a_batch.1, b_batch.1), lhs_batch_dims={0}, lhs_contracting_dims={1}, rhs_batch_dims={0}, rhs_contracting_dims={1}\n}\n\n",
    "input_shapes": [
      [
        8,
        11
      ],
      [
        8,
        11
      ]
    ],
    "output_shape": [
      8
    ]
  },
  {
    "name": "div_nypi",
    "python_source": "def div_nypi(x, y):\n    \"\"\"Division (safe) operation.\"\"\"\n    return x / (y + 1e-6)",
    "jaxpr": "{ lambda ; a:f32[4] b:f32[4]. let\n    c:f32[4] = add b 9.999999974752427e-07:f32[]\n    d:f32[4] = div a c\n  in (d,) }",
    "stablehlo": "module @jit_div_nypi attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4xf32>, %arg1: tensor<4xf32>) -> (tensor<4xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<9.99999997E-7> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<4xf32>\n    %1 = stablehlo.add %arg1, %0 : tensor<4xf32>\n    %2 = stablehlo.divide %arg0, %1 : tensor<4xf32>\n    return %2 : tensor<4xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_div_nypi, entry_computation_layout={(f32[4]{0}, f32[4]{0})->f32[4]{0}}\n\nENTRY main.1 {\n  x.1 = f32[4]{0} parameter(0)\n  y.1 = f32[4]{0} parameter(1)\n  constant.1 = f32[] constant(1e-06)\n  broadcast.1 = f32[4]{0} broadcast(constant.1), dimensions={}\n  add.1 = f32[4]{0} add(y.1, broadcast.1)\n  ROOT div.1 = f32[4]{0} divide(x.1, add.1)\n}\n\n",
    "input_shapes": [
      [
        4
      ],
      [
        4
      ]
    ],
    "output_shape": [
      4
    ]
  },
  {
    "name": "min_dsil",
    "python_source": "def min_dsil(x, y):\n    \"\"\"Element-wise min operation.\"\"\"\n    return jnp.minimum(x, y)",
    "jaxpr": "{ lambda ; a:f32[14,12] b:f32[14,12]. let c:f32[14,12] = min a b in (c,) }",
    "stablehlo": "module @jit_min_dsil attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<14x12xf32>, %arg1: tensor<14x12xf32>) -> (tensor<14x12xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.minimum %arg0, %arg1 : tensor<14x12xf32>\n    return %0 : tensor<14x12xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_min_dsil, entry_computation_layout={(f32[14,12]{1,0}, f32[14,12]{1,0})->f32[14,12]{1,0}}\n\nENTRY main.1 {\n  x.1 = f32[14,12]{1,0} parameter(0)\n  y.1 = f32[14,12]{1,0} parameter(1)\n  ROOT min.1 = f32[14,12]{1,0} minimum(x.1, y.1)\n}\n\n",
    "input_shapes": [
      [
        14,
        12
      ],
      [
        14,
        12
      ]
    ],
    "output_shape": [
      14,
      12
    ]
  },
  {
    "name": "composite_jkll",
    "python_source": "def composite_jkll(x):\n    \"\"\"Composite function with multiple operations.\"\"\"\n    return jnp.exp(-jnp.abs(x)) + jnp.sin(x)",
    "jaxpr": "{ lambda ; a:f32[3,4]. let\n    b:f32[3,4] = abs a\n    c:f32[3,4] = neg b\n    d:f32[3,4] = exp c\n    e:f32[3,4] = sin a\n    f:f32[3,4] = add d e\n  in (f,) }",
    "stablehlo": "module @jit_composite_jkll attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<3x4xf32>) -> (tensor<3x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.abs %arg0 : tensor<3x4xf32>\n    %1 = stablehlo.negate %0 : tensor<3x4xf32>\n    %2 = stablehlo.exponential %1 : tensor<3x4xf32>\n    %3 = stablehlo.sine %arg0 : tensor<3x4xf32>\n    %4 = stablehlo.add %2, %3 : tensor<3x4xf32>\n    return %4 : tensor<3x4xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_composite_jkll, entry_computation_layout={(f32[3,4]{1,0})->f32[3,4]{1,0}}\n\nENTRY main.1 {\n  x.1 = f32[3,4]{1,0} parameter(0)\n  abs.1 = f32[3,4]{1,0} abs(x.1)\n  neg.1 = f32[3,4]{1,0} negate(abs.1)\n  exp.1 = f32[3,4]{1,0} exponential(neg.1)\n  sin.1 = f32[3,4]{1,0} sine(x.1)\n  ROOT add.1 = f32[3,4]{1,0} add(exp.1, sin.1)\n}\n\n",
    "input_shapes": [
      [
        3,
        4
      ]
    ],
    "output_shape": [
      3,
      4
    ]
  },
  {
    "name": "abs_zhiu",
    "python_source": "def abs_zhiu(x):\n    \"\"\"Absolute value operation.\"\"\"\n    return jnp.abs(x)",
    "jaxpr": "{ lambda ; a:f32[15,11,14]. let b:f32[15,11,14] = abs a in (b,) }",
    "stablehlo": "module @jit_abs_zhiu attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<15x11x14xf32>) -> (tensor<15x11x14xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.abs %arg0 : tensor<15x11x14xf32>\n    return %0 : tensor<15x11x14xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_abs_zhiu, entry_computation_layout={(f32[15,11,14]{2,1,0})->f32[15,11,14]{2,1,0}}\n\nENTRY main.1 {\n  x.1 = f32[15,11,14]{2,1,0} parameter(0)\n  ROOT abs.1 = f32[15,11,14]{2,1,0} abs(x.1)\n}\n\n",
    "input_shapes": [
      [
        15,
        11,
        14
      ]
    ],
    "output_shape": [
      15,
      11,
      14
    ]
  },
  {
    "name": "matmul_tgmv",
    "python_source": "def matmul_tgmv(a, b):\n    \"\"\"Matrix multiplication of shapes (16,7) @ (7,15).\"\"\"\n    return jnp.dot(a, b)",
    "jaxpr": "{ lambda ; a:f32[16,7] b:f32[7,15]. let\n    c:f32[16,15] = dot_general[\n      dimension_numbers=(([1], [0]), ([], []))\n      preferred_element_type=float32\n    ] a b\n  in (c,) }",
    "stablehlo": "module @jit_matmul_tgmv attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<16x7xf32>, %arg1: tensor<7x15xf32>) -> (tensor<16x15xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.dot_general %arg0, %arg1, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<16x7xf32>, tensor<7x15xf32>) -> tensor<16x15xf32>\n    return %0 : tensor<16x15xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_matmul_tgmv, entry_computation_layout={(f32[16,7]{1,0}, f32[7,15]{1,0})->f32[16,15]{1,0}}\n\nENTRY main.1 {\n  a.1 = f32[16,7]{1,0} parameter(0)\n  b.1 = f32[7,15]{1,0} parameter(1)\n  ROOT dot_general.1 = f32[16,15]{1,0} dot(a.1, b.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n}\n\n",
    "input_shapes": [
      [
        16,
        7
      ],
      [
        7,
        15
      ]
    ],
    "output_shape": [
      16,
      15
    ]
  },
  {
    "name": "nn_layer_uraj",
    "python_source": "def nn_layer_uraj(x, w, b):\n    \"\"\"Neural network layer with sigmoid activation.\"\"\"\n    return jax.nn.sigmoid(jnp.dot(x, w) + b)",
    "jaxpr": "{ lambda ; a:f32[4,4] b:f32[4,6] c:f32[6]. let\n    d:f32[4,6] = dot_general[\n      dimension_numbers=(([1], [0]), ([], []))\n      preferred_element_type=float32\n    ] a b\n    e:f32[1,6] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 6)\n      sharding=None\n    ] c\n    f:f32[4,6] = add d e\n    g:f32[4,6] = logistic f\n  in (g,) }",
    "stablehlo": "module @jit_nn_layer_uraj attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x4xf32>, %arg1: tensor<4x6xf32>, %arg2: tensor<6xf32>) -> (tensor<4x6xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.dot_general %arg0, %arg1, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<4x4xf32>, tensor<4x6xf32>) -> tensor<4x6xf32>\n    %1 = stablehlo.broadcast_in_dim %arg2, dims = [1] : (tensor<6xf32>) -> tensor<1x6xf32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x6xf32>) -> tensor<4x6xf32>\n    %3 = stablehlo.add %0, %2 : tensor<4x6xf32>\n    %4 = stablehlo.negate %3 : tensor<4x6xf32>\n    %5 = stablehlo.exponential %4 : tensor<4x6xf32>\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %6 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<4x6xf32>\n    %7 = stablehlo.add %6, %5 : tensor<4x6xf32>\n    %cst_0 = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %8 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<4x6xf32>\n    %9 = stablehlo.divide %8, %7 : tensor<4x6xf32>\n    return %9 : tensor<4x6xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_nn_layer_uraj, entry_computation_layout={(f32[4,4]{1,0}, f32[4,6]{1,0}, f32[6]{0})->f32[4,6]{1,0}}\n\nENTRY main.1 {\n  constant.1 = f32[] constant(1)\n  broadcast.1 = f32[4,6]{1,0} broadcast(constant.1), dimensions={}\n  x.1 = f32[4,4]{1,0} parameter(0)\n  w.1 = f32[4,6]{1,0} parameter(1)\n  dot_general.1 = f32[4,6]{1,0} dot(x.1, w.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n  b.1 = f32[6]{0} parameter(2)\n  broadcast_in_dim.1 = f32[1,6]{1,0} reshape(b.1)\n  add.5 = f32[1,6]{1,0} broadcast(broadcast_in_dim.1), dimensions={0,1}\n  add.6 = f32[6]{0} reshape(add.5)\n  add.7 = f32[4,6]{1,0} broadcast(add.6), dimensions={1}\n  add.8 = f32[4,6]{1,0} add(dot_general.1, add.7)\n  neg.1 = f32[4,6]{1,0} negate(add.8)\n  exp.1 = f32[4,6]{1,0} exponential(neg.1)\n  add.9 = f32[4,6]{1,0} add(exp.1, broadcast.1)\n  ROOT div.1 = f32[4,6]{1,0} divide(broadcast.1, add.9)\n}\n\n",
    "input_shapes": [
      [
        4,
        4
      ],
      [
        4,
        6
      ],
      [
        6
      ]
    ],
    "output_shape": [
      4,
      6
    ]
  },
  {
    "name": "vmap_fjhm",
    "python_source": "def vmap_fjhm(a_batch, b_batch):\n    \"\"\"Batched operation using vmap.\"\"\"\n    def inner(a, b):\n        return jnp.maximum(a, b)\n    return jax.vmap(inner)(a_batch, b_batch)",
    "jaxpr": "{ lambda ; a:f32[2,14] b:f32[2,14]. let c:f32[2,14] = max a b in (c,) }",
    "stablehlo": "module @jit_vmap_fjhm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x14xf32>, %arg1: tensor<2x14xf32>) -> (tensor<2x14xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.maximum %arg0, %arg1 : tensor<2x14xf32>\n    return %0 : tensor<2x14xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_vmap_fjhm, entry_computation_layout={(f32[2,14]{1,0}, f32[2,14]{1,0})->f32[2,14]{1,0}}\n\nENTRY main.1 {\n  a_batch.1 = f32[2,14]{1,0} parameter(0)\n  b_batch.1 = f32[2,14]{1,0} parameter(1)\n  ROOT max.1 = f32[2,14]{1,0} maximum(a_batch.1, b_batch.1)\n}\n\n",
    "input_shapes": [
      [
        2,
        14
      ],
      [
        2,
        14
      ]
    ],
    "output_shape": [
      2,
      14
    ]
  },
  {
    "name": "sigmoid_oufl",
    "python_source": "def sigmoid_oufl(x):\n    \"\"\"Sigmoid activation operation.\"\"\"\n    return jax.nn.sigmoid(x)",
    "jaxpr": "{ lambda ; a:f32[10,15]. let b:f32[10,15] = logistic a in (b,) }",
    "stablehlo": "module @jit_sigmoid_oufl attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10x15xf32>) -> (tensor<10x15xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.negate %arg0 : tensor<10x15xf32>\n    %1 = stablehlo.exponential %0 : tensor<10x15xf32>\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10x15xf32>\n    %3 = stablehlo.add %2, %1 : tensor<10x15xf32>\n    %cst_0 = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<10x15xf32>\n    %5 = stablehlo.divide %4, %3 : tensor<10x15xf32>\n    return %5 : tensor<10x15xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_sigmoid_oufl, entry_computation_layout={(f32[10,15]{1,0})->f32[10,15]{1,0}}\n\nENTRY main.1 {\n  constant.1 = f32[] constant(1)\n  broadcast.1 = f32[10,15]{1,0} broadcast(constant.1), dimensions={}\n  x.1 = f32[10,15]{1,0} parameter(0)\n  neg.1 = f32[10,15]{1,0} negate(x.1)\n  exp.1 = f32[10,15]{1,0} exponential(neg.1)\n  add.1 = f32[10,15]{1,0} add(exp.1, broadcast.1)\n  ROOT div.1 = f32[10,15]{1,0} divide(broadcast.1, add.1)\n}\n\n",
    "input_shapes": [
      [
        10,
        15
      ]
    ],
    "output_shape": [
      10,
      15
    ]
  },
  {
    "name": "sigmoid_cwqh",
    "python_source": "def sigmoid_cwqh(x):\n    \"\"\"Sigmoid activation operation.\"\"\"\n    return jax.nn.sigmoid(x)",
    "jaxpr": "{ lambda ; a:f32[8]. let b:f32[8] = logistic a in (b,) }",
    "stablehlo": "module @jit_sigmoid_cwqh attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<8xf32>) -> (tensor<8xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.negate %arg0 : tensor<8xf32>\n    %1 = stablehlo.exponential %0 : tensor<8xf32>\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<8xf32>\n    %3 = stablehlo.add %2, %1 : tensor<8xf32>\n    %cst_0 = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %4 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<8xf32>\n    %5 = stablehlo.divide %4, %3 : tensor<8xf32>\n    return %5 : tensor<8xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_sigmoid_cwqh, entry_computation_layout={(f32[8]{0})->f32[8]{0}}\n\nENTRY main.1 {\n  constant.1 = f32[] constant(1)\n  broadcast.1 = f32[8]{0} broadcast(constant.1), dimensions={}\n  x.1 = f32[8]{0} parameter(0)\n  neg.1 = f32[8]{0} negate(x.1)\n  exp.1 = f32[8]{0} exponential(neg.1)\n  add.1 = f32[8]{0} add(exp.1, broadcast.1)\n  ROOT div.1 = f32[8]{0} divide(broadcast.1, add.1)\n}\n\n",
    "input_shapes": [
      [
        8
      ]
    ],
    "output_shape": [
      8
    ]
  },
  {
    "name": "composite_xwyf",
    "python_source": "def composite_xwyf(x):\n    \"\"\"Composite function with multiple operations.\"\"\"\n    return jnp.sin(x) + jnp.tanh(x) + x ** 2",
    "jaxpr": "{ lambda ; a:f32[16,15]. let\n    b:f32[16,15] = sin a\n    c:f32[16,15] = tanh a\n    d:f32[16,15] = add b c\n    e:f32[16,15] = integer_pow[y=2] a\n    f:f32[16,15] = add d e\n  in (f,) }",
    "stablehlo": "module @jit_composite_xwyf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<16x15xf32>) -> (tensor<16x15xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<16x15xf32>\n    %1 = stablehlo.tanh %arg0 : tensor<16x15xf32>\n    %2 = stablehlo.add %0, %1 : tensor<16x15xf32>\n    %3 = stablehlo.multiply %arg0, %arg0 : tensor<16x15xf32>\n    %4 = stablehlo.add %2, %3 : tensor<16x15xf32>\n    return %4 : tensor<16x15xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_composite_xwyf, entry_computation_layout={(f32[16,15]{1,0})->f32[16,15]{1,0}}\n\nENTRY main.1 {\n  x.1 = f32[16,15]{1,0} parameter(0)\n  sin.1 = f32[16,15]{1,0} sine(x.1)\n  tanh.1 = f32[16,15]{1,0} tanh(x.1)\n  add.2 = f32[16,15]{1,0} add(sin.1, tanh.1)\n  integer_pow.1 = f32[16,15]{1,0} multiply(x.1, x.1)\n  ROOT add.3 = f32[16,15]{1,0} add(add.2, integer_pow.1)\n}\n\n",
    "input_shapes": [
      [
        16,
        15
      ]
    ],
    "output_shape": [
      16,
      15
    ]
  },
  {
    "name": "mean_jfys",
    "python_source": "def mean_jfys(x):\n    \"\"\"Mean reduction along axis 2.\"\"\"\n    return jnp.mean(x, axis=2)",
    "jaxpr": "{ lambda ; a:f32[13,16,9]. let\n    b:f32[13,16] = reduce_sum[axes=(2,) out_sharding=None] a\n    c:f32[13,16] = div b 9.0:f32[]\n  in (c,) }",
    "stablehlo": "module @jit_mean_jfys attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<13x16x9xf32>) -> (tensor<13x16xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.reduce(%arg0 init: %cst) applies stablehlo.add across dimensions = [2] : (tensor<13x16x9xf32>, tensor<f32>) -> tensor<13x16xf32>\n    %cst_0 = stablehlo.constant dense<9.000000e+00> : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %cst_0, dims = [] : (tensor<f32>) -> tensor<13x16xf32>\n    %2 = stablehlo.divide %0, %1 : tensor<13x16xf32>\n    return %2 : tensor<13x16xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_mean_jfys, entry_computation_layout={(f32[13,16,9]{2,1,0})->f32[13,16]{1,0}}\n\nregion_0.1 {\n  reduce_sum.3 = f32[] parameter(0)\n  reduce_sum.4 = f32[] parameter(1)\n  ROOT reduce_sum.5 = f32[] add(reduce_sum.3, reduce_sum.4)\n}\n\nENTRY main.2 {\n  x.1 = f32[13,16,9]{2,1,0} parameter(0)\n  constant.3 = f32[] constant(0)\n  reduce_sum.7 = f32[13,16]{1,0} reduce(x.1, constant.3), dimensions={2}, to_apply=region_0.1\n  constant.2 = f32[] constant(9)\n  div.2 = f32[13,16]{1,0} broadcast(constant.2), dimensions={}\n  ROOT div.3 = f32[13,16]{1,0} divide(reduce_sum.7, div.2)\n}\n\n",
    "input_shapes": [
      [
        13,
        16,
        9
      ]
    ],
    "output_shape": [
      13,
      16
    ]
  },
  {
    "name": "nn_layer_vxlx",
    "python_source": "def nn_layer_vxlx(x, w, b):\n    \"\"\"Neural network layer with relu activation.\"\"\"\n    return jax.nn.relu(jnp.dot(x, w) + b)",
    "jaxpr": "{ lambda ; a:f32[7,31] b:f32[31,9] c:f32[9]. let\n    d:f32[7,9] = dot_general[\n      dimension_numbers=(([1], [0]), ([], []))\n      preferred_element_type=float32\n    ] a b\n    e:f32[1,9] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 9)\n      sharding=None\n    ] c\n    f:f32[7,9] = add d e\n    g:f32[7,9] = custom_jvp_call[\n      name=relu\n      call_jaxpr={ lambda ; h:f32[7,9]. let\n          i:f32[7,9] = jit[\n            name=relu\n            jaxpr={ lambda ; h:f32[7,9]. let\n                i:f32[7,9] = max h 0.0:f32[]\n              in (i,) }\n          ] h\n        in (i,) }\n      jvp=jvp\n      symbolic_zeros=False\n    ] f\n  in (g,) }",
    "stablehlo": "module @jit_nn_layer_vxlx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x31xf32>, %arg1: tensor<31x9xf32>, %arg2: tensor<9xf32>) -> (tensor<7x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.dot_general %arg0, %arg1, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<7x31xf32>, tensor<31x9xf32>) -> tensor<7x9xf32>\n    %1 = stablehlo.broadcast_in_dim %arg2, dims = [1] : (tensor<9xf32>) -> tensor<1x9xf32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x9xf32>) -> tensor<7x9xf32>\n    %3 = stablehlo.add %0, %2 : tensor<7x9xf32>\n    %4 = call @relu(%3) : (tensor<7x9xf32>) -> tensor<7x9xf32>\n    return %4 : tensor<7x9xf32>\n  }\n  func.func private @relu(%arg0: tensor<7x9xf32>) -> tensor<7x9xf32> {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<7x9xf32>\n    %1 = stablehlo.maximum %arg0, %0 : tensor<7x9xf32>\n    return %1 : tensor<7x9xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_nn_layer_vxlx, entry_computation_layout={(f32[7,31]{1,0}, f32[31,9]{1,0}, f32[9]{0})->f32[7,9]{1,0}}\n\nrelu.1 {\n  Arg_0.1 = f32[7,9]{1,0} parameter(0)\n  constant.1 = f32[] constant(0)\n  max.2 = f32[7,9]{1,0} broadcast(constant.1), dimensions={}\n  ROOT max.3 = f32[7,9]{1,0} maximum(Arg_0.1, max.2)\n}\n\nENTRY main.2 {\n  x.1 = f32[7,31]{1,0} parameter(0)\n  w.1 = f32[31,9]{1,0} parameter(1)\n  dot_general.1 = f32[7,9]{1,0} dot(x.1, w.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n  b.1 = f32[9]{0} parameter(2)\n  broadcast_in_dim.1 = f32[1,9]{1,0} reshape(b.1)\n  add.4 = f32[1,9]{1,0} broadcast(broadcast_in_dim.1), dimensions={0,1}\n  add.5 = f32[9]{0} reshape(add.4)\n  add.6 = f32[7,9]{1,0} broadcast(add.5), dimensions={1}\n  add.7 = f32[7,9]{1,0} add(dot_general.1, add.6)\n  ROOT jit_relu_.1 = f32[7,9]{1,0} call(add.7), to_apply=relu.1\n}\n\n",
    "input_shapes": [
      [
        7,
        31
      ],
      [
        31,
        9
      ],
      [
        9
      ]
    ],
    "output_shape": [
      7,
      9
    ]
  },
  {
    "name": "composite_ihai",
    "python_source": "def composite_ihai(x):\n    \"\"\"Composite function with multiple operations.\"\"\"\n    return jnp.sin(x) + jnp.exp(-jnp.abs(x))",
    "jaxpr": "{ lambda ; a:f32[6,9]. let\n    b:f32[6,9] = sin a\n    c:f32[6,9] = abs a\n    d:f32[6,9] = neg c\n    e:f32[6,9] = exp d\n    f:f32[6,9] = add b e\n  in (f,) }",
    "stablehlo": "module @jit_composite_ihai attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<6x9xf32>) -> (tensor<6x9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.sine %arg0 : tensor<6x9xf32>\n    %1 = stablehlo.abs %arg0 : tensor<6x9xf32>\n    %2 = stablehlo.negate %1 : tensor<6x9xf32>\n    %3 = stablehlo.exponential %2 : tensor<6x9xf32>\n    %4 = stablehlo.add %0, %3 : tensor<6x9xf32>\n    return %4 : tensor<6x9xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_composite_ihai, entry_computation_layout={(f32[6,9]{1,0})->f32[6,9]{1,0}}\n\nENTRY main.1 {\n  x.1 = f32[6,9]{1,0} parameter(0)\n  sin.1 = f32[6,9]{1,0} sine(x.1)\n  abs.1 = f32[6,9]{1,0} abs(x.1)\n  neg.1 = f32[6,9]{1,0} negate(abs.1)\n  exp.1 = f32[6,9]{1,0} exponential(neg.1)\n  ROOT add.1 = f32[6,9]{1,0} add(sin.1, exp.1)\n}\n\n",
    "input_shapes": [
      [
        6,
        9
      ]
    ],
    "output_shape": [
      6,
      9
    ]
  },
  {
    "name": "vmap_toau",
    "python_source": "def vmap_toau(a_batch, b_batch):\n    \"\"\"Batched operation using vmap.\"\"\"\n    def inner(a, b):\n        return jnp.sum(a * b)\n    return jax.vmap(inner)(a_batch, b_batch)",
    "jaxpr": "{ lambda ; a:f32[7,15] b:f32[7,15]. let\n    c:f32[7,15] = mul a b\n    d:f32[7] = reduce_sum[axes=(np.int64(1),) out_sharding=None] c\n  in (d,) }",
    "stablehlo": "module @jit_vmap_toau attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x15xf32>, %arg1: tensor<7x15xf32>) -> (tensor<7xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<7x15xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [1] : (tensor<7x15xf32>, tensor<f32>) -> tensor<7xf32>\n    return %1 : tensor<7xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_vmap_toau, entry_computation_layout={(f32[7,15]{1,0}, f32[7,15]{1,0})->f32[7]{0}}\n\nregion_0.1 {\n  reduce_sum.3 = f32[] parameter(0)\n  reduce_sum.4 = f32[] parameter(1)\n  ROOT reduce_sum.5 = f32[] add(reduce_sum.3, reduce_sum.4)\n}\n\nENTRY main.2 {\n  a_batch.1 = f32[7,15]{1,0} parameter(0)\n  b_batch.1 = f32[7,15]{1,0} parameter(1)\n  mul.1 = f32[7,15]{1,0} multiply(a_batch.1, b_batch.1)\n  constant.1 = f32[] constant(0)\n  ROOT reduce_sum.7 = f32[7]{0} reduce(mul.1, constant.1), dimensions={1}, to_apply=region_0.1\n}\n\n",
    "input_shapes": [
      [
        7,
        15
      ],
      [
        7,
        15
      ]
    ],
    "output_shape": [
      7
    ]
  },
  {
    "name": "sub_eixw",
    "python_source": "def sub_eixw(x, y):\n    \"\"\"Subtraction operation.\"\"\"\n    return x - y",
    "jaxpr": "{ lambda ; a:f32[9] b:f32[9]. let c:f32[9] = sub a b in (c,) }",
    "stablehlo": "module @jit_sub_eixw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<9xf32>, %arg1: tensor<9xf32>) -> (tensor<9xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.subtract %arg0, %arg1 : tensor<9xf32>\n    return %0 : tensor<9xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_sub_eixw, entry_computation_layout={(f32[9]{0}, f32[9]{0})->f32[9]{0}}\n\nENTRY main.1 {\n  x.1 = f32[9]{0} parameter(0)\n  y.1 = f32[9]{0} parameter(1)\n  ROOT sub.1 = f32[9]{0} subtract(x.1, y.1)\n}\n\n",
    "input_shapes": [
      [
        9
      ],
      [
        9
      ]
    ],
    "output_shape": [
      9
    ]
  },
  {
    "name": "vmap_pzjx",
    "python_source": "def vmap_pzjx(a_batch, b_batch):\n    \"\"\"Batched operation using vmap.\"\"\"\n    def inner(a, b):\n        return jnp.maximum(a, b)\n    return jax.vmap(inner)(a_batch, b_batch)",
    "jaxpr": "{ lambda ; a:f32[7,5] b:f32[7,5]. let c:f32[7,5] = max a b in (c,) }",
    "stablehlo": "module @jit_vmap_pzjx attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x5xf32>, %arg1: tensor<7x5xf32>) -> (tensor<7x5xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.maximum %arg0, %arg1 : tensor<7x5xf32>\n    return %0 : tensor<7x5xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_vmap_pzjx, entry_computation_layout={(f32[7,5]{1,0}, f32[7,5]{1,0})->f32[7,5]{1,0}}\n\nENTRY main.1 {\n  a_batch.1 = f32[7,5]{1,0} parameter(0)\n  b_batch.1 = f32[7,5]{1,0} parameter(1)\n  ROOT max.1 = f32[7,5]{1,0} maximum(a_batch.1, b_batch.1)\n}\n\n",
    "input_shapes": [
      [
        7,
        5
      ],
      [
        7,
        5
      ]
    ],
    "output_shape": [
      7,
      5
    ]
  },
  {
    "name": "composite_tbcs",
    "python_source": "def composite_tbcs(x):\n    \"\"\"Composite function with multiple operations.\"\"\"\n    return jnp.tanh(x) + jnp.sin(x)",
    "jaxpr": "{ lambda ; a:f32[4,16]. let\n    b:f32[4,16] = tanh a\n    c:f32[4,16] = sin a\n    d:f32[4,16] = add b c\n  in (d,) }",
    "stablehlo": "module @jit_composite_tbcs attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<4x16xf32>) -> (tensor<4x16xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.tanh %arg0 : tensor<4x16xf32>\n    %1 = stablehlo.sine %arg0 : tensor<4x16xf32>\n    %2 = stablehlo.add %0, %1 : tensor<4x16xf32>\n    return %2 : tensor<4x16xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_composite_tbcs, entry_computation_layout={(f32[4,16]{1,0})->f32[4,16]{1,0}}\n\nENTRY main.1 {\n  x.1 = f32[4,16]{1,0} parameter(0)\n  tanh.1 = f32[4,16]{1,0} tanh(x.1)\n  sin.1 = f32[4,16]{1,0} sine(x.1)\n  ROOT add.1 = f32[4,16]{1,0} add(tanh.1, sin.1)\n}\n\n",
    "input_shapes": [
      [
        4,
        16
      ]
    ],
    "output_shape": [
      4,
      16
    ]
  },
  {
    "name": "log_gvsp",
    "python_source": "def log_gvsp(x):\n    \"\"\"Log (with safety) operation.\"\"\"\n    return jnp.log(jnp.abs(x) + 1e-6)",
    "jaxpr": "{ lambda ; a:f32[12]. let\n    b:f32[12] = abs a\n    c:f32[12] = add b 9.999999974752427e-07:f32[]\n    d:f32[12] = log c\n  in (d,) }",
    "stablehlo": "module @jit_log_gvsp attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<12xf32>) -> (tensor<12xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.abs %arg0 : tensor<12xf32>\n    %cst = stablehlo.constant dense<9.99999997E-7> : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<12xf32>\n    %2 = stablehlo.add %0, %1 : tensor<12xf32>\n    %3 = stablehlo.log %2 : tensor<12xf32>\n    return %3 : tensor<12xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_log_gvsp, entry_computation_layout={(f32[12]{0})->f32[12]{0}}\n\nENTRY main.1 {\n  x.1 = f32[12]{0} parameter(0)\n  abs.1 = f32[12]{0} abs(x.1)\n  constant.1 = f32[] constant(1e-06)\n  broadcast.1 = f32[12]{0} broadcast(constant.1), dimensions={}\n  add.1 = f32[12]{0} add(abs.1, broadcast.1)\n  ROOT log.1 = f32[12]{0} log(add.1)\n}\n\n",
    "input_shapes": [
      [
        12
      ]
    ],
    "output_shape": [
      12
    ]
  },
  {
    "name": "nn_layer_prkr",
    "python_source": "def nn_layer_prkr(x, w, b):\n    \"\"\"Neural network layer with relu activation.\"\"\"\n    return jax.nn.relu(jnp.dot(x, w) + b)",
    "jaxpr": "{ lambda ; a:f32[7,26] b:f32[26,13] c:f32[13]. let\n    d:f32[7,13] = dot_general[\n      dimension_numbers=(([1], [0]), ([], []))\n      preferred_element_type=float32\n    ] a b\n    e:f32[1,13] = broadcast_in_dim[\n      broadcast_dimensions=(1,)\n      shape=(1, 13)\n      sharding=None\n    ] c\n    f:f32[7,13] = add d e\n    g:f32[7,13] = custom_jvp_call[\n      name=relu\n      call_jaxpr={ lambda ; h:f32[7,13]. let\n          i:f32[7,13] = jit[\n            name=relu\n            jaxpr={ lambda ; h:f32[7,13]. let\n                i:f32[7,13] = max h 0.0:f32[]\n              in (i,) }\n          ] h\n        in (i,) }\n      jvp=jvp\n      symbolic_zeros=False\n    ] f\n  in (g,) }",
    "stablehlo": "module @jit_nn_layer_prkr attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<7x26xf32>, %arg1: tensor<26x13xf32>, %arg2: tensor<13xf32>) -> (tensor<7x13xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.dot_general %arg0, %arg1, contracting_dims = [1] x [0], precision = [DEFAULT, DEFAULT] : (tensor<7x26xf32>, tensor<26x13xf32>) -> tensor<7x13xf32>\n    %1 = stablehlo.broadcast_in_dim %arg2, dims = [1] : (tensor<13xf32>) -> tensor<1x13xf32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [0, 1] : (tensor<1x13xf32>) -> tensor<7x13xf32>\n    %3 = stablehlo.add %0, %2 : tensor<7x13xf32>\n    %4 = call @relu(%3) : (tensor<7x13xf32>) -> tensor<7x13xf32>\n    return %4 : tensor<7x13xf32>\n  }\n  func.func private @relu(%arg0: tensor<7x13xf32>) -> tensor<7x13xf32> {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<7x13xf32>\n    %1 = stablehlo.maximum %arg0, %0 : tensor<7x13xf32>\n    return %1 : tensor<7x13xf32>\n  }\n}\n",
    "xla_hlo": "HloModule jit_nn_layer_prkr, entry_computation_layout={(f32[7,26]{1,0}, f32[26,13]{1,0}, f32[13]{0})->f32[7,13]{1,0}}\n\nrelu.1 {\n  Arg_0.1 = f32[7,13]{1,0} parameter(0)\n  constant.1 = f32[] constant(0)\n  max.2 = f32[7,13]{1,0} broadcast(constant.1), dimensions={}\n  ROOT max.3 = f32[7,13]{1,0} maximum(Arg_0.1, max.2)\n}\n\nENTRY main.2 {\n  x.1 = f32[7,26]{1,0} parameter(0)\n  w.1 = f32[26,13]{1,0} parameter(1)\n  dot_general.1 = f32[7,13]{1,0} dot(x.1, w.1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n  b.1 = f32[13]{0} parameter(2)\n  broadcast_in_dim.1 = f32[1,13]{1,0} reshape(b.1)\n  add.4 = f32[1,13]{1,0} broadcast(broadcast_in_dim.1), dimensions={0,1}\n  add.5 = f32[13]{0} reshape(add.4)\n  add.6 = f32[7,13]{1,0} broadcast(add.5), dimensions={1}\n  add.7 = f32[7,13]{1,0} add(dot_general.1, add.6)\n  ROOT jit_relu_.1 = f32[7,13]{1,0} call(add.7), to_apply=relu.1\n}\n\n",
    "input_shapes": [
      [
        7,
        26
      ],
      [
        26,
        13
      ],
      [
        13
      ]
    ],
    "output_shape": [
      7,
      13
    ]
  }
]