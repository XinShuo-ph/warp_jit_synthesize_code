{
  "name": "vmap_qxje",
  "python_source": "def vmap_qxje(a_batch, b_batch):\n    \"\"\"Batched operation using vmap.\"\"\"\n    def inner(a, b):\n        return jnp.maximum(a, b)\n    return jax.vmap(inner)(a_batch, b_batch)",
  "jaxpr": "{ lambda ; a:f32[2,4] b:f32[2,4]. let c:f32[2,4] = max a b in (c,) }",
  "stablehlo": "module @jit_vmap_qxje attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<2x4xf32>, %arg1: tensor<2x4xf32>) -> (tensor<2x4xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.maximum %arg0, %arg1 : tensor<2x4xf32>\n    return %0 : tensor<2x4xf32>\n  }\n}\n",
  "xla_hlo": "HloModule jit_vmap_qxje, entry_computation_layout={(f32[2,4]{1,0}, f32[2,4]{1,0})->f32[2,4]{1,0}}\n\nENTRY main.1 {\n  a_batch.1 = f32[2,4]{1,0} parameter(0)\n  b_batch.1 = f32[2,4]{1,0} parameter(1)\n  ROOT max.1 = f32[2,4]{1,0} maximum(a_batch.1, b_batch.1)\n}\n\n",
  "input_shapes": [
    [
      2,
      4
    ],
    [
      2,
      4
    ]
  ],
  "output_shape": [
    2,
    4
  ]
}