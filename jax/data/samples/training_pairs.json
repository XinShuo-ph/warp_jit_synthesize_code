[
  {
    "id": 0,
    "kernel_name": "scalar_arr_qahf",
    "python": "def scalar_arr_qahf(alpha, x, y):\n    \"\"\"Scalar-array combined operation.\"\"\"\n    return alpha * x + y",
    "type": "generate_scalar_array_op",
    "hlo_forward": "module @jit_scalar_arr_qahf attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<10xf32>, %arg2: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %2 = stablehlo.multiply %1, %arg1 : tensor<10xf32>\n    %3 = stablehlo.add %2, %arg2 : tensor<10xf32>\n    return %3 : tensor<10xf32>\n  }\n}\n",
    "hlo_backward": "module @jit_loss_fn attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<10xf32>) -> (tensor<f32> {jax.result_info = \"result[0]\"}, tensor<10xf32> {jax.result_info = \"result[1]\"}, tensor<10xf32> {jax.result_info = \"result[2]\"}) {\n    %0 = stablehlo.convert %arg0 : tensor<f32>\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %1 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %2 = stablehlo.broadcast_in_dim %0, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %3 = stablehlo.multiply %2, %1 : tensor<10xf32>\n    %4 = stablehlo.multiply %1, %arg1 : tensor<10xf32>\n    %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %5 = stablehlo.reduce(%4 init: %cst_0) applies stablehlo.add across dimensions = [0] : (tensor<10xf32>, tensor<f32>) -> tensor<f32>\n    %6 = stablehlo.convert %5 : tensor<f32>\n    return %6, %3, %1 : tensor<f32>, tensor<10xf32>, tensor<10xf32>\n  }\n}\n",
    "hlo_optimized": "HloModule jit_scalar_arr_qahf, is_scheduled=true, entry_computation_layout={(f32[], f32[10]{0}, f32[10]{0})->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jax/code/synthesis/pipeline.py\"\n2 \"/workspace/jax/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmph4z8hujc.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"extract_hlo_text\"\n6 \"scalar_arr_qahf\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=308 end_line=308 column=8 end_column=93}\n2 {file_name_id=1 function_name_id=2 line=253 end_line=255 column=59 end_column=17}\n3 {file_name_id=1 function_name_id=3 line=132 end_line=132 column=13 end_column=42}\n4 {file_name_id=2 function_name_id=4 line=152 end_line=152 column=15 end_column=43}\n5 {file_name_id=2 function_name_id=5 line=53 end_line=53 column=14 end_column=40}\n6 {file_name_id=3 function_name_id=6 line=6 end_line=6 column=11 end_column=20}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=6}\n\n\n%fused_computation (param_0.1: f32[10], param_1.2: f32[10], param_2.1: f32[]) -> f32[10] {\n  %param_2.1 = f32[] parameter(2)\n  %mul.1 = f32[10]{0} broadcast(%param_2.1), dimensions={}, metadata={op_name=\"jit(scalar_arr_qahf)/mul\" stack_frame_id=6}\n  %param_1.2 = f32[10]{0} parameter(1)\n  %mul.0 = f32[10]{0} multiply(%mul.1, %param_1.2), metadata={op_name=\"jit(scalar_arr_qahf)/mul\" stack_frame_id=6}\n  %param_0.1 = f32[10]{0} parameter(0)\n  ROOT %add.0 = f32[10]{0} add(%mul.0, %param_0.1), metadata={op_name=\"jit(scalar_arr_qahf)/add\" stack_frame_id=6}\n}\n\nENTRY %main.1 (alpha.1: f32[], x.1: f32[10], y.1: f32[10]) -> f32[10] {\n  %alpha.1 = f32[] parameter(0), metadata={op_name=\"alpha\"}\n  %x.1 = f32[10]{0} parameter(1), metadata={op_name=\"x\"}\n  %y.1 = f32[10]{0} parameter(2), metadata={op_name=\"y\"}\n  ROOT %multiply_add_fusion = f32[10]{0} fusion(%y.1, %x.1, %alpha.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(scalar_arr_qahf)/add\" stack_frame_id=6}\n}\n\n"
  },
  {
    "id": 1,
    "kernel_name": "elementwise_bsdm",
    "python": "def elementwise_bsdm(a, b):\n    \"\"\"Elementwise operation on two arrays.\"\"\"\n    return a * b",
    "type": "generate_simple_elementwise",
    "hlo_forward": "module @jit_elementwise_bsdm attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>, %arg1: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<10xf32>\n    return %0 : tensor<10xf32>\n  }\n}\n",
    "hlo_backward": "module @jit_loss_fn attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>, %arg1: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result[0]\"}, tensor<10xf32> {jax.result_info = \"result[1]\"}) {\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %1 = stablehlo.multiply %arg0, %0 : tensor<10xf32>\n    %2 = stablehlo.multiply %0, %arg1 : tensor<10xf32>\n    return %2, %1 : tensor<10xf32>, tensor<10xf32>\n  }\n}\n",
    "hlo_optimized": "HloModule jit_elementwise_bsdm, is_scheduled=true, entry_computation_layout={(f32[10]{0}, f32[10]{0})->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jax/code/synthesis/pipeline.py\"\n2 \"/workspace/jax/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmptdbio_je.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"extract_hlo_text\"\n6 \"elementwise_bsdm\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=308 end_line=308 column=8 end_column=93}\n2 {file_name_id=1 function_name_id=2 line=253 end_line=255 column=59 end_column=17}\n3 {file_name_id=1 function_name_id=3 line=132 end_line=132 column=13 end_column=42}\n4 {file_name_id=2 function_name_id=4 line=152 end_line=152 column=15 end_column=43}\n5 {file_name_id=2 function_name_id=5 line=53 end_line=53 column=14 end_column=40}\n6 {file_name_id=3 function_name_id=6 line=6 end_line=6 column=11 end_column=16}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=6}\n\n\n%wrapped_multiply_computation (param_0: f32[10], param_1: f32[10]) -> f32[10] {\n  %param_0 = f32[10]{0} parameter(0)\n  %param_1 = f32[10]{0} parameter(1)\n  ROOT %mul.0 = f32[10]{0} multiply(%param_0, %param_1), metadata={op_name=\"jit(elementwise_bsdm)/mul\" stack_frame_id=6}\n}\n\nENTRY %main.1 (a.1: f32[10], b.1: f32[10]) -> f32[10] {\n  %a.1 = f32[10]{0} parameter(0), metadata={op_name=\"a\"}\n  %b.1 = f32[10]{0} parameter(1), metadata={op_name=\"b\"}\n  ROOT %wrapped_multiply = f32[10]{0} fusion(%a.1, %b.1), kind=kLoop, calls=%wrapped_multiply_computation, metadata={op_name=\"jit(elementwise_bsdm)/mul\" stack_frame_id=6}\n}\n\n"
  },
  {
    "id": 2,
    "kernel_name": "vec_kowe",
    "python": "def vec_kowe(a, b):\n    \"\"\"Vector dot product.\"\"\"\n    return jnp.sum(a * b, axis=-1)",
    "type": "generate_vector_kernel",
    "hlo_forward": "module @jit_vec_kowe attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>, %arg1: tensor<10xf32>) -> (tensor<f32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg0, %arg1 : tensor<10xf32>\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst) applies stablehlo.add across dimensions = [0] : (tensor<10xf32>, tensor<f32>) -> tensor<f32>\n    return %1 : tensor<f32>\n  }\n}\n",
    "hlo_backward": "module @jit_loss_fn attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>, %arg1: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result[0]\"}, tensor<10xf32> {jax.result_info = \"result[1]\"}) {\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %1 = stablehlo.multiply %arg0, %0 : tensor<10xf32>\n    %2 = stablehlo.multiply %0, %arg1 : tensor<10xf32>\n    return %2, %1 : tensor<10xf32>, tensor<10xf32>\n  }\n}\n",
    "hlo_optimized": "HloModule jit_vec_kowe, is_scheduled=true, entry_computation_layout={(f32[10]{0}, f32[10]{0})->f32[]}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jax/code/synthesis/pipeline.py\"\n2 \"/workspace/jax/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmp4cuhb4af.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"extract_hlo_text\"\n6 \"vec_kowe\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=308 end_line=308 column=8 end_column=93}\n2 {file_name_id=1 function_name_id=2 line=253 end_line=255 column=59 end_column=17}\n3 {file_name_id=1 function_name_id=3 line=132 end_line=132 column=13 end_column=42}\n4 {file_name_id=2 function_name_id=4 line=152 end_line=152 column=15 end_column=43}\n5 {file_name_id=2 function_name_id=5 line=53 end_line=53 column=14 end_column=40}\n6 {file_name_id=3 function_name_id=6 line=6 end_line=6 column=11 end_column=34}\n7 {file_name_id=3 function_name_id=6 line=6 end_line=6 column=19 end_column=24}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=6}\n7 {file_location_id=7 parent_frame_id=6}\n\n\n%region_0.1 (reduce_sum.3: f32[], reduce_sum.4: f32[]) -> f32[] {\n  %reduce_sum.3 = f32[] parameter(0), metadata={op_name=\"reduce_sum\"}\n  %reduce_sum.4 = f32[] parameter(1), metadata={op_name=\"reduce_sum\"}\n  ROOT %reduce_sum.5 = f32[] add(%reduce_sum.3, %reduce_sum.4), metadata={op_name=\"jit(vec_kowe)/reduce_sum\" stack_frame_id=6}\n}\n\n%fused_computation (param_0.2: f32[10], param_1.2: f32[10]) -> f32[] {\n  %param_0.2 = f32[10]{0} parameter(0)\n  %param_1.2 = f32[10]{0} parameter(1)\n  %mul.0 = f32[10]{0} multiply(%param_0.2, %param_1.2), metadata={op_name=\"jit(vec_kowe)/mul\" stack_frame_id=7}\n  %constant.0 = f32[] constant(0)\n  ROOT %reduce_sum.0 = f32[] reduce(%mul.0, %constant.0), dimensions={0}, to_apply=%region_0.1, metadata={op_name=\"jit(vec_kowe)/reduce_sum\" stack_frame_id=6}\n}\n\nENTRY %main.2 (a.1: f32[10], b.1: f32[10]) -> f32[] {\n  %a.1 = f32[10]{0} parameter(0), metadata={op_name=\"a\"}\n  %b.1 = f32[10]{0} parameter(1), metadata={op_name=\"b\"}\n  ROOT %multiply_reduce_fusion = f32[] fusion(%a.1, %b.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(vec_kowe)/reduce_sum\" stack_frame_id=6}\n}\n\n"
  },
  {
    "id": 3,
    "kernel_name": "loop_hmci",
    "python": "def loop_hmci(a, n):\n    \"\"\"Loop-based accumulation using lax.fori_loop.\"\"\"\n    def body_fn(i, acc):\n        return acc + a\n    return jax.lax.fori_loop(0, n, body_fn, jnp.zeros_like(a))",
    "type": "generate_loop_kernel",
    "hlo_forward": "module @jit_loop_hmci attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>, %arg1: tensor<i32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %cst = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %c = stablehlo.constant dense<0> : tensor<i32>\n    %1:4 = stablehlo.while(%iterArg = %arg0, %iterArg_0 = %c, %iterArg_1 = %arg1, %iterArg_2 = %0) : tensor<10xf32>, tensor<i32>, tensor<i32>, tensor<10xf32>\n    cond {\n      %2 = stablehlo.compare  LT, %iterArg_0, %iterArg_1,  SIGNED : (tensor<i32>, tensor<i32>) -> tensor<i1>\n      stablehlo.return %2 : tensor<i1>\n    } do {\n      %c_3 = stablehlo.constant dense<1> : tensor<i32>\n      %2 = stablehlo.add %iterArg_0, %c_3 : tensor<i32>\n      %3 = stablehlo.add %iterArg_2, %iterArg : tensor<10xf32>\n      stablehlo.return %iterArg, %2, %iterArg_1, %3 : tensor<10xf32>, tensor<i32>, tensor<i32>, tensor<10xf32>\n    }\n    return %1#3 : tensor<10xf32>\n  }\n}\n",
    "hlo_optimized": "HloModule jit_loop_hmci, is_scheduled=true, entry_computation_layout={(f32[10]{0}, s32[])->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jax/code/synthesis/pipeline.py\"\n2 \"/workspace/jax/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmpmjduh4so.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"extract_hlo_text\"\n6 \"loop_hmci\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=308 end_line=308 column=8 end_column=93}\n2 {file_name_id=1 function_name_id=2 line=253 end_line=255 column=59 end_column=17}\n3 {file_name_id=1 function_name_id=3 line=132 end_line=132 column=13 end_column=42}\n4 {file_name_id=2 function_name_id=4 line=152 end_line=152 column=15 end_column=43}\n5 {file_name_id=2 function_name_id=5 line=53 end_line=53 column=14 end_column=40}\n6 {file_name_id=3 function_name_id=6 line=8 end_line=8 column=44 end_column=61}\n7 {file_name_id=3 function_name_id=6 line=8 end_line=8 column=11 end_column=62}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=6}\n7 {file_location_id=7 parent_frame_id=6}\n\n\n%wrapped_broadcast_computation (param_0: f32[]) -> f32[10] {\n  %param_0 = f32[] parameter(0)\n  ROOT %broadcast.0 = f32[10]{0} broadcast(%param_0), dimensions={}\n}\n\n%wrapped_add_computation (param_0.1: s32[], param_1: s32[]) -> s32[] {\n  %param_0.1 = s32[] parameter(0)\n  %param_1 = s32[] parameter(1)\n  ROOT %add.0 = s32[] add(%param_0.1, %param_1), metadata={op_name=\"jit(loop_hmci)/while/body/add\" stack_frame_id=7}\n}\n\n%wrapped_add_computation.1 (param_0.2: f32[10], param_1.1: f32[10]) -> f32[10] {\n  %param_0.2 = f32[10]{0} parameter(0)\n  %param_1.1 = f32[10]{0} parameter(1)\n  ROOT %add.1 = f32[10]{0} add(%param_0.2, %param_1.1), metadata={op_name=\"jit(loop_hmci)/while/body/add\" stack_frame_id=7}\n}\n\n%region_0.1 (arg_tuple.1: (s32[], f32[10], s32[], f32[10])) -> (s32[], f32[10], s32[], f32[10]) {\n  %arg_tuple.1 = (s32[], f32[10]{0}, s32[], f32[10]{0}) parameter(0)\n  %constant.3 = s32[] constant(1)\n  %get-tuple-element.19 = f32[10]{0} get-tuple-element(%arg_tuple.1), index=3\n  %get-tuple-element.18 = s32[] get-tuple-element(%arg_tuple.1), index=2\n  %get-tuple-element.9 = f32[10]{0} get-tuple-element(%arg_tuple.1), index=1\n  %get-tuple-element.8 = s32[] get-tuple-element(%arg_tuple.1), index=0\n  %wrapped_add.1 = f32[10]{0} fusion(%get-tuple-element.9, %get-tuple-element.19), kind=kLoop, calls=%wrapped_add_computation.1, metadata={op_name=\"jit(loop_hmci)/while/body/add\" stack_frame_id=7}\n  %wrapped_add = s32[] fusion(%get-tuple-element.8, %constant.3), kind=kLoop, calls=%wrapped_add_computation, metadata={op_name=\"jit(loop_hmci)/while/body/add\" stack_frame_id=7}\n  ROOT %tuple.3 = (s32[], f32[10]{0}, s32[], f32[10]{0}) tuple(%wrapped_add, %wrapped_add.1, %get-tuple-element.18, %get-tuple-element.19)\n}\n\n%wrapped_compare_computation (param_0.3: s32[], param_1.2: s32[]) -> pred[] {\n  %param_0.3 = s32[] parameter(0)\n  %param_1.2 = s32[] parameter(1)\n  ROOT %lt.0 = pred[] compare(%param_0.3, %param_1.2), direction=LT, metadata={op_name=\"jit(loop_hmci)/while/cond/lt\" stack_frame_id=7}\n}\n\n%region_1.2 (arg_tuple.3: (s32[], f32[10], s32[], f32[10])) -> pred[] {\n  %arg_tuple.3 = (s32[], f32[10]{0}, s32[], f32[10]{0}) parameter(0)\n  %get-tuple-element.14 = s32[] get-tuple-element(%arg_tuple.3), index=2\n  %get-tuple-element.12 = s32[] get-tuple-element(%arg_tuple.3), index=0\n  ROOT %wrapped_compare = pred[] fusion(%get-tuple-element.12, %get-tuple-element.14), kind=kLoop, calls=%wrapped_compare_computation, metadata={op_name=\"jit(loop_hmci)/while/cond/lt\" stack_frame_id=7}\n}\n\n%while.5_computation (tuple.4: (s32[], f32[10], s32[], f32[10])) -> (s32[], f32[10], s32[], f32[10]) {\n  %tuple.4 = (s32[], f32[10]{0}, s32[], f32[10]{0}) parameter(0)\n  ROOT %while.0 = (s32[], f32[10]{0}, s32[], f32[10]{0}) while(%tuple.4), condition=%region_1.2, body=%region_0.1, metadata={op_name=\"jit(loop_hmci)/while\" stack_frame_id=7}\n}\n\nENTRY %main.3 (a.1: f32[10], n.1: s32[]) -> f32[10] {\n  %a.1 = f32[10]{0} parameter(0), metadata={op_name=\"a\"}\n  %n.1 = s32[] parameter(1), metadata={op_name=\"n\"}\n  %constant.4 = s32[] constant(0)\n  %constant.5 = f32[] constant(0)\n  %copy.6 = s32[] copy(%constant.4)\n  %wrapped_broadcast = f32[10]{0} fusion(%constant.5), kind=kLoop, calls=%wrapped_broadcast_computation\n  %tuple = (s32[], f32[10]{0}, s32[], f32[10]{0}) tuple(%copy.6, %wrapped_broadcast, %n.1, %a.1)\n  %call = (s32[], f32[10]{0}, s32[], f32[10]{0}) call(%tuple), to_apply=%while.5_computation, frontend_attributes={xla_cpu_small_call=\"true\"}, metadata={op_name=\"jit(loop_hmci)/while\" stack_frame_id=7}\n  ROOT %while.7 = f32[10]{0} get-tuple-element(%call), index=1, metadata={op_name=\"jit(loop_hmci)/while\" stack_frame_id=7}\n}\n\n"
  },
  {
    "id": 4,
    "kernel_name": "scalar_arr_xkpw",
    "python": "def scalar_arr_xkpw(alpha, x, y):\n    \"\"\"Scalar-array combined operation.\"\"\"\n    return alpha + x * y",
    "type": "generate_scalar_array_op",
    "hlo_forward": "module @jit_scalar_arr_xkpw attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<f32>, %arg1: tensor<10xf32>, %arg2: tensor<10xf32>) -> (tensor<10xf32> {jax.result_info = \"result\"}) {\n    %0 = stablehlo.multiply %arg1, %arg2 : tensor<10xf32>\n    %1 = stablehlo.convert %arg0 : tensor<f32>\n    %2 = stablehlo.broadcast_in_dim %1, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %3 = stablehlo.add %2, %0 : tensor<10xf32>\n    return %3 : tensor<10xf32>\n  }\n}\n",
    "hlo_backward": "module @jit_loss_fn attributes {mhlo.num_partitions = 1 : i32, mhlo.num_replicas = 1 : i32} {\n  func.func public @main(%arg0: tensor<10xf32>, %arg1: tensor<10xf32>) -> (tensor<f32> {jax.result_info = \"result[0]\"}, tensor<10xf32> {jax.result_info = \"result[1]\"}, tensor<10xf32> {jax.result_info = \"result[2]\"}) {\n    %cst = stablehlo.constant dense<1.000000e+00> : tensor<f32>\n    %0 = stablehlo.broadcast_in_dim %cst, dims = [] : (tensor<f32>) -> tensor<10xf32>\n    %cst_0 = stablehlo.constant dense<0.000000e+00> : tensor<f32>\n    %1 = stablehlo.reduce(%0 init: %cst_0) applies stablehlo.add across dimensions = [0] : (tensor<10xf32>, tensor<f32>) -> tensor<f32>\n    %2 = stablehlo.convert %1 : tensor<f32>\n    %3 = stablehlo.multiply %arg0, %0 : tensor<10xf32>\n    %4 = stablehlo.multiply %0, %arg1 : tensor<10xf32>\n    return %2, %4, %3 : tensor<f32>, tensor<10xf32>, tensor<10xf32>\n  }\n}\n",
    "hlo_optimized": "HloModule jit_scalar_arr_xkpw, is_scheduled=true, entry_computation_layout={(f32[], f32[10]{0}, f32[10]{0})->f32[10]{0}}, allow_spmd_sharding_propagation_to_parameters={true,true,true}, allow_spmd_sharding_propagation_to_output={true}\n\nFileNames\n1 \"/workspace/jax/code/synthesis/pipeline.py\"\n2 \"/workspace/jax/code/extraction/ir_extractor.py\"\n3 \"/tmp/tmph8lgej9n.py\"\n\nFunctionNames\n1 \"<module>\"\n2 \"generate_batch_to_jsonl\"\n3 \"extract_ir_from_source\"\n4 \"extract_ir\"\n5 \"extract_hlo_text\"\n6 \"scalar_arr_xkpw\"\n\nFileLocations\n1 {file_name_id=1 function_name_id=1 line=308 end_line=308 column=8 end_column=93}\n2 {file_name_id=1 function_name_id=2 line=253 end_line=255 column=59 end_column=17}\n3 {file_name_id=1 function_name_id=3 line=132 end_line=132 column=13 end_column=42}\n4 {file_name_id=2 function_name_id=4 line=152 end_line=152 column=15 end_column=43}\n5 {file_name_id=2 function_name_id=5 line=53 end_line=53 column=14 end_column=40}\n6 {file_name_id=3 function_name_id=6 line=6 end_line=6 column=19 end_column=24}\n7 {file_name_id=3 function_name_id=6 line=6 end_line=6 column=11 end_column=24}\n\nStackFrames\n1 {file_location_id=1 parent_frame_id=1}\n2 {file_location_id=2 parent_frame_id=2}\n3 {file_location_id=3 parent_frame_id=3}\n4 {file_location_id=4 parent_frame_id=4}\n5 {file_location_id=5 parent_frame_id=5}\n6 {file_location_id=6 parent_frame_id=6}\n7 {file_location_id=7 parent_frame_id=6}\n\n\n%fused_computation (param_0.1: f32[10], param_1.2: f32[10], param_2.1: f32[]) -> f32[10] {\n  %param_2.1 = f32[] parameter(2)\n  %add.1 = f32[10]{0} broadcast(%param_2.1), dimensions={}, metadata={op_name=\"jit(scalar_arr_xkpw)/add\" stack_frame_id=7}\n  %param_0.1 = f32[10]{0} parameter(0)\n  %param_1.2 = f32[10]{0} parameter(1)\n  %mul.0 = f32[10]{0} multiply(%param_0.1, %param_1.2), metadata={op_name=\"jit(scalar_arr_xkpw)/mul\" stack_frame_id=6}\n  ROOT %add.0 = f32[10]{0} add(%add.1, %mul.0), metadata={op_name=\"jit(scalar_arr_xkpw)/add\" stack_frame_id=7}\n}\n\nENTRY %main.1 (alpha.1: f32[], x.1: f32[10], y.1: f32[10]) -> f32[10] {\n  %alpha.1 = f32[] parameter(0), metadata={op_name=\"alpha\"}\n  %x.1 = f32[10]{0} parameter(1), metadata={op_name=\"x\"}\n  %y.1 = f32[10]{0} parameter(2), metadata={op_name=\"y\"}\n  ROOT %multiply_add_fusion = f32[10]{0} fusion(%x.1, %y.1, %alpha.1), kind=kLoop, calls=%fused_computation, metadata={op_name=\"jit(scalar_arr_xkpw)/add\" stack_frame_id=7}\n}\n\n"
  }
]