# Technical Report: JIT Code Synthesis Datasets

**Date:** December 2025  
**Purpose:** Python→IR training data for LLM code generation

---

## Executive Summary

This project generates high-quality training pairs for LLM code translation using two JIT compilation backends:

1. **NVIDIA Warp** → Python kernels to C++/CUDA code
2. **Google JAX** → Python functions to HLO/XLA IR

Both datasets include **forward and backward (gradient) passes**, essential for differentiable programming tasks.

---

## Dataset Comparison

| Metric | Warp | JAX |
|--------|------|-----|
| Total Samples | 1,500 | 1,500 |
| File Size | 18 MB | 7.2 MB |
| Format | JSONL | JSONL |
| Kernel Types | 10 | 10+ (extended) |
| Forward Pass | ✅ C++/CUDA | ✅ HLO |
| Backward Pass | ✅ Adjoint | ✅ Gradient HLO |
| Optimized | N/A | ✅ XLA-optimized HLO |

---

## Warp Dataset Details

### Sample Structure
```json
{
  "id": 0,
  "kernel_name": "kernel_xyz",
  "python": "@wp.kernel\ndef kernel_xyz(...):\n    ...",
  "cpp": "void kernel_cpu_kernel_forward(...) {...}\nvoid kernel_cpu_kernel_backward(...) {...}",
  "cuda": "void kernel_cuda_kernel_forward(...) {...}\nvoid kernel_cuda_kernel_backward(...) {...}",
  "type": "generate_..."
}
```

### Code Characteristics

**CPU Code**:
- Sequential execution via `for (task_index = 0; ...)`
- Args passed via struct pointer

**CUDA Code**:
- Parallel execution via `blockIdx`, `threadIdx`
- Grid-stride loop pattern
- Direct parameter passing

---

## JAX Dataset Details

### Sample Structure
```json
{
  "id": 0,
  "kernel_name": "kernel_xyz",
  "python": "def kernel_xyz(a, b):\n    return a + b",
  "hlo_forward": "HloModule jit_kernel_xyz...",
  "hlo_backward": "HloModule jit_grad...",
  "hlo_optimized": "HloModule optimized...",
  "type": "generate_..."
}
```

### HLO Characteristics

**Forward HLO**:
- Unoptimized XLA intermediate representation
- Direct mapping from Python operations

**Backward HLO**:
- Gradient computation via reverse-mode autodiff
- Generated by `jax.grad`

**Optimized HLO**:
- After XLA optimization passes (fusion, layout, etc.)
- Shows actual compilation patterns

---

## Why Both Backends?

### Warp (Low-Level)
- Learn actual GPU kernel implementation
- Understand CPU vs GPU code differences
- Direct memory access patterns

### JAX (High-Level)
- Learn compiler optimization patterns
- Hardware-agnostic representation
- Standard ML framework IR

### Complementary Training
Together, these datasets enable models to:
1. Understand high-level optimization (JAX)
2. Generate low-level implementations (Warp)
3. Bridge abstraction levels

---

## Kernel Type Distribution

| Type | Description | Coverage |
|------|-------------|----------|
| Elementwise | `a + b`, `a * b` | ~10% |
| Scalar-Array | `alpha * x + y` | ~10% |
| Unary | `sin(a)`, `sqrt(a)` | ~10% |
| Branch | `if a > 0: ...` | ~10% |
| Loop | `for i in range(n): ...` | ~10% |
| Reduction | `sum(a)`, `mean(a)` | ~10% |
| Vector | `dot(a, b)`, `norm(a)` | ~10% |
| Multi-Statement | Chained operations | ~10% |
| Nested Branch | Nested conditionals | ~10% |
| Compound | Mixed patterns | ~10% |

JAX also includes extended ML kernels: matmul, softmax, attention, layernorm, gelu, batchnorm.

---

## Technical Approach

### Warp Extraction
```python
import warp._src.context as ctx

hasher = ctx.ModuleHasher(module)
builder = ctx.ModuleBuilder(module, options, hasher)
cpp_code = builder.codegen("cpu")
cuda_code = builder.codegen("cuda")
```

### JAX Extraction
```python
import jax

lowered = jax.jit(fn).lower(*inputs)
hlo_forward = lowered.as_text()

grad_fn = jax.grad(lambda *args: jnp.sum(fn(*args)))
hlo_backward = jax.jit(grad_fn).lower(*inputs).as_text()

hlo_optimized = lowered.compile().as_text()
```

---

## Usage

### Warp Generation
```bash
cd jit
python3 code/synthesis/pipeline.py \
    --count 5000 \
    --output data/large.jsonl \
    --jsonl \
    --device both
```

### JAX Generation
```bash
cd jax
python3 code/synthesis/pipeline.py \
    --count 5000 \
    --output data/large.jsonl \
    --jsonl \
    --mode both \
    --extended  # Include ML kernels
```

---

## Files Summary

| Path | Description |
|------|-------------|
| `jit/data/training_all.jsonl` | Warp dataset (1,500 pairs) |
| `jax/data/training_all.jsonl` | JAX dataset (1,500 pairs) |
| `jit/code/synthesis/pipeline.py` | Warp pipeline |
| `jax/code/synthesis/pipeline.py` | JAX pipeline |
| `jit/code/extraction/ir_extractor.py` | Warp IR extraction |
| `jax/code/extraction/ir_extractor.py` | JAX HLO extraction |

---

## Conclusion

This project provides comprehensive training data for LLM code translation:

- **Two complementary backends** (Warp C++/CUDA, JAX HLO)
- **Forward and backward passes** for differentiable programming
- **10+ diverse kernel types** for balanced training
- **Production-ready JSONL format** for easy integration

The JIT-based approach guarantees correctness and enables unlimited scaling.
