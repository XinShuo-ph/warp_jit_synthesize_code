{
  "id": "58fda1404d6b",
  "kernel_name": "scalar_pdtvli",
  "kernel_type": "scalar_param",
  "python_source": "@wp.kernel\ndef scalar_pdtvli(x: wp.array(dtype=float), out: wp.array(dtype=float), scale: float, offset: float):\n    tid = wp.tid()\n    out[tid] = x[tid] - scale + offset\n",
  "cuda_ir_forward": "__global__ void scalar_pdtvli_5b9290e6_cuda_kernel_forward(\n    wp::launch_bounds_t dim,\n    wp::array_t<wp::float32> var_x,\n    wp::array_t<wp::float32> var_out,\n    wp::float32 var_scale,\n    wp::float32 var_offset)\n{\n    wp::tile_shared_storage_t tile_mem;\n\n    for (size_t _idx = static_cast<size_t>(blockDim.x) * static_cast<size_t>(blockIdx.x) + static_cast<size_t>(threadIdx.x);\n         _idx < dim.size;\n         _idx += static_cast<size_t>(blockDim.x) * static_cast<size_t>(gridDim.x))\n    {\n            // reset shared memory allocator\n        wp::tile_shared_storage_t::init();\n\n        //---------\n        // primal vars\n        wp::int32 var_0;\n        wp::float32* var_1;\n        wp::float32 var_2;\n        wp::float32 var_3;\n        wp::float32 var_4;\n        //---------\n        // forward\n        // def scalar_pdtvli(x: wp.array(dtype=float), out: wp.array(dtype=float), scale: float, offset: float):       <L 160>\n        // tid = wp.tid()                                                                         <L 161>\n        var_0 = builtin_tid1d();\n        // out[tid] = x[tid] - scale + offset                                                     <L 162>\n        var_1 = wp::address(var_x, var_0);\n        var_3 = wp::load(var_1);\n        var_2 = wp::sub(var_3, var_scale);\n        var_4 = wp::add(var_2, var_offset);\n        wp::array_store(var_out, var_0, var_4);\n    }\n}",
  "cuda_ir_backward": "__global__ void scalar_pdtvli_5b9290e6_cuda_kernel_backward(\n    wp::launch_bounds_t dim,\n    wp::array_t<wp::float32> var_x,\n    wp::array_t<wp::float32> var_out,\n    wp::float32 var_scale,\n    wp::float32 var_offset,\n    wp::array_t<wp::float32> adj_x,\n    wp::array_t<wp::float32> adj_out,\n    wp::float32 adj_scale,\n    wp::float32 adj_offset)\n{\n    wp::tile_shared_storage_t tile_mem;\n\n    for (size_t _idx = static_cast<size_t>(blockDim.x) * static_cast<size_t>(blockIdx.x) + static_cast<size_t>(threadIdx.x);\n         _idx < dim.size;\n         _idx += static_cast<size_t>(blockDim.x) * static_cast<size_t>(gridDim.x))\n    {\n            // reset shared memory allocator\n        wp::tile_shared_storage_t::init();\n\n        //---------\n        // primal vars\n        wp::int32 var_0;\n        wp::float32* var_1;\n        wp::float32 var_2;\n        wp::float32 var_3;\n        wp::float32 var_4;\n        //---------\n        // dual vars\n        wp::int32 adj_0 = {};\n        wp::float32 adj_1 = {};\n        wp::float32 adj_2 = {};\n        wp::float32 adj_3 = {};\n        wp::float32 adj_4 = {};\n        //---------\n        // forward\n        // def scalar_pdtvli(x: wp.array(dtype=float), out: wp.array(dtype=float), scale: float, offset: float):       <L 160>\n        // tid = wp.tid()                                                                         <L 161>\n        var_0 = builtin_tid1d();\n        // out[tid] = x[tid] - scale + offset                                                     <L 162>\n        var_1 = wp::address(var_x, var_0);\n        var_3 = wp::load(var_1);\n        var_2 = wp::sub(var_3, var_scale);\n        var_4 = wp::add(var_2, var_offset);\n        // wp::array_store(var_out, var_0, var_4);\n        //---------\n        // reverse\n        wp::adj_array_store(var_out, var_0, var_4, adj_out, adj_0, adj_4);\n        wp::adj_add(var_2, var_offset, adj_2, adj_offset, adj_4);\n        wp::adj_sub(var_3, var_scale, adj_1, adj_scale, adj_2);\n        wp::adj_address(var_x, var_0, adj_x, adj_0, adj_1);\n        // adj: out[tid] = x[tid] - scale + offset                                                <L 162>\n        // adj: tid = wp.tid()                                                                    <L 161>\n        // adj: def scalar_pdtvli(x: wp.array(dtype=float), out: wp.array(dtype=float), scale: float, offset: float):  <L 160>\n        continue;\n    }\n}",
  "generated_at": "2025-12-28T22:59:56.578761",
  "metadata": {
    "num_params": 4,
    "num_lines": 2,
    "device": "cuda"
  }
}