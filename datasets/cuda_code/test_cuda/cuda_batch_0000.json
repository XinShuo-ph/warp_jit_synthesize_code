[
  {
    "python_source": "@wp.kernel\ndef cuda_arith_qahftr(a: wp.array(dtype=float), b: wp.array(dtype=float), c: wp.array(dtype=float), out: wp.array(dtype=float)):\n    tid = wp.tid()\n    val = a[tid] + b[tid]\n    out[tid] = val / c[tid] + -9.36\n",
    "ir_code": "void cuda_arith_qahftr_16f94eb1_cuda_kernel_forward(\n    wp::launch_bounds_t dim,\n    wp::array_t<wp::float32> var_a,\n    wp::array_t<wp::float32> var_b,\n    wp::array_t<wp::float32> var_c,\n    wp::array_t<wp::float32> var_out)\n{\n    wp::tile_shared_storage_t tile_mem;\n\n    for (size_t _idx = static_cast<size_t>(blockDim.x) * static_cast<size_t>(blockIdx.x) + static_cast<size_t>(threadIdx.x);\n         _idx < dim.size;\n         _idx += static_cast<size_t>(blockDim.x) * static_cast<size_t>(gridDim.x))\n    {\n            // reset shared memory allocator\n        wp::tile_shared_storage_t::init();\n\n        //---------\n        // primal vars\n        wp::int32 var_0;\n        wp::float32* var_1;\n        wp::float32* var_2;\n        wp::float32 var_3;\n        wp::float32 var_4;\n        wp::float32 var_5;\n        wp::float32* var_6;\n        wp::float32 var_7;\n        wp::float32 var_8;\n        const wp::float32 var_9 = 9.36;\n        const wp::float32 var_10 = -9.36;\n        wp::float32 var_11;\n        //---------\n        // forward\n        // def cuda_arith_qahftr(a: wp.array(dtype=float), b: wp.array(dtype=float), c: wp.array(dtype=float), out: wp.array(dtype=float)):       <L 4>\n        // tid = wp.tid()                                                                         <L 5>\n        var_0 = builtin_tid1d();\n        // val = a[tid] + b[tid]                                                                  <L 6>\n        var_1 = wp::address(var_a, var_0);\n        var_2 = wp::address(var_b, var_0);\n        var_4 = wp::load(var_1);\n        var_5 = wp::load(var_2);\n        var_3 = wp::add(var_4, var_5);\n        // out[tid] = val / c[tid] + -9.36                                                        <L 7>\n        var_6 = wp::address(var_c, var_0);\n        var_8 = wp::load(var_6);\n        var_7 = wp::div(var_3, var_8);\n        var_11 = wp::add(var_7, var_10);\n        wp::array_store(var_out, var_0, var_11);\n    }\n}",
    "kernel_name": "cuda_arith_qahftr",
    "backend": "cuda"
  },
  {
    "python_source": "@wp.kernel\ndef cuda_reduce_fnafqo(values: wp.array(dtype=float), result: wp.array(dtype=float)):\n    tid = wp.tid()\n    val = values[tid] * -5.59\n    wp.atomic_max(result, 0, val)\n",
    "ir_code": "void cuda_reduce_fnafqo_12c9a1ce_cuda_kernel_forward(\n    wp::launch_bounds_t dim,\n    wp::array_t<wp::float32> var_values,\n    wp::array_t<wp::float32> var_result)\n{\n    wp::tile_shared_storage_t tile_mem;\n\n    for (size_t _idx = static_cast<size_t>(blockDim.x) * static_cast<size_t>(blockIdx.x) + static_cast<size_t>(threadIdx.x);\n         _idx < dim.size;\n         _idx += static_cast<size_t>(blockDim.x) * static_cast<size_t>(gridDim.x))\n    {\n            // reset shared memory allocator\n        wp::tile_shared_storage_t::init();\n\n        //---------\n        // primal vars\n        wp::int32 var_0;\n        wp::float32* var_1;\n        const wp::float32 var_2 = 5.59;\n        const wp::float32 var_3 = -5.59;\n        wp::float32 var_4;\n        wp::float32 var_5;\n        const wp::int32 var_6 = 0;\n        wp::float32 var_7;\n        //---------\n        // forward\n        // def cuda_reduce_fnafqo(values: wp.array(dtype=float), result: wp.array(dtype=float)):       <L 4>\n        // tid = wp.tid()                                                                         <L 5>\n        var_0 = builtin_tid1d();\n        // val = values[tid] * -5.59                                                              <L 6>\n        var_1 = wp::address(var_values, var_0);\n        var_5 = wp::load(var_1);\n        var_4 = wp::mul(var_5, var_3);\n        // wp.atomic_max(result, 0, val)                                                          <L 7>\n        var_7 = wp::atomic_max(var_result, var_6, var_4);\n    }\n}",
    "kernel_name": "cuda_reduce_fnafqo",
    "backend": "cuda"
  },
  {
    "python_source": "@wp.kernel\ndef cuda_stencil_wtekhf(input: wp.array(dtype=float), output: wp.array(dtype=float), n: int):\n    tid = wp.tid()\n    if tid > 0 and tid < n - 1:\n        left = input[tid - 1] * 5.27\n        center = input[tid]\n        right = input[tid + 1] * 5.27\n        output[tid] = left + center + right\n",
    "ir_code": "void cuda_stencil_wtekhf_f33552c1_cuda_kernel_forward(\n    wp::launch_bounds_t dim,\n    wp::array_t<wp::float32> var_input,\n    wp::array_t<wp::float32> var_output,\n    wp::int32 var_n)\n{\n    wp::tile_shared_storage_t tile_mem;\n\n    for (size_t _idx = static_cast<size_t>(blockDim.x) * static_cast<size_t>(blockIdx.x) + static_cast<size_t>(threadIdx.x);\n         _idx < dim.size;\n         _idx += static_cast<size_t>(blockDim.x) * static_cast<size_t>(gridDim.x))\n    {\n            // reset shared memory allocator\n        wp::tile_shared_storage_t::init();\n\n        //---------\n        // primal vars\n        wp::int32 var_0;\n        const wp::int32 var_1 = 0;\n        bool var_2;\n        const wp::int32 var_3 = 1;\n        wp::int32 var_4;\n        bool var_5;\n        bool var_6;\n        const wp::int32 var_7 = 1;\n        wp::int32 var_8;\n        wp::float32* var_9;\n        const wp::float32 var_10 = 5.27;\n        wp::float32 var_11;\n        wp::float32 var_12;\n        wp::float32* var_13;\n        wp::float32 var_14;\n        wp::float32 var_15;\n        const wp::int32 var_16 = 1;\n        wp::int32 var_17;\n        wp::float32* var_18;\n        const wp::float32 var_19 = 5.27;\n        wp::float32 var_20;\n        wp::float32 var_21;\n        wp::float32 var_22;\n        wp::float32 var_23;\n        //---------\n        // forward\n        // def cuda_stencil_wtekhf(input: wp.array(dtype=float), output: wp.array(dtype=float), n: int):       <L 4>\n        // tid = wp.tid()                                                                         <L 5>\n        var_0 = builtin_tid1d();\n        // if tid > 0 and tid < n - 1:                                                            <L 6>\n        var_2 = (var_0 > var_1);\n        var_4 = wp::sub(var_n, var_3);\n        var_5 = (var_0 < var_4);\n        var_6 = var_2 && var_5;\n        if (var_6) {\n            // left = input[tid - 1] * 5.27                                                       <L 7>\n            var_8 = wp::sub(var_0, var_7);\n            var_9 = wp::address(var_input, var_8);\n            var_12 = wp::load(var_9);\n            var_11 = wp::mul(var_12, var_10);\n            // center = input[tid]                                                                <L 8>\n            var_13 = wp::address(var_input, var_0);\n            var_15 = wp::load(var_13);\n            var_14 = wp::copy(var_15);\n            // right = input[tid + 1] * 5.27                                                      <L 9>\n            var_17 = wp::add(var_0, var_16);\n            var_18 = wp::address(var_input, var_17);\n            var_21 = wp::load(var_18);\n            var_20 = wp::mul(var_21, var_19);\n            // output[tid] = left + center + right                                                <L 10>\n            var_22 = wp::add(var_11, var_14);\n            var_23 = wp::add(var_22, var_20);\n            wp::array_store(var_output, var_0, var_23);\n        }\n    }\n}",
    "kernel_name": "cuda_stencil_wtekhf",
    "backend": "cuda"
  },
  {
    "python_source": "@wp.kernel\ndef cuda_vec_ccwpus(pos: wp.array(dtype=wp.vec3), vel: wp.array(dtype=wp.vec3), acc: wp.array(dtype=wp.vec3), output: wp.array(dtype=wp.vec3)):\n    tid = wp.tid()\n    dt = 0.72\n    new_vel = vel[tid] + acc[tid] * dt * 9.46\n    new_pos = pos[tid] + new_vel * dt\n    output[tid] = new_pos\n",
    "ir_code": "void cuda_vec_ccwpus_f4eafe31_cuda_kernel_forward(\n    wp::launch_bounds_t dim,\n    wp::array_t<wp::vec_t<3, wp::float32>> var_pos,\n    wp::array_t<wp::vec_t<3, wp::float32>> var_vel,\n    wp::array_t<wp::vec_t<3, wp::float32>> var_acc,\n    wp::array_t<wp::vec_t<3, wp::float32>> var_output)\n{\n    wp::tile_shared_storage_t tile_mem;\n\n    for (size_t _idx = static_cast<size_t>(blockDim.x) * static_cast<size_t>(blockIdx.x) + static_cast<size_t>(threadIdx.x);\n         _idx < dim.size;\n         _idx += static_cast<size_t>(blockDim.x) * static_cast<size_t>(gridDim.x))\n    {\n            // reset shared memory allocator\n        wp::tile_shared_storage_t::init();\n\n        //---------\n        // primal vars\n        wp::int32 var_0;\n        const wp::float32 var_1 = 0.72;\n        wp::vec_t<3, wp::float32>* var_2;\n        wp::vec_t<3, wp::float32>* var_3;\n        wp::vec_t<3, wp::float32> var_4;\n        wp::vec_t<3, wp::float32> var_5;\n        const wp::float32 var_6 = 9.46;\n        wp::vec_t<3, wp::float32> var_7;\n        wp::vec_t<3, wp::float32> var_8;\n        wp::vec_t<3, wp::float32> var_9;\n        wp::vec_t<3, wp::float32>* var_10;\n        wp::vec_t<3, wp::float32> var_11;\n        wp::vec_t<3, wp::float32> var_12;\n        wp::vec_t<3, wp::float32> var_13;\n        //---------\n        // forward\n        // def cuda_vec_ccwpus(pos: wp.array(dtype=wp.vec3), vel: wp.array(dtype=wp.vec3), acc: wp.array(dtype=wp.vec3), output: wp.array(dtype=wp.vec3)):       <L 4>\n        // tid = wp.tid()                                                                         <L 5>\n        var_0 = builtin_tid1d();\n        // dt = 0.72                                                                              <L 6>\n        // new_vel = vel[tid] + acc[tid] * dt * 9.46                                              <L 7>\n        var_2 = wp::address(var_vel, var_0);\n        var_3 = wp::address(var_acc, var_0);\n        var_5 = wp::load(var_3);\n        var_4 = wp::mul(var_5, var_1);\n        var_7 = wp::mul(var_4, var_6);\n        var_9 = wp::load(var_2);\n        var_8 = wp::add(var_9, var_7);\n        // new_pos = pos[tid] + new_vel * dt                                                      <L 8>\n        var_10 = wp::address(var_pos, var_0);\n        var_11 = wp::mul(var_8, var_1);\n        var_13 = wp::load(var_10);\n        var_12 = wp::add(var_13, var_11);\n        // output[tid] = new_pos                                                                  <L 9>\n        wp::array_store(var_output, var_0, var_12);\n    }\n}",
    "kernel_name": "cuda_vec_ccwpus",
    "backend": "cuda"
  },
  {
    "python_source": "@wp.kernel\ndef cuda_scan_chqxje(data: wp.array(dtype=float), out: wp.array(dtype=float)):\n    tid = wp.tid()\n    acc = data[tid]\n    for i in range(3):\n        acc = acc + data[tid] * float(i + 1)\n    out[tid] = acc\n",
    "ir_code": "void cuda_scan_chqxje_3a21b7a0_cuda_kernel_forward(\n    wp::launch_bounds_t dim,\n    wp::array_t<wp::float32> var_data,\n    wp::array_t<wp::float32> var_out)\n{\n    wp::tile_shared_storage_t tile_mem;\n\n    for (size_t _idx = static_cast<size_t>(blockDim.x) * static_cast<size_t>(blockIdx.x) + static_cast<size_t>(threadIdx.x);\n         _idx < dim.size;\n         _idx += static_cast<size_t>(blockDim.x) * static_cast<size_t>(gridDim.x))\n    {\n            // reset shared memory allocator\n        wp::tile_shared_storage_t::init();\n\n        //---------\n        // primal vars\n        wp::int32 var_0;\n        wp::float32* var_1;\n        wp::float32 var_2;\n        wp::float32 var_3;\n        const wp::int32 var_4 = 0;\n        wp::float32* var_5;\n        const wp::int32 var_6 = 1;\n        wp::int32 var_7;\n        wp::float32 var_8;\n        wp::float32 var_9;\n        wp::float32 var_10;\n        wp::float32 var_11;\n        const wp::int32 var_12 = 1;\n        wp::float32* var_13;\n        const wp::int32 var_14 = 1;\n        wp::int32 var_15;\n        wp::float32 var_16;\n        wp::float32 var_17;\n        wp::float32 var_18;\n        wp::float32 var_19;\n        const wp::int32 var_20 = 2;\n        wp::float32* var_21;\n        const wp::int32 var_22 = 1;\n        wp::int32 var_23;\n        wp::float32 var_24;\n        wp::float32 var_25;\n        wp::float32 var_26;\n        wp::float32 var_27;\n        //---------\n        // forward\n        // def cuda_scan_chqxje(data: wp.array(dtype=float), out: wp.array(dtype=float)):         <L 4>\n        // tid = wp.tid()                                                                         <L 5>\n        var_0 = builtin_tid1d();\n        // acc = data[tid]                                                                        <L 6>\n        var_1 = wp::address(var_data, var_0);\n        var_3 = wp::load(var_1);\n        var_2 = wp::copy(var_3);\n        // for i in range(3):                                                                     <L 7>\n        // acc = acc + data[tid] * float(i + 1)                                                   <L 8>\n        var_5 = wp::address(var_data, var_0);\n        var_7 = wp::add(var_4, var_6);\n        var_8 = wp::float(var_7);\n        var_10 = wp::load(var_5);\n        var_9 = wp::mul(var_10, var_8);\n        var_11 = wp::add(var_2, var_9);\n        var_13 = wp::address(var_data, var_0);\n        var_15 = wp::add(var_12, var_14);\n        var_16 = wp::float(var_15);\n        var_18 = wp::load(var_13);\n        var_17 = wp::mul(var_18, var_16);\n        var_19 = wp::add(var_11, var_17);\n        var_21 = wp::address(var_data, var_0);\n        var_23 = wp::add(var_20, var_22);\n        var_24 = wp::float(var_23);\n        var_26 = wp::load(var_21);\n        var_25 = wp::mul(var_26, var_24);\n        var_27 = wp::add(var_19, var_25);\n        // out[tid] = acc                                                                         <L 9>\n        wp::array_store(var_out, var_0, var_27);\n    }\n}",
    "kernel_name": "cuda_scan_chqxje",
    "backend": "cuda"
  }
]