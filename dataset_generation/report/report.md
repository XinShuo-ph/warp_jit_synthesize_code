# JIT Code Synthesis Dataset Report

**Date:** December 28, 2025
**To:** Chief Scientist

## Executive Summary
This report documents the generation of a synthetic dataset for training Large Language Models (LLMs) to understand and optimize Just-In-Time (JIT) compiled code. We have produced over 400MB of paired data mapping Python kernels to their C++ (CPU) and CUDA (GPU) Intermediate Representations (IR) using NVIDIA Warp.

## 1. Introduction

### 1.1 Just-In-Time (JIT) Compilation and IR
JIT compilation is a technique where code is compiled during execution rather than prior to execution. This allows for runtime optimizations based on the specific hardware and data types. The Intermediate Representation (IR) is a crucial low-level code representation generated by the compiler frontend (Python parser) before being translated to machine code. Understanding IR is key for AI models to perform code optimization, translation, and verification.

### 1.2 NVIDIA Warp
NVIDIA Warp is a Python framework designed for high-performance simulation and graphics. It features a rich JIT compilation pipeline that takes decorated Python functions (`@wp.kernel`) and transpiles them into efficient C++ or CUDA C++ code. This makes it an ideal source for generating high-quality, ground-truth data linking high-level Python logic to low-level GPU/CPU kernels.

## 2. Dataset Overview

We have successfully generated two distinct datasets focusing on CPU and CUDA backends.

### 2.1 CPU Code Dataset
- **Volume:** ~200 MB
- **Sample Count:** ~1,200 pairs
- **Content:**
  - Input: Python source code (Warp kernels).
  - Output: C++ source code generated for CPU execution.
  - Features: Arithmetic, loops, conditionals, vector operations, atomic operations.

### 2.2 CUDA Code Dataset
- **Volume:** ~230 MB
- **Sample Count:** ~1,300 pairs
- **Content:**
  - Input: Same distribution of Python kernels.
  - Output: CUDA C++ source code (kernels with `blockDim`, `threadIdx`, shared memory usage).
  - **Note:** Generated via cross-compilation (codegen) mode, ensuring valid syntax and structure even without a physical GPU.

### 2.3 Data Format
Each sample is stored as a JSON file containing:
- `python_source`: The high-level kernel definition.
- `kernel_name`: Name of the entry point.
- `cpp_ir_full`: The complete generated C++/CUDA source file.
- `cpp_ir_forward` / `cuda_ir_forward`: Extracted body of the forward pass kernel.
- `metadata`: Complexity metrics (lines of code, parameter counts) and backend type (`cpu` or `cuda`).

## 3. Methodology

The dataset was produced using a custom synthesis pipeline:
1. **Procedural Generation**: A random kernel generator (`KernelGenerator`) constructs valid Python kernels using a grammar of operations (Math, Control Flow, Vectors).
2. **Compilation/Codegen**: The kernels are passed to the Warp JIT compiler.
   - For CPU: Fully compiled to C++.
   - For CUDA: Transpiled to CUDA C++ using Warp's codegen capabilities.
3. **Extraction**: The resulting C++ source is captured from the Warp cache, and relevant IR sections are extracted.
4. **Validation**: Hash-based deduplication ensures dataset uniqueness.

## 4. Conclusion
This dataset provides a robust foundation for training models on the Python-to-Hardware mapping problem. The structured nature of Warp's generated code, combined with the diversity of our procedural generation, covers a wide range of computational patterns essential for scientific computing and graphics workloads.
