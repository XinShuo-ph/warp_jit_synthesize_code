{
  "python_source": "@wp.kernel\ndef math_nvvejh(x: wp.array(dtype=float), out: wp.array(dtype=float)):\n    tid = wp.tid()\n    val = wp.exp(x[tid] * -1.28)\n    out[tid] = wp.log(val)\n",
  "cuda_forward": "extern \"C\" __global__ void math_nvvejh_21b0b347_cuda_kernel_forward(\n    wp::launch_bounds_t dim,\n    wp::array_t<wp::float32> var_x,\n    wp::array_t<wp::float32> var_out)\n{\n    wp::tile_shared_storage_t tile_mem;\n\n    for (size_t _idx = static_cast<size_t>(blockDim.x) * static_cast<size_t>(blockIdx.x) + static_cast<size_t>(threadIdx.x);\n         _idx < dim.size;\n         _idx += static_cast<size_t>(blockDim.x) * static_cast<size_t>(gridDim.x))\n    {\n            // reset shared memory allocator\n        wp::tile_shared_storage_t::init();\n\n        //---------\n        // primal vars\n        wp::int32 var_0;\n        wp::float32* var_1;\n        const wp::float32 var_2 = 1.28;\n        const wp::float32 var_3 = -1.28;\n        wp::float32 var_4;\n        wp::float32 var_5;\n        wp::float32 var_6;\n        wp::float32 var_7;\n        //---------\n        // forward\n        // def math_nvvejh(x: wp.array(dtype=float), out: wp.array(dtype=float)):                 <L 71>\n        // tid = wp.tid()                                                                         <L 72>\n        var_0 = builtin_tid1d();\n        // val = wp.exp(x[tid] * -1.28)                                                           <L 73>\n        var_1 = wp::address(var_x, var_0);\n        var_5 = wp::load(var_1);\n        var_4 = wp::mul(var_5, var_3);\n        var_6 = wp::exp(var_4);\n        // out[tid] = wp.log(val)                                                                 <L 74>\n        var_7 = wp::log(var_6);\n        wp::array_store(var_out, var_0, var_7);\n    }\n}",
  "metadata": {
    "kernel_name": "math_nvvejh",
    "category": "math",
    "device": "cuda"
  }
}