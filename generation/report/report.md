# JIT Code Synthesis Report

## 1. Introduction to Concepts

### Just-In-Time (JIT) Compilation
JIT compilation involves compiling code during execution rather than prior to execution. In the context of high-performance computing and deep learning, JIT allows frameworks to optimize kernels based on runtime information (e.g., tensor shapes, device availability) and generate specialized machine code for the target architecture.

### Intermediate Representation (IR)
An Intermediate Representation (IR) is a data structure or code used internally by a compiler or virtual machine to represent source code. It sits between the high-level source language (e.g., Python) and the low-level machine code. IRs are crucial for optimization, as they allow transformations to be applied independently of the source and target languages. In this project, we focus on extracting the C++ and CUDA IR generated by the Warp framework.

### Nvidia Warp
Nvidia Warp is a Python framework for writing high-performance simulation and graphics code. It takes Python functions decorated with `@wp.kernel`, compiles them into C++/CUDA, and executes them on the CPU or GPU. Warp uses a differentiable programming model, making it suitable for physics simulation and training data generation for AI.

## 2. Dataset Overview

We have produced a synthetic dataset of paired Python kernels and their corresponding generated IR (C++ for CPU, CUDA C++ for GPU).

### Statistics

| Metric | CPU Dataset | CUDA Dataset | Total |
|--------|-------------|--------------|-------|
| **Size** | 228 MB | 233 MB | 461 MB |
| **Pairs** | 45,000 | 45,000 | 90,000 |
| **Format** | JSON | JSON | - |

### Data Format
Each sample is a JSON file containing:
- `python_source`: The original Python code defining the Warp kernel.
- `cpp_forward` / `cuda_forward`: The extracted forward pass function from the generated C++/CUDA code.
- `metadata`: Kernel name, category, and target device.

### Categories
The dataset covers 10 kernel categories to ensure diversity:
- Arithmetic
- Atomic Operations
- Combined Patterns
- Conditional Logic
- Loops
- Math Functions
- Multi-conditionals
- Nested Loops
- Scalar Parameters
- Vector Operations

## 3. Workflow & Methodology

The dataset was generated using a batched synthesis pipeline:
1. **Procedural Generation**: A random kernel generator creates valid Warp Python kernels with varied logic and parameters.
2. **Batch Compilation**: Kernels are grouped into modules (10 per module) to amortize compilation overhead.
3. **Code Generation**: We utilize Warp's `ModuleBuilder` to generate C++ or CUDA source code without requiring a full build chain (GCC/NVCC), enabling rapid data generation (~500 pairs/sec).
4. **Extraction**: The relevant kernel functions are extracted from the generated source using regex patterns.

## 4. Conclusion
We successfully produced over 400MB of high-quality synthetic training data linking Python source code to low-level IR. This dataset serves as a foundation for training Large Language Models (LLMs) to understand and optimize kernel code, bridge the gap between high-level frameworks and hardware instructions, and potentially assist in automated kernel optimization.
