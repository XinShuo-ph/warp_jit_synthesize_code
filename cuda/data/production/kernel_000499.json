{
  "python_source": "@wp.kernel\ndef reduce_dpnwsb(input: wp.array(dtype=float), output: wp.array(dtype=float)):\n    tid = wp.tid()\n    val = input[tid]\n    # Note: atomic_mul not available, using loop\n    if val != 1.0:\n        wp.atomic_add(output, 0, val - 1.0)\n",
  "ir_forward": "void reduce_dpnwsb_b64938a6_cuda_kernel_forward(\n    wp::launch_bounds_t dim,\n    wp::array_t<wp::float32> var_input,\n    wp::array_t<wp::float32> var_output)\n{\n    wp::tile_shared_storage_t tile_mem;\n\n    for (size_t _idx = static_cast<size_t>(blockDim.x) * static_cast<size_t>(blockIdx.x) + static_cast<size_t>(threadIdx.x);\n         _idx < dim.size;\n         _idx += static_cast<size_t>(blockDim.x) * static_cast<size_t>(gridDim.x))\n    {\n            // reset shared memory allocator\n        wp::tile_shared_storage_t::init();\n\n        //---------\n        // primal vars\n        wp::int32 var_0;\n        wp::float32* var_1;\n        wp::float32 var_2;\n        wp::float32 var_3;\n        const wp::float32 var_4 = 1.0;\n        bool var_5;\n        const wp::int32 var_6 = 0;\n        const wp::float32 var_7 = 1.0;\n        wp::float32 var_8;\n        wp::float32 var_9;\n        //---------\n        // forward\n        // def reduce_dpnwsb(input: wp.array(dtype=float), output: wp.array(dtype=float)):        <L 4>\n        // tid = wp.tid()                                                                         <L 5>\n        var_0 = builtin_tid1d();\n        // val = input[tid]                                                                       <L 6>\n        var_1 = wp::address(var_input, var_0);\n        var_3 = wp::load(var_1);\n        var_2 = wp::copy(var_3);\n        // if val != 1.0:                                                                         <L 8>\n        var_5 = (var_2 != var_4);\n        if (var_5) {\n            // wp.atomic_add(output, 0, val - 1.0)                                                <L 9>\n            var_8 = wp::sub(var_2, var_7);\n            var_9 = wp::atomic_add(var_output, var_6, var_8);\n        }\n    }\n}",
  "metadata": {
    "kernel_name": "reduce_dpnwsb",
    "category": "reduction",
    "description": "Parallel product reduction (simplified)",
    "device": "cuda",
    "has_backward": true,
    "operation": "prod",
    "seed": 652
  },
  "ir_backward": "void reduce_dpnwsb_b64938a6_cuda_kernel_backward(\n    wp::launch_bounds_t dim,\n    wp::array_t<wp::float32> var_input,\n    wp::array_t<wp::float32> var_output,\n    wp::array_t<wp::float32> adj_input,\n    wp::array_t<wp::float32> adj_output)\n{\n    wp::tile_shared_storage_t tile_mem;\n\n    for (size_t _idx = static_cast<size_t>(blockDim.x) * static_cast<size_t>(blockIdx.x) + static_cast<size_t>(threadIdx.x);\n         _idx < dim.size;\n         _idx += static_cast<size_t>(blockDim.x) * static_cast<size_t>(gridDim.x))\n    {\n            // reset shared memory allocator\n        wp::tile_shared_storage_t::init();\n\n        //---------\n        // primal vars\n        wp::int32 var_0;\n        wp::float32* var_1;\n        wp::float32 var_2;\n        wp::float32 var_3;\n        const wp::float32 var_4 = 1.0;\n        bool var_5;\n        const wp::int32 var_6 = 0;\n        const wp::float32 var_7 = 1.0;\n        wp::float32 var_8;\n        wp::float32 var_9;\n        //---------\n        // dual vars\n        wp::int32 adj_0 = {};\n        wp::float32 adj_1 = {};\n        wp::float32 adj_2 = {};\n        wp::float32 adj_3 = {};\n        wp::float32 adj_4 = {};\n        bool adj_5 = {};\n        wp::int32 adj_6 = {};\n        wp::float32 adj_7 = {};\n        wp::float32 adj_8 = {};\n        wp::float32 adj_9 = {};\n        //---------\n        // forward\n        // def reduce_dpnwsb(input: wp.array(dtype=float), output: wp.array(dtype=float)):        <L 4>\n        // tid = wp.tid()                                                                         <L 5>\n        var_0 = builtin_tid1d();\n        // val = input[tid]                                                                       <L 6>\n        var_1 = wp::address(var_input, var_0);\n        var_3 = wp::load(var_1);\n        var_2 = wp::copy(var_3);\n        // if val != 1.0:                                                                         <L 8>\n        var_5 = (var_2 != var_4);\n        if (var_5) {\n            // wp.atomic_add(output, 0, val - 1.0)                                                <L 9>\n            var_8 = wp::sub(var_2, var_7);\n            // var_9 = wp::atomic_add(var_output, var_6, var_8);\n        }\n        //---------\n        // reverse\n        if (var_5) {\n            wp::adj_atomic_add(var_output, var_6, var_8, adj_output, adj_6, adj_8, adj_9);\n            wp::adj_sub(var_2, var_7, adj_2, adj_7, adj_8);\n            // adj: wp.atomic_add(output, 0, val - 1.0)                                           <L 9>\n        }\n        // adj: if val != 1.0:                                                                    <L 8>\n        wp::adj_copy(var_3, adj_1, adj_2);\n        wp::adj_address(var_input, var_0, adj_input, adj_0, adj_1);\n        // adj: val = input[tid]                                                                  <L 6>\n        // adj: tid = wp.tid()                                                                    <L 5>\n        // adj: def reduce_dpnwsb(input: wp.array(dtype=float), output: wp.array(dtype=float)):   <L 4>\n        continue;\n    }\n}"
}