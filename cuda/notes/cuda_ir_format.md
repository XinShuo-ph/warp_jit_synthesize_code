# CUDA IR Format Documentation

## Overview
CUDA IR generated by Warp differs from CPU IR in function signatures, execution model, and GPU-specific constructs. This document details the CUDA IR structure.

## Function Signature

### CUDA Kernel Function
```cpp
extern "C" __global__ void kernel_name_hash_cuda_kernel_forward(
    wp::launch_bounds_t dim,
    wp::array_t<wp::float32> var_arg1,
    wp::array_t<wp::float32> var_arg2,
    ...)
{
    // Function body
}
```

**Key Components:**
- `extern "C"`: C linkage for CUDA runtime
- `__global__`: CUDA kernel decorator (device code callable from host)
- `void`: Kernels don't return values
- `_cuda_kernel_forward`: Suffix indicates CUDA device and forward pass
- Arguments: Passed directly (not via struct pointer like CPU)

### CPU Kernel Function (for comparison)
```cpp
void kernel_name_hash_cpu_kernel_forward(
    wp::launch_bounds_t dim,
    size_t task_index,
    wp_args_kernel_name_hash *_wp_args)
{
    // Function body
}
```

## Execution Model

### CUDA Grid-Stride Loop
All CUDA kernels use a grid-stride loop pattern:

```cpp
for (size_t _idx = static_cast<size_t>(blockDim.x) * static_cast<size_t>(blockIdx.x) + static_cast<size_t>(threadIdx.x);
     _idx < dim.size;
     _idx += static_cast<size_t>(blockDim.x) * static_cast<size_t>(gridDim.x))
{
    // Kernel body executes here for each thread
}
```

**Variables:**
- `blockDim.x`: Threads per block (typically 256)
- `blockIdx.x`: Block index in grid
- `threadIdx.x`: Thread index within block
- `gridDim.x`: Total number of blocks
- `_idx`: Global thread index (computed)
- `dim.size`: Total number of elements to process

**Why Grid-Stride?**
- Allows kernel to handle any array size
- Works with arbitrary number of threads
- Efficient for large datasets

### CPU Task Loop (for comparison)
```cpp
// CPU uses simple task_index parameter
// No loop - each invocation processes one element
```

## Thread Indexing

### CUDA Thread ID Macro
```cpp
#define builtin_tid1d() wp::tid(_idx, dim)
```

The `_idx` variable is computed from CUDA thread/block indices.

### CPU Thread ID Macro
```cpp
#define builtin_tid1d() wp::tid(task_index, dim)
```

Uses the `task_index` parameter passed to function.

## Memory Constructs

### Shared Memory
CUDA kernels have shared memory support:

```cpp
wp::tile_shared_storage_t tile_mem;  // Declared at function start

// Inside loop:
wp::tile_shared_storage_t::init();   // Reset allocator
```

Used for tile-based operations and thread cooperation within a block.

### CPU Memory
No shared memory constructs in CPU version.

## Complete Example

### Python Source
```python
@wp.kernel
def simple_add(a: wp.array(dtype=float), b: wp.array(dtype=float), c: wp.array(dtype=float)):
    tid = wp.tid()
    c[tid] = a[tid] + b[tid]
```

### Generated CUDA IR
```cpp
extern "C" __global__ void simple_add_9ad1d227_cuda_kernel_forward(
    wp::launch_bounds_t dim,
    wp::array_t<wp::float32> var_a,
    wp::array_t<wp::float32> var_b,
    wp::array_t<wp::float32> var_c)
{
    wp::tile_shared_storage_t tile_mem;

    for (size_t _idx = static_cast<size_t>(blockDim.x) * static_cast<size_t>(blockIdx.x) + static_cast<size_t>(threadIdx.x);
         _idx < dim.size;
         _idx += static_cast<size_t>(blockDim.x) * static_cast<size_t>(gridDim.x))
    {
        wp::tile_shared_storage_t::init();

        //---------
        // primal vars
        wp::int32 var_0;
        wp::float32* var_1;
        wp::float32* var_2;
        wp::float32 var_3;
        wp::float32 var_4;
        wp::float32 var_5;
        //---------
        // forward
        // tid = wp.tid()
        var_0 = builtin_tid1d();
        // c[tid] = a[tid] + b[tid]
        var_1 = address(var_a, var_0);
        var_3 = load(var_1);
        var_2 = address(var_b, var_0);
        var_4 = load(var_2);
        var_5 = add(var_3, var_4);
        var_1 = address(var_c, var_0);
        store(var_1, var_5);
    }
}
```

## Key Differences Summary

| Aspect | CPU | CUDA |
|--------|-----|------|
| **Function Decorator** | `void` | `extern "C" __global__ void` |
| **Parameters** | Struct pointer | Direct parameters |
| **Thread Index** | `task_index` param | Computed from blockIdx/threadIdx |
| **Execution** | Single element | Grid-stride loop |
| **Shared Memory** | None | `tile_shared_storage_t` |
| **Suffix** | `_cpu_kernel_forward` | `_cuda_kernel_forward` |

## Atomic Operations

CUDA kernels support atomic operations:

```python
@wp.kernel
def atomic_add(values: wp.array(dtype=float), result: wp.array(dtype=float)):
    tid = wp.tid()
    wp.atomic_add(result, 0, values[tid])
```

Generated CUDA code includes:
```cpp
atomic_add(&var_result[var_1], var_3);  // GPU atomic operation
```

## Vector and Matrix Operations

Vector/matrix types map directly to CUDA types:
- `wp.vec3` → CUDA vector operations
- `wp.mat33` → CUDA matrix operations

Example:
```python
wp.dot(a[tid], b[tid])  # Vector dot product
```

Generates efficient CUDA vector math operations.

## Control Flow

Branch divergence considerations:
```python
if a[tid] > threshold:
    out[tid] = 1.0
else:
    out[tid] = 0.0
```

Generated CUDA code preserves branches. Note: threads in same warp with different branches will serialize execution.

## File Extension

- CPU: `.cpp` files
- CUDA: `.cu` files

## Summary

CUDA IR from Warp is production-ready GPU code that:
1. Uses standard CUDA constructs (`__global__`, grid-stride loops)
2. Efficiently maps Python kernels to GPU execution
3. Supports all Warp operations (math, vector, matrix, atomic)
4. Can be compiled with `nvcc` on systems with CUDA toolkit
5. Requires no GPU to generate (can develop on CPU-only systems)
